<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time and memory costs jointly determine a speed-accuracy trade-off and set-size effects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuze</forename><surname>Liu</surname></persName>
							<email>shuzeliu@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">PhD Program in Neuroscience</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">PhD Program in Neuroscience</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology and Center for Brain Science</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Brains, Minds, and Machines</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><forename type="middle">A</forename><surname>Bari</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">McLean Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Belmont</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Joseph</surname></persName>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<addrLine>52 Oxford St</addrLine>
									<postCode>02138</postCode>
									<settlement>Neuroscience, Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Time and memory costs jointly determine a speed-accuracy trade-off and set-size effects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>resource rationality</term>
					<term>decision making</term>
					<term>information theory</term>
					<term>reinforcement learning</term>
					<term>policy compression</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Policies, the mappings from states to actions, require memory. The amount of memory is dictated by the mutual information between states and actions, or the policy complexity. High-complexity policies preserve state information and generally lead to greater reward compared to low-complexity policies, which require less memory by discarding state information and exploiting environmental regularities. Under this theory, high-complexity policies incur a time cost: they take longer to decode than low-complexity policies. This naturally gives rise to a speed-accuracy trade-off, in which acting quickly necessitates inaccuracy (via low-complexity policies) and acting accurately necessitates acting slowly (via high-complexity policies). Furthermore, the relationship between policy complexity and decoding speed accounts for set-size effects: response times grow as a function of the number of possible states because larger state sets encourage higher policy complexity. Across three experiments, we tested these predictions by manipulating inter-trial intervals, environmental regularities, and state set sizes. In all cases, we found that humans are sensitive to both time and memory costs when modulating policy complexity. Altogether, our theory suggests that policy complexity constraints may underlie some speed-accuracy trade-offs and set-size effects.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time and memory costs jointly determine a speed-accuracy trade-off and set-size effects All computational systems-the human brain included-are subject to physical constraints that limit the ability to store and transmit information. Decision making taxes these limited cognitive resources, bounding achievable performance. The framework of resource rationality formalizes this idea, treating decision making as a constrained optimization problem that considers not only performance but also the costs associated with making decisions <ref type="bibr" target="#b4">(Bhui et al., 2021;</ref><ref type="bibr" target="#b37">Lieder &amp; Griffiths, 2020)</ref>. In biological systems, these costs are often formalized as time costs and memory costs, thought to be key factors underlying cognitive resource limitations <ref type="bibr" target="#b4">(Bhui et al., 2021;</ref><ref type="bibr" target="#b10">Callaway et al., 2023;</ref><ref type="bibr" target="#b37">Lieder &amp; Griffiths, 2020;</ref><ref type="bibr" target="#b60">Vul et al., 2014)</ref>. Importantly, time and memory costs are typically studied in isolation, although it seems plausible that both should interact to influence behavior.</p><p>Time costs are typically studied in tasks where speed and accuracy trade off against one another <ref type="bibr" target="#b21">(Garrett, 1922;</ref><ref type="bibr" target="#b62">Woodworth, 1899)</ref>. In the domain of decision making, speed-accuracy trade-offs have been widely observed across numerous perceptual and memory-based decision tasks <ref type="bibr" target="#b1">(Balci et al., 2011;</ref><ref type="bibr" target="#b6">Bogacz et al., 2010;</ref><ref type="bibr" target="#b28">Heitz, 2014;</ref><ref type="bibr" target="#b29">Hick, 1952)</ref>. People can be made to trace out a speed-accuracy function through explicit instruction or with task designs that sharply favor particular strategies <ref type="bibr" target="#b28">(Heitz, 2014;</ref><ref type="bibr" target="#b61">Wickelgren, 1977;</ref><ref type="bibr" target="#b63">Wu et al., 2023)</ref>.</p><p>Early attempts at conceptualizing the computational logic underlying speed-accuracy trade-offs suggested that they arise from the increased information processing required for accurate responses, resulting in longer response times (RTs; <ref type="bibr" target="#b61">Wickelgren, 1977)</ref>. More recent work has argued that speed-accuracy trade-offs may arise from an imperative to maximize time-averaged reward (that is, reward per unit time; <ref type="bibr" target="#b52">Simen et al., 2009)</ref>. Evidence in favor of this normative principle has been observed in both perceptual decision making and cognitive control domains <ref type="bibr" target="#b1">(Balci et al., 2011;</ref><ref type="bibr" target="#b6">Bogacz et al., 2010;</ref><ref type="bibr" target="#b16">Drugowitsch et al., 2015;</ref><ref type="bibr" target="#b45">Otto &amp; Daw, 2019)</ref>.</p><p>Despite this rich literature, gaps remain in our understanding of speed-accuracy trade-offs. There is limited work on how humans navigate this trade-off in multi-alternative, value-based task settings, a domain commonly studied in the reinforcement learning literature. This setting has relevance for more ecologically-meaningful behaviors <ref type="bibr" target="#b47">(Pirrone et al., 2014)</ref>. While there have been efforts to extend sequential sampling-a commonly used perceptual modeling framework-to characterize maximization of time-averaged reward in value-based decisions <ref type="bibr" target="#b58">(Tajima et al., 2016</ref><ref type="bibr" target="#b57">(Tajima et al., , 2019</ref>, the resulting models primarily address tasks in which the decision-maker deliberates between stimuli, such that each stimulus elicits noisy evidence for one unique action. These models do not naturally extend to naturalistic value-based settings, in which humans must choose one of sometimes many actions in response to an environment state.</p><p>None of the models discussed above address the additional influence of memory on decision making. A separate literature has demonstrated that decision making degrades as memory requirements (i.e., the amount of information needed to implement the optimal policy) increase <ref type="bibr" target="#b11">(Collins, 2018;</ref><ref type="bibr" target="#b12">Collins &amp; Frank, 2012;</ref><ref type="bibr" target="#b35">Lai &amp; Gershman, 2024)</ref>. Models with limited memory capacity have been developed to explain this and related findings, but these models do not typically address the speed-accuracy trade-off.</p><p>Central to the present paper is the idea that time and memory costs are deeply intertwined: information stored in memory must be "decoded" into action, and this decoding process takes longer when more information is stored . Memory incurs a time cost, and therefore decision-makers actually face a speed-accuracy-memory trade-off. Our goal is to understand this trade-off theoretically and explore it empirically.</p><p>In this paper, we develop a normative framework that jointly considers how time costs and memory constraints influence decisions, and test its predictions in three instrumental learning experiments.</p><p>We will show that across experimental conditions, human participants flexibly adjust their choice and RT profiles as predicted by the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The policy compression framework</head><p>This section describes our theoretical framework formally. We first define how to optimize decision making under memory constraints. We then introduce time costs and link them to the memory constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory-constrained policy optimization</head><p>The nervous system must contend with numerous constraints, including computational costs <ref type="bibr" target="#b7">(Bossaerts et al., 2019)</ref>, interference costs <ref type="bibr" target="#b44">(Musslick et al., 2016)</ref>, and metabolic costs <ref type="bibr" target="#b20">(Gailliot &amp; Baumeister, 2007)</ref>, among other costs <ref type="bibr" target="#b50">(Shenhav et al., 2017)</ref>. Here, we will focus on how channel capacity, an upper bound on the amount of information that can be transmitted across a noisy channel <ref type="bibr" target="#b43">(Miller, 1956;</ref><ref type="bibr" target="#b49">Shannon, 1948)</ref>, affects decision making-both in terms of decisions as well as how quickly those decisions are made ( <ref type="figure" target="#fig_0">Figure 1A,B</ref>).</p><p>The framework we propose in this manuscript is an application of rate-distortion theory to action selection. Rate-distortion theory describes how to construct an optimal channel that minimizes some notion of error (the distortion)-or, in our case, maximizes reward-subject to a constraint on the information transmission rate <ref type="bibr" target="#b54">(Sims, 2016)</ref>. The utility of rate-distortion theory lies in its generality: in addition to action selection , it has been applied to visual working memory <ref type="bibr" target="#b33">(Jakob &amp; Gershman, 2023;</ref><ref type="bibr" target="#b53">Sims, 2015;</ref><ref type="bibr" target="#b55">Sims et al., 2012)</ref>, perception <ref type="bibr" target="#b25">(Gershman &amp; Burke, 2023)</ref>, intertemporal decision-making <ref type="bibr" target="#b4">(Bhui et al., 2021)</ref>, economic behavior under imperfect information <ref type="bibr" target="#b39">(Maćkowiak et al., 2023)</ref>, the formation of cognitive abstractions <ref type="bibr" target="#b22">(Genewein et al., 2015)</ref> and task-switching costs <ref type="bibr">(Zenon et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TIME AND MEMORY COSTS JOINTLY INFLUENCE DECISIONS 5</head><p>For a resource-rational agent, we formalize memory usage as the mutual information between states s ∈ S and actions a ∈ A, which we call the policy complexity:</p><formula xml:id="formula_0">I π (S; A) = s P (s) a π(a|s) log π(a|s) P (a)<label>(1)</label></formula><p>where π(a|s) is the policy, a probabilistic mapping from states to actions, and P (a) = s P (s)π(a|s) is the marginal probability of choosing action a. High complexity policies are ones that preserve state information (e.g., deterministic mappings from states to actions) whereas low complexity policies discard state information (e.g., random actions).</p><p>In general, we assume that policies are subject to a capacity constraint, C, an upper bound on policy complexity. As we will elaborate on below, we allow agents to flexibly adjust C (up to an agent-specific upper bound) to maximize reward. Shannon's noisy channel theorem states that the minimum expected number of bits to transmit a signal across a noisy information channel without error is equal to the mutual information. Therefore, if the optimal policy requires more memory than the agent possesses, then the agent must compress its policy, or render it less state-dependent. We define the optimal policy, π * , as:</p><formula xml:id="formula_1">π * = argmax π V π , subject to I π (S; A) ≤ C<label>(2)</label></formula><p>where V π = s P (s) a π(a|s) Q(s, a) is the trial-averaged reward (i.e., reward per trial) under policy π(a|s), and Q(s, a) is the trial-averaged reward for taking action a in state s.</p><p>We can express the above constrained optimization problem in the following unconstrained Lagrange form:</p><formula xml:id="formula_2">π * = argmax π βV π − I π (S; A) + λ(s) a π(a|s) − 1<label>(3)</label></formula><p>where β ≥ 0, λ(s) ≥ 0 ∀s ∈ S are Lagrange multipliers. 1 Solving this equation yields the following optimal policy:</p><formula xml:id="formula_3">π * (a|s) ∝ exp[βQ(s, a) + log P * (a)]<label>(4)</label></formula><p>where P * (a) = s π * (a|s) p(s) is the optimal marginal action distribution.</p><p>The optimal policy takes the form of the familiar softmax distribution, common in the reinforcement learning literature. Here, the Lagrange multiplier, β, plays the role of the inverse temperature parameter. Note that although β typically takes on the role of balancing exploration/exploitation in reinforcement learning, we made no such appeals in deriving this policy.</p><p>Moreover, β is a function of the policy complexity:</p><formula xml:id="formula_4">β −1 = dV π dI π (S; A)<label>(5)</label></formula><p>At high policy complexity, when dV π dI π (S;A) is shallow, the optimal β is large and the policy is dominated by Q-values, which renders it state-dependent. At low policy complexity, the optimal β is close to 0, and Q-values have minimal impact on the policy. Moreover, low-complexity policies are dominated by the log P * (a) term, a form of perseveration (state-independent actions). In general, high-complexity policies yield more reward per trial than low-complexity policies. By varying β and calculating the optimal policy, we can trace out the reward-complexity frontier, which delimits the maximal trial-averaged reward obtainable for a given policy complexity ( <ref type="figure" target="#fig_0">Figure 1C</ref>).</p><p>The optimal policy derived here differs from the traditional softmax choice rule in reinforcement learning <ref type="bibr" target="#b56">(Sutton &amp; Barto, 2018)</ref> since it includes the additional contribution of P * (a). In tasks where P * (a) is uniform over actions, the optimal policy reduces to the traditional softmax choice rule (π(a|s) ∝ exp[βQ(s, a)]). However, in tasks where P * (a) is non-uniform and biased towards specific actions, then the influence of perseveration is non-trivial. We test this unique prediction of the theory in Experiments 2 and 3. Moreover, as stated above, the influence of perseveration is magnified at low policy complexity when β approaches 0. This prediction differs from a traditional softmax model, where the policy approaches a uniform distribution as β approaches 0. In recent work, we identified behavioral signatures unique to policy compression-and not predicted by the traditional softmax choice rule-in human data <ref type="bibr" target="#b35">(Lai &amp; Gershman, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time costs</head><p>Our formulation up to this point has ignored time costs. To understand why an agent would choose a low-complexity policy, let us assume states are represented as codewords through entropy coding and are decoded as actions, with the Huffman code as a canonical example <ref type="bibr" target="#b30">(Huffman, 1952)</ref>. The Huffman code corresponds to a binary tree in which leaf nodes correspond to decoded actions, where more complex state descriptions necessitate more leaf nodes, and therefore more bits. If we assume bits are inspected at a constant rate, then more complex state descriptions take longer to read out to reveal the decoded action.</p><p>This information-theoretic explanation for RTs has long been applied to tasks that vary the number of available actions, encapsulated in the Hick-Hyman Law. This law formalizes the empirical observation that RT increases logarithmically with the number of possible actions, or equivalently, linearly with the amount of information transmitted <ref type="bibr" target="#b29">(Hick, 1952;</ref><ref type="bibr" target="#b32">Hyman, 1953)</ref>. Under policy compression, policies of higher complexity require more complex state descriptions, which in turn necessitate more bits. Applying the same constant inspection rate assumption as the Hick-Hyman Law, reading out these policies should take longer, resulting in longer RTs. We have previously observed a significant correlation between RT and policy complexity <ref type="bibr" target="#b2">(Bari &amp; Gershman, 2023;</ref>. More specifically, given that bits are inspected at a constant rate, response times should be a linear function of policy complexity / description length, with some offset to reflect motor delay ( <ref type="figure" target="#fig_0">Figure 1D</ref>,E).</p><p>To see how the above theory predicts a speed-accuracy trade-off, let us assume agents attempt to maximize time-averaged reward under different inter-trial intervals (ITI). We chose this setup because ITI manipulations are a classic focus in the study of speed-accuracy trade-offs <ref type="bibr" target="#b28">(Heitz, 2014)</ref>, and the relationship between ITIs and time-averaged reward maximization has been of longstanding interest in the related field of perception <ref type="bibr" target="#b1">(Balci et al., 2011;</ref><ref type="bibr" target="#b6">Bogacz et al., 2010;</ref><ref type="bibr" target="#b16">Drugowitsch et al., 2015)</ref>. To make this concrete, let us take a simple premise where the agent perceives a state, selects an action after a response time, t RT , and waits through an ITI for t ITI seconds before the next trial. The time-averaged reward under these conditions takes the following form:</p><formula xml:id="formula_5">V π time (I(S; A)) = V π (I(S; A)) t RT (I(S; A)) + t ITI (6)</formula><p>where V π time (I(S; A)), is the time-averaged reward. V π (I(S; A)) is a function of policy complexity through the derivation of the optimal policy 2 ( <ref type="figure" target="#fig_0">Figure 1D</ref>) and t RT (I(S; A)) is a function of policy complexity through the assumption of a linear relationship between RT and policy complexity ( <ref type="figure" target="#fig_0">Figure 1E</ref>).</p><p>To see how the theory predicts a speed-accuracy trade-off, we can visualize the relationship between time-averaged reward and policy complexity in <ref type="figure" target="#fig_0">Figure 1F</ref>, where we varied the ITI. To maximize time-averaged reward, humans should decrease policy complexity when ITIs are short; although these policies result in less trial-averaged reward, they increase time-averaged reward because they allow agents to perform more actions due to smaller decoding time cost. Put in a more intuitive way, shorter ITIs enable agents to complete more trials within the same time frame. As a result, it is not worth spending too much time figuring out the best action for any single trial, because the agent could complete many more trials and earn much more reward in that time. Moreover, because the optimal policy includes a perseverative term (log P * (a)), the contribution of perseveration should be magnified at low policy complexity (low ITIs) because of the smaller β term. This is an interpretation of the speed-accuracy trade-off founded in the notion of resource rationality. Regarding set-size effects, in which "set size" refers to the number of possible states/stimuli, the theory predicts that response times should grow as a function of set size because larger sets require higher policy complexity (i.e., the policy must encode more states) to maximize time-averaged reward, which in turn demands longer decoding time ( <ref type="figure" target="#fig_0">Figure 1G</ref>,H).</p><p>Acting faster can yield greater time-averaged reward under some task conditions (short ITIs or <ref type="figure" target="#fig_0">Figures 1D</ref>,G, the optimal policy at policy complexity 0 obtains more reward than the apparent chance level (one over the number of actions). Chance level of reward is higher because reward is delivered with probability 0.25 for incorrect actions. In addition, for the set size 2 and 4 conditions in <ref type="figure" target="#fig_0">Figure 1G</ref>, the optimal policy is to only choose among the subset of small stimulus set sizes), but necessitates a commitment to greater errors through lower policy complexity.</p><formula xml:id="formula_6">2 In</formula><p>Under other conditions (long ITIs or large stimulus set sizes), one can have the guarantee of fewer errors through higher policy complexity, though with the requirement of acting slower. A host of other predictions fall out of this single relationship, which we will elaborate on in the Experiment sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory costs</head><p>In deriving the optimal policy, we treat memory as a point constraint on policy complexity and allow the constraint to be flexibly adjusted as a function of time costs. However, information transmission may also incur a memory cost (i.e., subjective mental effort) that increases monotonically with policy complexity. The idea that information complexity incurs a memory cost is consistent with prior work in perceptual decision and reinforcement learning paradigms <ref type="bibr" target="#b17">(Fang, 2021;</ref><ref type="bibr" target="#b25">Gershman &amp; Burke, 2023)</ref>.</p><p>Moreover, systematic deviations from reward-rate maximization have been interpreted as evidence that information transmission incurs a cost <ref type="bibr" target="#b16">(Drugowitsch et al., 2015)</ref>. We therefore hypothesize that policy complexity will manifest as subjective task difficulty and that subjects will systematically adopt policy complexity levels lower than predicted by the framework.  Panels A-C adapted from <ref type="bibr" target="#b35">(Lai &amp; Gershman, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>We aimed to test whether humans incorporate both time and memory costs when making decisions. In Experiment 1, we manipulated ITIs to test whether humans adjust policy complexity to maximize time-averaged reward. We made the following predictions related to time costs. Under longer ITIs, we predict 1) higher policy complexity, which, given the framework's proposed relationship between policy complexity and RT, leads to 2) slower RTs, because more complex policies take longer to decode.</p><p>According to the framework, this combination of policy complexity and RT maximizes time-averaged reward under longer ITIs ( <ref type="figure" target="#fig_0">Figure 1F</ref>). Next, since higher policy complexity dictates a more deterministic mapping from states to actions, we predict 3) decreased action stochasticity (the conditional entropy of actions conditioned on state, H(A|S)) with longer ITIs. As detailed in the Introduction, the optimal policy is proportional to the exponentiated action values, weighted by an inverse temperature parameter β, and the marginal action distribution. At higher policy complexity, β increases, which decreases the influence of the marginal action distribution P (a) on the policy. Assuming P (a) is estimated and updated on a trial-by-trial basis, we predict 4) decreased perseveration (the probability of repeating the same action) with longer ITIs. We return to the relationship between P (a) and choice repetition when we develop process models. Finally, we predict 5) decreased time-averaged reward; although higher policy complexity results in increased trial-averaged reward, this is offset by the longer time spent in the ITI.</p><p>We made two further predictions related to memory costs. If increased memory utilization is costly (Zenon et al., 2019), then we predict 6) longer ITIs should be associated with higher perceived difficulty, since maximizing time-averaged reward under longer ITIs necessitates greater policy complexity. Such difficulty measurements were available to us, as participants had ranked all experimental blocks by their perceived difficulty. Based on perceived difficulty and prior work arguing that information transmission incurs a cognitive cost <ref type="bibr" target="#b16">(Drugowitsch et al., 2015;</ref><ref type="bibr" target="#b17">Fang, 2021;</ref><ref type="bibr" target="#b24">Gershman &amp; Bhui, 2020;</ref><ref type="bibr" target="#b25">Gershman &amp; Burke, 2023)</ref>, we hypothesize that participants will show 7) a systematic leftward bias in policy complexity, in which their empirical policy complexity is lower than what is optimal. This is a non-trivial hypothesis, since one would not expect this simply from maximizing time-averaged reward, as implementing a policy of slightly less or greater complexity results in similar time-averaged reward and there should therefore be no bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>One-hundred participants (37 women, 62 men, 1 non-binary, 1 prefer not to say) were recruited.</p><p>We selected the sample size based on the lowest estimated effect size (Cohen's d = 0.312) among dependent Experiment 1 setup </p><formula xml:id="formula_7">1 2 3 4 A 1 A 2 A 3 A 4 S 4 S 3</formula><formula xml:id="formula_8">S 1 S 2 S 3 S 4 A 1 A 2 A 3 A 4</formula><p>In this block, ITI = 0.5 seconds.</p><p>In this block, you earned 20 pts. for the block. After the block ends, participants receive feedback on the total reward gained in the block.</p><p>Participants are informed of the block's ITI before starting.</p><p>variables, according to estimates from a separate group of N = 48 pilot participants (data excluded from final analysis). All analyses were preregistered at https://aspredicted.org/blind.php?x=VF2_NH6. We excluded 3 participants for having an average RT for any block exceeding 5 seconds, leaving data from 97 participants (35 women, 60 men, 1 non-binary, 1 prefer not to say) for subsequent analyses. Participants gave informed consent, and the Harvard University Committee on the Use of Human Subjects approved the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Each participant completed three blocks of trials with ITIs of 0s, 0.5s, and 2s respectively. The block order was randomized across participants. Participants were informed of the ITI of each block.</p><p>Participants were informed that they would receive a bonus proportional to their performance for each TIME AND MEMORY COSTS JOINTLY INFLUENCE DECISIONS block (i.e., relative to the maximum reward attainable for each block).</p><p>There were four possible states (images) and four available actions, which are shared across blocks.</p><p>Each stimulus was assigned a unique optimal action ( <ref type="figure" target="#fig_2">Figure 2B</ref>). Participants were informed that the mapping from stimulus to action was held fixed across all blocks. This was done to minimize the learning of action values within blocks.</p><p>On each trial, participants were presented with one image (state) and responded by pressing one of several possible keyboard keys (actions; <ref type="figure" target="#fig_2">Figure 2A</ref>). Stimulus presentation was counterbalanced within runs of 8 trials, where the stimulus presentation order was randomized within each run and each of the four images appeared exactly twice per run. We did this to ensure a uniform state distribution P (s), allowing us to better estimate policy complexity. See <ref type="figure" target="#fig_0">Supplementary Figure 1</ref> for evidence that participants did not exploit this regularity. Reward delivery was binary and probabilistic: each state was associated with one optimal action ( <ref type="figure" target="#fig_2">Figure 2B</ref>). After making a response, participants were given immediate feedback for 0.3s-either a green border around the image to indicate reward or a gray border to indicate no reward.</p><p>We did not use punishment feedback. A fixation cross then appeared throughout the ITI. Each block lasted until 3 minutes elapsed and the current run of trials finished. Participants could track the remaining time and reward earned during the block, which were displayed as red and green bars, respectively. At the end of each block, they were provided with feedback on the total reward they earned in that block ( <ref type="figure" target="#fig_2">Figure 2C</ref>).</p><p>Participants completed three 1-minute training blocks, one for each ITI condition, to familiarize themselves with the task and learn the mapping from stimulus to response. These data were not analyzed.</p><p>Participants then completed the three 3-minute blocks where ITI was varied, as mentioned above. After completing the whole experiment, participants ranked the perceived difficulty for each block. Participants additionally completed the Barratt Impulsiveness Scale, which we did not analyze for this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating policy complexity</head><p>We defined policy complexity as the mutual information between the observed states and chosen actions. Following prior work <ref type="bibr" target="#b24">(Gershman &amp; Bhui, 2020;</ref>, we estimated the policy complexity of each participant in each ITI condition using the Hutter estimator, which computes the posterior mean value of mutual information under a symmetric Dirichlet prior <ref type="bibr" target="#b31">(Hutter, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>Due to the directional nature of the framework's predictions, all statistical tests were one-sided paired t-tests except for the test of optimal minus empirical policy complexity, which was a one-sided</p><p>Wilcoxon signed-rank test due to strong non-normality. The one-sided test directions were preregistered.</p><p>In the main text, we report all pairwise comparisons, their effect sizes, and the 95% confidence intervals (CI) of the effect sizes in <ref type="table">Supplementary Tables 1 to 2.</ref> We fit a linear mixed-effects (LME) model to determine the participant-specific relationship between average RT and policy complexity for a block. The fixed effects were the intercept and policy complexity and random effects were intercept and policy complexity (independent from each other), grouped by participant. We obtained parameter estimates using maximum likelihood estimation with the "fitlme" function in MATLAB R2023a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>Data and code for this and subsequent experiments are available at https://github.com/LSZ2001/policycompression_timememorycosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Consistent with prior results, participants achieve near maximal trial-averaged reward as a function of policy complexity ( <ref type="figure">Figure 3A</ref>; <ref type="bibr" target="#b2">Bari &amp; Gershman, 2023;</ref><ref type="bibr" target="#b23">Gershman, 2020;</ref><ref type="bibr" target="#b35">Lai &amp; Gershman, 2024)</ref>. In line with our predictions that humans modulate policy complexity to maximize time-averaged reward, participants used policies of higher complexity in longer ITI blocks compared to shorter ITI blocks (t(96) = −6.40, p &lt; 10 −8 ; <ref type="figure">Figure 3B</ref>). Participants also adopted slower RTs in longer ITI blocks (t(96) = −7.44, p &lt; 10 −10 ; <ref type="figure">Figure 3C</ref>), consistent with the notion that higher complexity policies are slower to execute. Action stochasticity similarly decreased in longer ITI blocks as policies became concentrated on one action for each state (t(96) = 4.39, p &lt; 10 −4 ; <ref type="figure">Figure 3D</ref>). Perseveration, as predicted, decreased as a function of ITI (t(96) = 4.09, p &lt; 10 −4 ; <ref type="figure">Figure 3E</ref>). This overall led to a reduction in time-averaged reward as a function of ITI (t(96) = 25.6, p &lt; 10 −44 ; <ref type="figure">Figure 3F</ref>,G). All of these findings are consistent with the idea that humans are sensitive to time costs when adjusting policy complexity.</p><p>We next tested the hypothesis that humans are sensitive to memory costs when adjusting policy complexity. One readout of this is perceived task difficulty-if implementing a high complexity policy is costly, then participants should perceive it as more cognitively demanding. This was indeed the case, as participants ranked the ITI = 0s condition the easiest-which demands the lowest policy complexity-and ranked the ITI = 2s condition the most difficult-which demands the highest policy complexity (t(96) = −5.11, p &lt; 10 −6 ; <ref type="figure" target="#fig_5">Figure 4A</ref>). Furthermore, we computed the Spearman correlation for each participant between their perceived difficulty rating and empirical policy complexity across ITI conditions and confirmed they were positively correlated at the single-participant level (t(96) = 5.23, p &lt; 10 −6 , Cohen's d = 0.526, 95% CI [0.314, 0.736]). Note that one would have predicted the opposite if the motor cost of the task conditions dominated, since shorter ITI conditions demand a higher frequency of button presses to maximize reward.</p><p>Finally, since we confirmed that higher policy complexity is costlier, we tested our prediction that participants should exhibit a leftward bias in policy complexity. First, we validated the proposed linear relationship between policy complexity and RT by fitting LME models to predict average RT as a function of policy complexity. The fitted model yielded significant effects for the intercept (fixed effects 0.301 ± 0.0251, t(289) = 12.0, p &lt; 10 −26 ; random effects SD = 0.161) and policy complexity (fixed effects 0.445 ± 0.0400, t(289) = 11.1, p &lt; 10 −23 ; random effects SD = 0.0870). Visually, empirical RTs and the LME-predicted RTs correlated well with one another ( <ref type="figure" target="#fig_5">Figure 4B</ref>) and most participants shared linear time-cost functions that largely differed by intercept ( <ref type="figure" target="#fig_5">Figure 4C</ref>). We next used the fitted participant-specific linear time-cost functions to estimate their optimal policy complexity, defined as the policy complexity level that maximizes time-averaged reward for each participant in each condition. For each participant, we compared empirical policy complexity to optimal and confirmed a leftward policy time-averaged reward (G) across ITI conditions. All SEM errorbars were within-participant <ref type="bibr" target="#b13">(Cousineau et al., 2005)</ref>.</p><formula xml:id="formula_9">z = −8.20, p &lt; 10 −15 , Cliff's δ = −0.674, 95% CI [−0.794, −0.555] (Figure 4D-F).</formula><p>One alternative explanation is that the leftward bias arises became some fraction of participants reach their individual capacity limits (upper bound on C), which is less than the optimal policy complexity for a given task condition. On average, this will tend to produce an apparent leftward bias. To test this hypothesis, we identified a subgroup of participants with a leftward bias in the ITI = 0s condition and examined their policy complexity in the ITI = 2s condition. If these participants reached their capacity limits in the ITI = 0s condition that favors low policy complexity, then we would predict that they should maintain a similar policy complexity in the ITI = 2s condition that favors high policy complexity because they cannot exceed their capacity limits. We found that this rarefied group of subjects increased policy complexity in ITI = 2s relative to ITI = 0s (t(74) = 3.88, p &lt; 10 −3 , Cohen's d = 0.175, 95% CI [0.0701, 0.280]). This suggests that the leftward bias is not solely due to capacity limits.</p><p>We also used the above results to address an alternative explanation of our findings: perhaps a small subgroup of participants adjusted policy complexity but most of our findings can be attributed to a large subgroup of disengaged participants who consistently responded randomly across all ITI conditions.</p><p>To test this explanation, we used the leftward policy complexity bias for the ITI = 2s condition ( <ref type="figure" target="#fig_5">Figure   4F</ref>) and partitioned participants into two subgroups-a "low-complexity" and a "high-complexity" group.</p><p>Consistent with our prior findings, this "low-complexity" subgroup significantly modulated policy complexity and RT across ITI conditions ( <ref type="figure" target="#fig_2">Supplementary Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We found that participants in Experiment 1 were sensitive to ITI conditions, modulating their policy complexity and RT in the direction of time-averaged reward maximization. The experimental data supported all seven predictions of the policy compression framework, demonstrating that humans are sensitive to both time and memory costs when making decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>In Experiment 2, we tested an additional prediction of policy compression: humans should exploit environmental regularities (e.g., multiple states sharing the same optimal action) when compressing their policies (endogenized by the P * (a) term in the optimal policy). We designed Experiment 2 to test this unique prediction and to replicate findings from Experiment 1. We introduced environmental regularity by having two states (s 1 and s 2 ) share the same optimal action a 1 ( <ref type="figure">Figure 5A</ref>). This has the effect that the optimal marginal action distribution, P * (a) is non-uniform and favors that action. We hypothesized that the effect of the optimal marginal action distribution would be greatest at low policy complexity, since marginal actions influence the policy more strongly at low complexity (Equation 4). Difference between each participant's optimal (maximizing time-averaged reward according to LME predictions for that participant) and empirical policy complexity, for each ITI condition (left to right: 0s, 0.5s, 2s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Two-hundred participants (113 women, 83 men, 4 prefer not to say) were recruited. All participants did not participate in Experiment 1. We selected the sample size based on the lowest </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Task procedures were identical to those in Experiment 1, except that in Experiment 2, two of the four states (s 1 and s 2 ) shared the same optimal response a 1 . The specific images and key presses were randomized across participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating policy complexity</head><p>Policy complexity was estimated using the same procedures as in Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>Statistical testing and errorbar visualization procedures were identical to those in Experiment 1.</p><p>We report all pairwise comparisons, their effect sizes, and the 95% CIs of the effect sizes in <ref type="table">Supplementary   Tables 3 to 4</ref>. LME modeling procedures were identical to those in Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Our findings were largely identical to what we found in Experiment 1. Participants achieved near-maximal trial-averaged reward as a function of policy complexity (Supplementary <ref type="figure">Figure 3B)</ref>. Policy complexity increased as a function of ITI (t(197) = −9.74, p &lt; 10 −18 ; <ref type="figure">Figure 5B</ref>). RTs similarly slowed as a function of ITI (t(197) = −7.95, p &lt; 10 −13 ; <ref type="figure">Figure 5C</ref>). Action stochasticity (t(197) = 1.95, p = 0. According to the policy compression framework, participants should exploit the fact that states s 1 and s 2 share the same optimal action a 1 ( <ref type="figure">Figure 5A</ref>), increasingly choosing a 1 as ITI decreases, because this favors lower policy complexity. To gain an intuition, in the extreme case where policy complexity equals to zero, participants ignore the stimuli entirely and they should always pick action a 1 since this maximizes reward. We looked at the policies for stimuli s 3 and s 4 to identify the effect of the marginal action distribution. For stimuli s 3 and s 4 , under high policy complexity, a 1 should be chosen infrequently since this is not the reward-maximizing option. However, as policy complexity decreases and the marginal action distribution has greater influence on the policy, a 1 should be chosen more often. Consistent with our prediction, the probability of choosing a 1 decreased monotonically as a function of ITI (t(197) = 1.88, p = 0.0305; <ref type="figure">Figure 5D</ref>). However, the effect size was fairly small, with the 95% CI including 0 (Cohen's</p><formula xml:id="formula_10">d = 0.124, 95% CI [−0.006, 0.256])</formula><p>. One limitation of this preregistered analysis is it only compares the most extreme datapoints, at ITI = 0s and 2s. We therefore performed a posthoc LME regression analysis utilizing data from all 3 conditions. We included intercept and ITI condition as fixed effects and independent random effects. The resulting fixed effect for ITI was significant (coefficient estimate −0.0164 ± 0.00766, t(592) = −2.14, p = 0.0330, random effects SD = 0.00134), providing additional evidence for a monotonically decreasing probability of choosing a 1 with increased ITI.</p><p>We further replicated Experiment 1's findings related to memory costs. Participants ranked the ITI = 0s condition the easiest and the ITI = ; random effects SD = 0.0336) and policy complexity (fixed effects 1.02 ± 0.0115, t(592) = 8.89, p &lt; 10 −17 ; random effects SD = 0.617). We used the same procedure as Experiment 1 to calculate participant-specific optimal policy complexity, and we again found a leftward bias in the difference between empirical and optimal policy complexity for the ITI = 2s condition, z = −6.21, p &lt; 10 −9 , Cliff's <ref type="figure">Figure 3F)</ref>. We did not find a leftward bias for the ITI = 0s and 0.5s conditions (ITI = 0s, z = 11.7, p = 1.00, Cliff's δ = 0.829, 95% CI [0.756, 0.902]; ITI = 0.5s, z = 7.57, p = 1.00, Cliff's δ = 0.427, 95% CI [0.332, 0.523]; <ref type="figure">Supplementary Figure 3F)</ref>, likely because the optimal policy complexity of most participants for these ITI conditions (ITI = 0s, mean±SEM = 0.00345 ± 0.00144 bits; ITI = 0.5s, 0.0616 ± 0.00756 bits) was already very close to 0-the lowest possible policy complexity level. In contrast, in Experiment 1, the optimal policy complexity level for ITI = 0s was 0.227 ± 0.0153 bits, higher than the corresponding optimal policy complexity for Experiment 2. We conducted the same subgroup partition as in Experiment 1 and found that the "low-complexity" subgroup still significantly modulated policy complexity and RT as a function of ITI (see <ref type="figure" target="#fig_2">Supplementary Figure 2</ref>).</p><formula xml:id="formula_11">δ = −0.444, 95% CI [−0.546, −0.343] (Supplementary</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TIME AND MEMORY COSTS JOINTLY INFLUENCE DECISIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In Experiment 2, we replicated findings from Experiment 1, which demonstrates their robustness.</p><p>Most importantly, the data support an important prediction of the framework: participants exploit environmental regularities by incorporating them into the marginal action distribution, P (a), and that this effect is most pronounced at low policy complexity. This finding contributes to recent work demonstrating that participants exploit environmental regularities at low policy complexity <ref type="bibr" target="#b35">(Lai &amp; Gershman, 2024)</ref>. We did, however, observe a systematic deviation from the predictions of the theory. Quantitatively, policy compression predicts that as policy complexity approaches 0, agents should deterministically choose the shared action a 1 for states s 3 and s 4 ; our participants systematically had higher entropy policies (i.e., their policies were more stochastic), leading to smaller effect sizes than predicted by the framework. Overall, this systematic deviation merits future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3</head><p>We have so far demonstrated that humans modulate policy complexity in response to ITI manipulations to maximize time-averaged reward. However, manipulating ITI is not the only task condition that should modulate policy complexity and RTs. The relationship between policy complexity and decoding speed predicts set-size effects, a seemingly disparate domain: response times should grow as a function of set size because larger sets require higher policy complexity. As the set size grows and more stimuli must be encoded by the policy, the optimal policy complexity also grows to maximize time-averaged reward ( <ref type="figure" target="#fig_0">Figure 1G,H)</ref>.</p><p>In Experiment 3, we manipulated stimulus set size while keeping ITI fixed. We made the following predictions related to time costs: larger set sizes should be associated with 1) higher policy complexity, 2) slower RTs, 3) decreased perserveration, and 4) decreased time-averaged reward. We also made similar predictions related to memory costs: 5) greater set sizes should be associated with higher perceived difficulty and 6) we should observe a systematic leftward bias in empirical policy complexity relative to optimal.</p><p>In Experiment 3, we vary the state set size (up to 6) but keep the number of available actions fixed at the largest set size (at 6). Hence, in conditions where the set size is less than the number of actions, there exist actions that are suboptimal for all possible states. The framework predicts that such actions should never be chosen, assigning them 0 probability in the optimal policy π * (a|s). This yields a prediction unique to policy compression: at low policy complexity, the optimal policy will assign non-zero probability to suboptimal actions in a given state, as long as they are optimal for some other state (e.g., if a 1 is optimal in s 1 , then a 1 will be chosen with some non-zero probability in other states). Further, the probability of choosing these suboptimal actions will increase at lower values of policy complexity. We therefore predict that 7) choosing a suboptimal action in a given state, conditioned on that action being optimal for some other state, will be more probable for smaller set sizes when policy complexity is the smallest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>One hundred and one participants (54 women, 44 men, 2 non-binary, 1 prefer not to say) were recruited. All participants did not participate in either Experiment 1 or 2. We selected the sample size based on the lowest estimated effect size (Cohen's d = 0.459) among dependent variables of interest, according to analyses of a separate group of N = 48 pilot participants (data excluded from final analysis).</p><p>All analyses were preregistered at https://aspredicted.org/ZSW_HFY. The inclusion criterion was identical to Experiments 1 and 2. A total of 99 participants (53 women, 43 men, 2 non-binary, 1 prefer not to say) met this inclusion criteria and were therefore included. Participants gave informed consent, and the Harvard University Committee on the Use of Human Subjects approved the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The three test blocks had stimuli set sizes of 2, 4, and 6 stimuli respectively, and their order was randomized across participants. Each set size used unique images in order to make each set-size manipulation as independent as possible. The action set size was fixed at 6 across all set-size conditions.</p><p>We used ITI = 2s for each block. Participants were informed of the ITI and set size of each block.</p><p>Each stimulus was associated with a unique optimal action. Like Experiments 1 and 2, optimal actions yielded reward with probability 0.75 and suboptimal actions yielded reward with probability 0.25.</p><p>Stimuli were randomized and presented in counterbalanced runs of 8, 8, and 10 trials for set sizes 2, 4, and 6 respectively (each stimulus therefore appeared 4 times, 2 times, and 2 times respectively within each run).</p><p>For each set-size condition, participants first completed three training blocks with ITI = 0s, 0.5s, and 2s, similar to Experiments 1 and 2. We did this to encourage learning and minimize the length of training. To ensure similar learning across set-size conditions, we presented each stimulus 48 times during training (24 for ITI = 0s, 16 for ITI = 0.5s, and 8 for ITI = 2s) rather than training for a fixed time duration. Participants were told that the mapping from stimuli to actions remained fixed between training and test blocks. After completing the three training blocks, participants proceeded to the 3-minute test block of the same set-size condition. The structure, visual display, and duration of blocks were identical to Experiments 1 and 2.</p><p>After completing the whole experiment, participants ranked the perceived difficulty for each block.</p><p>Participants additionally completed the Barratt Impulsiveness Scale, which we did not analyze for this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating policy complexity</head><p>Policy complexity was estimated for each participant in each set size condition. Other details were identical to that in Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>Statistical testing and errorbar visualization procedures were identical to those in Experiment 1 and 2, except that tests were carried out between set-size conditions instead of ITI conditions. In the main text, we report comparisons between set size = 2 vs 6. We report all pairwise comparisons, their effect sizes, and the 95% CIs of the effect sizes in <ref type="table">Supplementary Tables 5 to 6</ref>. LME modeling procedures were identical to those in Experiment 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We found support for our predictions that humans are sensitive to time costs in response to set-size manipulations. Participants achieved near-maximal trial-averaged reward as a function of policy complexity ( <ref type="figure">Figure 6A</ref>). Consistent with our predictions, policy complexity increased as a function of set size (t(98) = −10.5, p &lt; 10 −17 ; <ref type="figure">Figure 6B</ref>) and RTs slowed (t(98) = −3.97, p &lt; 10 −4 ; <ref type="figure">Figure 6C</ref>).</p><p>Perseveration decreased as a function of set size (t(98) = 4.00, p &lt; 10 −4 ; <ref type="figure">Figure 6D</ref>). The probability of choosing a suboptimal action, conditioned on that action being optimal for some other state, increased for smaller set sizes (t(98) = 2.84, p = 0.00275; <ref type="figure">Figure 6E</ref>). Finally, time-averaged reward decreased as a function of set size (t(98) = 5.28, p &lt; 10 −6 ).</p><p>We additionally found support for our predictions regarding memory costs. Participants ranked the set size 2 condition as the easiest and the set size 6 condition as the hardest (t(98) = −7.48, p &lt; 10 −10 ; <ref type="figure">Figure 6F</ref>), and this trend held at the single-participant level (t(98) = 4.59, p &lt; 10 −5 , Cohen's d = 0.458, 95% CI [0.251, 0.663]). We fitted the LME and identified significant effects for the intercept (fixed effects 0.354 ± 0.0287, t(295) = 12.4, p &lt; 10 −27 ; random effects SD = 0.0381) and policy complexity (fixed effects 0.568 ± 0.0977, t(295) = 5.82, p &lt; 10 −7 ; random effects SD = 0.568). We used the same procedure as Experiment 1 to calculate participant-specific optimal policy complexity, and we again found a leftward bias in the difference between empirical and optimal policy complexity for all set-size conditions: set size 2,  <ref type="figure">Figure 6G</ref>).</p><p>Similar to Experiment 1, we find that capacity limits alone are insufficient to explain the leftward bias. We isolated the subgroup of participants with a leftward bias in the set size 2 condition, and examined their policy complexity in the set size 6 condition. If capacity limits were sufficient to explain the leftward bias, then this subgroup should not increase policy complexity in the set size 6 condition. Counter to hypothesis, this subgroup increased policy complexity in the set size 6 condition compared to set size 2 (t(87) = 10.1, p &lt; 10 −15 , Cohen's d = 0.397, 95% CI [0.303, 0.492]). This suggests that the leftward bias in set size 2 is not solely due to capacity limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In Experiment 3, we have demonstrated the applicability of policy compression across a different form of task manipulation-stimulus set size, as opposed to ITI studied in Experiments 1 and 2.</p><p>Importantly, both ITI and set-size manipulations induce human behavioral changes predicted by the framework. In addition, Experiment 3 shows that the framework is well-suited for a variety of value-based tasks, in which the environmental state does not feature multiple stimuli each favoring a different action. Process-level modeling with evidence-accumulation models</p><p>We next sought to understand whether standard formulations of prior models could account for our experimental results. We focus on evidence accumulation models as they are capable of generating choice and RT data in multi-alternative choice settings. These models assume agents integrate sequential noisy observations to a decision bound to make a choice, with the RT determined by the time it took to accumulate evidence <ref type="bibr" target="#b19">(Forstmann et al., 2016)</ref>. These models have been extended to value-based instrumental learning tasks by assuming agents sequentially retrieve (noisy) Q-values from memory <ref type="bibr" target="#b18">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b40">McDougle &amp; Collins, 2021;</ref><ref type="bibr" target="#b42">Miletić et al., 2020;</ref><ref type="bibr" target="#b46">Pedersen et al., 2017;</ref><ref type="bibr" target="#b57">Tajima et al., 2019)</ref>.</p><p>We focus specifically on the linear ballistic accumulator (LBA) class of models, as they are naturally well-suited for multi-alternative choice settings like our experiments <ref type="bibr" target="#b8">(Brown &amp; Heathcote, 2008;</ref><ref type="bibr" target="#b9">Busemeyer et al., 2019;</ref><ref type="bibr" target="#b15">Donkin et al., 2009</ref><ref type="bibr" target="#b14">Donkin et al., , 2011</ref>, and have frequently been used in model studies that synthesize memory and value-based decision making <ref type="bibr" target="#b40">(McDougle &amp; Collins, 2021;</ref><ref type="bibr" target="#b42">Miletić et al., 2020;</ref><ref type="bibr" target="#b59">van Ravenzwaaij et al., 2020)</ref>. The simplest LBA assumes the following: each available action corresponds to one independent accumulator. On each trial, the start point of each accumulator is sampled uniformly</p><formula xml:id="formula_12">between [0, A]. The drift rate k i of each accumulator i is sampled from a Gaussian distribution N (v i , s 2 ),</formula><p>where v i is the mean drift rate of the accumulator and s 2 is the variance. As is standard, v i takes one of two values: v i = v correct for the correct action and v incorrect otherwise where v incorrect &lt; v correct <ref type="bibr" target="#b14">(Donkin et al., 2011)</ref>. We fixed s = 0.1 to ensure parameter identifiability <ref type="bibr" target="#b15">(Donkin et al., 2009;</ref><ref type="bibr" target="#b40">McDougle &amp; Collins, 2021</ref>). On each trial, each accumulator ballistically accumulates evidence and action i is taken when the first accumulator reaches a decision bound b. The trial's response time is the time taken for this first accululator to reach the bound plus some nondecision time t 0 : RT = t 0 + b−A ki . The simplest LBA model contains 5 free parameters: A, b, v correct , v incorrect , t 0 . This model formulation allows us to express the likelihood of observing a trial's choice and RT for a given set of parameter values. Therefore, the LBA can be fitted jointly on the choice and RT data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard LBA models</head><p>We first fit three variants of the LBA model to data from each experiment, each with increasing degrees of freedom. Model 1 (LBA 1) is the LBA we have just described, with five free parameters shared across all three ITI/set-size conditions. Model 2 (LBA 2) fits the bound height parameter b independently for each ITI/set size condition. Model 3 (LBA 3) fits all five parameters separately for each ITI and set size condition to allow for the greatest flexibility. After fitting these three models, we simulated choice and RT data over the same task conditions seen by participants.</p><p>Across the three experiments, the fitted LBA models demonstrate an increase in policy complexity as a function of ITIs or set size, just like human participants ( <ref type="figure">Figure 7A</ref>). The more complex LBA 2 and 3 also qualitatively captured the increase in RT, due to condition-specific parameters. However, these LBA models could not capture the perseverative tendencies of the subjects. They fail to repeat previous actions as frequently as subjects ( <ref type="figure">Figure 7C</ref>) and fail to take advantage of the optimal marginal action distribution ( <ref type="figure">Figure 7D</ref>). This failure is particularly prominent in Experiment 3, where LBAs diverge further from human behavior at low set size conditions. This is because the LBAs allocate probability across all actions in the set size 2 and 4 conditions, even for actions that are suboptimal across all states. Given these model failures, we sought to augment the LBAs with a mechanism for capitalizing on the statistical regularities within each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LBA models augmented with perseveration</head><p>To investigate whether perseveration towards P * (a)-as predicted by policy compression-could provide a more parsimonious explanation for human behavior, we designed and fit a new LBA model (LBA 4), that builds on LBA 2 (which allows the bound height b to vary across conditions). LBA 4 updates a marginal action distribution, P (a), on a trial-by-trial basis and uses this estimate to bias action selection.</p><p>This estimate of P (a) provides a bonus to the mean drift rate v(a) of each accumulator/action a in a manner that is similar to Equation 4, the optimal policy. Mathematically, this model takes the following form:</p><formula xml:id="formula_13">P 0 (a) ∝ 1 (7) P τ (a) ∝ P τ −1 (a) + α P • I(a = a τ −1 ) (8) v τ (a) = exp (v reward (a|s τ ) + α V • log (P τ (a))) (9) (a τ , RT τ ) ∼ LBA(A, b, t 0 , v τ , s = 0.1)<label>(10)</label></formula><p>where τ denotes the current trial, (τ − 1) denotes the previous trial, I(•) denotes the indicator function, v reward (a|s τ ) equals v correct or v incorrect depending on whether a is an optimal action for the current state</p><formula xml:id="formula_15">s τ , and v τ = (v τ (a 1 ), v τ (a 2 )</formula><p>, ...) denote the mean drift rates for all accumulators in the current trial.</p><p>Specifically, Equation 8 updates the action distribution based on the previous trial's chosen action a τ −1 , using a delta learning rule with learning rate α P . Equation 9 mimics Equation 4, the optimal policy, in allowing reward and the marginal action distribution to jointly affect choices, where the trade-off parameter α V acts similarly to β and is fitted independently across task conditions. The joint influence of both rewards and past actions manifest in the mean drift rates of each accumulator, which are used to generate choice and response time data for the current trial (a τ , RT τ ) using the usual LBA setup. LBA 4 offers a plausible process-level implementation of policy compression, connecting trial-by-trial repetition of previous actions with a bias towards more frequently rewarded actions.</p><p>Overall, LBA 4 contains 11 free parameters:</p><formula xml:id="formula_16">(A, b 1 , b 2 , b 3 , t 0 , v correct , v incorrect , α P , α V1 , α V2 , α V3 ),</formula><p>where b and α V are fitted independently for each task condition (indexed by 1, 2, and 3). We fit LBA 4 to data from each experiment, and visualize its simulated data in <ref type="figure">Figure 7</ref>. Behaviorally, LBA 4 performs similarly to LBAs 1 to 3 in replicating the increasing trends of policy complexity and RT ( <ref type="figure">Figure 7A-B</ref>).</p><p>This is unsurprising because it build on top of LBA 2 and should do at least as well as that model. Importantly, it better captures the probability of repeating previous actions ( <ref type="figure">Figure 7C</ref>), as well as the increasing trends in P (a 1 |s 3 or s 4 ) in Experiment 2 and P (suboptimal action optimal for other states) in Experiment 3. Quantitative model comparison using BIC revealed that LBA 4 is the most parsimonious model for every experiment (Supplementary <ref type="figure">Figure 5)</ref>. Row 2: Experiment 2 predictions. Same as Row 1, with (D) being a replicate of <ref type="figure">Figure 5D</ref>, additionally with LBA predictions overlaid. Row 3: Experiment 3 predictions. These four panels are replicates of <ref type="figure">Figure 6B</ref>-E, additionally with LBA predictions overlaid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>By fitting various LBA models, we demonstrate that standard parameterization could not capture key predictions of policy compression nor of human behavior-perseveration towards the marginal action distribution P * (a), nor its plausible process-level implementation of repeating previous actions. In contrast, by introducing a mechanism for perseveration inspired by policy compression, the resultant LBA 4 better captured these predictions, in a way more similar to human participants. Further, model comparison results favored LBA 4, highlighting the novel contribution of policy compression in explaining a nontrivial component of human behavior.</p><p>Nevertheless, LBA 4 remains imperfect in capturing perseveration during Experiment 2, systematically underestimating P (a 1 |s 3 or s 4 ) for all three ITI conditions (although it still performs better than other LBAs). We allude to the deviations of human subjects from the theory's predictions in the Experiment 2 Discussion (i.e., humans tend to adopt higher entropy policies than predicted by the theory).</p><p>However, another possible contributor is a key disparity between LBA 4 and policy compression: it is unlikely that the competition between different LBA accumulators acts equivalently to the normalizing operation in Equation 4. These remaining disparities call for more refined descriptive models that match the logic of policy compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Here, we developed a theoretical decision making framework that jointly considers both time and memory costs. Across three human instrumental learning tasks, we tested its predictions and validated its explanatory breadth in seemingly disparate domains. Our findings reveal that humans are sensitive to the time cost of decoding policies when striving to maximize time-averaged reward. Importantly, the task manipulations-ITIs and set sizes-did not impose rigid constraints on participants' behavior, allowing them to freely adjust their RTs and the complexity of their policies based on the task context. These features enable us to interpret changes in policy complexity and RT as reflections of sensitivity to time and memory costs, rather than hard constraints (e.g., response deadlines) dictated by our experimental design.</p><p>The policy compression framework contributes to our understanding of value-based decision making in several ways. First, by considering both time and memory costs, the framework unifies several well-studied behavioral and cognitive variables that appear seemingly disparate. It links choice and RT data for value-based decisions under a single coherent, normative framework, a longstanding goal of cognitive science, and separate from the approach of combining sequential sampling models with reinforcement learning models to generate both choices and RTs <ref type="bibr" target="#b18">(Fontanesi et al., 2019;</ref><ref type="bibr" target="#b40">McDougle &amp; Collins, 2021;</ref><ref type="bibr" target="#b42">Miletić et al., 2020;</ref><ref type="bibr" target="#b46">Pedersen et al., 2017;</ref><ref type="bibr" target="#b57">Tajima et al., 2019)</ref>. This points to an area of future work for the policy compression framework, which does not naturally generate RT distributions. In this framework, policy complexity is the minimum average description length of the codewords used to decode actions, and therefore only maps onto average RT-a point statistic. In contrast, sequential sampling models are naturally suited for fitting and generating full RT distributions.</p><p>While descriptive models, such as LBAs, can generate a speed-accuracy trade-off (i.e., by lowering the decision bound, actions become faster but less accurate) and are capable of generating policies of higher complexity as a function of ITI and set sizes, we demonstrated here that a factorial combination of "vanilla" LBA parameters is insufficient to explain the host of predictions made by policy compression. For example, they were unable to account for perseverative effects. This is because the LBAs did not include a mechanism for "remembering" prior actions, and using action history to bias the tendency of future actions.</p><p>One can imagine incorporating action history by specifying a prior over Q-values that favors previously chosen actions, or by converting past action frequency (in units of probability) to Q-values (in units of reward), but such decisions remain largely ad hoc. By showing the higher-quality LBA 4 model fits, we demonstrate that policy compression can provide both justification and insight for future modeling decisions (e.g., normalized drift rates, adjusting accumulator start points as a function of action history) to better fit empirical behavior.</p><p>Policy compression offers normative insight into a broad set of seemingly disparate task manipulations-ITIs, environmental regularities, and stimulus set sizes. The framework made clear predictions that could readily be factored into the maximization of time-averaged reward (Equation 6).</p><p>Previous normative frameworks are more limited in their explanatory breadth in this regard. For example, there is a normative account for how speed and accuracy should trade off as a function of ITI manipulations in drift diffusion models <ref type="bibr" target="#b19">(Forstmann et al., 2016)</ref>, but this framework does not readily generalize to tasks where there are more than two possible actions. The race model proposed by <ref type="bibr" target="#b57">Tajima et al. (2019)</ref> accommodates multialternative settings, but their framework is limited to settings in which agents must choose one of multiple stimuli displayed simultaneously (i.e., each stimulus provides noisy evidence for its unique corresponding action). In this case, the number of actions is constrained to be the number of simultaneous stimuli. In our tasks where participants observed a single state and could make one of up to 6 actions, the normative insights provided by these past models-which would likely need to assume that a single stimulus elicits Q-value retrieval for all actions simultaneously-become less clear.</p><p>The policy compression framework, in contrast, provides a single explanation for both the speed-accuracy trade-off effects and set-size effects we observed: these arise due to the time cost of decoding policies. Our finding that policy complexity and RT are linked generalizes to task settings where RTs are imposed by response deadlines, rather than fully determined by participants <ref type="bibr" target="#b35">(Lai &amp; Gershman, 2024)</ref>. This is consistent with the idea that actions are generated by time-dependent decoding.</p><p>One of the key features of policy compression is the inclusion of a state-independent term, P * (a), in modulating behavior, which we have argued provides a normative basis for perseveration <ref type="bibr" target="#b23">(Gershman, 2020)</ref>. Importantly, at low policy complexity, the influence of P * (a) is greatest, because this is when policies are highly state-independent. According to the framework, one role of P * (a) is to exploit environmental regularities when they exist and allow agents to maximize reward for no increase in the memory cost (since it does not require encoding state information). In Experiments 2 and 3, we validated this nontrivial prediction. Further, assuming participants estimate P * (a) on a trial-by-trial basis (e.g., via</p><p>an iterative update process <ref type="bibr" target="#b3">(Bari et al., 2024;</ref><ref type="bibr" target="#b35">Lai &amp; Gershman, 2024)</ref>), then there should be a greater tendency to repeat actions at low complexity. Across manipulations of ITIs, environmental regularities, and set sizes, this is what we observed.</p><p>A distinctive hypothesis of the policy compression framework is a linear relationship between RT and policy complexity. This was supported by LME fits across all experiments (see also , for additional evidence). In another experiment featuring 5 different set sizes, and therefore 5 independent measurements of policy complexity and RT for each participant, we again identified a linear relationship, suggesting that our identification of a linear fit was not a consequence of only having 3 datapoints per participant (Supplementary <ref type="figure">Figure 6A-F)</ref>. To accommodate the possibility of nonlinear relationships between RT and policy complexity, we fit regressions testing multiple possible functional forms and found support for the LME model used throughout this manuscript (Supplementary <ref type="figure">Figure 6G-I)</ref>. The success of our chosen LME fits hints at the possibility that a discrete, bit-by-bit action decoding process, as opposed to sequential sampling of noisy evidence from memory, may better explain RTs in value-based decision making paradigms. Distinguishing this action decoding account from sequential sampling accounts appears to be a valuable area of inquiry. Generally speaking, it is unlikely that a Huffman code is exactly how the brain transmits information, since it likely requires unreasonably high precision. In support of this idea, the relationship between set size and RT in working memory tasks flattens for large set sizes <ref type="bibr" target="#b38">(Longstreth, 1988;</ref><ref type="bibr" target="#b48">Seibel, 1963)</ref>. However, given the relatively sparse state space of our experiments, we likely operated within the linear regime, explaining the quality of our RT-policy complexity fits.</p><p>In addition to time costs, our results also demonstrate that humans are sensitive to memory costs.</p><p>Participants reported lower perceived difficulty in task conditions where they used low-complexity policies, across both ITI and set-size manipulations. This suggests that there is an intrinsic costliness to information gain. Consistent with this view, we observed a systematic leftward bias in empirical policy complexity, compared to optimal as predicted by the framework (which does not incorporate such memory costs). These memory costs have previously been suggested to be a function of the amount of information required to update a prior distribution (i.e., via Kullback-Leibler divergence between prior and posterior distributions; Zenon et al., 2019). If we take the prior as P * (a) and the posterior as π(a|s), then this measure of divergence can explain the memory costs we have observed here. Future studies should mathematically formalize this notion of memory costs so that the leftward bias we observed may be seen as a prediction of the theory (with the inclusion of a memory cost), rather than a systematic deviation. How this memory cost is instantiated in biological hardware remains a subject of future research.</p><p>Although the LME fits pointed to a linear RT to policy complexity relationship within participant, the same relationship across participants plateaued at high complexity (Supplementary <ref type="figure">Figure 3C</ref>). This could be due to a tendency for participants with steeper RT-policy complexity relationships to use low-complexity policies, whereas those with flatter relationships preferred to use high-complexity policies.</p><p>Such behavior could be viewed as rational-if high-complexity policies cost too much time, it can make sense to employ low-complexity policies instead. This could be a valuable area of future work, since such relationships may explain a tendency towards impulsive behaviors.</p><p>The information-theoretic approach underlying policy compression resonates with past work on choice and memory retrieval. Policy compression is related to the Hick-Hyman Law, which also assumes constant information inspection rates (i.e., linear decoding time costs), but pertains to the number of allowable actions <ref type="bibr" target="#b29">(Hick, 1952)</ref>. Policy compression provides predictions for behavior even when the total number of actions is fixed. Further, the framework's prediction regarding perseveration also connects with rational analyses of memory, which propose that memory retrieval is optimized to capture statistical regularities in the environment, favoring items that are relevant across contexts <ref type="bibr" target="#b0">(Anderson &amp; Milson, 1989)</ref>. Future research integrating memory and decision-making could provide a more unified understanding of how information shapes human behavior across both domains.</p><p>Another promising avenue for future research is to identify the neural mechanisms that approximate policy compression, which could shed light on the state and action representations used by the brain and clarify why behavior sometimes deviates from this normative framework. In prior applications of rate-distortion theory, we found that tonic dopamine controls the allocation of cognitive resources <ref type="bibr" target="#b2">(Bari &amp; Gershman, 2023;</ref><ref type="bibr" target="#b3">Bari et al., 2024;</ref><ref type="bibr" target="#b41">Mikhael et al., 2021)</ref>. We have begun to explore how phasic midbrain dopamine is modulated by policy complexity <ref type="bibr" target="#b27">(Gershman &amp; Lak, 2024)</ref>. In the domain of visual working memory, we found that rate-distortion theory predicts novel firing rate properties of dorsolateral prefrontal cortex, a region critical for the maintenance of information in working memory <ref type="bibr" target="#b33">(Jakob &amp; Gershman, 2023)</ref>.</p><p>We posit that resource-rational connectionist models may be particularly valuable since they can better approximate biological structural motifs (e.g., recurrent loops) <ref type="bibr" target="#b5">(Binz et al., 2022)</ref>. Understanding these neural processes could significantly advance our knowledge of how the brain manages cognitive resources in complex decision-making settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraints on generality</head><p>We recruited participants from Amazon Mechanical Turk, using English as the instruction language. We acknowledge the possibility of behavioral differences induced by online versus in-person task presentation formats, as well as cross-cultural differences. However, we have no evidence suggesting that such variations would change our results significantly. Future work should be carried out to assess the robustness of our results to population group changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In summary, by considering both time and memory as concurrent resources consumed by decisions, we have developed a normative framework that specifies the relationship between habitual and goal-directed components of behavior, as well as their manifestation in choice and RT profiles. We have shown that the framework can predict speed-accuracy trade-offs and set-size effects, which demonstrates the potential of resource-rational analysis-interpreting decisions as optimizing a balance between reward and resource expenditure-in explaining human decisions. We believe future work that jointly considers multiple cognitive costs promises to have broad explanatory breadth of human behavior.</p><p>Zenon, A., <ref type="bibr">Solopchuk, O., &amp; Pezzulo, G. (2019)</ref>. An information-theoretic perspective on the costs of cognition. Neuropsychologia, 123, 5-18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>This supplementary materials file contains the full set of statistical test results and effect sizes, as well as more comprehensive supplementary figures. <ref type="table">Tables   Supplementary Table 1</ref> Experiment Note. Red color denotes effect directions opposite to the framework's prediction, or CIs that include 0. <ref type="figure" target="#fig_0">Supplementary Figure 1</ref> Trial-averaged reward as a function of trials within each run Note. Mean±SEM errorbars are scaled for within-participant visualization <ref type="bibr" target="#b13">(Cousineau et al., 2005)</ref>. Note. For each Experiment, we divided participants into a "low complexity" and "high complexity" subgroup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figure 2</head><p>Row 1: Mean±SEM of policy complexity and average RT, for Experiment 1 participants in the low and high-complexity subgroups. Row 2: Experiment 2 results. Row 3: Experiment 3 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figure 3</head><p>Reward-complexity relationships and LME model fits for all experiments Note. Optimal policy complexity is defined as the policy complexity level that maximizes time-averaged reward, predicted by the LME for each participant in each condition. Panels correspond to each experiment, and color denotes different conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figure 5</head><p>Model comparison for LBA models, using the Bayesian information criterion (BIC) Note. Panels correspond to BIC of each experiment, and summed BIC across experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The policy as a communication channel. A state distribution P (s) generates states s that are encoded into memory via an encoder, e(s), yielding a codeword c. The codeword is then mapped onto an action a according to P (a|c). Together, encoding and action selection produce the policy π(a|s) that maps states to actions. (B) The optimal policy includes a state-dependent term, Q(s, a), and a state-independent term, log P (a). The log P (a) term biases choices towards actions that are frequently chosen across all states. The β parameter determines the relative contribution of Q(s, a) and log P (a), controlling the state-dependence of the policy. We highlight distributions for an example state. (C) A limit on the channel capacity results in a trade-off between reward and compression. The β parameter increases monotonically with policy complexity. We highlight two example optimal policies at different policy complexity levels. The optimal policies trace out the reward-complexity frontier, which delimits achievable performance for a givenpolicy complexity. (D) Reward-complexity frontier for Experiment 1. (E) Proposed linear relationship between RT and policy complexity. (F) For Experiment 1, time-averaged reward as a function of policy complexity for each ITI under the linear RT-to-policy complexity relationship in (E); the policy complexity that maximizes time-averaged reward for each condition is highlighted (vertical lines). (G) Reward-complexity frontiers for Experiment 3. (H) For Experiment 3, time-averaged reward as a function of policy complexity for each set-size condition under the linear RT-to-policy complexity relationship in (E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The four possible states (images) and the corresponding optimal actions (key presses). The mapping between images and keys was randomized across participants. (B) Experiment 1 reward structure Q(s, a). The optimal action for each state is indicated by a green border. (C) On every trial, the participant observes an image (state) and responds by pressing a key (action). Then, reward feedback is provided as a green border around the image if the action was rewarded or a gray border if action was not rewarded. After the feedback, a fixation cross is displayed throughout the intertrial interval (ITI), before the next trial starts. Participants are able to track remaining time (red bar) and cumulative reward (green bar)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>complexity bias for each ITI condition: ITI = 0s, z = −2.48, p = 0.00665, Cliff's δ = −0.468, 95% CI [−0.630, −0.307]; ITI = 0.5s, z = −2.14, p = 0.0160, Cliff's δ = −0.401, 95% CI [−0.568, −0.233]; ITI = 2s, . reward (/s) G Note. (A) Trial-averaged reward of participants across ITI conditions (color), and the theoretical upper bound (reward-complexity frontier; black) at each policy complexity level. Some data points lie above the optimal frontier due to the stochastic nature of reward delivery. (B-G) Mean±SEM of participant policy complexity (B), average RT (C), action stochasticity (D), perseveration (E), trial-averaged reward (F), and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Experiment 1 linear mixed-effects modeling results and sensitivity to memory costs A) Mean±SEM of perceived difficulty rankings (1 denotes easiest block; 3 denotes hardest) for each ITI condition. (B) Model-predicted RT and empirical RT, for each participant in each ITI condition. (C)Model-predicted linear relationship between average RT and policy complexity for each participant. (D-F)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>estimated effect size (Cohen's d = 0.245) among dependent variables of interest, according to analyses of a separate group of N = 50 pilot participants (data excluded from final analysis). All analyses were preregistered at https://aspredicted.org/blind.php?x=VF2_NH6. The inclusion criterion was identical to Experiment 1. A total of 198 participants (112 women, 82 men, 4 prefer not to say) met this inclusion criteria. Participants gave informed consent, and the Harvard University Committee on the Use of Human Subjects approved the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0264), perseveration (t(197) = 7.61, p &lt; 10 −12 ), and time-averaged reward (t(197) = 34.8, p &lt; 10 −85 ) each decreased as a function of ITI. However, the 95% CI for the effect size of action stochasticity included 0 (Cohen's d = 0.123, 95% CI [−0.002, 0.248]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Reward probability for each state-action pair. (B-D) Mean±SEM of participant policy complexity (B), average RT (C), and mean probability of choosing action a 1 in states s 3 and s 4 (D), across ITI conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>2s condition the hardest (t(197) = −12.0, p &lt; 10 −24 ), and this trend held at the single-participant level (t(197) = 10.6, p &lt; 10 −20 , Cohen's d = 0.756, 95% CI [0.597, 0.913]). The LME had significant effects for the intercept (fixed effects 0.214 ± 0.0151, t(592) = 14.2, p &lt; 10 −38</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Trial-averaged reward across set-size conditions (color). The solid line denotes the theoretical upper bound (reward-complexity frontier) for each set size. Note that some datapoints lie above the upper bound due to the stochastic nature of reward delivery. (B-E) Mean±SEM for empirical (black) policy complexity (B), RT (C), perserveration (D),perceived difficulty (E), and the probability of choosing a suboptimal action that is optimal for some other possible state, averaged over all eligible actions (F), across set-size conditions. (G) Difference between each participant's optimal policy complexity (i.e., policy complexity that maximizes time-averaged reward) and empirical policy complexity, for each set-size condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Row 1: Experiment 1 predictions. (A-C) Empirical (black) and LBA-predicted (color) mean±SEM policy complexity (A), RT (B), and perseveration (C) of participants, as a function of ITI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Row 1: Experiment 1 results. (A) Reward structure. (B) Reward-complexity frontier, overlaid with empirical policy complexity and trial-averaged reward (colors denote task condition). (C) Empirical average RT to policy complexity relationship. (D) LME model-predicted RT and empirical RT, for each participant in each condition. (E) LME model-fitted linear relationship between RT and policy complexity for each participant. (F) Histogram of policy complexity difference between a participant's LME-predicted optimal policy complexity (defined as maximizing time-averaged reward) and that participant's empirical policy complexity. Panels A, B, D, E, F are replicated from the main Figures. Row 2: Experiment 2 results. Panels A and F are replicated from the main Figures. Row 3: Experiment 3 results. Panels B and F are replicated from the main figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Note. Red colors denotes statistically nonsignificant tests under the p &lt; 0.05 cutoff.Note. Red color denotes effect directions opposite to the framework's prediction, or CIs that include 0.Experiment 2 statistical test p-values ITI = 0s vs 0.5s ITI = 0.5s vs 2s ITI = 0s vs 2s Note. Red colors denotes statistically nonsignificant tests under the p &lt; 0.05 cutoff. Note. Red color denotes effect directions opposite to the framework's prediction, or CIs that include 0. Note. Red colors denotes statistically nonsignificant tests under the p &lt; 0.05 cutoff. Experiment 3 Cohen's d effect sizes and their 95% confidence intervals</figDesc><table><row><cell>Supplementary Table 2 Supplementary Table 3 Supplementary Table 4 Supplementary Table 5 Supplementary Table 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Experiment 1 Cohen's d effect sizes and their 95% confidence intervals Experiment 2 Cohen's d effect sizes and their 95% confidence intervals Experiment 3 statistical test p-values</cell></row><row><cell></cell><cell cols="3">ITI = 0s vs 0.5s ITI = 0s vs 0.5s Set size 2 vs 4 Set size 4 vs 6 Set size 2 vs 6 ITI = 0.5s vs 2s ITI = 0.5s vs 2s Set size 2 vs 4 Set size 4 vs 6</cell><cell>ITI = 0s vs 2s ITI = 0s vs 2s Set size 2 vs 6</cell></row><row><cell>Policy complexity (bits) Policy complexity (bits) Policy complexity (bits) Policy complexity (bits) Policy complexity (bits)</cell><cell cols="4">−0.244 [−0.377, −0.117] −0.202 [−0.311, −0.0979] −0.452 [−0.614, −0.303] 1.99 × 10 −7 1.11 × 10 −9 7.78 × 10 −19 −0.190 [−0.265, −0.117] −0.288 [−0.385, −0.195] −0.497 [−0.613, −0.388] 6.64 × 10 −11 0.00552 4.66 × 10 −18 −0.757 [−1.00, −0.531] −0.253 [−0.456, −0.0582] −1.05 [−1.31, −0.813]</cell></row><row><cell>Average RT (s) Average RT (s) Average RT (s) Average RT (s) Average RT (s)</cell><cell cols="4">−0.302 [−0.425, −0.189] −0.238 [−0.361, −0.122] −0.550 [−0.726, −0.391] 9.43 × 10 −8 1.09 × 10 −9 7.22 × 10 −14 −0.219 [−0.303, −0.137] −0.444 [−0.594, −0.300] −0.603 [−0.769, −0.446] 2.02 × 10 −4 0.0991 6.82 × 10 −5 −0.383 [−0.604, −0.172] −0.148 [−0.378, 0.0790] −0.487 [−0.748, −0.239]</cell></row><row><cell cols="2">1 statistical test p-values Action stochasticity 0.129 [−0.0282, 0.291] Action stochasticity 0.0305 Action stochasticity 0.0983 [−0.00472, 0.203] Perserveration 3.54 × 10 −4 Perserveration 0.396 [0.169, 0.635]</cell><cell cols="2">0.268 [0.122, 0.422] 0.374 0.0264 0.0190 [−0.0982, 0.136] 0.580 6.14 × 10 −5 −0.0202 [−0.220, 0.179]</cell><cell>0.402 [0.216, 0.620] 0.123 [−0.00168, 0.248] 0.390 [0.193, 0.598]</cell></row><row><cell>(H(A|S); bits) (H(A|S); bits) (H(A|S); bits) (P(repeat previous action)) (P(repeat previous action))</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">ITI = 0s vs 0.5s ITI = 0.5s vs 2s ITI = 0s vs 2s</cell></row><row><cell>Policy complexity (bits) Perserveration Perserveration Perserveration Time-averaged reward Time-averaged reward</cell><cell>8.92 × 10 −5 0.337 [0.172, 0.512] 1.23 × 10 −4 0.160 [0.0750, 0.248] 1.22 × 10 −6 0.606 [0.358, 0.871]</cell><cell cols="3">8.22 × 10 −5 0.0842 [−0.0839, 0.255] 2.82 × 10 −9 3.74 × 10 −9 5.43 × 10 −13 0.285 [0.190, 0.385] 0.456 [0.332, 0.586] 0.406 [0.205, 0.620] 0.0585 3.92 × 10 −7 0.176 [−0.0454, 0.402] 0.754 [0.460, 1.07]</cell></row><row><cell>Average RT (s) (P(repeat previous action)) (P(repeat previous action)) (P(repeat previous action)) (reward/s) (reward/s)</cell><cell>1.49 × 10 −7</cell><cell>3.64 × 10 −5</cell><cell cols="2">2.15 × 10 −11</cell></row><row><cell>Action stochasticity (H(A|S); bits) Time-averaged reward Time-averaged reward Time-averaged reward Perceived difficulty Perceived difficulty (reward/s) (reward/s) (reward/s) P(suboptimal action P(suboptimal action</cell><cell cols="4">0.0528 2.34 [1.96, 2.79] 3.70 × 10 −65 2.19 [1.92, 2.47] 4.39 × 10 −8 −0.903 [−1.25, −0.579] 3.66 × 10 −6 1.78 × 10 −4 3.18 [2.72, 3.73] 1.15 × 10 −100 1.77 × 10 −86 3.68 [3.30, 4.11] 1.58 × 10 −11 −0.778 [−1.14, −0.443] −1.413 [−1.86, −1.01] 3.71 [3.16, 4.37] 3.46 [3.08, 3.87] 1.48 × 10 −5 0.0142 0.123 0.00275 0.290 [0.0303, 0.559] 0.154 [−0.108, 0.420] 0.387 [0.115, 0.671]</cell></row><row><cell>Perceived difficulty Perceived difficulty Perceived difficulty optimal for other states) optimal for other states)</cell><cell>−0.599 [−0.943, −0.273] 1.17 × 10 −12 −0.880 [−1.13, −0.638]</cell><cell cols="3">−0.673 [−1.00, −0.367] 1.02 × 10 −20 1.55 × 10 −25 −1.00 [−1.44, −0.599] −1.10 [−1.35, −0.873] −1.62 [−1.95, −1.32]</cell></row><row><cell>Perserveration (P(repeat previous action)) Policy complexity (bits), P (a 1 |s 3 or s 4 ) P (a 1 |s 3 or s 4 ) Policy complexity (bits), Policy complexity (bits),</cell><cell>3.66 × 10 −5 −0.203 [−0.268, −0.148] 0.331 0.0242 [−0.0850, 0.134] 1.15 × 10 −6 −0.783 [−1.15, −0.456]</cell><cell cols="3">0.161 −0.267 [−0.573, 0.0258] 4.56 × 10 −5 −0.549 [−0.877, −0.249] 0.0478 0.0305 0.0961 [−0.0172, 0.211] 0.124 [−0.00600, 0.256] 0.476 1.09 × 10 −10 −0.00823 [−0.286, 0.269] −1.23 [−1.65, −0.860]</cell></row><row><cell>Time-averaged reward (reward/s) low complexity subgroup Policy complexity (bits), low complexity subgroup low complexity subgroup Policy complexity (bits), Policy complexity (bits), low complexity subgroup low complexity subgroup Policy complexity (bits), Policy complexity (bits),</cell><cell cols="4">7.86 × 10 −35 3.17 × 10 −13 −0.701 [−0.901, −0.514] −0.423 [−0.715, −0.164] −0.415 [−0.682, −0.178] 2.09 × 10 −49 6.47 × 10 −21 1.13 × 10 −26 −1.04 [−1.28, −0.829] −1.28 [−1.53, −1.45] 5.15 × 10 −45 −0.851 [−1.21, −0.549] 6.71 × 10 −7 1.19 × 10 −4 2.71 × 10 −10 −0.731 [−1.07, −0.445] −0.581 [−0.928, −0.278] −1.20 [−1.65, −0.834]</cell></row><row><cell>Perceived difficulty high complexity subgroup Policy complexity (bits), Policy complexity (bits), high complexity subgroup high complexity subgroup</cell><cell>1.77 × 10 −4 4.18 × 10 −4 −0.577 [−0.960, −0.245]</cell><cell cols="3">1.10 × 10 −5 0.763 0.135 [−0.250, 0.532] 8.36 × 10 −7 0.0150 −0.414 [−0.826, −0.0385]</cell></row><row><cell>Policy complexity (bits), low complexity subgroup low complexity subgroup Average RT (s), Average RT (s), low complexity subgroup low complexity subgroup Average RT (s), high complexity subgroup high complexity subgroup Average RT (s), Average RT (s),</cell><cell cols="4">1.49 × 10 −12 1.17 × 10 −7 −0.365 [−0.508, −0.228] −0.575 [−0.775, −0.386] −0.750 [−0.968, −0.544] 0.0359 2.21 × 10 −9 1.08 × 10 −12 2.05 × 10 −4 −0.398 [−0.580, −0.237] −0.279 [−0.466, −0.106] −0.690 [−0.947, −0.467] 0.105 0.918 0.461 −0.175 [−0.461, 0.103] 0.164 [−0.0695, 0.4046] −0.0146 [−0.321, 0.291]</cell></row><row><cell>Policy complexity (bits), high complexity subgroup high complexity subgroup Average RT (s), Average RT (s), high complexity subgroup high complexity subgroup Average RT (s), low complexity subgroup low complexity subgroup Average RT (s), Average RT (s),</cell><cell cols="4">8.22 × 10 −4 0.0376 −0.242 [−0.534, 0.0281] −0.334 [−0.575, −0.117] −0.322 [−0.596, −0.0712] −0.664 [−1.04, −0.339] 3.76 × 10 −4 0.0671 0.00762 −0.117 [−0.283, 0.0391] −0.337 [−0.639, −0.0652] 5.22 × 10 −8 2.27 × 10 −4 0.0334 6.41 × 10 −6 −0.718 [−1.17, −0.325] −0.462 [−0.997, 0.0372] −1.15 [−1.74, −0.650]</cell></row><row><cell>high complexity subgroup high complexity subgroup</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average RT (s),</cell><cell>1.28 × 10 −6</cell><cell>8.78 × 10 −4</cell><cell cols="2">2.13 × 10 −9</cell></row><row><cell>low complexity subgroup</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average RT (s),</cell><cell>0.00140</cell><cell>0.00604</cell><cell cols="2">4.74 × 10 −5</cell></row><row><cell>high complexity subgroup</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">λ(s) terms ensure proper normalization: a π(a|s) = 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Jan Drugowitsch for helpful discussions. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figure 6</head><p>Experiment 3 pilot behavioral and modeling results. the LME model used throughout the main text; "intercept" contains only fixed and random effects on the intercept; "quad" and "cubic" add second and third-order polynomial terms of policy complexity as both fixed and random effects; "logx" and "logy" apply a log transformation to policy complexity and RT respectively, before fitting the LME models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human memory: An adaptive perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">703</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Acquisition of decision making criteria: Reward rate ultimately beats accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Balci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="640" to="657" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Undermatching is a consequence of policy compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="457" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Computationally-informed insights into anhedonia and treatment by κ-opioid receptor antagonism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Krystal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pizzagalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
			<biblScope unit="page" from="2024" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Resource-rational decision making. Current Opinion in Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Heuristics from bounded meta-learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Endres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1042</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do humans produce the speed-accuracy trade-off that maximizes reward rate?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="863" to="891" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertainty and computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bossaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Murawski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<date type="published" when="1766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The simplest complete model of choice response time: Linear ballistic accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cognitive and neural bases of multi-attribute, multi-alternative, value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal metacognitive control of memory recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The tortoise and the hare: Interactions between reinforcement learning and working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1422" to="1432" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How much of reinforcement learning is working memory, not reinforcement learning? a behavioral, computational, and neurogenetic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1024" to="1035" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Confidence intervals in within-subject designs: A simpler solution to loftus and masson&apos;s method. Tutorials in quantitative methods for psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cousineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="42" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Drawing conclusions from choice response time models: A tutorial using the linear ballistic accumulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The overconstraint of response time models: Rethinking the scaling problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1129" to="1135" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tuning the speed-accuracy trade-off to maximize reward rate in multisensory decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Deangelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Angelaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<editor>T. Behrens</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>eLife, 4.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computationally rational reinforcement learning: Modeling the influence of policy and representation complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Cognitive Modelling</title>
		<meeting>the 19th International Conference on Cognitive Modelling</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A reinforcement learning diffusion decision model for value-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fontanesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Spektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rieskamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1099" to="1121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequential sampling models in cognitive neuroscience: Advantages, applications, and extensions. Annual review of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="641" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The physiology of willpower: Linking blood glucose to self-control. Personality and social psychology review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Gailliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Baumeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="303" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A study of the relation of accuracy to speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Garrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1922" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Columbia university</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bounded rationality, abstraction, and hierarchical decision-making: An information-theoretic optimality principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Leibfried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grau-Moya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Robotics and AI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Origin of perseveration in the trade-off between reward and complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rationally inattentive intertemporal choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3365</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mental control of uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="475" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The reward-complexity trade-off in schizophrenia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Psychiatry</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lak</surname></persName>
		</author>
		<title level="m">Policy complexity suppresses dopamine responses. bioRxiv</title>
		<imprint>
			<date type="published" when="2024" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The speed-accuracy tradeoff: History, physiology, methodology, and behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Heitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">86875</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the rate of gain of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distribution of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stimulus information as a determinant of reaction time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rate-distortion theory of neural coding and its implications for working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">79450</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy compression: An information bottleneck in action selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="195" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human decision making balances reward maximization and policy compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1371/journal.pcbi.1012057</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1012057" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hick&apos;s law: Its limit is 3 bits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Longstreth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Psychonomic Society</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="10" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rational inattention: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maćkowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Matějka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiederholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Literature</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="226" to="273" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mcdougle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="20" to="39" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rational inattention and tonic dopamine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Mikhael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1008659</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mutual benefits: Combining reinforcement learning with sequential sampling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miletić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">107261</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The magic number seven plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parallel processing capability versus efficiency of representation in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ozcimder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The opportunity cost of time modulates cognitive effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The drift diffusion model as the choice rule in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1234" to="1251" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">When natural selection should optimize speed-accuracy trade-offs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">80741</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Discrimination reaction time for a 1,023-alternative task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">215</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Toward a rational and mechanistic account of mental effort. Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="99" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reward rate optimization in two-alternative decision making: Empirical tests of theoretical predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1865</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The cost of misremembering: Inferring the loss function in visual working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="2" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rate-distortion theory and human perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="181" to="198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An ideal observer analysis of visual working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">807</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Optimal policy for multi-alternative decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1503" to="1511" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimal policy for value-based decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12400</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Accumulating advantages: A new conceptualization of rapid multiple choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Ravenzwaaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">One and done? optimal decisions from very few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="637" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Speed-accuracy tradeoff and information processing dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Wickelgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="85" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Accuracy of voluntary movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Woodworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Psychological Review: Monograph Supplements</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Chunking as a rational solution to the speed-accuracy trade-off in a serial reaction time task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Éltető</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7680</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
