<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Imperative of Interpretable Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
							<email>stoyanovich@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering &amp; Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<addrLine>370 Jay Street, 11th Floor Brooklyn</addrLine>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
							<email>jay.vanbavel@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Associate Professor of Psychology &amp; Neural Sciences</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><forename type="middle">V</forename><surname>West</surname></persName>
							<email>tessa.west@nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Associate Professor of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Imperative of Interpretable Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The Imperative of Interpretable Machines We are in the midst of a global trend to regulate the use of algorithms, AI, and Automated Decision Systems. As reported by the One Hundred Year Study on Artificial Intelligence [1]: &quot;AI technologies already pervade our lives. As they become a central force in society, the field is shifting from simply building systems that are intelligent to building intelligent systems that are human-aware and trustworthy.&quot; Major cities, states, and national governments are establishing task forces, passing laws, and issuing guidelines about responsible development and use of technology, often starting with its use in government itself, where there is, at least in theory, less friction between organizational goals and societal values.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the US, New York City has made a public commitment to opening the black box of the government's use of technology: In 2018, an Automated Decision Systems (ADS) Task Force was convened, the first such in the nation, and charged with providing recommendations to New York City's government agencies for how to become transparent and accountable in their use of ADS. In a 2019 report, the Task Force recommended using ADS where they are beneficial, reduce potential harm, and promote fairness, equity, accountability, and transparency <ref type="bibr" target="#b1">[2]</ref>. Can these principles become policy in the face of the apparent lack of trust in the government's ability to manage AI in the interest of the public? We argue that overcoming this mistrust hinges on our ability to engage in substantive multi-stakeholder conversations around ADS, bringing with it the imperative of interpretability-allowing humans to understand and, if necessary, contest the computational process and its outcomes.</p><p>Remarkably little is known about how humans perceive and evaluate algorithms and their outputs, what makes a human trust or mistrust an algorithm <ref type="bibr" target="#b2">[3]</ref>, and how we can empower humans to exercise agency-to adopt or challenge an algorithmic decision. Consider, for example, scoring and ranking-data-driven algorithms that prioritize entities such as individuals, schools, or products and services. These algorithms may be used to determine credit worthiness, and desirability for college admissions or employment. Scoring and ranking are as ubiquitous and powerful as they are opaque. Despite their importance, members of the public often know little about why one person is ranked higher than another by a resume screening or a credit scoring tool, how the ranking process is designed, and whether its results can be trusted.</p><p>As an interdisciplinary team of scientists in computer science and social psychology, we propose a framework that forms connections between interpretability and trust, and develops actionable explanations for a diversity of stakeholders, recognizing their unique perspectives and needs. We focus on three questions about making machines interpretable: (1) What are we explaining? <ref type="bibr" target="#b1">(2)</ref> To whom are we explaining and for what purpose? And (3) how do we know that an explanation is effective? By asking-and charting the path towards answering-these questions, we can promote greater trust in algorithms, and improve fairness and efficiency of algorithm-assisted decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What are we explaining?</head><p>Existing legal and regulatory frameworks, such as the US Fair Credit Reporting Act and the EU General Data Protection Regulation, differentiate between two kinds of explanations. The first concerns the outcome: What are the results for an individual, a demographic group, or the population as a whole? The second concerns the logic behind the decision-making process: what features help an individual or group get a higher score, or, more generally, what are the rules by which the score is computed? Selbst and Barocas <ref type="bibr" target="#b3">[4]</ref> argue for an additional kind of an explanation that considers the justification: Why are the rules what they are? Much has been written about explaining outcomes <ref type="bibr" target="#b4">[5]</ref>, so we focus on explaining and justifying the process.</p><p>Procedural justice aims to ensure that algorithms are perceived as fair and legitimate. Research demonstrates that, as long as a process is seen as fair, people will accept outcomes that may not benefit them. This finding is supported in numerous domains, including hiring and employment, legal dispute resolution, and citizen reactions to police and political leaders <ref type="bibr" target="#b5">[6]</ref>, and it remains relevant when decisions are made with the assistance of algorithms. A recent lawsuit against Harvard University, filed by "Students for Fair Admissions", stems, at least in part, from a lack of transparency and sense of procedural justice among some applicant groups. Similar allegations of injustice were leveled against the New York City Department of Education when only seven black students (out of 895 spots) had been admitted into New York's most selective high school <ref type="bibr" target="#b6">[7]</ref>. To increase feelings of procedural justice, interests of different stakeholders should be taken into account when building and evaluating algorithms, prior to observing any outcomes <ref type="bibr" target="#b7">[8]</ref>.</p><p>Data transparency is a dimension of explainability unique to algorithm-assisted-rather than purely human-decision making. In applications involving predictive analytics, data is used to customize generic algorithms for specific situations: algorithms are trained using data. The same algorithm may exhibit radically different behavior-make different predictions and different kinds of mistakes-when trained on two different data sets. Without access to the training data, it is impossible to know how an algorithm will behave. For example, predictive policing algorithms often reproduce the systemic historical bias toward poor or black neighborhoods because of their reliance on historical policing data. This can amplify historical patterns of discrimination, rather than provide insight into crime patterns <ref type="bibr" target="#b8">[9]</ref>. Transparency of the algorithm alone is insufficient to understand and counteract these particular errors.</p><p>The requirement for data transparency is in keeping with the justification dimension of interpretability: If the rules derived by the algorithm are due to the data on which it was trained, then justifying these rules must entail explaining the rationale behind the data selection and collection process. Why was this particular dataset used, or not used? It is also important to make statistical properties of the data available and interpretable, along with the methodology that was used to produce it, substantiating the fitness for use of the data for the task at hand <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To whom are we explaining and why?</head><p>Different stakeholder groups take on distinct roles in algorithm-assisted decision making, and so have different interpretability requirements. While much important work focuses on interpretability for computing professionals <ref type="bibr" target="#b4">[5]</ref>-those who design, develop, and test technical solutions-less is known about the interpretability needs of others. These include members of the public who are affected by algorithmic decisions: doctors, judges, and college admissions officers who make-and take responsibility for-these decisions; and auditors, policy makers, and regulators who assess the systems' legal compliance and alignment with societal norms.</p><p>Social identity is key to understanding the values, beliefs, and interpretations of the world held by members of a group <ref type="bibr" target="#b10">[11]</ref>. People tend to trust in-group members more than out-group members, and if their group is not represented during decision-making, they will not trust the system to make judgments that are in their best interest <ref type="bibr" target="#b11">[12]</ref>. Numerous identities may play a critical role in how algorithms are evaluated and whether the results they produced should be trusted. One recent case that highlights the contentious role of group identity is the effect of political ideology on search engines and news feeds. Liberal and conservative politicians both demand that technology platforms like Facebook become "neutral" <ref type="bibr" target="#b12">[13]</ref>, and have repeatedly criticized Google for embedding bias in its algorithms <ref type="bibr" target="#b13">[14]</ref>. In this case, the identity of the programmers can overshadow more central features of the algorithm, such as the accuracy of the news source.</p><p>Moral cognition is concerned with how people determine whether an action or outcome is morally right or wrong. Moral cognition is influenced by intuitions, and therefore is often inconsistent with reasoning <ref type="bibr" target="#b14">[15]</ref>. A large body of evidence suggests that people evaluate decisions made by humans differently from those made by computers (although this may be changing, see <ref type="bibr" target="#b15">[16]</ref>); as such, they may be uncomfortable delegating certain types of decisions to algorithms. Consider the case of driverless vehicles. Even though people approve of autonomous vehicles that might sacrifice passengers to save a larger number of non-passengers, they would prefer not to ride in such vehicles <ref type="bibr" target="#b16">[17]</ref>. Thus, utilitarian algorithms designed to minimize net harm may ironically increase harm by making objectively safer technology aversive to consumers. Failing to understand how people evaluate the moral programming of algorithms could thus unwittingly cause harm to large groups of people. The problem is compounded by the fact that moral preferences for driverless vehicles vary dramatically across cultures <ref type="bibr" target="#b17">[18]</ref>. Solving these sorts of problems will require an understanding of social dilemmas, since self-interest might come directly in conflict with collective interest <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Are explanations effective?</head><p>A promising approach for interpretability is to develop labels for data and models analogous to nutritional labels used in the food industry, where simple, standard labels convey information about the ingredients and nutritional value. Nutritional labels are designed to inform specific decisions rather than provide exhaustive information. Proposals for hand-designed labels for data, models, or both have been suggested in the literature (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>). We advocate instead for generating such labels automatically or semi-automatically as a part of the computational process itself, embodying the paradigm of interpretability-by-design <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>We expect that data and model labels will inform different design choices by computer scientists and data scientists who implement algorithms and deploy them in complex multi-step decision-making processes. These processes typically use a combination of proprietary and third-party algorithms that may encode hidden assumptions, and rely on datasets that are often repurposed-used outside of the original context for which they were intended. Labels will help determine the "fitness of use" of a given model or dataset, and assess the methodology that was used to produce it.</p><p>Information disclosure does not always have the intended effect. For instance, nutritional labeling for food and calorie labeling are in broad use today. However, the information conveyed in them does not always affect calorie consumption <ref type="bibr" target="#b22">[23]</ref>. A plausible explanation is that, "When comparing a $3 Big Mac at 540 calories with a similarly priced chicken sandwich with 360 calories, the financially strapped consumer [...] may well conclude that the Big Mac is a better deal in terms of calories per dollar" <ref type="bibr" target="#b22">[23]</ref>. It is therefore important to understand, with the help of experimental studies, what kinds of disclosure are effective, and for what purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The integration of expertise from behavioral science and computer science is essential to making algorithmic systems interpretable by a wide range of stakeholders, allowing people to exercise agency and ultimately building trust. Individuals and groups who distrust algorithms may be less likely to harness the potential benefits of new technology, and, in this sense, interpretability intimately relates to equity. Education is an integral part of making explanations effective. Recent studies found that individuals who are more familiar with AI fear it less, and are more optimistic about its potential societal impacts (e.g., <ref type="bibr" target="#b23">[24]</ref>). We share this cautious optimism, but predicate it on helping different stakeholders move beyond the extremes of unbounded techno optimism and techno criticism, and into a nuanced and productive conversation about the role of technology in society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research questions</head><p>What are we explaining? Do people trust algorithms more or less than they would trust an individual making the same decisions? What are the perceived trade-offs between data disclosure and the privacy of individuals whose data is being analyzed, in the context of interpretability? Which potential sources of bias are most likely to trigger distrust in algorithms? What is the relationship between the perceptions about a dataset's fitness for use and the overall trust in the algorithmic system?</p><p>To whom are we explaining and why? How do group identities shape perceptions about algorithms? Do people lose trust in algorithmic decisions when they learn that outcomes produce disparities? Is this only the case when these disparities harm their in-group? Are people more likely to see algorithms as biased if members of their own group were not involved in algorithm construction? What kinds of transparency will promote trust, and when will transparency decrease trust? Do people trust the moral cognition embedded within algorithms? Does this apply to some domains (e.g., pragmatic decisions, like clothes shopping) more than others (e.g., moral domains, like criminal justice sentencing)? Are certain decisions taboo to delegate to algorithms (e.g., religious advice)?</p><p>Are explanations effective? Do people understand the label? What kinds of explanations allow individuals to exercise agency: make informed decisions, modify their behavior in light of the information, or challenge the results of the algorithmic process? Does the nutrition label help create trust? Can the creation of nutrition labels lead programs to alter the algorithm?</p><p>The authors declare no competing interests.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One Hundred Year Study on Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Report of the 2015-2016 Study Panel</title>
		<meeting><address><addrLine>Stanford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">New York City Automated Decision Systems Task Force</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NYC.gov</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rovatsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="497" to="498" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fordham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1085" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Oxford Handbook of Justice in the Workplace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bobocel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shapiro</surname></persName>
		</author>
		<ptr target="http://www.nytimes.com/2019/03/18/nyregion/black-students-nyc-high-schools.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Significance</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoyanovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alfano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huijts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Trust and Philosophy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cbnc</surname></persName>
		</author>
		<ptr target="https://www.cnbc.com/2019/08/20/republican-report-of-facebook-anti-conservative-bias-suggests-changes.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Guardian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
		<ptr target="https://www.theguardian.com/technology/2018/dec/04/google-facebook-anti-conservative-bias-claims" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Science</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="998" to="1002" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alterovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonnefon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="1573" to="1576" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Awad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">563</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A M</forename><surname>Van Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joireman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organ. Behav. Hum. Decis. Process</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chmielinski</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.03677" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM FAT*</title>
		<imprint>
			<biblScope unit="page" from="220" to="229" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1773" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Clin. Nutr</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="679" to="680" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dafoe</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: American Attitudes and Trends</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Center for the Governance of AI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
