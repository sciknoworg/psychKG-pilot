<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using artificial neural networks to reveal the human confidence computation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medha</forename><surname>Shekhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshad</forename><surname>Rafiei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dobromir</forename><surname>Rahnev</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Medha Shekhar School of Psychology Georgia Institute of Technology 654 Cherry Str NW Atlanta</orgName>
								<address>
									<postCode>30332</postCode>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using artificial neural networks to reveal the human confidence computation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial neural networks</term>
					<term>confidence</term>
					<term>perceptual decision making</term>
					<term>visual metacognition</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Humans can evaluate the accuracy of their decisions by providing confidence judgements. Traditional cognitive models have tried to capture the mechanisms underlying this process but remain mostly limited to two-choice tasks with simple stimuli. How confidence is given for naturalistic stimuli with multiple choice alternatives is not well understood. We recently developed a convolutional neural network (CNN) model-RTNet-that exhibits several important signatures of human decision making and outperforms existing alternatives in predicting human responses. Here, we use RTNet&apos;s image computability to test four different confidence strategies on a task involving digit discrimination with eight alternatives. Specifically, we tested confidence strategies that consider the entire evidence distribution across choices (Softmax and Entropy), the difference between the winning choice and the second-best choice (Top2Diff), or only the evidence for the winning choice (Positive Evidence). Across 60 subjects, the Top2Diff model provided the best quantitative and qualitative fits to the data and the best predictions of humans&apos; image-by-image confidence ratings. These results support the notion that human confidence is based on the difference of evidence between the top two choices and demonstrate that Softmaxcurrently the standard way of deriving confidence from CNNs-is inadequate for modelling human confidence computations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As humans, we can introspect on our own decisions and provide confidence estimates that reflect the likelihood of these decisions being correct <ref type="bibr" target="#b24">(Koriat, 2006;</ref><ref type="bibr" target="#b27">Mamassian, 2016;</ref><ref type="bibr" target="#b31">Metcalfe &amp; Shimamura, 1994)</ref>. However, the computations behind confidence remain debated and thus revealing them is seen as one of the central medium-term goals for the field <ref type="bibr" target="#b38">(Rahnev et al., 2022)</ref>. Popular cognitive modelling frameworks including signal detection theory <ref type="bibr" target="#b4">(Bang et al., 2019;</ref><ref type="bibr" target="#b5">Boundy-Singer et al., 2023;</ref><ref type="bibr" target="#b13">Green, D.M. and Swets, 1966;</ref><ref type="bibr" target="#b44">Rausch et al., 2020;</ref><ref type="bibr" target="#b43">Rausch &amp; Zehetleitner, 2016;</ref><ref type="bibr" target="#b46">Shekhar &amp; Rahnev, 2018</ref><ref type="bibr" target="#b48">, 2024a</ref>, sequential sampling <ref type="bibr" target="#b35">(Pleskac &amp; Busemeyer, 2010;</ref><ref type="bibr" target="#b40">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b41">Ratcliff &amp; Starns, 2009</ref><ref type="bibr" target="#b42">, 2013</ref> and Bayesian inference <ref type="bibr" target="#b2">(Adler &amp; Ma, 2018b</ref><ref type="bibr" target="#b1">, 2018a</ref><ref type="bibr" target="#b9">Denison et al., 2018;</ref><ref type="bibr" target="#b10">Fleming &amp; Daw, 2017;</ref><ref type="bibr" target="#b26">Li &amp; Ma, 2020)</ref> have all attempted to capture the mechanisms underlying confidence generation and successfully provided insights into these processes. However, there are two major limitations associated with these models. Firstly, cognitive modelling has almost exclusively been applied to tasks involving only two choices <ref type="bibr" target="#b37">(Rahnev, 2020)</ref>. Despite notable attempts <ref type="bibr" target="#b26">(Li &amp; Ma, 2020)</ref>, extending these models to more complex tasks involving multiple choices has proved difficult. For instance, classic SDT is fundamentally limited to 2-choices tasks and Bayesian computations become intractable for a large number of alternatives . Secondly, these models have only been applied to tasks involving simple visual stimuli such as Gabor patches and moving dots since traditional models of decision making are not image computable (i.e. they cannot process images involving complex features).</p><p>Therefore, it is still unclear how confidence is generated for decisions involving naturalistic stimuli and multiple alternatives.</p><p>One promising approach for addressing this question is to use convolutional neural networks (CNNs). The advantage of CNNs is that they are image computable models of vision that are capable of human-level object recognition <ref type="bibr" target="#b6">(Bowers et al., 2023;</ref><ref type="bibr" target="#b55">Wichmann &amp; Geirhos, 2023)</ref>. Further, they can easily be applied to decision making tasks with several alternatives. These features of CNNs can potentially be leveraged to model confidence mechanisms in tasks involving naturalistic images and multiple alternatives. However, although CNNs perform object classification tasks with high levels of accuracy, they cannot yet be considered as good models of human vision as it is unclear whether their decisionmaking mechanisms are similar to humans <ref type="bibr" target="#b6">(Bowers et al., 2023;</ref><ref type="bibr" target="#b55">Wichmann &amp; Geirhos, 2023)</ref>. Particularly, their behavior has not been extensively validated on human data for tasks involving simple psychological manipulations. Therefore, standard CNNs may not yet be suitable for modelling and inferring the processes occurring in humans.</p><p>To address these challenges, we recently developed a dynamic neural network model, called RTNet, which combines the image processing capabilities of CNNs with mechanisms drawn from empirically tested cognitive models <ref type="bibr" target="#b36">(Rafiei et al., 2024)</ref>. Specifically, RTNet has two modules: (1) a CNN with stochastic weights that generates noisy evidence at each time step and (2) an evidence accumulator which integrates this noisy evidence towards a threshold ( <ref type="figure" target="#fig_0">Figure 1A)</ref>. Critically, RTNet reproduces several key signatures of human perceptual decisions and also predicts human RT, choice and confidence for novel, individual images.</p><p>Further, RTNet clearly outperformed other biologically inspired CNN architectures involving processes such as recurrence <ref type="bibr">(BLNet;</ref><ref type="bibr" target="#b50">Spoerer et al., 2020)</ref>, parallel processing (CNet ; <ref type="bibr" target="#b18">Iuzzolino et al., 2021)</ref>, and resource-efficient processing (MSDNet; <ref type="bibr" target="#b17">Huang et al., 2017)</ref>.</p><p>Based on these findings, RTNet can be considered as a promising model of human decision making that can be further applied towards examining confidence mechanisms in humans. The first is a Bayesian neural network whose weights are stochastic such that at each processing step, a unique feedforward CNN is instantiated. As a result of the stochastic weights, the network processes the same image at each time step and generates noisy evidence through the activations of its output layer. The evidence for each choice option is then accumulated by an evidence accumulator module towards a pre-defined threshold. (B) Task. Subjects performed a digit discrimination task where they were presented with a noisy hand-written image of a digit between 1 and 8. The Subjects decided which digit was presented and then reported their confidence on a scale from 1-4. (C) The four experimental conditions. Task difficulty as well as speed-accuracy trade-off instructions were manipulated in a 2x2 factorial design. Task difficulty was adjusted by adding noise to the images, whereas speed-accuracy trade-off was manipulated by instructing subjects to focus on either accuracy or speed.</p><p>Previous studies have proposed a range of different strategies for computing confidence from the available perceptual evidence. For instance, confidence can be computed using all the evidence available for the perceptual decision. A popular example of this strategy is the Softmax transformation, which is currently the standard method for computing confidence in neural networks. Softmax normalizes evidence by accounting for its spread across all choice options <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>. However, it has not been tested in humans using cognitive models. Another confidence strategy that also uses the spread across all choice options is to compute the entropy of the evidence distribution across choices (Entropy strategy; <ref type="bibr" target="#b26">Li &amp; Ma, 2020;</ref><ref type="bibr" target="#b50">Spoerer et al., 2020)</ref>. In contrast to Softmax and Entropy, other proposed strategies only utilize a subset of the available evidence. Specifically, confidence may be computed as the difference in evidence between the two most likely options (Top2Diff strategy; <ref type="bibr" target="#b26">Li &amp; Ma, 2020)</ref>. Alternatively, another popular theory is that confidence only considers the evidence in favor of the chosen response (i.e., the "positive evidence"), neglecting all evidence against the choice (PE strategy; <ref type="bibr" target="#b23">Koizumi et al., 2015;</ref><ref type="bibr" target="#b29">Maniscalco et al., , 2021</ref><ref type="bibr" target="#b33">Odegaard et al., 2018;</ref><ref type="bibr" target="#b34">Peters et al., 2017)</ref>.</p><p>Despite the fact that different confidence strategies have been proposed, there is currently little empirical work to distinguish between them. Perhaps the only attempt to quantitatively compare some of these strategies in the context of a multi-alternative task was made by <ref type="bibr" target="#b26">Li &amp; Ma (2020)</ref>. The authors applied Bayesian modelling to a 3-choice task and showed that human confidence was most likely to be based on the difference in probability between the two most likely choices, rather than on the probability of the choice itself or the entropy associated with distribution of probabilities. However, Li &amp; Ma's task was unique in that, unlike most of the literature, none of the alternatives were objectively correct on any given trial. Therefore, whether these findings would extend to tasks that feature more complex stimuli, higher number of alternatives, and the presence of an objective correct answer remains to be tested.</p><p>Here, we compared human confidence judgments in a complex, 8-choice digit discrimination task ( <ref type="figure" target="#fig_0">Figure 1B)</ref> with confidence ratings generated by RTNet using each of the four confidence strategies described above: Softmax, Entropy, Top2Diff, and PE. The task involved discriminating hand-written MNIST stimuli <ref type="bibr" target="#b8">(Deng, 2012)</ref> and featured difficulty and speed-accuracy trade-off manipulations ( <ref type="figure" target="#fig_0">Figure 1C)</ref>. We found that the Top2Diff strategy emerged as the best model in providing quantitative fits, reproducing qualitative signatures of human confidence, and predicting confidence for individual images. These results suggest that confidence in humans is computed as the difference in evidence between the top two options and establish CNNs as promising models for inferring mechanisms underlying human decisions in naturalistic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compared the ability of four confidence generation strategies -Softmax, Entropy, Top2Diff, and PE -to fit human confidence in an 8-choice digit discrimination task. Each strategy was implemented within our recently-developed neural network model, RTNet <ref type="bibr" target="#b36">(Rafiei et al., 2024)</ref>. In our previous work, we had trained RTNet on the MNIST dataset and then fit the noise and boundary parameters (see Methods) to match human performance for difficulty and speed-accuracy trade-off manipulations. Here, we fit the same pre-trained RTNet model to the human confidence ratings using the activations in the last layer of RTNet. Specifically, for each confidence strategy, we fit a set of three confidence criteria that transform the network's continuous, raw confidence output into 4-point confidence ratings. We fit each confidence strategy to the data of each human subject separately. We then assessed the quality of model fits and the predictions of each strategy for the confidence associated with previously unseen individual images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Top2Diff strategy yields the best fit to human confidence</head><p>We first compared how well the four confidence strategies fit the observed data using AIC scores. Lower AIC scores indicate better fits to the data. AIC comparisons revealed that the Top2Diff strategy significantly outperformed all other strategies by generating the lowest AIC values <ref type="figure">(Figure 2A)</ref> points. These differences correspond to the Top2Diff strategy being, on average, 6.3 x 10 9 times more likely than PE, 4.2 x 10 12 times more likely than Softmax, and 1.0 x 10 37 times more likely than Entropy. In other words, the Top2Diff strategy provided a much better quantitative fit to the human data compared to any of the other three strategies. <ref type="figure">Figure 2</ref>. Comparing the ability of Top2Diff, PE, Softmax, and Entropy to fit human confidence ratings. We examined both the quantitative and qualitative fits of the four confidence strategies (Top2Diff, PE, Softmax, and Entropy). Each strategy was built on the output of RTNet. A) The Top2Diff strategy outperformed all other confidence strategies by at least 45 AIC points. Error bars depict 95% confidence intervals for the difference in AIC scores between Top2Diff and each of the remaining strategies. B) Confidence decreases with task difficulty. All confidence strategies are able to reproduce this qualitative pattern, but the Top2Diff model provided the closest fits to the data. C) There is no significant change in confidence between the speed and accuracy focus conditions. All models incorrectly predict that confidence should be higher in the accuracy-compared to the speed-focus condition, but Top2Diff model again provides the closest fits to the data. D) Confidence increases with task performance for correct trials but decreases with task performance for incorrect trials giving rise to a folded-X pattern. All strategies except Entropy can reproduce this qualitative pattern, but the Top2Diff model provides the closest fits to the data. Error bars depict SEM. SSE, sum of squared errors (smaller values indicate better fits).</p><p>To gain further insight into the performance of the four confidence strategies, we compared how well each strategy can fit different qualitative patterns of confidence.</p><p>Specifically, we examined the effects of the two behavioral manipulations (task difficulty and speed-accuracy trade-off) on confidence. In addition, we examined the folded-X pattern which is popularly regarded as a critical signature of human confidence <ref type="bibr" target="#b15">(Hangya et al., 2016;</ref><ref type="bibr" target="#b45">Sanders et al., 2016)</ref>.</p><p>First, we evaluated the effect of task difficulty manipulation on confidence. For humans, we found that confidence decreased from 3.50 in the easy to 3.14 in the difficult condition (t(59) = 11.768, p &lt; 0.001; <ref type="figure">Figure 2B</ref>). All strategies could replicate this general pattern (Top2Diff: easy = 3.49, difficult = 3.25; PE: easy = 3.39, difficult = 3.28; Softmax: easy = 3.48, difficult = 3.25; Entropy: easy = 3.27, difficult = 3.07). However, the Top2Diff strategy most closely matched the observed magnitude of change in confidence as measured by the average sum of squared errors across subjects (Top2Diff = 0.048; PE = 0.067; Softmax = 0.135; Entropy = 0.275). Further, these errors were significantly lower on average for the Top2Diff model compared to Softmax (t(59) = 2.103, p = 0.040) and Entropy (t(59) = 3.845, p &lt; 0.001) but not PE (t(59) = 1.904, p = 0.062).</p><p>Second, we evaluated the effect of the speed-accuracy trade-off manipulation on confidenceClick or tap here to enter text.. In humans, there was no significant change in confidence from the speed-focus to the accuracy-focus condition (speed-focus = 3.31, accuracy-focus = 3.33; t(59) = 0.612, p = 0.543; <ref type="figure">Figure 2C</ref>). However, all models showed an increase in confidence from the speed-focus to accuracy-focus condition (Top2Diff: 3.16 vs. 3.58; PE: 2.90 vs. 3.77; Softmax: 3.05 vs. 3.68; Entropy: 2.47 vs. 3.87). Once again, computing the average sum of squared errors across subjects showed that the Top2Diff model provided the closest match to human confidence (Top2Diff = 0.152; PE = 0.466; Softmax = 0472; Entropy = 1.420). These errors were significantly lower for the Top2Diff model compared to all other models (PE: t(59) = 9.755, p &lt; 0.001; Softmax: t(59) = 3.006, p = 0.004; Entropy: t(59) = 7.339, p &lt; 0.001).</p><p>Finally, we assessed whether our human subjects and models generated the "folded-X pattern" <ref type="bibr" target="#b15">(Hangya et al., 2016;</ref><ref type="bibr" target="#b45">Sanders et al., 2016)</ref>where, as the task gets easier, confidence for correct trials increases but confidence for error trials decreases. Indeed, human confidence followed this folded-X pattern, such that the easy condition exhibited higher confidence for correct trials (difficult = 3.39, easy = 3.66; t(59) = 9.727, p &lt; 0.001; <ref type="figure">Figure 2D</ref>) but lower confidence for error trials (difficult = 2.74, easy = 2.68; t(59) = -2.095, p = 0.041). All strategies predicted that confidence for correct trials should be higher for the easy compared to the difficult condition (Top2Diff: difficult = 3.46, easy = 3.62; PE: difficult = 3.38, easy = 3.45; Softmax: difficult = 3.43, easy = 3.57; Entropy: difficult = 3.20, easy = 3.33). However, there was more variability in this pattern for error trials, with the Entropy and Softmax strategies failing to predict a clear difference in confidence for error trials (Top2Diff: difficult = 2.95, easy = 2.80; PE: difficult = 3.14, easy = 3.04; Softmax: difficult = 2.99, easy = 2.97; Entropy: difficult = 2.89, easy = 2.91). The Top2Diff model produced the most precise fits to the folded-X pattern observed in humans as measured by the sum of squared errors across subjects (Top2Diff = 0.208; PE = 0.554; Softmax = 0.499; Entropy = 0.699). Again, these errors were significantly lower on average for the Top2Diff model compared to all other models (PE: t(59) = 6.202, p &lt; 0.001; Softmax: t(59) = 3.703, p &lt; 0.001; Entropy: t(59) = 5.918, p &lt; 0.001). Overall, as may be expected from the AIC fits, the Top2Diff model produced the most consistently accurate matches to the observed patterns of human confidence across all three qualitative patterns.</p><p>The Top2Diff strategy yields the best predictions of confidence for individual images Unlike traditional cognitive models which are not image computable, CNNs can generate predictions of confidence at the level of individual images. Therefore, we used this ability of the CNNs to test which confidence strategy generated the best predictions of confidence for novel, individual images. We first compared the observed and predicted distributions of confidence by plotting histograms of average confidence for all 480 unique images. The average human confidence ratings for individual images showed a continuous, unimodal distribution with a peak near the middle of the range ( <ref type="figure" target="#fig_1">Figure 3A)</ref>. Similarly, the Top2Diff strategy also generated a continuous, unimodal distribution that also peaked near the middle of the range. In contrast, the PE, Softmax, and Entropy strategies showed deviations from this pattern. While the PE strategy also showed a unimodal distribution, it predicted a peak for the highest values of the range.</p><p>Most strikingly, Softmax and Entropy both generated strong bimodal distributions. In particular, the Entropy model, generated no confidence values that fell within the middle of the observed range (between 2.98 and 3.68). Overall, the Top2Diff model generated the closest match to the observed distribution of image-by-image average confidence. The distributions of confidence were unimodal and continuous for humans, Top2Diff, and PE. However, Softmax and Entropy generated strong bimodal distributions. (B) We correlated image-by-image predictions of average group confidence with observed average confidence in humans. The Top2Diff model's predictions produced the highest correlations with group confidence in humans. Dots represent the 480 unique images. (C) Comparing the predictions of group confidence between models from panel B. The correlations between model predictions and human confidence were significantly higher for Top2Diff compared to all other models. (D) The models' image-by-image predictions of confidence for individual subjects. The predictions of the Top2Diff model showed significantly higher correlations with individual subject confidence compared to all other strategies. Dots represent individual subjects. Error bars show SEM. *p&lt;0.05; ***p&lt;0.001.</p><p>We next turned to the main question of assessing the models' ability to predict human confidence for individual images. We correlated the image-by-image confidence value predicted by each strategy with the average image-by-image confidence values across the group of human subjects. All strategies yielded correlations that were significantly greater than zero (Top2Diff: r = 0.469; PE: r = 0.341, Softmax: r = 0.199, Entropy: r = 0.161; all p's &lt; 0.001; <ref type="figure" target="#fig_1">Figure 3B</ref>). Critically, the Top2Diff strategy produced significantly higher correlations than all other strategies (PE: z(479) = 2.37, p = 0.018; Softmax: z(479) = 4.75, p &lt; 0.001; Entropy: z(479) = 5.35; p &lt; 0.001, test for comparing r-values; <ref type="figure" target="#fig_1">Figure   3C</ref>). Notably, the Softmax and Entropy models produced substantially lower correlations, likely due to their non-human-like bimodal distribution of average imageby-image confidence. Overall, the Top2Diff strategy generated image-by-image predictions for confidence that provided the best match to image-by-image average human confidence.</p><p>Beyond assessing average confidence across the group, we also examined how well each model could predict image-by-image confidence for individual subjects. We simulated the models separately for each subject using the confidence criteria estimated from model fitting to each individual. Then, we correlated observed and predicted image-by-image confidence for each subject. Again, all confidence strategies yielded correlations that were significantly greater than zero (Top2Diff: r = 0.18, PE: r = .073, Softmax: r = .135, Entropy: r = .064; all p's &lt; 0.001; <ref type="figure" target="#fig_1">Figure 3D</ref>). However, the Top2Diff strategy yielded significantly higher correlations compared to all other strategies (PE: t(59) = 12.95, p &lt; 0.001, Softmax: t(59) = 5.38, p &lt; 0.001, Entropy: t(59) = 12.367, p &lt; 0.001, pairwise t-tests). We also assessed how well individual confidence could be predicted by the rest of the group's confidence -an estimate of the noise ceiling or the upper bound for model performances given inter-subject variability. The Top2Diff strategy's predictions were at 45.1% of the noise ceiling, whereas the other models' predictions were much lower (PE: 18.0%, Softmax: 33.2% and Entropy: 15.8%).</p><p>Together, these findings show that the Top2Diff strategy provides the best image-byimage predictions of confidence for both the group and individual subjects.</p><p>Only the Top2Diff strategy predicts group confidence in humans better than individual subjects Individual subjects can themselves serve as models of group behavior. Here, we tested how well our models can predict group behavior compared to individual subjects. We correlated the image-by-image confidence predictions of models as well as the imageby-image confidence of individual subjects to the corresponding average confidence of the group. The difference between the model-to-group correlations and subject-togroup correlations provide a measure of how well the model predicts the group over individual subjects. We found that only the Top2Diff model was able to significantly predict group confidence better than individuals (Δ = 0.050, t(59) = 2.481, p = 0.016; <ref type="figure" target="#fig_2">Figure 4A</ref>). In fact, all other models performed significantly worse than the individual subjects in predicting group confidence (PE: Δ = -0.215, t(59) = -10.566, p &lt; 0.001; Softmax: Δ = -0.067, t(59) = -2.602, p = 0.012; Entropy: Δ = -0.229, t(59) = -9.302, p &lt; 0.001). Further, the Top2Diff model outperformed 61.7% of individual subjects in predicting the group, whereas the PE, Softmax and Entropy models only performed better than 10%, 41.6%, and 15% of individual subjects, respectively. These findings show that Top2Diff was the only strategy that served as a better model for predicting group confidence compared to individual human subjects. Models' predictions of average group confidence compared to predictions from individual subjects. Only the Top2Diff model was significantly better than individual subjects at predicting the group (better than 37 out of 60 subjects). Dots represent individual subjects. Error bars show SEM. **p&lt;0.01; ***p&lt;0.001. (B) The relationship between a subject's similarity to the group and their similarity to the model. Only for the Top2Diff model, there was a significant relationship between these two quantities, indicating that the model successfully captured the data from subjects who were the most similar to the group data. Dots represent individual subjects.</p><p>Subjects more similar to the group mean are better predicted by the Top2Diff strategy <ref type="figure" target="#fig_2">Figure 4A</ref> revealed substantial variability in how similar subjects were to the group mean. Subjects who were worse in predicting the group mean may have had especially strong biases or simply provided noisy data. If so, good models of human behavior are likely to be better predictors for individuals who are more similar to the group than for individuals who are most different from the group. Indeed, we found that subjects whose confidence had a higher image-by-image correlation with the group mean were also better predicted by the Top2Diff model (r = 0.483, p &lt; 0.001; <ref type="figure" target="#fig_2">Figure 4B</ref>). However, for all other strategies, there was no significant relationship between the subject's similarity to the group and the predictions of each strategy (PE: r = 0.074, p = 0.57; Softmax: r = 0.137, p = 0.30; Entropy: r = 0.034, p = 0.80), suggesting that these models were unable to encapsulate the relationship of individual subjects to the group.</p><p>Other CNN architectures perform substantially worse than RTNet and fail to generate consistent support for any confidence strategy</p><p>Our results demonstrate that the Top2Diff strategy clearly outperforms all other strategies when instantiated in RTNet. However, it is not clear how the four strategies would perform within other CNN architectures. Testing across architectures would enable us to understand how well other CNNs architectures can fit human data relative to RTNet and whether the same confidence strategies are consistently supported across model architectures. For conciseness, we only report three analyses: a) AIC comparisons, b) fits to the folded-X pattern and c) image-by-image predictions of individual subject confidence. These three analyses are representative of the three main aspects of our model assessment because they test models on their ability to 1) quantitatively fit the data, 2) capture qualitative patterns in the data, and 3) predict human responses for individual images. Similar to what was done for RTNet, we used three other CNN architectures -MSDNet <ref type="bibr" target="#b17">(Huang et al., 2017)</ref>, BLNet <ref type="bibr" target="#b50">(Spoerer et al., 2020)</ref> and CNet <ref type="bibr" target="#b18">(Iuzzolino et al., 2021)</ref> -that were previously trained on MNIST and fit to human choice data <ref type="bibr" target="#b36">(Rafiei et al., 2024)</ref>. We then fit subject-specific confidence criteria to these models to obtain confidence on a 4-point scale.</p><p>We first assessed the models' fits to the data by computing average AIC values across the 60 subjects for the 16 models (4 architectures x 4 confidence strategies). We found that all four RTNet models generated substantially lower AIC values compared to the remaining 12 models (3 architectures x 4 confidence strategies) ( <ref type="figure" target="#fig_3">Figure 5A</ref>, <ref type="table">Supplementary Table 1</ref>). The Top2Diff strategy instantiated within RTNet remained the best model, with other strategies instantiated within RTNet falling between 45-170 AIC points behind it. In contrast, the four confidence strategies instantiated within MSDNet were 430-778 points worse than the RTNet-Top2Diff model. BLNet and CNet produced even worse fits with strategies instantiated in BLNet falling behind the RTNet-Top2Diff model by 2860-3087 AIC points and strategies within CNet falling behind RTNet-Top2Diff by 3245-3528 AIC points. All AIC comparisons were significant as assessed by computing bootstrapped 95% confidence intervals. These results corroborate our previous findings that RTNet provides the best fits to human data <ref type="bibr" target="#b36">(Rafiei et al., 2024)</ref>. The poor fits to data by other CNN architectures suggest that these architectures are unable to simultaneously fit the observed patterns of choices and confidence. However, since fitting models to choices and confidence was done in separate steps, it is important to rule out the possibility that these poor fits are due to a failure of models to fit confidence alone. Therefore, we correlated each subject's average confidence to the model's average confidence for that subject. We found that all models and strategies produced very high correlations (median r = 0.934, all r's &gt; 0.75, all p's &lt; 0.001), confirming that the fitting procedure was successful ( <ref type="figure" target="#fig_3">Figure 5B)</ref>. Instead, the poor fits for MSDNet, BLNet, and CNet were likely a result of the failure of these architectures to fit the pattern of human choices underlying confidence, in spite of being able to fit the overall confidence for each individual subject well.</p><p>Having established that RTNet provides the best overall fits to human data, we proceeded to look at how different confidence strategies perform within other CNN architectures. We examined the architectures in the order of the quality of their overall fit, starting with MSDNet and proceeding to BLNet and CNet.  <ref type="figure" target="#fig_4">Figure 6A</ref>).</p><p>Further, all models except PE were able to produce the folded-X pattern, but the Top2Diff model produced the closest fits to the observed pattern as measured by the average sum of squared errors across subjects (Top2Diff = 0.361; PE = 0.553; Softmax = 0.532; Entropy = 0.374). The errors produced by Top2Diff were significantly lower compared to the PE and Softmax models but not the Entropy model (PE: t(59) = 3.557, p &lt; 0.001; Softmax: t(59) = 4.524, p &lt; 0.001; Entropy: t(59) = 0.440, p = 0.661). Surprisingly, despite the PE strategy providing the worst AIC scores and folded-X pattern predictions, it produced the best predictions of individual confidence (Top2Diff: r = 0.122; PE: r = 0.169; Softmax: r = 0.118; Entropy = 0.118; all p's &lt; 0.001) and all comparisons of correlations between PE and other models were significant (Top2Diff: t(59) = 6.01, p &lt; 0.001; Softmax: t(59) = 5.90, p &lt; 0.001; Entropy: t(59) = 7.29, p &lt; 0.001). Second, we examined the results for BLNet, which was the third best-performing CNN architecture far behind both RTNet and MSDNet. Quantitative comparisons showed the PE, Entropy, and Top2Diff models having similar performance and far outperforming Softmax <ref type="figure" target="#fig_4">(Figure 6B)</ref> comparisons showed that all models were able to fit the observed folded-X pattern, with the PE, Top2Diff, and Entropy models producing similar fits and Softmax producing the worst fit (Top2Diff = 0.446; PE = 0.439; Softmax = 0.596; Entropy = 0.466). Indeed, the PE model, which produced the lowest errors, was only significantly better compared to Softmax but not to Top2Diff and Entropy (Top2Diff: t(59) = 0.149, p = 0.882; Softmax: t(59) = 2.037, p = 0.046; Entropy: t(59) = 0.460, p = 0.647). Image-level predictions were relatively similar across strategies, with the Top2Diff model producing highest correlations (Top2Diff: r = 0.20; PE: r = 0.184; Softmax: r = 0.191; Entropy = 0.196; all p's &lt; 0.001), which were significantly better compared to PE (t(59) = 4.71; p &lt; 0.001) but not compared to Softmax (t(59) = 1.52; p = 0.135) or Entropy (t(59) = 1.372; p = 0.175).</p><p>Finally, we examined the results for CNet, which was the worst-performing CNN architecture. Quantitative comparisons showed that the Entropy model outperformed  <ref type="figure" target="#fig_4">Figure 6C</ref>). All models except PE were able to predict the folded-X pattern (the PE model predicted a decrease in confidence from the difficult to easy condition for both correct and error trials). Among the other models, Entropy produced the closest fits to observed pattern (Top2Diff = 0.454; Softmax = 0.737; Entropy = 0.446). The fits yielded by Entropy were only significantly better compared to the Softmax model but not the Top2Diff model (Top2Diff: t(59) = 0.177, p = 0.860; Softmax: t(59) = 3.217, p = 0.002).</p><p>Finally, the Top2Diff, Entropy, and Softmax models produced similar image-by-image correlations with human confidence (Top2Diff: r = 0.177; PE: r = 0.129; Softmax: r = 0.168; Entropy = 0.170; all p's &lt; 0.001). Nevertheless, the predictions of Top2Diff, the best performing model on this test, were significantly better than those made by PE (t(59) = 5.60; p &lt; 0.001) and Softmax (t(59) = 2.23; p = 0.029) but not better than those made by the Entropy model (t(59) = 1.846; p = 0.07).</p><p>Overall, these results highlight the fact that the performance of different confidence strategies strongly depends on the CNN architecture. Importantly, only within the best performing CNN architecture, RTNet, we find a single confidence strategy (Top2Diff) that consistently gives the best qualitative and quantitative fits as well the best imagelevel predictions of confidence. For all other architectures, no single strategy was found to simultaneously produce the best fits to data as well as the best image-by-image predictions of confidence. Nevertheless, the Top2Diff strategy was consistently among the best models for all architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We instantiated four different confidence strategies in a recently-developed neural network model of perceptual decisions, RTNet. We found that human confidence is best described by a strategy that computes confidence as the difference in evidence between the top-two choices (Top2Diff). The Top2Diff strategy not only produced the best quantitative fits to human data but also reproduced all qualitative signatures of human confidence and generated the best image-by-image predictions of confidence ratings. These findings demonstrate that Softmax, which is currently the dominant way of obtaining confidence from neural network models, is not appropriate for deriving human-like confidence.</p><p>Our findings suggest that confidence considers only a specific subset of the available evidence, rather than the entire distribution of evidence. Our results are in line with previous findings from <ref type="bibr" target="#b26">Li &amp; Ma (2020)</ref> and <ref type="bibr" target="#b57">Xue et al. (2024)</ref>who found that the Top2Diff strategy provided the best description of human confidence. Importantly, both of these previous papers employed 3-choice tasks with relatively simple stimuli that consisted of colored dots. Our study extends these findings to more complex stimuli and tasks that involve many more choice options.</p><p>These findings add to a growing body of evidence against the notion that the positive evidence heuristic underlies human confidence. In spite of its popularity <ref type="bibr" target="#b23">(Koizumi et al., 2015;</ref><ref type="bibr" target="#b29">Maniscalco et al., , 2021</ref><ref type="bibr" target="#b34">Peters et al., 2017)</ref>, the PE mechanism has been challenged by numerous recent findings from cognitive as well as neural network modelling <ref type="bibr" target="#b44">(Rausch et al., 2020;</ref><ref type="bibr" target="#b48">Shekhar &amp; Rahnev, 2024a</ref><ref type="bibr" target="#b49">, 2024b</ref><ref type="bibr" target="#b54">Webb et al., 2023)</ref>. These studies have shown that the PE model consistently produces some of the worst fits to human data compared to other existing models and that the PE mechanism is, in fact, not necessary to account for behavioral effects that were previously assumed to be its signatures <ref type="bibr" target="#b23">(Koizumi et al., 2015;</ref>.</p><p>Our results also challenge Softmax as a model of human confidence. Softmax is used in most neural network models to derive the confidence of the network <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>. This is a natural choice due to the simplicity of the method and its desirable mathematical properties. Consequently, the few papers that have compared confidence in neural networks vs. humans have also used Softmax <ref type="bibr" target="#b36">(Rafiei et al., 2024;</ref><ref type="bibr" target="#b49">Shekhar &amp; Rahnev, 2024b;</ref><ref type="bibr" target="#b54">Webb et al., 2023)</ref> but never compared these results to other confidence strategies.</p><p>Our results demonstrate that Softmax is inappropriate for the purposes of building models of human confidence.</p><p>Finally, we also demonstrate that the Entropy strategy is also not a good model of human confidence. This finding is in line with the results of <ref type="bibr" target="#b26">(Li &amp; Ma, 2020)</ref> for simpler 3-choice tasks. One reason for why both Entropy and Softmax do not perform well may be that both strategies derive confidence ratings using the entire distribution of sensory evidence.</p><p>However, while this type of computation may not match well what humans do, it must be noted that it produces more informative confidence ratings. Therefore, Entropy and Softmax may indeed be better suited to derive confidence from neural networks when the goal is to maximize insight into the network's performance rather than model human behavior.</p><p>A prominent theory of confidence generation that we could not test here is the Bayesian confidence hypothesis, which postulates that confidence is computed optimally as the probability of being correct <ref type="bibr" target="#b3">(Aitchison et al., 2015;</ref><ref type="bibr" target="#b15">Hangya et al., 2016;</ref><ref type="bibr" target="#b45">Sanders et al., 2016)</ref>.</p><p>Recent studies have strongly challenged this assumption by showing that models that derive confidence directly from perceptual evidence substantially outperform models which use probability computations <ref type="bibr" target="#b2">(Adler &amp; Ma, 2018b</ref><ref type="bibr" target="#b1">, 2018a</ref><ref type="bibr" target="#b9">Denison et al., 2018;</ref><ref type="bibr" target="#b26">Li &amp; Ma, 2020;</ref><ref type="bibr" target="#b44">Rausch et al., 2020;</ref><ref type="bibr" target="#b48">Shekhar &amp; Rahnev, 2024a;</ref><ref type="bibr" target="#b56">Xue et al., 2023)</ref>. However, in the current study, we could not directly test the Bayesian confidence hypothesis since this would require knowledge of the probability distributions governing the evidence generated by the networks. Most cognitive models include explicit assumptions about the distributions of sensory evidence, which enable us to compute the posterior probabilities associated with choices. In contrast, the statistical properties of the evidence generated by neural networks performing tasks using complex stimuli and involving multiple choices are not well understood and are unlikely to be well described by simple assumptions used in cognitive models. Therefore, in order to test Bayesian confidence hypothesis using neural networks, one needs to first model the internal distributions of evidence generated by these networks and also make the assumption that humans can derive and use these distributions to correctly compute the probability of being correct.</p><p>Why does human confidence resemble the Top2Diff strategy despite the suboptimality of its underlying computation? One possibility is that the Top2Diff strategy may be preferred over more optimal strategies that use all available information since these optimal strategies can require very complex computations. Particularly, in situations where there are many alternatives and the possible choices are not clearly defined, Bayesian and Entropy computations would become intractable as they require the system to compute probabilities associated with all possible choices <ref type="bibr" target="#b37">(Rahnev, 2020)</ref>. On the other hand, the PE heuristic minimizes computational effort but neglects too much available information, thus producing confidence that may not be sufficiently informative about one's perceptual accuracy. Therefore, the Top2Diff strategy may have emerged as a viable solution to the trade-off between maximizing the information content of confidence ratings and minimizing computational costs.</p><p>We observed strikingly large differences in quantitative model fits between the four CNN architectures we tested (RTNet, MSDNet, BLNet and CNet). It is surprising that BLNet and CNet, which incorporate biological mechanisms of recurrence <ref type="bibr" target="#b12">(Goetschalckx et al., 2023;</ref><ref type="bibr" target="#b21">Kar et al., 2019;</ref><ref type="bibr" target="#b20">Kar &amp; DiCarlo, 2021;</ref><ref type="bibr" target="#b22">Koivisto et al., 2011;</ref><ref type="bibr" target="#b50">Spoerer et al., 2020)</ref> and parallel processing <ref type="bibr" target="#b18">(Iuzzolino et al., 2021)</ref>, performed substantially worse than RTNet and MSDNet. While RTNet is expected to perform well as it implements an empirically validated mechanism of evidence accumulation <ref type="bibr" target="#b11">(Forstmann et al., 2016;</ref><ref type="bibr" target="#b39">Ratcliff &amp; McKoon, 2008;</ref><ref type="bibr" target="#b40">Ratcliff &amp; Rouder, 1998;</ref><ref type="bibr" target="#b42">Ratcliff &amp; Starns, 2013)</ref>, it is less clear why MSDNet -which is not built based on biologically relevant mechanisms -outperformed BLNet and CNet. One thing to note is that MSDNet and RTNet are smaller networks compared to CNet and BLNet.</p><p>Indeed, CNet uses the deeper ResNet-18 architecture, and BLNet is a seven-layer network that gets unrolled across eight time steps into a deeper network. It may be that very deep networks trained on simple tasks like MNIST make for poor models of human perceptual decision making. Another possibility that explains these AIC differences is that the biological mechanisms implemented by BLNet and CNet may be unnecessary to explain human perceptual decisions for relatively easy tasks such as digit discrimination, compared to more complex perceptual tasks, such as scene discrimination <ref type="bibr" target="#b12">(Goetschalckx et al., 2023)</ref>, or specific manipulations, such as masking <ref type="bibr" target="#b52">(Tang et al., 2018)</ref>. On the other hand, the evidence accumulation mechanism described by RTNet may be more important for decision making that involves simple perceptual discrimination.</p><p>One limitation of our study was the inability to simultaneously fit both choice and confidence data to the neural network models. Since neural networks have parameters in the order of millions, it becomes computationally intractable to simultaneously tune all these parameters and fit the observed human data. Therefore, we used a general-purpose training method to teach the models to perform the task and then optimized two critical parameters that controlled the models' performance levels. Even then, we fit these two parameters to the average human data (instead of each subject separately) because all CNN models required significant computational resources and processing time to generate responses even on a single trial. Instead, the only parameters we could fit on a subject-bysubject basis using traditional exhaustive search standard in cognitive models <ref type="bibr" target="#b0">(Acerbi &amp; Ma, 2017;</ref><ref type="bibr" target="#b32">Nelder &amp; Mead, 1965)</ref> were the three confidence criteria. Future work should focus on developing methods that would allow traditional maximum likelihood estimation to be applied to neural network models of decision making.</p><p>In conclusion, we show that a novel neural network model of human decision making, RTNet, can be used to infer the mechanisms of confidence generation in humans for multichoice tasks with relatively complex stimuli. Particularly, the strategy which computes confidence as the difference in evidence between the top-two choices (Top2Diff) emerged as the clear winner over other strategies that base confidence on the whole distribution of evidence or neglect choice-incongruent evidence. Our results highlights neural network models as promising tools to test hypotheses about the mechanisms of decision-making under naturalistic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli and task</head><p>Sixty-four subjects (31 female; age, 18-32 years) with normal or corrected-to-normal vision participated in the experiment. All subjects signed informed consent and were compensated monetarily for their participation. The protocol was approved by the Georgia Institute of Technology Institutional Review Board (protocol no. H15308). This experiment has been previously reported in <ref type="bibr" target="#b36">Rafiei et al. (2024)</ref> and all the details can be found in the original publication. Below, we briefly describe the experimental design.</p><p>Subjects performed a digit discrimination task with eight choices and reported confidence. On each trial, a fixation cross was presented for 500-1000 ms followed the presentation of a stimulus for 300 ms. The stimulus was a digit between 1-8 and superimposed with noise. Subjects reported the perceived digit by pressing a key between 1-8 and subsequently reported their confidence on a scale from 1-4 via another key press (where 1 corresponds to lowest confidence and 4 corresponds to highest confidence). The response screens stayed on until subjects made a response.</p><p>We manipulated task difficulty as well as speed-accuracy trade-off (SAT) instructions.</p><p>Task difficulty was manipulated by corrupting the pixels with uniform noise. Easier stimuli contained lower pixel noise on average. SAT was manipulated by instructing subjects to make their responses as fast or as accurate as possible. Trials containing easy and difficult stimuli were interleaved whereas trials with different SAT instructions were blocked and presented alternately.</p><p>The stimuli were obtained from the publicly available MNIST dataset containing 60,000 training images and 10,000 testing images. The stimuli shown to subjects were taken only from the testing set to ensure that both subjects and networks were tested on novel images. 480 images were randomly selected and evenly distributed across the four experimental conditions. Subjects completed 4 blocks of 60 trials each (960 trials in total as each of the 480 images was presented twice). The experiment was designed in the MATLAB v.2020b environment using Psychtoolbox 3 <ref type="bibr" target="#b7">(Brainard, 1997)</ref>. The stimuli were presented on a 21.5-inch Dell P2217H monitor (1,920 × 1,080 pixel resolution, 60</p><p>Hz refresh rate). The subjects were seated 60 cm away from the screen and provided their responses using the keyboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral analyses</head><p>We excluded subjects based on preregistered criteria (https://osf.io/kmraq). These criteria resulted in the exclusion of four subjects in total (two subjects for not following SAT instructions and two subjects for ceiling effects on confidence). For each individual subject, we computed average confidence as a function of task difficulty and SAT condition to assess how these factors affect confidence. We also assessed confidence ratings for the folded-X pattern, which is popularly regarded to be a signature of human confidence. The folded-X pattern is obtained by computing confidence as a function of task difficulty separately for correct and error trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architecture and implementation</head><p>We first briefly describe the four network architectures and their implementation. We trained 60 instances of each network architecture using different random initializations of the network's weights to allow for individual differences in learning. All networks were implemented in Python (version 3.10.11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTNet</head><p>The RTNet architecture consists of two modules. The first module is a Bayesian neural network (BNN) whose weights are learned as posterior probability distributions rather than point estimates <ref type="bibr" target="#b19">(Jospin et al., 2020)</ref>. Due to stochasticity in the BNN's weights, a unique feedforward network gets sampled at each time step and repeated processing of the same image generates variable activations in the network's final layer. The second module consists of an evidence accumulator that receives these noisy activations and integrates the evidence towards a pre-defined threshold. Evidence is accumulated independently for each choice option and the model chooses the option for which the evidence first hits the threshold. Response time corresponds to the number of evidence samples that were required to reach the threshold. The network was implemented within the AlexNet architecture <ref type="bibr" target="#b25">(Krizhevsky et al., 2012)</ref> consisting of five convolutional layers and three fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNet</head><p>The Cascaded parallel network (CNet) uses parallel processing and introduces propagation delays between convolutional layers using skip connections <ref type="bibr" target="#b18">(Iuzzolino et al., 2021)</ref>. Even though all the network's layers process input parallelly at any given time, propagation delays cause early layers to receive input faster and achieve stable activations sooner. For the same amount of processing time, the later layers receive only partial input from the earlier layers and take more processing steps to achieve stabilization of their output. As a result, simple features propagate faster through the network while more complex features require greater processing time, naturally leading to the trade-off between processing speed and stimulus complexity found in humans.</p><p>The network's decision can be generated by setting a threshold at the output layer and the decision time is determined by the number of processing steps that are required to reach the threshold. The network was implemented within the ResNet-18 architecture since it requires network architectures with skip connections <ref type="bibr" target="#b16">(He et al., 2015)</ref>. ResNet-18 contains 17 convolutional layers organized in eight residual blocks and one fully connected output layer with a softmax activation function that generates the decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLNet</head><p>BLNet is a recurrent convolutional neural network (RCNN) that uses bottom-up and lateral connections to recurrently feed each layer's input back to itself <ref type="bibr" target="#b50">(Spoerer et al., 2020)</ref>. Therefore, each layer receives feedforward input from the previous convolutional layer as well as recurrent input from itself in the form of its own activations at the previous time step. Time steps are defined in terms of feedforward sweeps and after each feedforward sweep, the network's readout is evaluated. If the readout crosses a predefined threshold, the network chooses the option with the highest readout. The number of feedforward sweeps preceding the decision determines the response time.</p><p>Recurrent processing is a biologically inspired mechanism that can dynamically adjust a network's computational power. For instance, setting a higher threshold will lead to the network undergoing a larger number of feedforward and recurrent computations, effectively resulting in a deeper network being unrolled. We implemented BLNet as described in the original publication that introduced this network <ref type="bibr" target="#b50">(Spoerer et al., 2020)</ref>, which consists of seven convolutional layers and a final readout layer with a softmax activation function. The network was unrolled across time for a maximum of eight time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSDNet</head><p>MSDNet uses a standard feedforward CNN with early-exit classifiers after each convolutional layer <ref type="bibr" target="#b17">(Huang et al., 2017)</ref>. The classifiers compute the evidence at each layer using a softmax function and if the evidence crosses a pre-defined threshold, the network stops processing input and generates a response based on that layer's output.</p><p>The decision corresponds to the choice option that generates the highest softmax value and the response time corresponds to the layer at which the decision was generated. As in the original publication, MSDNet was implemented with the AlexNet architecture with five convolutional layers and three fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fitting CNN models to human choices</head><p>Our central goal was to compare model fits to human confidence across network architectures and confidence strategies. However, due to computational constraints, it is not possible to simultaneously fit both choice and confidence data to the models via traditional maximum likelihood estimation (MLE) methods. Therefore, we followed a step-wise approach where we 1) trained the models on the general task performed by humans, 2) fit the models to human choices by optimizing two additional parameters (noise in the stimuli and decision threshold) that gave us the closest match to the average human performance, and 3) generated confidence using each of the four strategies and fit three confidence criteria separately for each individual human subject in order to create subject-specific fits for the confidence ratings. In Step 1, we trained all four networks on the MNIST dataset to classify handwritten digits. All the networks were trained to achieve an accuracy of &gt;97% on the training set. The details of the training procedure can be found in the original publication. Training the models and tuning the choice parameters (Steps 1 and 2) were previously done by <ref type="bibr" target="#b36">Rafiei et al. (2024)</ref>. In the current study, we simulated these pre-trained and tuned networks to generate confidence using four different strategies (Step 3). Importantly, in Step 2, we fit two choice parameters which were the same for all human subjects, but in Step 3, we fit individual confidence criteria that differed from subject to subject. Below, we describe the procedures for Steps 2 and 3 in more detail.</p><p>Fitting the pre-trained models to human choices (Step 2 of fitting)</p><p>To fit the models to human choices, we matched all the models' accuracy levels to those observed in humans by tuning two additional parameters. We separately matched the network's performance in each experimental condition to the average accuracy observed for that condition. To match accuracies across the difficulty levels, we adjusted the noise in the images. Images with higher levels of noise lead to lower accuracy. To mimic the effect of the speed-accuracy trade-off manipulation on accuracy, we adjusted the networks' thresholds. A higher threshold leads to longer processing times and higher accuracies.</p><p>We estimated the parameters using a coarse search followed by a fine-grained search and chose the parameters that gave us the closest match to average human accuracy for each condition. For RTNet, the best match to human accuracy was achieved for noise levels of 2.1 for easy images and 4.1 for difficult images, and threshold values of 3 for the speed condition and 6 for the accuracy condition. For CNet, the closest match was achieved for noise levels of 1.42 for easy images and 1.83 for difficult images and threshold values of 0.83 for the speed condition and 0.9 for the accuracy condition and.</p><p>For BLNet, the best match to human accuracy was obtained when the noise levels were set to 0.55 for easy images and 1.2 for difficult images and when thresholds were set to 0.4 for the speed condition and 0.95 for the accuracy focus condition. For MSDNet, the closest match was achieved for noise levels of 1.9 for easy images and 3.0 for difficult images and for threshold values of 0.58 for the speed condition and 0.82 for the accuracy condition.</p><p>In addition to the procedure above where we fit the two parameters to optimize the fit to average accuracy in each condition, we performed a separate fitting procedure that fit each network's responses to the entire 8x8 response matrix using the maximum likelihood estimation (MLE). Neither methods provided significantly better overall AIC scores compared to the other (t(15) = 0.891; p = 0.387, two-sided paired t-test on model AIC scores). Therefore, we only report findings from the first method here and report findings from the second method in the Supplementary <ref type="figure" target="#fig_0">(Supplementary Methods;   Supplementary Figures 1-4; Supplementary Table 2</ref>).</p><p>Fitting the model responses to human confidence (Step 3 of fitting)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementing different confidence strategies</head><p>We implemented four confidence strategies within each network architecture: PE, Top2Diff, Softmax and Entropy. We first obtained the logits or raw activations from the networks' output layer for each of the eight choice options, = [ 1 , 2 , … 8 ]. The positive evidence hypothesis posits that confidence is based on the evidence in favor of the chosen option. Therefore, under the PE model, confidence was computed as the activation associated with the chosen response i.e. = max( ). According to the Top2Diff model, confidence was computed as the difference in activation between the chosen response and the response that generated the second highest evidence, such that 2 = max( ) − 2( )where max2( ) is the second highest value in the vector . Since the activations are unbounded, they can take arbitrarily large values which can be problematic when fitting models to the data. Therefore, for both the PE and Top2Diff models, the raw activation scores across trials were normalized by their standard deviations to restrict their ranges. For the Softmax model, we first applied the softmax transformation to the raw activations, such that = ∑ 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>, where is the softmax value associated with the ℎ choice. The softmax function transforms the activations into probability scores where ∈ [0,1] and ∑ = 1 8 =1</p><p>. Softmax confidence was then obtained as = max( ). Finally, for the entropy model, the entropy,</p><p>, associated with the output distribution was computed based on the probability scores obtained from the softmax transformation as = − ∑ * log( )</p><formula xml:id="formula_0">8 =1</formula><p>.</p><p>Since confidence should increase with decreasing uncertainty or entropy, we defined confidence as the negative of entropy such that = − .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performing model fitting</head><p>Human confidence responses were obtained on a four-point scale. However, the models' confidence was derived from their raw and continuous activations, making it difficult to directly compare the models' confidence to humans. Therefore, we ran a model fitting procedure to fit the models' confidence responses separately for each individual human subject. We defined a set of three confidence criteria, = [ 1 , 2 , 3 ], for each subject that would transform the continuous confidence responses into subject-specific 4-point confidence, thus allowing direct comparisons with humans.</p><p>Model fitting was based a maximum likelihood estimation (MLE) procedure that aimed to find a set of parameters that maximized the log-likelihood of the model associated with the full probability distribution of responses. The probability distribution of responses was computed as an 8 x 8 x 4 response matrix representing the eight stimulus classes and 8 x 4 responses (eight choices with four confidence ratings for each choice). For each model, we pooled the responses across the 60 model instances and binned them according to their associated stimulus, choice, and confidence to obtain the response probability matrix. Log-likelihood was computed as:</p><formula xml:id="formula_1">∑ log( ) × , ,</formula><p>, where refers to the response probability (computed from the model's simulated responses) and refers to the number of trials associated with stimulus class = {1,2, … , 8}, choice = {1,2, … , 8} and, confidence response = {1,2,3,4}. The parameter search was conducted using the Bayesian Adaptive Direct search (BADS) toolbox <ref type="bibr" target="#b0">(Acerbi &amp; Ma, 2017)</ref>. Quantitative model comparisons were made using a goodness-of-fit measure called the Akaike information criterion (AIC) which is computed as = −2 + 2 , where refers to the log-likelihood associated with the maximum-likelihood estimates of the parameters and refers to the number of model parameters. For all models, was fixed as three (corresponding to the three confidence criteria) because we were only fitting the model's responses to confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating the model's confidence responses for individual subjects</head><p>After obtaining the MLE of the confidence criteria for each individual subject, we used these parameters to simulate subject-specific confidence ratings. To generate these ratings, we first derived raw confidence for each trial by applying the confidence generation strategies defined above. To generate subject-specific confidence, we applied that subject's estimated confidence criteria to the raw confidence values after aggregating the networks' output across all 60 instances. The confidence criteria were defined as = [ 0 , 1 , 2 , 3, , 4 ], where 0 = −∞ and 4 = ∞, and were applied to the raw confidence values ( ) such that falling within the interval [ −1 , )</p><p>resulted in a confidence rating of where ∈ {1, 2, 3, 4}.</p><p>Qualitatively assessing models' confidence responses and predictions</p><p>We analyzed whether the models' confidence ratings reproduced the qualitative patterns of confidence observed in humans. Specifically, we simulated each subject's confidence separately using their individual parameter estimates and analyzed whether these simulated confidence ratings reproduced the observed effects of the difficulty manipulation, the SAT manipulation, and the folded-X pattern. Mean confidence for each condition was computed separately for each simulated subject and then averaged across subjects.</p><p>We further tested how well models could predict human confidence for novel, individual images at the level of the group as well as individual subjects. For testing group level confidence, we computed the average confidence observed for each unique image across the 60 human subjects and for the 60 simulated subjects (separately for each model). The correlation between the observed and predicted image-by-image group confidence was computed using Pearson's correlation coefficient. Further, to test whether these correlations were significantly different from each other, we used the Fisher's r-to-z transformation and two-sided paired t-tests. To assess confidence predictions for individual subjects, we computed image-by-image confidence separately for each simulated subject and correlated these quantities with the imageby-image confidence observed for that subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and code availability</head><p>All data and code have been made publicly available at https://osf.io/xcv98/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>RT architecture and task. (A) RTNet architecture. RTNet consists of two modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Assessing model predictions of confidence for unseen, individual images. (A) Histograms of confidence for humans and models across the 480 unique images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Assessing characteristics of group-level predictions of confidence. (A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparing model fits across all CNN architectures and confidence strategies. (A) Quantitative comparisons using AIC scores. The Top2Diff strategy within the RTNet model gave the best AIC fits the data. Positive AIC differences indicate support for the RTNet-Top2Diff model. Regardless of the confidence strategy, the RTNet architecture significantly outperformed all other CNN architectures. Error bars show bootstrapped 95% confidence intervals. (B) Model fits compared to observed mean confidence for individual subjects. All CNN architectures and confidence strategies are able to fit the observed mean confidence for individual subjects (median r = 0.934, all p's &lt; .001). Each dot represents an individual subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparing confidence strategies within other CNN architectures. Results for (A) MSDNet, (B) BLNet, and (C) CNet. Each panel shows AIC-based quantitative fits (left), qualitative fits to the folded-X pattern (middle), and image-by-image correlations between each confidence strategy and individual human subjects (right). The figure demonstrates that the performance of different confidence strategies strongly depends on the CNN architecture. Further, for all three models, no single strategy provides the best fits across the three metrics examined here. Dots represent individual subjects. Error bars show SEM. *p&lt;0.05; ***p&lt;0.001; n.s., not significant. SSE, sum of squared errors (smaller values indicate better fits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>.</head><label></label><figDesc>Specifically, Top2Diff outperformed the PE strategy by an average of 45.12 points (95% CI = [30.60, 57.86), the Softmax strategy by 58.15 points (95% CI = [43.94, 97.74]), and the Entropy strategy by 170.39 (95% CI = [121.70, 234.04])</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>First, we examined the results for MSDNet, the second best-performing CNN architecture after RTNet. For MSDNet, the Top2Diff strategy yielded the lowest AIC values. Specifically, Top2Diff outperformed the Entropy model by an average of 26.68 points (95% CI = [15.93, 37.91), the Softmax model by 95.53 points (95% CI = [39.22, 213.99]), and the PE model by 348.34 points (95% CI = [309.86, 397.35];</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.</head><label></label><figDesc>Specifically, PE, the best-performing model, outperformed Entropy by 3.70 AIC points (95% CI = [-40.05, 42.12]), Top2Diff by 6.65 AIC points (95% CI = [-42.96, 54.35]), and Softmax by 226.74 AIC points (95% CI = [129.73, 59.46]). Qualitative</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Top2Diff by 59.88 AIC points (95% CI = [30.62, 91.97]), Softmax by 149.20 AIC points (95% CI = [102.41, 211.89]), and PE by 282.51 AIC points (95% CI = [218.77, 349.79],</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head><p>The authors declared no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1836" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing Bayesian and non-Bayesian accounts of human confidence reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1006572</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1006572" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Limitations of Proposed Signatures of Bayesian Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_A_01141</idno>
		<ptr target="https://doi.org/10.1162/NECO_A_01141" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3327" to="3354" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Doubly Bayesian Analysis of Confidence in Perceptual Decision-Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004519</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1004519" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sensory noise increases metacognitive efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000511</idno>
		<ptr target="https://doi.org/10.1037/xge0000511" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="452" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Confidence reflects a noisy decision reliability estimate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Boundy-Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Ziemba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L T</forename><surname>Goris</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01464-x</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01464-x" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="154" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep problems with neural network models of human vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dujmović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Llera Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Biscione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puebla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Adolfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blything</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X22002813</idno>
		<ptr target="https://doi.org/DOI:10.1017/S0140525X22002813" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Psychophysics Toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/9176952" />
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The MNIST database of handwritten digit images for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2012.2211477</idno>
		<ptr target="https://doi.org/10.1109/MSP.2012.2211477" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1717720115</idno>
		<ptr target="https://doi.org/10.1073/pnas.1717720115" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="11090" to="11095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000045</idno>
		<ptr target="https://doi.org/10.1037/rev0000045" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="114" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1146/ANNUREV-PSYCH-122414-033645</idno>
		<ptr target="https://doi.org/10.1146/ANNUREV-PSYCH-122414-033645" />
	</analytic>
	<monogr>
		<title level="m">Sequential Sampling Models in Cognitive Neuroscience: Advantages, Applications, and Extensions</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">641</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computing a human-like reaction time metric from stable recurrent vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Sheinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.11582v2" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Signal Detection Theory and Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On Calibration of Modern Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.04599v2" />
	</analytic>
	<monogr>
		<title level="m">34th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2130" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Mathematical Framework for Statistical Decision Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hangya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kepecs</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00864</idno>
		<ptr target="https://doi.org/10.1162/NECO_a_00864" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-Scale Dense Networks for Resource Efficient Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.09844v5" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.09808v4" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="27631" to="27644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hands-on Bayesian Neural Networks --a Tutorial for Deep Learning Users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Jospin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCI.2022.3155327</idno>
		<ptr target="https://doi.org/10.1109/MCI.2022.3155327" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast Recurrent Processing via Ventrolateral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.NEURON.2020.09.035</idno>
		<ptr target="https://doi.org/10.1016/J.NEURON.2020.09.035" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="176" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evidence that recurrent circuits are critical to the ventral stream&apos;s execution of core object recognition behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-019-0392-5</idno>
		<ptr target="https://doi.org/10.1038/s41593-019-0392-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="974" to="983" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Processing in V1/V2 Contributes to Categorization of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Railo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Revonsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Salminen-Vaparanta</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3074-10.2011</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3074-10.2011" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2488</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Does perceptual confidence facilitate cognitive control? Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.3758/S13414-015-0843-3</idno>
		<ptr target="https://doi.org/10.3758/S13414-015-0843-3" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1295" to="1306" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Metacognition and consciousness. The Cambridge Handbook of Consciousness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koriat</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511816789.012</idno>
		<ptr target="https://doi.org/http://dx.doi.org/10.1017/CBO9780511816789.012" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="289" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/cuda-convnet/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-15581-6</idno>
		<ptr target="https://doi.org/10.1038/s41467-020-15581-6" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visual Confidence. Annual Review of Vision Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<idno type="DOI">10.1146/ANNUREV-VISION-111815-114630</idno>
		<ptr target="https://doi.org/10.1146/ANNUREV-VISION-111815-114630" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="459" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The signal processing architecture underlying subjective reports of sensory awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1093/nc/niw002</idno>
		<ptr target="https://doi.org/10.1093/nc/niw002" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience of Consciousness</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tuned inhibition in perceptual decision-making circuits can explain seemingly suboptimal confidence behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Odegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grimaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.1371/JOURNAL.PCBI.1008779</idno>
		<ptr target="https://doi.org/10.1371/JOURNAL.PCBI.1008779" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Heuristic use of perceptual evidence leads to dissociation between performance and metacognitive sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-016-1059-x</idno>
		<ptr target="https://doi.org/10.3758/s13414-016-1059-x" />
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="923" to="937" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Metacognition: Knowing about Knowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metcalfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Shimamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metcalfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Shimamura</surname></persName>
		</author>
		<ptr target="http://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=1738390" />
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT Press. -References -Scientific Research Publish. In MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Simplex Method for Function Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
		<idno type="DOI">10.1093/COMJNL/7.4.308</idno>
		<ptr target="https://doi.org/10.1093/COMJNL/7.4.308" />
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Superior colliculus neuronal ensemble activity signals optimal rather than subjective confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Odegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grimaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Basso</surname></persName>
		</author>
		<idno type="DOI">10.1073/PNAS.1711628115/SUPPL_FILE/PNAS.201711628SI.PDF</idno>
		<ptr target="https://doi.org/10.1073/PNAS.1711628115/SUPPL_FILE/PNAS.201711628SI.PDF" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptual confidence neglects decision-incongruent evidence in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuzniecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Devinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Halgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-017-0139</idno>
		<ptr target="https://doi.org/10.1038/s41562-017-0139" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stage dynamic signal detection: A theory of choice, decision time, and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pleskac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0019737</idno>
		<ptr target="https://doi.org/10.1037/a0019737" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="864" to="901" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The neural network RTNet exhibits the signatures of human perceptual decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-024-01914-8</idno>
		<ptr target="https://doi.org/10.1038/s41562-024-01914-8" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2024</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Confidence in the Real World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.05.005</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2020.05.005" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="590" to="591" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balsdon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Gardelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Faivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Filevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jehee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Odegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sackur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zylberberg</surname></persName>
		</author>
		<idno type="DOI">10.1177/17456916221075615</idno>
		<ptr target="https://doi.org/10.1177/17456916221075615" />
	</analytic>
	<monogr>
		<title level="m">Consensus Goals in the Field of Visual Metacognition</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1746" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The diffusion decision model: Theory and data for twochoice decision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2008.12-06-420</idno>
		<ptr target="https://doi.org/10.1162/neco.2008.12-06-420" />
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling Response Times for Two-Choice Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9280.00067</idno>
		<ptr target="https://doi.org/10.1111/1467-9280.00067" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling confidence and response time in recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Starns</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014086</idno>
		<ptr target="https://doi.org/10.1037/a0014086" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="83" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling confidence judgments, response times, and multiple choices in decision making: recognition memory and motion discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Starns</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0033152</idno>
		<ptr target="https://doi.org/10.1037/a0033152" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="697" to="719" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visibility is not equivalent to confidence in a low contrast orientation discrimination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zehetleitner</surname></persName>
		</author>
		<idno type="DOI">10.3389/FPSYG.2016.00591/BIBTEX</idno>
		<ptr target="https://doi.org/10.3389/FPSYG.2016.00591/BIBTEX" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">183184</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cognitive modelling reveals distinct electrophysiological markers of decision confidence and error monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zehetleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Maier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2020.116963</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2020.116963" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Signatures of a Statistical Computation in the Human Sense of Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hangya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kepecs</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016.03.025</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2016.03.025" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="499" to="506" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distinguishing the Roles of Dorsolateral and Anterior PFC in Visual Metacognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3484-17.2018</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3484-17.2018" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="5078" to="5087" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The nature of metacognitive inefficiency in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000249</idno>
		<ptr target="https://doi.org/10.1037/rev0000249" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="70" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How do humans give confidence? A comprehensive comparison of process models of perceptual metacognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1037/XGE0001524</idno>
		<ptr target="https://doi.org/10.1037/XGE0001524" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="656" to="688" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human-like dissociations between confidence and accuracy in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.02.01.578187</idno>
		<idno>2024.02.01.578187</idno>
		<ptr target="https://doi.org/10.1101/2024.02.01.578187" />
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Spoerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Kietzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mehrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Charest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<idno type="DOI">10.1371/JOURNAL.PCBI.1008215</idno>
		<ptr target="https://doi.org/10.1371/JOURNAL.PCBI.1008215" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hardesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="8835" to="8840" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
	<note>Recurrent computations for visual pattern completion. Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/PNAS.1719397115/SUPPL_FILE/PNAS.1719397115.SAPP.PDF</idno>
		<ptr target="https://doi.org/10.1073/PNAS.1719397115/SUPPL_FILE/PNAS.1719397115.SAPP.PDF" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Natural statistics support a rational account of confidence biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajananda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-023-39737-2</idno>
		<ptr target="https://doi.org/10.1038/s41467-023-39737-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Are Deep Neural Networks Adequate Behavioral Models of Human Visual Perception? Annual Review of Vision Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<idno type="DOI">10.1146/ANNUREV-VISION-120522-031739</idno>
		<ptr target="https://doi.org/10.1146/ANNUREV-VISION-120522-031739" />
		<imprint>
			<date type="published" when="2023" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="501" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Challenging the Bayesian confidence hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.31234/OSF.IO/MF5ZP</idno>
		<ptr target="https://doi.org/10.31234/OSF.IO/MF5ZP" />
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A novel behavioral paradigm reveals the nature of confidence computation in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<idno type="DOI">10.1167/JOV.24.10.407</idno>
		<ptr target="https://doi.org/10.1167/JOV.24.10.407" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="407" to="407" />
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
