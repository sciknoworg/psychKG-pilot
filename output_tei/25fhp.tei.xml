<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A dynamic model of context-based retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-08-27">August 27, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madison</forename><forename type="middle">D</forename><surname>Paron</surname></persName>
							<email>mparon@sas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">D</forename><surname>Paron</surname></persName>
							<email>jparon@wharton.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Finance</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
							<email>kahana@psych.upenn.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A dynamic model of context-based retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-27">August 27, 2024</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Memory</term>
					<term>Recall</term>
					<term>Decisions</term>
					<term>Context</term>
					<term>Response times</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose a comprehensive model of how experiences are encoded and retrieved from memory. At the core of the model is a dynamic retrieval process incorporating two essential mechanisms: iterative retrieval, whereby information is sequentially sampled from memory to access the full history of experiences; and competitive retrieval, whereby the most prominent features in memory inhibit the recollection of other features. Together with context-based encoding, the model quantitatively explains wellknown facts about response order and inter-response times in recall experiments. We show that our retrieval process maps closely to existing decision frameworks, such as drift-diffusion models, suggesting that the memory system plays a fundamental role in a wide-ranging set of decision-making settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider an individual faced with a decision-making problem. To obtain the information necessary to reach a decision, the individual typically needs to retrieve and accumulate evidence from memory. Indeed, decisions take time in large part because evidence must be sampled sequentially from memory <ref type="bibr" target="#b18">(Shadlen &amp; Shohamy, 2016)</ref>. This insight is missing from conventional memory models (see <ref type="bibr" target="#b7">Kahana, 2020)</ref>. These models provide detailed accounts of how memory dynamics govern the encoding of information, but appeal to separate evidenceaccumulation processes (e.g., <ref type="bibr" target="#b2">Brown &amp; Heathcote, 2008;</ref><ref type="bibr" target="#b14">Ratcliff, 1978;</ref><ref type="bibr" target="#b19">Usher &amp; McClelland, 2001</ref>) when describing the subsequent retrieval and response. They therefore say little about how experiences are sampled during retrieval and used to reach a response.</p><p>This paper argues that the same iterative memory process underlying encoding also underlies retrieval and response. We incorporate this insight into a temporal-context-based model of dynamic retrieval (TCM-DR), which provides a comprehensive account of how information is not only encoded, but also retrieved from memory to reach a decision. The model features agents (subjects) who perceive and encode information from an environment and subsequently retrieve that information in response to a cue or objective. Encoding of experiences is as in the standard temporal context model <ref type="bibr" target="#b5">(Howard &amp; Kahana, 2002;</ref><ref type="bibr" target="#b17">Sederberg, Howard, &amp; Kahana, 2008)</ref>: the subject perceives features of the environment and encodes them as neural representations in memory together with the current latent mental state, or context. Retrieval of features from memory is modeled through a new iterative retrieval process whereby the brain sequentially samples information from memory and maps it to recognizable objects or states from the environment. <ref type="figure">Figure 1</ref> conveys the basic idea of our model (the right panel) and contrasts it with existing models of memory (the left panel). In standard models, the encoding, retrieval, and response processes are separated. Some such models have no temporal retrieval dynamics at all. For instance, <ref type="bibr" target="#b5">Howard and Kahana (2002)</ref> propose a single-step retrieval process in which one set of retrieved features forms retrieval probabilities, then the subject randomly recalls items according to these probabilities. More recent context-based models do imply temporal dynamics. In <ref type="bibr" target="#b17">Sederberg et al. (2008)</ref>, for example, encoding is the result of endogenous  <ref type="figure">Figure 1</ref>: Comparison between the standard class of memory models and our model. Standard models separate retrieval of information in memory from the response process. Our model assumes that the same memory process underlying perception and encoding also underlies a joint retrieval and response process.</p><p>item-and context-reinstatement into memory, while retrieval and response are determined by an exogenous diffusion-based model that stochastically recalls item features. These separate response processes conceal the fact that memory is fundamentally involved not only in encoding, but also in retrieval and response. In reality, a large part of the exogenous noise driving these models is standing in for a sequential process whereby evidence is accumulated from memory.</p><p>In TCM-DR, there is a single, iterative retrieval and response process that follows the same dynamics as the encoding process. Each successful retrieval of a feature is the outcome of a sequential process in which information is sampled from memory over time ( <ref type="figure">Figure 1</ref>).</p><p>The subject follows a "train of thought" that is initiated by the current mental context and any relevant external cues. Features retrieve contexts, then these contexts retrieve features, and so on. Information is accumulated until a decision-relevant response is reached. There is no exogenous noise in the retrieval and response process.</p><p>We find that two psychological mechanisms are essential to explaining the dynamics of retrieval and response: iterative retrieval and competitive retrieval. Iterative retrieval is a recursive process by which the brain uses context to retrieve features and features to retrieve contexts until these retrieved features (thoughts) become stable. It is in line with behavioral and neurological evidence that decisions take time precisely because individuals sequentially sample information from memory <ref type="bibr" target="#b18">(Shadlen &amp; Shohamy, 2016)</ref>. We show that iterative retrieval allows the subject to access the full historical distribution of observed features. Therefore, it is helpful for forming a complete database and for generalizing beyond the present context. It is insufficient on its own, however, for two reasons: (i) it implies context-and cue-independent recall and (ii) it results in the recollection of an unrecognizable combination of past features instead of a single discernible feature.</p><p>Competitive retrieval resolves both of these issues. With competitive retrieval, the most prominent features brought to mind during retrieval are given outsized attention, suppressing less prominent features. This allows the "fuzzy" features retrieved from iterative retrieval to be "cleaned" and mapped back to recognizable features. This same mechanism is fundamental to a large class of models in computational cognitive science, notably Hopfield networks <ref type="bibr" target="#b12">(Masson, 1995;</ref><ref type="bibr" target="#b16">Rizzuto &amp; Kahana, 2001;</ref><ref type="bibr" target="#b6">Kahana, 2012)</ref>. We show that, when iterative retrieval and competitive retrieval operate simultaneously, retrieved features can converge to a distinct past experience that depends on the initial context and cue. The subject is able to retrieve distinct features that were encoded in the distant past (due to iterative retrieval) but related to the present context and cues (due to competitive retrieval). This yields not only variation in which features are retrieved, but also variation in how long it takes to converge to a response. Critically, this variation arises even without any exogenous noise in the response process.</p><p>TCM-DR is not just a theory, but a quantitative account of the data. We validate the retrieval process by replicating empirical facts from free-recall experiments. The simulated model empirically explains classic recall-order phenomena like the recency, primacy, and temporal contiguity effects. Perhaps more interestingly, the model explains multiple dimensions of inter-response-time (IRT) data well. This includes the cross-subject distribution of IRTs and the average IRTs between list transitions by serial-position lag. A striking property of the model is that it can generate significant variation in response order and response times, both across subjects and within subjects across trials, despite relying on only two core parameters (one for each retrieval mechanism) that are identical across subjects. 1 The only difference between subjects is their pre-experimental contexts and inter-item associations (memories). The reason is as just discussed: small differences in pre-experimental contexts can lead to large differences in what is ultimately recalled and in how long it takes for a particular feature to converge during retrieval. TCM-DR therefore explains, and provides a convincing basis for, the significant heterogeneity in retrieval we see across people and time.</p><p>Our framework stands among a large set of frameworks modeling how information is accumulated and used to reach a decision, including drift-diffusion models and Bayesian and statistical learning models. We believe that TCM-DR is not simply an alternative to these models, but a memory-based microfoundation that elucidates their deeper psychological origins. To show this, we compare TCM-DR with each class of model in turn, explaining the close mapping between them and providing a memory-based interpretation for these models' parameters and mechanisms. This mapping is especially close in diffusion-based models of retrieval, in which the basic functions of iterative retrieval and competitive retrieval are largely captured by mutual inhibition and memory drift. These insights suggest an important role for memory in a broader set of decision-making settings.</p><p>2 TCM-DR: A model with dynamic retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Environment</head><p>We consider an agent (the subject) who exists within an environment over time. The subject partitions dimensions of the environment into elements y of a countable set Y. At any given time, the subject's objective is either (i) to perceive and encode information from the environment or (ii) to retrieve past information in response to a cue. We describe how the agent pursues each of these objectives in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding with temporal context</head><p>Encoding is the process by which the subject stores observations of experienced features and the associations formed between them. Most of this process follows the temporal context model of <ref type="bibr" target="#b5">Howard and Kahana (2002)</ref> and its successors (e.g., <ref type="bibr" target="#b17">Sederberg et al., 2008)</ref>.</p><p>For ease of exposition, let us index the features y i ∈ Y by i ∈ {1, . . . , n}. When the subject perceives feature y i , it evokes a feature representation f i ∈ R n . Each f i is a standard basis vector, so that these elementary features are orthonormal. 2 Complex objects (e.g., a yellow square) are composites of these elementary features. At any given time t, the latent mental state of the subject's brain is represented by an m-dimensional unit vector c t , which we call context. Each element of context represents neural activity, like the activation of a synapse or a network of neurons. Memory is represented by an m × n matrix M of associations between features and contexts. The rows of the memory matrix correspond to the dimensions of context, while columns correspond to dimensions of features; therefore, the (j, i)th element of M represents the strength of the association between context element j and feature y i . This means that column i of M codes the contextual (neural) pattern that the brain uses to represent that feature. As we will see, distinct features become associated with each other when their contextual representations in M become more similar -that is, when they form links in the brain.</p><p>This way of modeling the encoding process is not conceptually different from the standard temporal context model <ref type="bibr" target="#b5">(Howard &amp; Kahana, 2002;</ref><ref type="bibr" target="#b17">Sederberg et al., 2008)</ref>. We do, however, make one slight mathematical modification by assuming that vector lengths are taken with respect to the L 1 norm (as opposed to L 2 ), as in <ref type="bibr">Wachter and Kahana (in press</ref>). This is not essential to the psychological intuition, but does yield three benefits. First, it makes the mathematical dynamics of retrieval more tractable. Second, it means there is only one memory matrix used for both encoding and retrieval. 3 Third, it means that a non-negative unit vector can naturally be interpreted as a vector of probabilities, a fact later shown to link our model to Bayesian learning and inference and statistical decision theory.</p><p>Now consider a sequence of experienced features y t over time t ∈ N. In the model, the subject starts the encoding of this sequence of experiences with some pre-existing context c 0 and memory M 0 . 4 The encoding process then occurs recursively. The observation of feature y t = y i evokes f t = f i , which cues an input context from memory according to</p><formula xml:id="formula_0">c in t = M t−1 (Γ col t−1 ) −1 f t ,<label>(1)</label></formula><p>where Γ col t−1 is an n × n diagonal matrix with jth diagonal element equal to the sum of the elements in the jth column of M t−1 . The matrix Γ col t−1 serves only to normalize the columns of M such that c in remains a unit vector. Input context c in retrieves past contexts experienced contemporaneously with features similar to those of item i. This input then updates retrieved context c t , the time-t state of the brain. Retrieved context evolves as a vector autoregression with rate of decay ζ ∈ [0, 1]:</p><formula xml:id="formula_1">c t = (1 − ζ)c t−1 + ζc in t .<label>(2)</label></formula><p>As long as ζ is less than one, context is autocorrelated, so that temporally contiguous features are stored with similar contexts. The association between this context and the observed features are then re-encoded into memory as</p><formula xml:id="formula_2">M t = M t−1 + ϕ t c t f ⊤ t .<label>(3)</label></formula><p>Because f t = f i is the ith standard basis vector, this simply means c t is added to the ith column of the memory matrix. Before being added to memory, this new association is scaled by the coefficient ϕ t ≥ 1, which allows for the possibility that certain salient positions in the sequence of features may be encoded more strongly. In the standard temporal context model of free recall, this coefficient is the primacy gradient, taking the form</p><formula xml:id="formula_3">ϕ t = 1 +φ 0 e −φ 1 (t−1) .<label>(4)</label></formula><p>It increases the strength of stored associations for earlier list items, declining from 1 +φ 0 at the first item down toward 1 over the study phase.</p><p>After the feature y t is encoded, the subject observes y t+1 and the process repeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval and response process</head><p>Retrieval is the process by which features y i are brought to mind. Using current context and any accessible features (i.e., cues), the subject searches the memory matrix M to retrieve information. Re-initializing time to 0, the subject has an initial context vector c −1 and a memory matrix M 0 . If there is a cue y cue , then this cue elicits an updated context:</p><formula xml:id="formula_4">c 0 = (1 − ζ)c −1 + ζc in cue , where c in cue = M 0 (Γ col 0 ) −1 f cue .</formula><p>If there is no cue, then the subject initiates recall from the current mental context:</p><formula xml:id="formula_5">c 0 = c −1 .</formula><p>This retrieved context c 0 then retrieves a set of featuresf in 0 from memory:</p><formula xml:id="formula_6">f in 0 = M ⊤ 0 (Γ row 0 ) −1 c 0 ,<label>(5)</label></formula><p>where Γ row is analogous to Γ col but with row sums of M (again, this is simply to normalize the outputf in ). This retrieval step will yield high weight on those features with (i) high contextual similarity with c 0 and (ii) high historical frequency of observation. These features f in 0 are then transformed into a vector f in 0 by a competitive-retrieval step described below. As Section 3 will explain in detail, it is not sufficient for the subject to retrieve features in one step. Rather, retrieval requires the sequential sampling of information from memory.</p><p>In the model, the subject can do this by iteratively retrieving new features and contexts.</p><p>We call this process iterative retrieval. We assume that, when the brain is in a retrieval state, intermediate feature information is not re-encoded into memory. 5 This means that the matrix of associations remains fixed at M t = M 0 . The set of equations underlying retrieval is essentially the same as in the encoding phase. The only difference is that, instead of retrieving a context from features of the environment f , the subject uses the retrieved features f in at each iteration step. Thus, as in equations (1) and (2), the subject updates context according to</p><formula xml:id="formula_7">c in t = M 0 (Γ col 0 ) −1 f in t−1 , c t = (1 − ζ)c t−1 + ζc in t .<label>(6)</label></formula><p>This new context then retrieves new features:</p><formula xml:id="formula_8">f in t = M ⊤ 0 (Γ row 0 ) −1 c t .<label>(7)</label></formula><p>Retrieved featuresf in will be linear combinations of elementary basis features f i ; hence, they will not, in general, clearly correspond to any y i ∈ Y. This motivates a second step by whichf in t becomes f in t . In order to retrieve a recognizable feature f in ≈ f i , the brain will need to implement a "cleaning" procedure by which it maps the "fuzzy" featuresf in to standard basis vectors. This is a common process in models of cognitive psychology, like, for instance, Hopfield networks <ref type="bibr" target="#b12">(Masson, 1995;</ref><ref type="bibr" target="#b16">Rizzuto &amp; Kahana, 2001;</ref><ref type="bibr" target="#b6">Kahana, 2012)</ref>. We call this competitive retrieval because it a process whereby larger elements of the vectorf in suppress the recollection of smaller elements. Mathematically, there is a competitive-retrieval function</p><formula xml:id="formula_9">F : R n → R n generating retrieved features as f in t = F (f in t ).<label>(8)</label></formula><p>The function F maps the unit vectorf in into a unit vector that is closer to a standard basis vector. We give F the following properties:</p><p>1. F is differentiable at each element of the input vectorf in .</p><p>2. F increases the larger values off in and decreases the smaller values off in : for any i</p><formula xml:id="formula_10">and i ′ such thatf in (i) &gt;f in (i ′ ), f in (i) −f in (i) &gt; f in (i ′ ) −f in (i ′ ).</formula><p>The first assumption is for analytical convenience. The second implies that the largest element off in increases and, to keep f in at unit length, the smallest element decreases. An example of a function F satisfying these properties is the power rule:</p><formula xml:id="formula_11">f in t = F (f in t ) = (f in t ) η ||(f in t ) η || ,<label>(9)</label></formula><p>where η &gt; 1, || • || is the length of a vector under the L 1 norm, and exponentiation is an element-wise operation. 6 (When η = 1, we revert back to a case without competitive retrieval, in which f in =f in .) While our results do not depend on this specific functional form of competitive retrieval, we will continue to use it to model this step throughout the rest of the paper.</p><p>This set of equations defines how the subject retrieves f in t from f in t−1 , using context as an intermediate step. This recursive process iterates forward on the retrieved features until a response rule is satisfied. The response rule determines (i) the point at which the iteration described above should stop; and (ii) whether, upon stopping, the retrieved feature vector is recognizable as some f i (i.e., some y i ∈ Y). We assume two main criteria. First, retrieved features must converge to some stable vector f in ∞ . In particular, the process must reach some point at which ||f in t − f in t−1 || &lt; ε for some small threshold ε &gt; 0. Second, the convergent retrieved features must be sufficiently close to a feature from the environment (a standard basis vector) that it is recognizable enough to be recalled. This means that the maximum element of f in t must exceed some threshold f thresh close to 1. If and once these two criteria are satisfied, the subject recalls the item corresponding to the maximum element of f in t ; if the criteria are not satisfied after a given decision-time limit, nothing is recalled.</p><p>In summary, a response is completed either when convergence occurs or when some time limit is reached. To re-initiate the retrieval process and recall another feature, the subject uses the current context and an environmental cue (if available) to re-start the process described above. <ref type="bibr">7</ref> The feature cue may be the most recently recalled item (e.g., in free recall) or might be an attribute of the initial cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamics of the retrieval process</head><p>Having laid out the assumptions of the model, we now analyze the underlying mechanisms and their consequences for retrieval over time. We do this by mathematically characterizing and discussing the dynamics of the retrieval induced by iterative retrieval and competitive retrieval. These properties are transparently illustrated by way of a simple two-item example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evolution of retrieved features</head><p>By combining the equations constituting the retrieval process, one can express the temporal dynamics of retrieved features f in t autoregressively. This eliminates the need to keep track of intermediate latent contexts. In particular, substituting context evolution (6) and recalled features (7) into the expression (8), we have the relation</p><formula xml:id="formula_12">f in t = F (1 − ζ)f in t−1 + ζΦf in t−1 ,<label>(10)</label></formula><p>where the matrix</p><formula xml:id="formula_13">Φ = M ⊤ 0 (Γ row 0 ) −1 M 0 (Γ col 0 ) −1 .<label>(11)</label></formula><p>This is a feature-to-feature transition matrix that summarizes the mapping from one set of retrieved features, to an intermediate retrieved context, back to a new set of retrieved features. In other words, Φ summarizes the transition rates between features of the environment based on their shared historical associations.</p><p>What does the dynamical system (10) tell us about how retrieved features evolve over time?</p><p>We can first answer this by decomposing the changes implied by (10) into its con-</p><formula xml:id="formula_14">stituent mechanisms. Letting ∆f in t denote the temporal differencef in t −f in t−1 , we have ∆f in t = ζ(Φ − I)f in t−1 iterative retrieval + ζ(f in t−1 −f in t−1 ) competitive retrieval .<label>(12)</label></formula><p>The first term in this expression is the effect of iterative retrieval. Multiplication with the transition matrix Φ represents a sampling of experiences, through contextual associations, using the features currently in mind. On its own, it appears as a standard linear dynam-ical system (i.e., a vector autoregression). The second term of (12) represents competitive retrieval. By assumption, the function F (f in ) increases large elements off in and decreases small elements. Thus, the difference f in −f in is strictly positive for the largest element and strictly negative for the smallest, summarizing the change induced by F . Together, these effects pull retrieved features in two directions: toward those that are most easily sampled from memory and toward those that are currently most prominent.</p><p>To get a more direct interpretation of the evolution of f in , we can calculate the change in actual retrieved features ∆f in , as opposed to the intermediate step ∆f in . Appendix A.1</p><p>proves that ∆f in can be expressed in the form</p><formula xml:id="formula_15">∆f in t = D F (f in t−1 )ζ(Φ − I)f in t−1 iterative retrieval + D F (f in t−1 )ζ(f in t−1 −f in t−1 ) competitive retrieval +o t−1 ,<label>(13)</label></formula><p>where the term o t−1 represents small terms that disappear as the time interval between iterations approaches zero (i.e., the continuous-time limit). 8 The matrix D F is an n × n Jacobian matrix with elements</p><formula xml:id="formula_16">D F t (i, i ′ ) = ∂F (f in t (i)) ∂f in t (i ′ )</formula><p>.</p><p>This Jacobian D F summarizes the non-linearity induced by the transformation F . Indeed, noting that</p><formula xml:id="formula_17">∆f in t = D F (f in t−1 )∆f in t + o t−1 ,</formula><p>we can see that updating occurs effectively in two separate parts. Starting with a prior set of retrieved featuresf in t−1 , competitive and iterative retrieval first yield an intermediate set of retrieved featuresf in t , as described above. Then, these changes are mapped toward a unit vector to complete the competitive retrieval step. The matrix D F summarizes the directions in which each component travels to accomplish this. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Understanding the mechanisms</head><p>Iterative retrieval and competitive retrieval each play essential roles in shaping the dynamics of retrieved features. To show this, we examine each in isolation. First, we shut down competitive retrieval and study the effects of iterative retrieval; second, we shut down iterative retrieval and study competitive retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Understanding iterative retrieval</head><p>Suppose there is no competitive retrieval: f in =f in . Retrieved features evolve according to the simple vector autoregression (VAR):</p><formula xml:id="formula_18">∆f in t = ζ(Φ − I)f in t−1 .<label>(14)</label></formula><p>This is a linear dynamical system; to understand whether (and to what) it converges, we need to study the properties of the transition matrix Φ. Note first that, iterating (14) backward to time 1, we can rewrite the system as an explicit function of time:</p><formula xml:id="formula_19">f in t = A t f in 0 , where A = (1 − ζ)I + ζΦ and f in 0 = M ⊤ 0 (Γ row 0 ) −1 c 0 .<label>(15)</label></formula><p>Establishing unique convergence of this system amounts to studying the eigenvalues of A.</p><p>Note first that, for an n-dimensional vector of ones ι n , we have</p><formula xml:id="formula_20">ι ⊤ n Φ = ι n .</formula><p>That is, the columns of Φ all sum to one, and ι n is a left eigenvector of Φ with corresponding eigenvalue equal to one. From (15), the same must be true of the matrix A. The elements of Φ and A are strictly positive, so these are regular, column-stochastic matrices. It follows from standard linear algebra results that Φ and A each have a well-defined right-eigenvector corresponding to the eigenvalue of one. All other eigenvalues are strictly less than one, so this system does indeed converge to a fixed point.</p><p>More specifically, Appendix B proves that the dynamical system (15) always converges to a positive unit vector f in ∞ , which is the unique solution to the linear system</p><formula xml:id="formula_21">f in ∞ = Φf in ∞ .<label>(16)</label></formula><p>This result not only guarantees that the solution is unique, but also reveals that it is a function of the memory matrix M only. The initial context c 0 with which the subject initiates recall is irrelevant to the convergence of the retrieval process. We can think of Φ as revealing ergodic probabilities of encoded features: the long-run historical frequencies with which certain items have been observed over time. Thus, iterative retrieval on its own represents a process by which the brain sequentially samples information from memory until it obtains a sufficiently representative sample from history.</p><p>This context-independent property of iterative retrieval is ideal for recalling as much historical information encoded in memory as possible; however, it has two obvious drawbacks. Context also allows the subject to sequentially cue contiguous words in the list, recognizing that they are in some way associated. Competitive retrieval resolves both of these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Understanding competitive retrieval</head><p>Let us now shut down iterative retrieval and study the role of competitive retrieval. Again, we can express retrieved features as a function of time only:</p><formula xml:id="formula_22">f in t ∝ (f in 0 ) ηt , where f in 0 = M ⊤ 0 (Γ row 0 ) −1 c 0 ,<label>(17)</label></formula><p>where, as before, the exponent is taken element-by-element. Provided that the initial set of features f in 0 has a unique maximum value, this expression will converge to a unit vector f in such that 10</p><formula xml:id="formula_23">f in ∞ (i) =      1 if f in 0 (i) &gt; f in 0 (i ′ ), ∀i ′ ̸ = i, 0 otherwise.<label>(18)</label></formula><p>The most prominent feature brought to mind by initial cues and context "wins" the competition for retrieval.</p><p>In contrast to iterative retrieval, which yields a set of retrieved features that are entirely independent of the initial cues and context, competitive retrieval yields a set of features that are solely a function of initial cues and context. This initial context recalls a set of features from memory, and the brain then maps those features to items from the environment at a rate governed by η. As already mentioned, this process is analogous to the decoding of noisy information in a Hopfield network. Using available information in context c 0 , the brain decodes from memory a noisy representation f in 0 of an item y i from the environment Y. It then processes this noisy representation via a "cleaning" process to render it recognizable; this process takes time, so that recognition of the initial representation is not immediate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Understanding the combined effects</head><p>Iterative retrieval and competitive retrieval operate together, resulting in a recall process that depends on both long-run historical associations and the initial cues and context at the time of recall. Because of the non-linearities in this joint process, the system characterizing recall dynamics appears much more complicated. Still, we can show that it has stable convergence properties and that the ultimate point of convergence depends on the point of recall initiation.</p><p>Combining both mechanisms means that the dynamical system (10) converges to a positive unit vector f in ∞ , which is one of the solutions to the nonlinear system</p><formula xml:id="formula_24">f in ∞ = F (Φf in ∞ ).<label>(19)</label></formula><p>This system may have multiple solutions due to the non-linearity of the function F . Under 10 In the unusual, knife-edge case when f in 0 has N ≥ 2 identical maxima, the corresponding elements of f in ∞ become 1/N ≤ 1/2. Under our decision rule f thresh , this would be too small to recall any value, so nothing would be recalled.</p><p>our power rule, the expression (19) can be rewritten as the system</p><formula xml:id="formula_25">f in ∞ (i) n i=1 n i ′ =1 Φ(i, i ′ )f in ∞ (i ′ ) η = n i ′ =1 Φ(i, i ′ )f in ∞ (i ′ ) η</formula><p>for every i ∈ {1, . . . , n}. Expanding these sums and products results in a polynomial system with n equations in n unknowns. As this polynomial is of degree greater than one (because η &gt; 1), the solution need not be unique as in the linear (η = 1) case.</p><p>The particular solution to which this system converges over time, then, depends on the initial position f in 0 , which itself is a function of the initial context c 0 . Different initial contexts can yield dramatically different convergent retrieved features. Another way in which to view this is using the dynamics (13), which shows that the evolution of retrieved features is the net effect of an iterative retrieval term and a competitive retrieval term. Iterative retrieval moves current features in the direction implied by Φ, as described in Section 3.2.1. Competitive retrieval moves features toward a unit vector. The sum of these two effects determines not only the final vector to which the process will converge, but also the response time. If these effects move retrieved features in the same direction, convergence will be faster; if, instead, they work in opposite directions, convergence can be very slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Illustration: A two-item setting</head><p>To illustrate the convergence properties of our retrieval and response process more concretely, we examine it in a setting with only two items (n = 2). The example has the added benefit of helping to interpret our model in light of the common scenario in which an individual must choose between two given alternatives. The two-alternative forced choice (TAFC) task is one classic experimental example of this <ref type="bibr" target="#b1">(Bogacz, Brown, Moehlis, Holmes, &amp; Cohen, 2006)</ref>.</p><p>For simplicity, suppose that the memory matrix M 0 takes the symmetric form</p><formula xml:id="formula_26">M 0 ∝   1 − p p p 1 − p  </formula><p>for some p ∈ (0, 1). This means that the two item features have been experienced with equal historical frequency (have equal column sums) and have some shared associations (p / ∈ {0, 1}). Assume p ̸ = 1/2, so that these items do not have trivially identical representations in memory. This memory matrix implies a symmetric transition matrix</p><formula xml:id="formula_27">Φ =   1 − ϕ ϕ ϕ 1 − ϕ   ,</formula><p>where ϕ = 2p(1 − p) ∈ (0, 1/2). Notice that the ergodic probability vector for Φ -the</p><formula xml:id="formula_28">vector f IR ∞ satisfying f IR ∞ = Φf IR ∞ -is equal to f IR ∞ =   1/2 1/2   .</formula><p>We use the "IR" superscript to emphasize that this is the solution to the model with only iterative retrieval (η = 1). Absent competitive retrieval, all initial cues and contexts result in this set of retrieved features.</p><p>Because</p><formula xml:id="formula_29">f in t (1) = 1 − f in t (2)</formula><p>, we can describe the whole system with just one element; consider the first element f in t (1). From the fixed-point formula (19), we have that</p><formula xml:id="formula_30">f in ∞ (1) = [ϕ + (1 − 2ϕ)f in ∞ (1)] η [ϕ + (1 − 2ϕ)f in ∞ (1)] η + [1 − (ϕ + (1 − 2ϕ)f in ∞ (1))] η .<label>(20)</label></formula><p>In the special case of η = 2, for example, this fixed-point equation reduces to a cubic function of f in ∞ (1), to which there are three unique and distinct solutions. One solution is f in ∞ (1) = 1/2, the knife-edge case in which the memory-iteration and competitive-retrieval solutions are the same. The other two solutions are symmetrically above and below 1/2: one is in the interval (0, 1/2) and the other is in <ref type="figure" target="#fig_1">(1/2, 1)</ref>. Thus, these two final retrieved features vectors lie somewhere between the ergodic vector with f in ∞ (1) = 1/2 decoded from memory and the extreme binary outcomes f in ∞ (1) = 0 or f in ∞ (1) = 1 from competitive retrieval. How do retrieved features evolve between these fixed points? The difference equation (12) implies that, from t − 1 to t, the change in the first element off in equals</p><formula xml:id="formula_31">∆f in t (1) = ζϕ(1 − 2f in t−1 (1)) iterative retrieval + ζ(f in t−1 (1) −f in t−1 (1)) competitive retrieval .</formula><p>The iterative retrieval term is strictly positive for all f in t−1 (1) &lt; 1/2 and strictly negative for f in t−1 (1) &gt; 1/2. The opposite is true of the competitive-retrieval term: it is positive for f in t−1 (1) ∈ (1/2, 1) and negative for f in t−1 (1) ∈ (0, 1/2). At f in t−1 = 1/2, these effects are both zero, because f in t = f IR ∞ and both elements equal the maximum of f in t . Iterative retrieval dominates for f in (1) near 0 or 1 and competitive retrieval dominates for f in (1) near 1/2. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the dynamics of the retrieval process in this two-item example via a phase diagram. Panel A corresponds to the η = 1 case; Panel B corresponds to the η &gt; 1 case. In each case, the solid line represents the deviation of current retrieved features f in t (1) from the fixed-point condition (20). In particular, the deviation equals the difference</p><formula xml:id="formula_32">Deviation(f in t (1)) = [ϕ + (1 − 2ϕ)f in t (1)] η [ϕ + (1 − 2ϕ)f in t (1)] η + [1 − (ϕ + (1 − 2ϕ)f in t (1))] η − f in t (1).<label>(21)</label></formula><p>Of course, (20) implies that this deviation is equal to zero at every f in ∞ . The arrows in the figures illustrate that, between these fixed points, f in t (1) increases when the deviation is positive and decreases when it is negative. In the η = 1 case, regardless of the current (and therefore initial) condition, retrieved features converge over time to f IR ∞ , consistent with historical frequencies embedded in Φ. In the η &gt; 1 case, there are three possible points of convergence. The point f IR ∞ is an unstable fixed point: retrieved features converge here if and only if the initial context is such that f in 0 (1) = 1/2. The other two points are stable: for all c 0 such that f in 0 (1) &lt; 1/2, retrieved features converge to the low fixed point and the subject chooses item 2; and for all f in 0 (1) &gt; 1/2, features converge to the high fixed point and the subject chooses item 1.</p><p>This two-item example is evidently stylized and hence abstracts from some of the complexities of settings with many items. As the dimensionality of the feature space (and perhaps η) becomes larger, the number of possible fixed points and the dynamics between them will change, as we will see in our quantitative evaluation of the model. This will yield a larger variance in both the possible convergence points and the time it takes to reach them. Still, the core logic remains the same. The interaction of iterative retrieval and competitive retrieval represents a trade-off among three forces: the decoding of historical information from memory, the mapping of internal representations to features from the environment, and the utilization of current context to make more relevant decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application to free-recall data</head><p>To validate our framework, we apply TCM-DR to the experimental setting of a free-recall experiment. Our objective with this application is to study whether the model is capable of quantitatively explaining patterns in recall order and inter-response times. To do this, we calibrate the model parameters and simulate multiple subjects studying and recalling a list of items. We find that the model simulation reproduces multiple dimensions of responses and inter-response times (IRTs) in data from a standard free-recall experiment. In particular, it is able to match the substantial within-and between-subject variation in recall behavior observed in the data, despite being fully deterministic and relying on only two core parameters (ζ and η) that are assumed to be identical across time and subjects. Importantly, our goal with this application is to explain as much as possible without adding additional assumptions to the model. Thus, this version of the model does not include the full set of machinery incorporated in state-of-the art temporal context models like TCM-A <ref type="bibr" target="#b17">(Sederberg et al., 2008)</ref> and CMR <ref type="bibr" target="#b13">(Polyn, Norman, &amp; Kahana, 2009)</ref>. For example, we do not extend our model to include the forward-asymmetry parameter (γ FC ) or the weighting of temporal versus source context (L FC ) that help these models to explain additional facts. Adding these ingredients to TCM-DR would be straightforward, but would do little to help with the validation exercise in this section.  <ref type="table" target="#tab_1">Table 1</ref> summarizes the calibration of our model and the relevant assumptions behind the simulation. As shown in Panel A, the parameters governing the encoding and retrieval processes are the rate of context updating ζ, the power rule for competitive retrieval η, and the two parameters governing primacy ϕ from (4). Primacy parameters are chosen to match recall probabilities by serial position. As we showed above, higher ζ and η will govern average inter-response times. Lower ζ will also increase the strength of temporal contiguity, as low ζ means context is more correlated during the study and retrieval phases. Higher η increases the within-and cross-agent variance of recalls and IRTs. These moments determine which parameter combination can best explain our set of empirical facts. Unif. dist. Panel B of <ref type="table" target="#tab_1">Table 1</ref> summarizes the response-rule parameters. Recall that the retrieval and response process converges when ||∆f in t || is less than some very small number ε; we choose ε = 0.001. Conditional on convergence, the maximum element of f in t is chosen if and only if that element exceeds some threshold f thresh conv . We choose a threshold of 0.8. Finally, in the rare case that the process is not yet convergent but retrieved features are nearly identical to a unit vector, the subject recalls that item "early." We consider this to have happened if the maximum element of such a vector exceeds f thresh early = 0.999. 11 Under this calibration, we can simulate the experiment for multiple subjects. Consider first Panel C of the table. We assume an experiment with n = 25 distinct items and a latent context with m = 25 different elements. 12 In our simulation, all subjects study a list of n = 25 words and, as Section 2 describes, subsequently attempt to freely recall as many of these items as possible, in any order. After a successful retrieval, the column of M corresponding to the just-recalled item is suppressed near zero to prevent an unreasonable amount of repetition. Strictly speaking, this suppression step is not necessary to explain recall behavior, but does reduce average inter-response times between retrievals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and experimental methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model calibration and simulation</head><p>Subjects differ in their initial (pre-experimental) contexts and memories. This is the only dimension along which subjects are heterogeneous; therefore, all dispersion in recall order and IRTs can be traced back to these differences. As stated in Panel D, we assume that the subject-specific initial conditions are simply drawn at random from a set of independent uniform distributions. We choose a uniform distribution to emphasize that we do not need to impose a specific kind of structural cross-subject heterogeneity to generate realistic recall behavior. The randomness of the initial contexts reflects the fact that each subject comes into the laboratory with a unique set of recent experiences. <ref type="bibr">13</ref> The memory matrix is an equally weighted average of a common component (an identity matrix), reflecting distinctiveness of the items; and an idiosyncratic component (a uniform random matrix), reflecting differences in prior associations between words that happen to be presented on the list. Note that, because we do not specify the actual items in the list, we do not take a stand on the innate semantic similarity between list items. One could easily add that possibility to the model by assuming that, in the common component of subjects' initial memory matrices, certain items tend to share certain contexts. 14</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results: Free-recall responses 4.3.1 Recall order</head><p>We first show that the simulated model explains classic phenomena characterizing the order in which subjects recall items. This includes the recency and primacy effects present in probabilities of first recall and the temporal contiguity effect present in lag conditional response probabilities (CRPs). The model also succeeds in explaining other facts (e.g., serial position curves). We focus on these two because they illustrate two essential properties of the model: the context-dependence of retrieval and the importance of temporal contiguity for associative learning. We assume immediate free recall throughout, but it is straightforward A. First recall probabilities B. Conditional response probabilities <ref type="figure">Figure 3</ref>: A. Probability of first recall in the model. Serial positions correspond to the order in which an item was presented during study. B. Lag conditional response probabilities (CRPs) in the model. Each lag CRP is the probability of making a transition from item i to item i + Lag during recall, conditional on that transition being available. Both panels A. and B. assume immediate free recall.</p><p>to introduce a distractor into the model, which will serve to suppress recency effects.</p><p>Panel A of <ref type="figure">Figure 3</ref> plots the model-implied probability of first recall. Because this is immediate free recall, the context used to initiate retrieval is very similar to the subject's context at the time the last list item was encoded, so the last item is very likely to be recalled first. This is the recency effect. Importantly, the most recent item is not guaranteed to be the first-recalled item: some subjects first recall items in other serial positions, and a noticeable number start at the beginning of the list (a primacy effect). In our model, small between-subject differences in pre-experimental contexts and associations lead to noticeable differences in realized responses.</p><p>Panel B of <ref type="figure">Figure 3</ref> plots the lag CRPs. The CRP at lag j represents the probability that a subject recalls item i+j after having just recalled item i, conditional on that transition being possible. The peaks at lags ±1 characterize the temporal contiguity effect: subjects are more likely to make transitions to words that were encoded nearby in time. This is a consequence of contextual autocorrelation through the parameter ζ. There is a slight forward asymmetry, both in our model-implied CRP and in the data: subjects tend to advance forward in the list slightly more often than they move backward. The CRPs also increase for extreme negative lags, since the primacy effect increases the likelihood that subjects will transition to words at the beginning of the list. 15 Overall, the recall probabilities and CRPs generated by the model accord well with corresponding plots in the data (e.g., <ref type="bibr" target="#b9">Kahana et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Response times</head><p>TCM-DR is certainly not the first context-based model to explain recency, primacy, or temporal contiguity effects. It is, however, the first to do so without appealing to an exogenous one-step or diffusion-based retrieval process to generate variation in the simulated data. A more revealing test of our model is whether it can also explain variation in inter-response times (IRTs), both between subjects and within subjects across trials. <ref type="bibr">16</ref> We find that the model explains multiple dimensions of the response-time data well, including the distribution of IRTs and the average IRTs between retrievals of varying lags. Note that our model characterizes time in terms of number of iterations, while the data are in seconds. To match average response times, we assume that 1 second in the data corresponds to 10 iterations.</p><p>Panel A of <ref type="figure">Figure 4</ref> compares the IRT distributions in the data and model. The data suggest a highly variable, right-skewed distribution. 17 In the model, small differences in initial context lead to large differences in IRTs. Sometimes, initial context is such that the iterative retrieval and competitive retrieval effects work in the same direction, or one dominates, resulting in a fast response; other times, they work in offsetting directions so that convergence takes a long time.</p><p>Panel B of <ref type="figure">Figure 4</ref> compares the average lag IRTs in the data and model. These curves represent the response-time analogues to the CRP curve above: each point at lag j is the average time taken between a transition from item i to item i + j. IRTs tend to be much shorter for close transitions, since these items tend to be encoded with similar contexts and therefore are easy to retrieve in succession. IRTs also tend to be slightly shorter at long lags (from one end of the list to the other) because of the primacy and recency effects.</p><p>15 The model fails to produce the fact that CRPs tend to be slightly elevated for extreme positive lags. The reason for this is that we do not include the forward-asymmetry parameter in the model that is included in more recent versions of the temporal context model (e.g., <ref type="bibr" target="#b17">Sederberg et al., 2008;</ref><ref type="bibr" target="#b13">Polyn et al., 2009)</ref>.</p><p>16 Drift-diffusion models typically achieve this by assuming exogenous stochastic drift rates across agents and trials <ref type="bibr" target="#b15">(Ratcliff &amp; Tuerlinckx, 2002)</ref>; our model is able to achieve this solely by introducing heterogeneity in pre-experimental context.</p><p>17 It is clearly non-normal, as shown by the normal fit and quantile plot in the data figure.</p><p>Overall, even without any parameter heterogeneity or exogenous noise, TCM-DR succeeds at capturing response-time behavior in recall. 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with existing decision models</head><p>An important contribution of TCM-DR is that it can be used to better understand existing models of recall and decisions through the lens of the principles of memory. This section discusses how our model relates to two well-known categories of models: diffusion-based psychological models of retrieval and response and models of Bayesian and reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Diffusion-based models of retrieval and choice</head><p>The iterative retrieval process at the center of TCM-DR sheds light on existing models of retrieval and choice that appeal to separate models with reduced-form drift rates and exogenous noise. As a state-of-the-art example, let us consider the leaky-accumulator model of <ref type="bibr" target="#b17">Sederberg et al. (2008)</ref> and <ref type="bibr" target="#b13">Polyn et al. (2009)</ref>, which is based on the decision process in <ref type="bibr" target="#b19">Usher and McClelland (2001)</ref>. In this model, the subject first retrieves a set of features f in 0 from memory, as in (5). Instead of iteratively updating these features, though, the subject now initiates and keeps track of a new state variable x t , an n-dimensional vector with ith element corresponding to the ith item feature. Let ∆x t denote the change x t − x t−1 .</p><p>Beginning with initial condition x 0 = ⃗ 0, x evolves autoregressively according to the process</p><formula xml:id="formula_33">∆x t = −τ (κ + λ(L − I))x t−1 mutual inhibition + τ C V f in 0 memory + τ σϵ t noise ,<label>(22)</label></formula><p>x</p><formula xml:id="formula_34">t (i) → max{x t (i), 0}, ∀i.<label>(23)</label></formula><p>This retrieval process ends in a response once one of the accumulators x t (i) reaches a threshold, at which point that item is recalled.</p><p>The first term on the right-hand side of (22) represents the mutual inhibition of item features: κ &gt; 0 modulates the extent to which individual features inhibit themselves, while λ &gt; 0 and L (a matrix filled with ones) govern how different features inhibit each other. The second term is the drift toward the initial retrieved-features vector f in 0 . It is scaled by the coefficient of variation C V of f in 0 , a scalar which is larger when f in 0 is closer to a standard basis vector. This speeds up convergence when the initial feature representation is already close to an item from the environment. The third and final term is an exogenous diffusion with vector of shocks ϵ t iid ∼ N (0, I) scaled by standard deviation σ. The scalar τ &gt; 0 governs the speed of convergence of the entire process.</p><p>This conventional diffusion-based model incorporates a striking number of mechanisms that arise endogenously from our memory model. We discuss these similarities between <ref type="formula" target="#formula_15">13</ref>and each element of the process (22) in turn. The first mechanism is mutual inhibition of item features with themselves and each other. In our context-based model, this occurs through both iterative retrieval and competitive retrieval. As we have shown above, iterative retrieval causes retrieved features to drift toward the historical distribution of features implied by memory through Φ. If a given feature element f in t (i) is "too large" relative to this distribution, it will be suppressed relative to other elements. Unlike in the diffusion-based model, some features will have to grow, but this is only because we need to keep the vector on the unit circle. <ref type="bibr">19</ref> Competitive retrieval also has an obvious mutual-inhibition-like feature in the property that large features inhibit small features.</p><p>The second mechanism in (22) is the drift term proportional to f in 0 . In a sense, this term constitutes a simplified view of our model. The initial retrieved-features vector f in 0 is the result of the first iteration of contextual retrieval; it reflects non-iterative decoding from memory. Like the competitive retrieval rule, it results in a recall process that puts weight on the initial context c 0 in the ultimate recall decision. The coefficient of variation C V operates in much the same way as competitive retrieval via η, which more strongly directs retrieved features toward a standard basis vector if f in 0 is itself closer to a standard basis vector.</p><p>The third component of (22) is the diffusion term σϵ t , which induces a dispersion in both choices and decision times across trials and across subjects. To an extent, our model provides a microfoundation that does not need exogenous shocks to explain this dispersion. For a given subject, cross-trial choice and decision-time variability arises naturally from changing temporal context. Between subjects, choices and decision times will vary due to differences in pre-experimental context and memory. As we have shown, the parameter η increases the variability of responses and response times in the same way as σ. The two-item example showed clearly how η &gt; 1 increases the number of possible vectors to which f in can converge, explaining the variance in retrieved features. The non-linearity introduced by competitive retrieval also leads to varying convergence speeds that depend on the initial condition.</p><p>Finally, there is an obvious correspondence between the parameter τ in the diffusion-based model and the rate of context updating ζ. When individuals update their contexts quickly, they have faster response times. 20 Similarly, the competitive-retrieval parameter η scales the updating process (the Jacobian D F scales with η), increasing the speed of updating. 21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models of Bayesian and reinforcement learning</head><p>TCM-DR is a psychology-based model of learning and inference. The standard precedents for this are Bayesian learning, reinforcement learning, and statistical decision theory. We show that the encoding and retrieval processes underlying TCM-DR have a very natural relation to these frameworks. This comparison is useful for understanding the extent to which these mechanisms might be supported by principles of formal statistical learning.</p><p>We can see the Bayesian view by interpreting memory and context as arrays of probabilities. 22 Let p t denote the subject's time-t subjective probability measure. Recall that the rows of the memory matrix M correspond to elements of context, while the columns correspond to item features. Hence, each element of M represents the frequency with which a given item has been observed with a given context. Let us defineM as the memory matrix M rescaled so that all of its elements sum to one:</p><formula xml:id="formula_35">M t = M t ι ⊤ m M t ι n ,</formula><p>where ι k is a k-vector of ones. It follows that each element ofM represents the historical, unconditional joint probability of observing a given item in a given context:</p><formula xml:id="formula_36">M t (j, i) = p t (c j , f i ),</formula><p>wherec j denotes the jth m-dimensional standard basis vector. Under the additional assumption that elements of context represent unconditional probabilities over context elements (i.e., c t (j) = p t (c j )), we show that the context-based model implies Bayesian learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Encoding as Bayesian learning</head><p>Suppose features y i are presented and encoded in increasing order. Upon the perception of feature y i the subject retrieves input context c in i as the ith column of the normalized matrix</p><formula xml:id="formula_37">M i−1 (Γ col i−1 ) −1 (recall (1)). Because M i−1 (Γ col i−1 )</formula><p>−1 scales the columns of M i−1 by their total frequency, Bayes' rule implies that the jth element of its ith column equals p i (c j |f i ), the conditional probability of the jth context given the observation of item i. Context then updates as a weighted average of input context and past context, such that</p><formula xml:id="formula_38">c i (j) = p i (c j ) = (1 − ζ)p i−1 (c j ) + ζp i (c j |f i ).</formula><p>This is of the same form as standard temporal-difference reinforcement learning rules. 23 This context is then interacted with features and stored back into memory aŝ</p><formula xml:id="formula_39">M i = (1 − θ i )M i−1 + θ i c i f ⊤ i ,</formula><p>where the weights</p><formula xml:id="formula_40">θ i = ϕ i ι ⊤ m M 0 ι n + i i ′ =1 ϕ i ′ .</formula><p>This is another temporal-difference learning rule, except that now the learning rate declines over time. <ref type="bibr">24</ref> Notice that the ith column of the matrix c i f ⊤ i simply equals p i (c j ). This means that the joint probabilities stored inM becomê</p><formula xml:id="formula_41">M i (j, i ′ ) = p i (c j , f i ′ ) =      (1 − θ i )p i−1 (c j , f i ′ ) + θ i p i (c j ) if i ′ = i, (1 − θ i )p i−1 (c j , f i ′ ) if i ′ ̸ = i.</formula><p>The unconditional probability of observing the current item increases, and this increase is distributed across context rows in proportion to the current context vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Retrieval as Bayesian inference</head><p>We begin by shutting down competitive retrieval (η = 1). During the retrieval and response phase, the subject begins by retrieving a set of features f in 0 from the initial context c 0 and the normalized matrix</p><formula xml:id="formula_42">M ⊤ 0 (Γ row 0 ) −1 . By Bayes' rule, the columns of M ⊤ 0 (Γ row 0 ) −1 store the conditional probabilities p(f i |c j )</formula><p>. Therefore, again using Bayes' rule, retrieved features equal</p><formula xml:id="formula_43">f in t (i) = m j=1 p(f i |c j )p t (c j ) = p t (f i ).</formula><p>That is, f in is simply a vector of individual item probabilities.</p><p>We now turn to the role of iterative retrieval. Recalling the definition of Φ from (11), some algebra reveals that</p><formula xml:id="formula_44">Φ(i, i ′ ) = m k=1 p(f i |c k )p(c k |f i ′ ).</formula><p>In words, elements of Φ map features f to contexts c (through p(c|f )), then map contexts back to features (through p(f |c)) in probability space. To see this more clearly, consider the updating rule for ∆f in , which we can write out as</p><formula xml:id="formula_45">f in t = (1 − ζ)f in t−1 + ζΦf in t−1 .</formula><p>At the beginning of period t, the subject has a set of time-(t − 1) probability assessments stored in f in t−1 . Let us denote these by f in t−1 (i) = p t−1|t−1 (f i ) to emphasize that these probabilities were formed at t − 1 using contemporaneous information. The subject then uses Φ to extract new information from these probabilities via Bayes' rule: the ith element of the product</p><formula xml:id="formula_46">Φf in t−1 is [Φf in t−1 ](i) = n i ′ =1 m k=1 p(f i |c k )p(c k |f i ′ )p t−1|t−1 (f i ′ ) = m k=1 p(f i |c k )p t|t−1 (c k ) = p t|t−1 (f i ).</formula><p>The notation p t|t−1 emphasizes that this is an updated probability at time t using information from the previous period. Finally, the subject weights this updated probability with the previous assessment to get a fully updated probability</p><formula xml:id="formula_47">f in t (i) = p t|t (f i ) = (1 − ζ)p t−1|t−1 (f i ) + ζp t|t−1 (f i ).</formula><p>Yet again, we have a temporal-difference learning rule, this time over item probabilities.</p><p>When we reintroduce competitive retrieval, probabilitiesp t|t (i) =f in t (i), calculated as above, undergo the nonlinear transformation F to generate decision probabilities</p><formula xml:id="formula_48">p t|t (i) =p t|t (i) η n i ′ =1p t|t (i ′ ) η .</formula><p>While this kind of transformation is not a standard element of Bayesian learning, it is a very common element of the statistical learning literature, going back as far as <ref type="bibr" target="#b3">Bush and Mosteller (1955)</ref> and <ref type="bibr" target="#b11">Luce (1959)</ref>. This kind of transformation is also commonplace in the reinforcement-learning literature, which assigns choice probabilities via a convex transformation of perceived action values (e.g., a softmax function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding remarks</head><p>Although theories of memory have come to account for varied aspects of memory encoding and retrieval, these theories have not offered a mechanistic account of how the memory system retrieves encoded information and maps it to the environment. Various retrieval rules that have been offered merely assume that, somehow, the memory system retrieves the most similar target among a sea of competitors. Here we propose a solution to this problem within the setting of retrieved-context theories of memory retrieval.</p><p>The idea behind our model is simple: the same processes that underlie the encoding of information into memory also underlie retrieval. Individuals use memory to call experiences to mind, and those recalled experiences serve as a jumping-off point for the recollection of more experiences. People follow a train of thought until it leads to the recovery of information that is useful for the task at hand.</p><p>We formalize this notion of following a train of thought via two new mechanisms. The first is an iterative retrieval process, whereby experiences are sequentially sampled from memory.</p><p>The presence of a latent mental context serves an important role in this sequential sampling, because it facilitates the mapping between features of the environment and representations in the mind. Context retrieves features, then features retrieve context, and so on.</p><p>The second core mechanism underlying our model is a competitive retrieval process.</p><p>Under competitive retrieval, those features that come to mind most prominently suppress the recollection of less prominent features. Competitive retrieval serves two purposes. First, it allows individuals to map retrieved information back to recognizable features from the environment. Second, it ensures that the features that do ultimately come to mind are related to the circumstances at the time of retrieval. This is especially important in decision-making situations, because most decisions require context-dependent information.</p><p>To validate our framework, we apply the model to the standard setting of a free-recall experiment in which a subject studies a series of items and subsequently attempts to recall as many as possible in any order. Simulating a reduced-form version of retrieved-context theory that includes our iterative retrieval model produces the classic effects of recency, primacy, and temporal contiguity. Of particular interest in the present study, we examined whether the model could also produce the fat-tailed distribution of inter-response times seen in the data and the faster inter-response times seen when transitioning between neighboring list items. This is indeed the case, suggesting that our framework can provide an accurate quantitative account of the retrieval and response process.</p><p>Our framework sheds light on the role of memory in evidence-accumulation models of retrieval and decisions (e.g., <ref type="bibr" target="#b2">Brown &amp; Heathcote, 2008;</ref><ref type="bibr" target="#b14">Ratcliff, 1978;</ref><ref type="bibr" target="#b19">Usher &amp; McClelland, 2001</ref>). In diffusion-based retrieval models, some initially retrieved information serves as the input to a noisy accumulation process that evolves to a boundary. We show how the key mechanisms in diffusion-based models can be mapped back to our two novel mechanismsiterative and competitive retrieval -suggesting that our framework can explain how the memory system underlies these popular accumulation processes. Notably, our model features no exogenous noise in the retrieval process and therefore provides a new explanation for the large variation in choices and response times observed in many settings.</p><p>By better understanding how the memory system is involved in retrieval and response times, we believe that our framework makes an important stride in reconciling principles of memory with principles of decision-making. The encoding and retrieval of information no doubt plays a central role in how individuals make choices. Our hope is that the present framework can serve as a stepping stone for future work pursuing the much loftier goal of understanding how memory shapes decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We thank Sudeep Bhatia, Greg Cox, David Halpern, Benjamin Hébert, Marc Howard, Andrei Shleifer, Jessica Wachter, and conference and seminar participants at the Context and Episodic Memory Symposium, the Society for Mathematical Psychology Conference, and the University of Pennsylvania for helpful comments.</p><p>where U is an orthonormal matrix of eigenvectors and Λ is a diagonal matrix of eigenvalues:</p><formula xml:id="formula_49">Λ(i, i) =      1 if i = 1, λ i ∈ (−1, 1) if i ̸ = 1.</formula><p>Using the fact that U U ⊤ = I, it follows that</p><formula xml:id="formula_50">A = (1 − ζ)I + ζU ΛU ⊤ = UΛU ⊤ , whereΛ = (1 − ζ)I + ζΛ =      1 if i = 1, λ i = (1 − ζ) + ζλ i if i ̸ = 1.</formula><p>That is, A also has an eigenvalue decomposition with one eigenvalue equal to one and all others |λ i | &lt; 1, and with corresponding eigenvectors identical to those of Φ. In particular, this means that lim t→∞Λ t = lim t→∞ Λ t , and so</p><formula xml:id="formula_51">f in ∞ = lim t→∞ A t f in 0 = lim t→∞ UΛ t U ⊤ f in 0 = lim t→∞ U Λ t U ⊤ f in 0 = lim t→∞ Φ t f in 0 .</formula><p>In other words, f in ∞ is also the right eigenvector of Φ corresponding to its eigenvalue of one, and is hence the unique vector satisfying</p><formula xml:id="formula_52">f in ∞ = Φf in ∞ ,</formula><p>as claimed in the main text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>First</head><label></label><figDesc>, it fails to map retrieved features f in to items from the environment. It leads instead to combinations of item features distributed according to their historical frequencies. Second, it eliminates the ability of the brain to use cues and temporal context as important conditioning information. In most decision-making settings, one should presumably rely on context. Take as an example a free-recall task. Context in the recall phase allows the subject to recall items in the most recent list, as opposed to items in other lists or outside of the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Phase diagrams for the two-item example with and without competitive retrieval. The solid lines are the deviation of f in t (1) from a convergent fixed point, as defined in (21). Each intersection of these lines with zero represents a fixed point f in ∞ (1). Arrows along the zero line represent the direction in which f in t (1) moves over time given that initial position. Panel A assumes η = 1 and Panel B assumes η = 2. Both plots assume ϕ = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Data are taken from Experiment 4 of the Penn Electrophysiology of Encoding and Retrieval Study (PEERS4), a high-quality dataset comprising 682,032 spoken responses gathered from 98 subjects who each participated in 23 experimental sessions. This study involved delayedrecall of long lists (24 items), making trials with perfect recall rare. Accuracy data from this experiment are reported by<ref type="bibr" target="#b8">Kahana, Aggarwal, and Phan (2018)</ref> and<ref type="bibr" target="#b0">Aka, Phan, and Kahana (2021)</ref>; response-time data are analyzed in detail in<ref type="bibr" target="#b4">Greene, Goldman, and Kahana (2024)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>STANDARD MODELS ENVIRONMENT PERCEPTION PERCEPTION ENVIRONMENT OUR MODEL</head><label></label><figDesc></figDesc><table><row><cell>Encoding information</cell><cell>Encoding information</cell></row><row><cell>RETRIEVAL</cell><cell>RETRIEVAL</cell></row><row><cell>Features retrieved from memory</cell><cell>AND RESPONSE</cell></row><row><cell></cell><cell>Features are retrieved from memory</cell></row><row><cell></cell><cell>iteratively until thoughts converge to an</cell></row><row><cell></cell><cell>item from the environment</cell></row><row><cell>RESPONSE</cell><cell></cell></row><row><cell>Retrieved features enter a separate decision process</cell><cell>DECISION</cell></row><row><cell>(E.g., winner-take-all, DDM, mutual inhibition (O-U))</cell><cell>Iterative feedback</cell></row><row><cell>DECISION</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Calibration of the simulated model. See the main text for details.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is in contrast with standard drift-diffusion models, which assume exogenous noise and often assume stochastic drift rates that vary across subjects and trials<ref type="bibr" target="#b15">(Ratcliff &amp; Tuerlinckx, 2002)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This assumption does not mean features can have no innate similarity; semantic similarity between attributes will show up as shared contextual representations in memory.3 In the classic temporal context model, feature-to-context and context-to-feature associations are stored separately in the matrices M FC and M CF , respectively. Because we use the L 1 norm, M FC is the transpose of M CF , so we need only keep track of one matrix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In a free-recall experiment, for instance, M 0 will be a subject-specific matrix of pre-experimental semantic associations between words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This assumption is consistent with recent research showing that encoding or retrieval (but not both) can happen at a given time<ref type="bibr" target="#b10">(Long &amp; Kuhl, 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">That is, the operation takes the power of each individual element: for some arbitrary vector x, the ith element of x η equals x(i) η .7 Additional retrieval and response criteria may be added depending on the situation. For example, in a free-recall task, one usually assumes that the subject will suppress already-recalled features in M , preventing repetition. We implement this in the application in Section 4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Equivalently, the term o t−1 represents second-and higher-order terms in a Taylor expansion of f in t . 9 Appendix A.2 derives the explicit form of D F under our power rule with parameter η.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We add this only because it is reasonable; it does not impact the results.12 The number of context elements is not particularly important, so long as this number is larger than the number of features m ≥ n. If m &lt; n, then the subject can only form a limited number of distinct inter-item associations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">For instance, someone who came to the lab from the zoo will likely start with a context closer to that of the word "lion" than someone who just came from the beach.14 Again, item features themselves are represented by standard basis vectors, so no two items are explicitly similar in feature space; rather, semantic similarity is captured by shared contexts between items in M .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">Like the CRPs inFigure 3, the absence of the forward-asymmetry parameter of TCM-A and CMR means that we get slightly longer IRTs for positive lags than for negative lags in our model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">More formally, one could conceive of an isomorphism T : R n → R n that maps unit vectors f in t to some non-unit vectors x t . It may be that all elements of x decrease from one period to the next, even though some elements of f in must increase. Because T is an isomorphism, ∆f in and ∆x are equivalent ways of describing the retrieval dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">One might think, then, that ζ = 1 is "optimal" for decision-makers who value faster response times; however, there may be a trade-off because high ζ also means low autocorrelation of context during encoding, which is problematic if decision-making depends on understanding contiguity of events. Thus, the "optimal" ζ is likely strictly between 0 and 1.21 Unlike ζ, however, η also governs the degrees of mutual inhibition and decision variance, so that its total effect on the decision process is much more complicated than just multiplying the response time.22  Wachter and Kahana (in press) give this same interpretation to their context-based encoding process. In that paper, context is interpreted as an unobservable feature, not a neural state. The math is the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23">In the language of the reinforcement-learning literature, we may think of p i−1 (c j ) − p i (c j |f i ) as a prediction error and of ζ as a learning-rate parameter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">Again comparing this to reinforcement learning, the prediction error is now c i f ⊤ i −M i−1 and the learning rate is θ i . This parameter is a decreasing function of time, so this is a constant-gain learning rule.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (i) ηf in t (i ′ ) η−1 ( n i ′ =1f in t (i ′ ) η ) 2 if i ̸ = i ′ , ηf in t (i) η−1 n i ′ =1f in t (i ′ ) η − ηf in t (i) 2η−1 ( n i ′ =1f in t (i ′ ) η ) 2 if i = i ′ . (A.11)   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. IRT distribution (model) A. IRT distribution (data) B. Average IRT by transition lag (model) B. Average IRT by transition lag (data) <ref type="figure">Figure 4</ref>: Inter-response times (IRTs) in the model and data. Panel A plots the distribution of IRTs in seconds (1 iteration = 0.1 seconds in the model). In the data panel (right), the normal-distribution fit and quantile plot display the non-normality of the distribution. Note that the scales of the y-axes are different for the model and data because the model panel is a histogram frequency and the data panel is a smoothed density. Panel B plots the average inter-response time for transitions between item i and item i + Lag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A Derivations for Section 3</head><p>A.1 Derivation of retrieved-feature dynamics</p><p>We first translate the expressions from Section 2 to their continuous-time analogues. Note that we can substitute the expression (1) into (2) and rewrite it in terms of changes:</p><p>The continuous-time equivalent of this context evolution is:</p><p>To get the evolution of f in t , we need only take a time derivative; this is feasible due to the assumption that the competitive-retrieval function F is differentiable at all of its elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each element of f in</head><p>t is (potentially) a function of all the elements off in t . Thus, by the chain rule, the derivative of the ith element of f in t with respect to time t is</p><p>Substituting (A.2) into this expression, we have</p><p>where Φ row (i) is the ith row of the matrix Φ, as defined in equation <ref type="formula">11</ref>. In matrix notation, this means that the full system evolves according to</p><p>where D F is an n × n Jacobian matrix with elements</p><p>Adding and subtracting ζD F (f in t )f in t dt to the right-hand side of (A.7), we have</p><p>the continuous-time version of the discrete-time process (13) stated in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Derivation for power rule</head><p>If the competitive-retrieval rule F takes a power form (9), then</p><p>In this case, the derivatives of D F are</p><p>Now, using the definition of f in , this simplifies to</p><p>Collecting terms,</p><p>We can re-express this in matrix notation as</p><p>where ι n is an n-dimensional column vector of ones and D ratio (f in t ) is an n × n diagonal matrix with ith diagonal</p><p>B Proof of convergence in iterative retrieval Recall (15) from the main text: absent competitive retrieval (i.e., for η = 1), retrieved features at time t equal f in t = A t f in 0 , A = (1 − ζ)I + ζΦ.</p><p>Because, as we explain in the main text, Φ has one eigenvalue equal to one and all other eigenvalues less than one in magnitude, it has an eigenvalue decomposition Φ = U ΛU ⊤ ,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting recall of words and lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000964</idno>
		<ptr target="http://dx.doi.org/10.1037/xlm0000964" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="765" to="784" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moehlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.113.4.700</idno>
		<ptr target="https://doi.org/10.1037/0033-295x.113.4.700" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="700" to="765" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The simplest complete model of choice reaction time: Linear ballistic accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stochastic models for learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Inter-response times in free recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
	<note>Manuscript in preparation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A distributed representation of temporal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmps.2001.1388</idno>
		<ptr target="https://doi.org/10.1006/jmps.2001.1388" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="299" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Foundations of human memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computational models of memory search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-010418-103358</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="138" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The variability puzzle in human memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000553</idno>
		<ptr target="https://doi.org/10.1037/xlm0000553" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1857" to="1863" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Penn electrophysiology of encoding and retrieval study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Lohnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Broitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Crutchley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Weidemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename></persName>
		</author>
		<idno type="DOI">10.1037/xlm0001319</idno>
		<ptr target="https://doi.org/10.1037/xlm0001319" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning Memory and Cognition</title>
		<imprint>
			<date type="published" when="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoding the tradeoff between encoding and retrieval to predict memory for overlapping events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Kuhl</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2019.07.014</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2019.07.014" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page">116001</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Individual choice behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A distributed memory model of semantic priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E J</forename><surname>Masson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0278-7393.21.1.3</idno>
		<ptr target="https://doi.org/10.1037/0278-7393.21.1.3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A context maintenance and retrieval model of organizational processes in free recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Polyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014420</idno>
		<ptr target="https://doi.org/10.1037/a0014420" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="156" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<idno type="DOI">438-481.10.3758/BF03196302</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An autoassociative neural network model of paired-associate learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rizzuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976601750399317</idno>
		<ptr target="https://doi.org/10.1162/089976601750399317" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2075" to="2092" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A context-based theory of recency and contiguity in free recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0013396</idno>
		<ptr target="https://doi.org/10.1037/a0013396" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="912" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decision making and sequential sampling from memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shohamy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016.04.036</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2016.04.036" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="927" to="966" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The time course of perceptual choice: The leaky, competing accumulator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.108.3.550</idno>
		<ptr target="https://doi.org/10.1037/0033-295x.108.3.550" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="592" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A retrieved-context theory of financial decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<idno type="DOI">10.3386/w26200</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Economics</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
