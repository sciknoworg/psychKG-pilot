<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMPARING TRUST PROCESSES BETWEEN HUMANS AND SYSTEMS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Langer</surname></persName>
							<email>markus.langer@uni-saarland.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychologie</orgName>
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><forename type="middle">J</forename><surname>König</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychologie</orgName>
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Back</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychologie</orgName>
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Hemsing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychologie</orgName>
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universität des Saarlandes</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<addrLine>Arbeits-&amp; Organisationspsychologie, Campus A1 3</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Universität des Saarlandes</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COMPARING TRUST PROCESSES BETWEEN HUMANS AND SYSTEMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial intelligence</term>
					<term>Trust</term>
					<term>Personnel Selection</term>
					<term>AI ethics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Automated systems based on artificial intelligence (AI) increasingly support decisions where ethical considerations are central to trust development in human-system collaboration. However, insights regarding trust in automated systems predominantly stem from contexts where the main driver of trust is that systems produce accurate outputs (e.g., alarm systems for monitoring tasks). It remains unclear whether what we know about trust in automated systems translates to application scenarios where ethical considerations (e.g., fairness) play a crucial role in trust development. In a personnel selection context, as an example of task where ethical considerations are important, we investigate trust processes in light of a trust violation relating to unfair bias and a trust repair intervention. Specifically, participants evaluated applicant preselection outcomes by either a human or an automated system across twelve personnel selection tasks. We additionally varied information regarding imperfection of the human and automated system. In task rounds five through eight, the preselected applicants were predominantly male, thus constituting a trust violation due to potential unfair bias. Before task round nine, participants received an excuse for the biased preselection (i.e., a trust repair intervention). Results showed that participants initially perceived automated systems to be less trustworthy. Furthermore, the trust violation and the trust repair intervention had weaker effects for the automated system. Those effects were partly stronger when highlighting system imperfection. We conclude that insights from classical areas of automation only partially translate to the many emerging application contexts of such systems where ethical considerations are central to trust processes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent years have seen an upsurge in the use of automated systems based on artificial intelligence (AI) to support or even automate decision-making. Whereas classical application areas of automation were production or monitoring <ref type="bibr" target="#b20">(Endsley, 2017)</ref>, systems are now employed in tasks that affect the fate and future of individuals such as in medicine, jurisdiction, or management <ref type="bibr" target="#b26">(Grgić-Hlača et al., 2019;</ref><ref type="bibr" target="#b40">Longoni et al., 2019;</ref><ref type="bibr" target="#b56">Raisch &amp; Krakoswki, 2021)</ref>. In management alone, AI-based systems are on the verge of changing decision processes in personnel selection, performance management, or promotion <ref type="bibr" target="#b8">(Cheng &amp; Hackett, 2021;</ref><ref type="bibr" target="#b59">Tambe et al., 2019)</ref>.</p><p>Consequently, managers can increasingly choose to assign tasks to either human trustees (the party that is trusted) or automated systems as trustees, which warrants the need for managers as trustors (the party that trusts) to continuously assess the trustworthiness of humans and automated systems in order to decide whether to rely on a respective trustee to perform a certain task.</p><p>Whereas previous research has shown that there are similarities in trustworthiness assessment and in related longitudinal trust processes for humans and automated systems as trustees <ref type="bibr" target="#b13">(de Visser et al., 2018;</ref><ref type="bibr" target="#b24">Glikson &amp; Woolley, 2020)</ref>, this research predominantly stems from classical application contexts of automation (e.g., monitoring). In these contexts, what trustors evaluate when assessing trustworthiness is focused on classical performance measures associated with effectiveness and efficiency (e.g., prediction accuracy). In comparison, there is a paucity of research investigating trust processes in contexts where automated systems support decisions that affect the fate of individuals (e.g., personnel selection; <ref type="bibr" target="#b36">Langer et al., 2020)</ref>. In such contexts, practitioners, researchers, and policy-makers are commonly concerned about ethical issues when using automated systems as decision support <ref type="bibr" target="#b34">(Jobin et al., 2019;</ref><ref type="bibr" target="#b43">Martin, 2019)</ref>. Especially outcome fairness is important in such contexts <ref type="bibr" target="#b55">(Raghavan et al., 2020)</ref> and determines trustworthiness assessments beyond classical performance measures. In fact, fairness issues (e.g., biases introduced or amplified by automated systems) have led to companies like Amazon losing trust and abandoning automated systems for managerial decisions <ref type="bibr" target="#b9">(Dastin, 2018)</ref>.</p><p>As previous research on trust in automation is dominated by work investigating trust processes associated with classical performance measures (e.g., whether systems produce accurate outputs), there is a lack of insight regarding trust processes in tasks where there is potential for violations of ethical standards such as fairness. For instance, although it is a matter of life and death to adequately trust in air-traffic-control systems, such systems usually violate operators' trust through misses or false alarms <ref type="bibr" target="#b53">(Parasuraman &amp; Riley, 1997)</ref>. In such illustrative contexts for the majority of previous research on trust in automated systems, fairness issues do not play an obvious role. However, such issues are salient throughout many novel application areas of automated systems <ref type="bibr" target="#b55">(Raghavan et al., 2020)</ref>. Thus, it remains unclear whether effects found in classical trust in automation research translate to novel application contexts where accuracy is only one of many factors determining trust in those systems.</p><p>This study compares trust processes (initial perceptions, trust violations, trust repair interventions; <ref type="bibr" target="#b13">de Visser et al., 2018)</ref> between human trustees and automated system as trustees in a novel application context for automated systems and in light of a potential violation of ethical standards. Specifically, participants took part in twelve rounds of a personnel selection task, where they received decision support from either a human colleague or an automated system. In every task round, participants received a preselection of applicants. After the fourth task, participants repeatedly received biased preselection outputs (i.e., trustees predominantly preselected male applicants) constituting trust violations due to violations of ethical standards. Before round nine, participants received an excuse for the biased preselection constituting a trust repair intervention.</p><p>We investigate initial perceptions of trustworthiness, trust and reliance, as well as perceptions in response to the trust violation and the trust repair intervention. Furthermore, we investigate expectations of perfection (high performance, high consistency in performance) associated with automated systems as a driver of potential differences in trust processes in relation to human and automated trustees <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background and Hypotheses Development Interpersonal Trust and Trust in Automation</head><p>Trust processes are central when trustors consider delegating tasks to trustees and when they receive advice or decision-support <ref type="bibr" target="#b6">(Bonaccio &amp; Dalal, 2006)</ref>. In those cases, the task fulfillment or the work output is important to trustors <ref type="bibr" target="#b44">(Mayer et al., 1995)</ref>. Consequently, relying on trustees' work outputs involves risk as trustees might not fulfill the trustors' expectations. This is true for interpersonal trust processes and in the case of trust in automation (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004</ref>).</p><p>Theoretical models on interpersonal trust <ref type="bibr" target="#b44">(Mayer et al., 1995)</ref> and trust in automation (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004)</ref> show that independently whether support is provided by a human or an automated system, main concepts in trust processes are trustworthiness, trust, and reliance. Trustor's trustworthiness assessments arise from known or perceived characteristics of the trustee as well as trustees' performance in a task (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b44">Mayer et al., 1995)</ref>. In the current study, we chose a personnel selection task which means that trustors will assess trustworthiness in regard to their goals in relation to this task. We decided for personnel selection as it reflects a context demanding complex, ethically sensitive decisions, and where AI 1 -based automated systems are already a viable options for decision-support <ref type="bibr" target="#b29">(Hickman et al., 2021)</ref>.</p><p>Trustworthiness of humans and automated systems is usually conceptualized with several facets (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b44">Mayer et al., 1995)</ref>. In this study, we capture four facets of trustworthiness that trustors may consider in relation to human and automated trustees: ability, flexibility, integrity, and benevolence <ref type="bibr" target="#b31">(Höddinghaus et al., 2020;</ref><ref type="bibr">Wang &amp; Benbasat, 2005</ref>). In the case of personnel selection, ascribing high ability means that trustors believe that trustees can successfully select suitable applicants <ref type="bibr" target="#b36">(Langer et al., 2020)</ref>. Flexibility reflects whether trustors assume that trustees can react flexibly to changes in a given task <ref type="bibr" target="#b31">(Höddinghaus et al., 2020)</ref>.</p><p>Integrity captures whether trustors believe that trustees provide unbiased recommendations and is in line with evaluating if trustees follow ethical standards that trustors value <ref type="bibr" target="#b14">(Den Hartog &amp; De Hoogh, 2009;</ref><ref type="bibr" target="#b22">Gilliland, 1993)</ref>. Finally, benevolence reflects if people believe that trustees consider trustors' interests, goals, and values <ref type="bibr" target="#b31">(Höddinghaus et al., 2020)</ref>. The second main concept within trust processes is trust. <ref type="bibr">Mayer et al. (1995 p. 712</ref>) define trust as a willingness to be vulnerable towards the actions of trustees without explicitly controlling or supervising trustees. Meeßen et al.</p><p>(2020) describe trust as an intention to trust trustees. In interpersonal trust and trust in automation, if people estimate trustworthiness of trustees to be high, trust will tend to also be high and thus it will be more likely that trustors will actually rely on trustees to fulfil a given task. Reliance then is the behavioral outcome of trust (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004)</ref>. This means that trustors actually delegate tasks to trustees or follow advice provided to them. This implies that trustors accept the risk that they may be disappointed by the outcome produced, may need to respond to a failure in a task, or may receive bad advice <ref type="bibr" target="#b44">(Mayer et al., 1995)</ref>. Depending on the respective outcome (e.g., the work output, the quality of advice), trustors will re-evaluate trustees' trustworthiness starting another cycle in the trust process (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b44">Mayer et al., 1995)</ref>.</p><p>Thus, trust processes are dynamic <ref type="bibr" target="#b24">(Glikson &amp; Woolley, 2020)</ref>. Specifically, there is an initial level of trustworthiness, trust, and reliance (i.e., when initially interacting with trustees) and respective levels can increase over time if trustors perceive trustees to perform successfully (J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b44">Mayer et al., 1995)</ref>. However, relying on someone or something always runs the risk of unfulfilled expectations, and trust violations can reduce trustworthiness, trust, and reliance <ref type="bibr" target="#b35">(Kim et al., 2006)</ref>. It is, however, possible to repair trust (e.g., through excuses) which can then rebuild, and positively affect perceptions of trustworthiness, trust and reliance <ref type="bibr" target="#b13">(de Visser et al., 2018;</ref><ref type="bibr">Tomlinson &amp; Mayer, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences in Trust Processes over Time</head><p>Whereas the basic concepts as well as dynamics (e.g., effects of trust violations or trust repair interventions) exist for interpersonal trust and for trust in automation, previous research has indicated that assessment of trustworthiness, trust, and reliance might differ for human versus systems as trustees <ref type="bibr" target="#b13">(de Visser et al., 2018;</ref><ref type="bibr" target="#b42">Madhavan &amp; Wiegmann, 2007)</ref>. These differences might be present as a direct effect on initial trustworthiness assessments. Additionally, they might be reflected in moderating effects regarding reactions to outcomes, behavior, and information associated with the trustee. In other words, there might be different reactions to trust violation and trust repair interventions depending on whether the trustee is human or an automated system <ref type="bibr" target="#b11">(de Visser et al., 2016)</ref>. <ref type="figure">Figure 1</ref> summarizes the main concepts and their relation in trust processes and locates our hypotheses and research question in the trust process as proposed by <ref type="bibr" target="#b44">Mayer et al. (1995)</ref>. The following sections provide the rationale for our proposed hypotheses and research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Trustworthiness Assessments</head><p>There is research assuming that initial trustworthiness assessments regarding automated systems might be comparably higher than for human trustees. For instance, <ref type="bibr">Madhaven and Wiegmann (2007)</ref> integrate research on trust in automated systems in classical application contexts for such systems and propose that humans initially believe that systems are designed for a specific purpose, and have been tested before they are deployed. Thus they should work as intended leading to high levels of initial trustworthiness <ref type="bibr" target="#b51">(Parasuraman &amp; Manzey, 2010)</ref>. Consequently, trustors should start with lower levels of trustworthiness for human trustees because of more uncertainty about their abilities, values, and intentions <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>.</p><p>In contrast to the propositions by Madhaven and Wiegmann, there is research indicating that trustworthiness assessments could be lower for automated systems compared to human trustees especially in tasks that affect the fate of individuals such as personnel selection (M. K. <ref type="bibr" target="#b39">Lee, 2018;</ref><ref type="bibr" target="#b40">Longoni et al., 2019)</ref>. This might be partly because people believe automated systems are less capable of fulfilling such tasks (M. K. <ref type="bibr" target="#b39">Lee, 2018)</ref>. Specifically, people may believe that automated evaluation of individuals' characteristics does not adequately capture respective task complexity, is dehumanizing, and does not consider ethical peculiarities in such contexts <ref type="bibr" target="#b27">(Grove &amp; Meehl, 1996;</ref><ref type="bibr" target="#b48">Newman et al., 2020)</ref>.</p><p>Overall, research stemming from classical areas of automation would suggest high levels of initial trustworthiness regarding automated systems. In novel application contexts, there are arguments indicating that initial assessments of system trustworthiness could be higher or lower compared to human trustees. Considering that trust and reliance should result from initial trustworthiness assessments, we thus pose the following research question:</p><p>Research Question (RQ) 1 2 : Is there an initial difference for trustworthiness assessments, trust, and reliance between the automated system and the human trustee?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of Trust Violations</head><p>The re-evaluation of trustworthiness after trust violations might be moderated by different expectations and assumptions regarding human and automated systems as trustees. In fact, trustworthiness assessments seem to suffer comparably strong as trustors experience the first error by an automated system <ref type="bibr" target="#b0">(Bahner et al., 2008;</ref><ref type="bibr" target="#b18">Dzindolet et al., 2003)</ref>. It is commonly assumed that this first-error-effect results from people having high expectations and high initial levels of trustworthiness regarding automated systems, as they believe systems are tested and delivered to work as intended <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>. This is also associated with perfection assumptions regarding systems, meaning that systems should show consistently high performance <ref type="bibr" target="#b16">(Dietvorst &amp; Bharti, 2020;</ref><ref type="bibr" target="#b51">Parasuraman &amp; Manzey, 2010)</ref>. When recognizing first errors, people realize that systems are not perfect, and that system performance varies, thereby strongly negatively affecting trustworthiness assessment <ref type="bibr" target="#b18">(Dzindolet et al., 2003)</ref>. In contrast, trustors might not expect human trustees to constantly work near-to-perfection, thus errors might be in line with what can be expected leading to comparably weaker trust violation effects <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>.</p><p>However, research suggesting this moderating effect also predominantly stems from classical application areas of automation where trust violations are mainly associated with trustee characteristics that relate to the trustworthiness facet ability. For instance, in such studies, trustees would miss alarms, produce false alarms, or provide less than perfect predictions <ref type="bibr" target="#b32">(Hoff &amp; Bashir, 2015)</ref>. In novel application areas such as personnel selection, trust violations can also be abilityassociated meaning that a trustee recommends unsuitable applicants. Additionally, trustees can produce ethically questionable outcomes (i.e., discriminating minority applicants) that can affect trustworthiness assessments <ref type="bibr" target="#b35">(Kim et al., 2006)</ref>. In cases where trust violations are based on violations of ethical considerations, people might have stronger negative reactions for human trustees as they may believe that automated systems do not actively discriminate against specific groups of people <ref type="bibr" target="#b4">(Bigman et al., 2020)</ref>. However, previous trust in automation research would suggest that errors by automated systems result in strong negative effects regarding trustworthiness which is why we propose: Hypothesis 1 3 : After a trust violation, trustworthiness, trust, and reliance towards the automated system as trustee will decrease more compared to the human trustee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of Trust Repair Interventions</head><p>Similar moderating effects associated with the nature of the trustee might also occur for trust repair interventions. Specifically, systems are deployed with a specific set of functions and level of quality. Any system improvements might require system updates or a completely new system <ref type="bibr" target="#b31">(Höddinghaus et al., 2020)</ref>. In contrast, human trustees might be expected to be more adaptable. If there are trust repair interventions for human trustees (e.g., excuses), trustors might assume that the human trustee will do their best to not let this error happen again <ref type="bibr" target="#b13">(de Visser et al., 2018;</ref><ref type="bibr">Tomlinson &amp; Mayer, 2009)</ref>. If there are trust repair interventions associated with automated trustees, trustors might still believe that a system will produce similar errors in future as they may perceive systems to possess a fixed set of attributes <ref type="bibr" target="#b17">(Dietvorst et al., 2015)</ref>. Thus, we conclude, Hypothesis 2: After the trust repair intervention, perceptions of trustworthiness, trust, and reliance towards an automated system as trustee will increase less compared to a human trustee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences in Trustworthiness Facets</head><p>The previously mentioned differences between human and automated trustees might also have implications for single facets of trustworthiness. Regarding ability, trustors might start off with higher levels of ability assessments regarding automated systems, but ability perceptions could suffer more strongly from trust violations and might be less affected by trust repair interventions <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>. Regarding flexibility, automated systems might be perceived as less flexible than human trustees <ref type="bibr" target="#b17">(Dietvorst et al., 2015</ref>), but we are not aware of research indicating how this changes through trust violation and trust repair interventions.</p><p>Regarding integrity, automated systems might be assessed as more consistent and less biased compared to human trustees <ref type="bibr" target="#b37">(Langer et al., 2019;</ref><ref type="bibr" target="#b39">M. K. Lee, 2018)</ref>. In addition, for human trustees, trust violation associated with ethical considerations (e.g., a biased preselection) might have stronger effects as people might be more outraged by such trust violations in the case of a human trustee <ref type="bibr" target="#b4">(Bigman et al., 2020)</ref>. For trust repair effects, it may be possible to assume stronger effects for human trustees as people believe that humans can learn from their mistakes <ref type="bibr">(Tomlinson &amp; Mayer, 2009)</ref>. However, it is also possible that humans may not believe that biased human trustees can change (i.e., assuming that biases could reflect stable attitudes). For benevolence, humans might be perceived as more benevolent than automated systems as they are more likely to be able to consider trustors' interests <ref type="bibr" target="#b31">(Höddinghaus et al., 2020)</ref>. However, we are not aware of research that investigated benevolence of human and automated systems with respect to trust violations and trust repair interventions. This list of tentative assumptions regarding the facets of trustworthiness subsume under the following research question: RQ2: Is there an initial difference and different effects for trust violations and trust repair interventions for the facets of trustworthiness regarding human and automated systems as trustees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumptions of Perfection as a Driver of Differences Between Human and Automated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trustees?</head><p>The previous sections indicate that one of the main arguments from previous research regarding differences between human trustees and automated systems as trustees is an assumption of perfection regarding automated systems <ref type="bibr" target="#b32">(Hoff &amp; Bashir, 2015;</ref><ref type="bibr" target="#b42">Madhavan &amp; Wiegmann, 2007)</ref>.</p><p>We thus experimentally manipulate this assumption by emphasizing that the automated system or the human trustee might not always produce error-free outputs, thus highlighting their potential imperfection and potential for performance variations <ref type="bibr" target="#b0">(Bahner et al., 2008)</ref>. If the assumption of perfection causes differences in trust processes between human and automated systems as trustees, making potential imperfection salient might affect perceptions of automated systems in a way that is more similar to reactions to human trustees <ref type="bibr" target="#b11">(de Visser et al., 2016)</ref>. Thus, we propose:</p><p>Hypothesis 3: Trustworthiness, trust, and reliance in the automated system when there is information about imperfection presented will initially be lower compared to when no such information is presented.</p><p>Hypothesis 4: Following a trust violation, trustworthiness, trust and reliance in the automated system when there is information about imperfection presented will decrease to a lesser extent compared to when no such information is presented.</p><p>RQ3: Will there be interaction effects between the trust repair intervention and the information regarding imperfection for the automated system on trustworthiness, trust, and reliance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample</head><p>We determined the required sample size with G * Power <ref type="bibr" target="#b21">(Faul et al., 2009)</ref>. In the preregistration we proposed to conduct ANOVAs. For a within-between interaction effect of η²p = 0.01, N = 108 participants would be necessary for a power of 1-β = .80. Assuming a small to medium effect for a between groups effect of η²p = 0.04, N = 148 participants would be necessary to achieve a power of 1-β = .80. Therefore, we wanted to recruit between 108 and 148 participants.</p><p>Ultimately, we decided that analyzing the data using regressions and hierarchical linear models was more appropriate. This study was advertised to people interested in Human Resource Management. We posted the advertisement on different social media groups, around the campus of a German university, and in the downtown area of a German city.</p><p>As we anticipated potential issues during data collection (e.g., technical issues), we continued data collection until our sample consisted of N = 211 participants. We excluded 10 participants because they did not follow the instructions in the experimental procedure, 3 because they reported technical issues, 30 because they indicated that they received advice from a human agent when they actually were in the automated system condition and vice versa, 44 because they failed the manipulation check regarding imperfection (i.e., indicating they were not told that the trustee can make errors when they were actually told so or vice versa), and 3 participants because they were not able to recall that there were task rounds where male applicants had been predominantly selected. The final sample consisted of N 4 = 121 German participants (79% female) of which 93% studied psychology. The mean age was 23.56 years (SD = 5.47), participants took a mean of 32 minutes (SD = 10) to complete the study, had experienced a median of three personnel selection processes as applicants, and 24% of participants indicated experience in applicant selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>In a 2 (human vs. automated trustee) × 2 (no information regarding imperfection vs. information regarding imperfection) online experiment, participants were randomly assigned to one of the four groups. Participants were instructed to imagine that they were responsible for personnel selection in an HR department at a large insurance company operating in Germany. They were informed that their company is recruiting trainees for its subsidiaries. We chose trainees at an insurance company because of the equal gender distribution in insurance jobs in Germany <ref type="bibr" target="#b58">(Rudnicka, 2020)</ref>. Furthermore, participants were informed that they will receive support in selecting candidates through a preselection of applicants from a larger applicant pool across 12 situations (representing consecutive days where there are new applications). Participants were then informed that their task was to evaluate the quality of the applicant preselection in regard to three organizational goals. These goals were customer satisfaction, innovation, and diversity. For each goal, participants received a text reflecting how the company defines these goals (see Supplemental Material A). The purpose of this was to introduce diversity explicitly so that not providing a diverse preselection would indicate a trust violation. Note that we chose two more goals in order to (a) provide participants with quality criteria for the preselection, (b) make the task more realistic, and (c) make the focus on diversity less obvious.</p><p>After being introduced to their task and the company's goals, participants received a job description including the desired qualifications of applicants. Then, participants received a tutorial round introducing them to the general process of each upcoming task round. In the tutorial (and in each of the task rounds) participants received between six and eight application photo-like pictures of white male and female applicants (see Supplemental Material B). We chose to only include white applicants as balancing for racial diversity would have greatly increased study complexity.</p><p>The pictures were taken from https://generated.photos, a website that uses AI to produce realistic pictures of human faces.</p><p>For each applicant, there was additional information accompanying the picture (i.e., family name, age, years of job experience, final high school grade, strengths). During the tutorial, participants were informed that they should analyze the preselection against the company goals and the job description. They were also told that they will receive the question: "Do you want to see the statistics of the applicant pool?" If participants responded with "No" they were directed to the next page in the online tool. If participants responded with "Yes", they were directed to a page showing them the underlying statistics of the applicant pool for a given task round. In these statistics, participants were informed about the number of applicants, the percentage of male and female applicants as well as provided with a list of means and distribution information for further information (e.g., "the mean number of years of job experience of today's applicant pool was 1.3 with a deviation of 0.3"). In the tutorial, participants were instructed to respond with "Yes" so that every participant would see what happens if they request the applicant pool statistics. We included the option to view the applicant pool to increase psychological fidelity, as well as giving participants the option to check whether there was gender diversity in the applicant pool.</p><p>At the beginning of the actual experiment, participants received information that there was an increasing number of applicants making it necessary to have a preselection stage where they as hiring manager receive support. They were told that this support is a colleague (an automated system) who analyzes and preselects applicants. In the information regarding imperfection condition, participants were additionally informed that "the colleague (automated system) usually produces good work outcomes but that there are always possibilities for errors." Participants were then informed that they had the opportunity to evaluate the preselection and accept or reject it.</p><p>Participants then saw the information of the preselected applicants.</p><p>Afterwards, participants responded to the fairness item, to three items assessing whether the trustee adhered to the company's goals (customer satisfaction, innovation, diversity), to the trustworthiness (i.e., ability, flexibility, integrity, benevolence; only in situations 1, 3, 5, 7, 9, and 12) and trust items. Participants then indicated whether they wanted to see the applicant pool statistics. After clicking "No," or after seeing those statistics, participants were asked: "Do you accept this preselection?", which was used as a measure of reliance. This process was repeated for twelve task rounds.</p><p>In task rounds five through eight, there were predominantly male applicants included in the preselection (see Supplemental Material B). Thus, rounds five through eight constitute the trust violation phase. Before round nine, participants received the following information: "Dear colleague, in previous preselection outcomes, there were more male than female applicants, although the applicant statistics indicated that there were about as much women as there were men in the applicant pool. We were made aware of this issue and it has been solved. We apologize for this. Applicant preselection in the future should follow the goals of the organization again. Thank you for your understanding." This constituted the trust repair intervention. After the last task round, participants responded to exploratory items asking them whether they were satisfied with the advisor and if they would want to work together with this advisor in future. Finally, we measured propensity to trust in humans and in technology and collected demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>Since participants in the main study needed to report their evaluation of trustees several times throughout the study, we wanted to optimize the use of items to reduce participant fatigue.</p><p>In order to determine which items to keep, we conducted a pre-study with 54 student participants.</p><p>In this pre-study, participants imagined to work at an HR department. They were told that they will receive support for a personnel selection task and were randomly assigned to either the human or the automated trustee condition. Participants then responded to fourteen items assessing perceptions of ability, flexibility, integrity, benevolence, and trust regarding the trustee. We removed four items because they negatively influenced Cronbach's Alpha reliability of the scales or because they did not work well in the human or the automated trustee condition (e.g., because participants indicated that they did not know how to respond to this item in regard to the respective trustee). 5</p><p>5 Results of the pre-test can be made available upon request.</p><p>Unless otherwise stated, participants responded to the items reflecting the dependent variables on a scale from 1 (strongly disagree) to 7 (strongly agree). To keep the gender of the human trustee undefined, the items did not mention the colleague's gender (using the inclusive "Kolleg/in" in German).</p><p>Ability was measured with two items taken from <ref type="bibr" target="#b31">Höddinghaus et al. (2020)</ref>. A sample item was "I believe the colleague/the automated system has the competency to consider all important information for the decision." Flexibility was measured with two items taken from Höddinghaus et al. (2020). A sample item was "I believe the colleague/the automated system is able to react flexibly to given contexts." Integrity was measured with two items taken from Höddinghaus et al.  <ref type="bibr" target="#b60">Thielsch et al. (2018)</ref>. A sample item was "I would strongly rely on the colleague/the automated system". Reliance was measured with the item "I accept this preselection" with the response options "Yes" or "No". Accepting the preselection reflects higher reliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manipulation Checks</head><p>To assess if the trust violation and trust repair intervention worked as intended, we used two manipulation check measures. First, after receiving the preselection, participants responded to the item "In your opinion, did the preselection adhere to the company's goals?" on a scale from 1 (disagree) to 5 (agree). Participants responded to this item three times, once for each of the three company goals (customer satisfaction, technology and innovation, diversity). This reminded participants of the company's goals and allowed us to assess if participants realized that the biased preselection violated the organization's diversity goal. Second, participants responded to the item "I perceived the preselection to be fair" which was meant to assess whether participants perceived a predominantly male preselection to be unfair. <ref type="table" target="#tab_0">Table 1</ref> displays correlations, means and SDs for the means of all variables across all tasks. <ref type="figure" target="#fig_3">Figures 2 and 3</ref> show line-graphs for the continuous dependent variables for each of the tasks and <ref type="table" target="#tab_1">Table 2</ref> shows contingency tables for participants' rejection and acceptance of the preselection of applicants. We used regressions and hierarchical linear models (HLM) to analyze the data. Within the HLM analyses, measures were combined into one measure for each of the three phases: the initial phase (task rounds one through four), trust violation phase (task rounds five through eight), and the trust repair phase (task rounds nine through twelve). Each phase was entered into the HLM analyses through backwards difference coding meaning that results regarding a respective phase can be interpreted as a comparison to the previously entered phase (e.g., trust violation phase in comparison to the initial phase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>HLM analyses were also used to analyze the manipulation checks. <ref type="table" target="#tab_2">Table 3</ref> shows that both perceptions of fairness and evaluations of the fulfillment of the diversity goal decreased under the trust violation and recovered through the trust repair intervention indicating that the manipulations had the intended effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Questions and Hypotheses 6</head><p>Tables 4 and 5 show the results of the regression analyses for initial trustworthiness and trust. <ref type="table" target="#tab_5">Tables 6 and 7</ref> show the results of the HLM analyses for the evaluation of trustworthiness, trust, and facets of trustworthiness over time.</p><p>Trustworthiness, trust and reliance. RQ1 asked whether there are initial difference for trustworthiness assessments, trust, and reliance between the human and automated systems as trustees. We used linear regressions to analyze differences in trustworthiness and trust for only the first task as it reflects the most appropriate point in time for initial evaluations of trustworthiness, trust and reliance. The results indicated that participants perceived the human trustee as more trustworthy and they more strongly intended to trust the human compared to the automated trustee <ref type="table" target="#tab_3">(Table 4</ref>). Regarding reliance, the percentage of participants rejecting the suggested preselection of applicants provided by the human (7.1% of participants rejected) or automated system (9.4% of participants rejected) in the first round showed no substantial differences. As such, we did not conduct further analyses. In sum, the answer to RQ1 is: trustworthiness and trust were initially higher for human trustees but there was no difference for reliance.</p><p>Hypothesis 1 proposed that after a trust violation, trustworthiness, trust, and reliance towards the automated system would decrease more compared to the human trustee. In contrast to this, <ref type="table" target="#tab_5">Table 6</ref> indicates that trustworthiness and trust decreased to a larger extent for the human trustee. For reliance, we initially analyzed the percentage of people who rejected the preselection depending on the conditions and the phases. For the initial phase and in the human condition, participants rejected the preselection in 27% of cases, and in the case of the automated system, 31% of cases were rejected. For the trust violation phase and in the human condition, participants rejected the preselection in 73% of cases compared to 72% in the case of the automated system indicating no differences in the decline of reliance. Overall, results did not support Hypothesis 1.</p><p>Hypothesis 2 suggested that after the trust repair intervention, trustworthiness, trust, and reliance towards the automated system would increase less than towards the human trustee. <ref type="table" target="#tab_5">Table   6</ref> shows that for trustworthiness and trust, this was the case. For reliance, the percentage of cases where participants rejected the human trustee's preselection in the trust repair phase was 28%, and for the automated system's preselection, 31% of cases were rejected showing no substantial differences. This supports Hypothesis 2 for trustworthiness and trust, but not for reliance.</p><p>Facets of trustworthiness. RQ2 asked whether there is an initial difference and different effects for trust violations and trust repair interventions for the facets of trustworthiness regarding human and automated systems as trustees. Tables 5 and 7 summarize the findings for trustworthiness facets. Regarding initial assessments, ability and flexibility were lower, and integrity was higher for the automated system <ref type="table" target="#tab_4">(Table 5)</ref>. Regarding trust violations, effects were weaker for the automated system for all facets of trustworthiness (see <ref type="table" target="#tab_6">Table 7</ref> TV×Trustee).</p><p>Similarly, trust repair intervention effects were weaker for the automated system for all facets of trustworthiness (see <ref type="table" target="#tab_6">Table 7</ref> TR×Trustee).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of information regarding imperfection. Hypothesis 3 proposed that</head><p>trustworthiness, trust, and reliance in the automated system when there is information about imperfection presented will initially be lower compared to when no such information is presented, however, we found no effects of information regarding imperfection and no interaction between the independent variables <ref type="table" target="#tab_3">(Table 4)</ref>. Thus, Hypothesis 3 was not supported.</p><p>Hypothesis 4 proposed that following a trust violation, trustworthiness, trust and reliance in the automated system when there is information about imperfection presented will decrease to a lesser extent compared to when no such information is presented. This was not supported for trustworthiness and trust, as seen in <ref type="table" target="#tab_5">Table 6</ref> (see Trust Violation×Trustee×Imperfection) and <ref type="figure" target="#fig_3">Figures 2 and 3</ref>. Instead, the effects of trust violations were stronger when information regarding imperfection was presented. Regarding reliance, in the initial phase the percentage of cases where participants rejected the preselection of the automated system when information regarding imperfection was presented was 29%, and in the case of the automated system with no such information presented it was 35%. In the trust violation phase, those rejection rates were 71% (information regarding imperfection presented) and 72% (no such information presented) respectively. Those results indicated that there was no difference in the decline of reliance. Overall, these results did not support Hypothesis 4. RQ3 asked whether there will be interaction effects between the trust repair intervention and the information regarding imperfection for the automated system on trustworthiness, trust, and reliance. Indeed, trust repair interventions were more effective at restoring trustworthiness and trust in the automated system when information regarding imperfection was presented (see Trust Repair×Trustee×Imperfection in <ref type="table" target="#tab_5">Table 6</ref>). Regarding reliance, the percentage of cases where participants rejected the preselection of the automated system when information regarding imperfection was presented was 26% compared to 38% when no such information was presented.</p><p>We calculated χ²-tests for the trust violation and the trust repair phase separately. For the trust violation phase, the difference between automated systems where information about imperfection was presented and where no such information was presented was not significant, χ²(1) = 0.01, p = .92, whereas there was a difference for the trust repair phase, χ²(1) = 4.13, p &lt; .05. As a benchmark, for the human trustee the percentages of rejection in the trust violation phase were 72% (information about imperfection presented) and 74% (no such information presented), and 28%</p><p>(information about imperfection presented) and 26% (no such information presented) in the trust repair phase which constituted no significant differences when calculating χ²-tests. The response to RQ3 therefore is: Trust repair interventions were more effective for automated systems as trustees when information regarding imperfection was presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>With the increasing use of AI-based automated systems in novel application context where they contribute to decisions over individuals' fates, it becomes crucial to understand trust processes in human-AI collaboration for such decisions <ref type="bibr" target="#b24">(Glikson &amp; Woolley, 2020;</ref><ref type="bibr" target="#b56">Raisch &amp; Krakoswki, 2021)</ref>. We built our hypotheses and research questions on theoretical models describing general trust processes and on research regarding trust violations and trust repair interventions in classical application contexts of automation <ref type="bibr" target="#b32">(Hoff &amp; Bashir, 2015;</ref><ref type="bibr" target="#b42">Madhavan &amp; Wiegmann, 2007)</ref>. Overall, our findings imply that theoretical assumptions and effects found in classical application areas of automated systems only partly translate to novel application contexts of automated systems such as personnel selection. Specifically, in classical application contexts, where trustworthiness assessments mainly stem from classical system performance measures, people seem to hold an assumption of perfection regarding automated systems associated with high and consistent performance. Our results indicate that people do not assume high performance for automated systems in personnel selection as initial trustworthiness assessments were lower for automated systems. However, there seems to remain an assumption of consistency, as trust violation associated with potentially unfair bias as well as trust repair effects were weaker for automated systems. This assumption of consistency was partly reduced by highlighting system imperfection leading to stronger trust violation and trust repair effects. In sum, our study suggests that research assessing trust in automated systems must be aware of the application context in which systems support decision-making because although assumptions of systems as being consistent might generalize, assumptions of high-performance might not. This seems to affect trust processes associated with automated systems in respective contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Implications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust in Automation Depends on the Use Context</head><p>Whereas in classical application contexts people seem to evaluate overall trustworthiness of automated systems to be higher than human trustworthiness <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>, we found the opposite to be true for personnel selection. Classical tasks for automated systems usually afford mechanical skills such as combining large amount of data and performing repetitive tasks (e.g., during monitoring tasks) (M. K. <ref type="bibr" target="#b39">Lee, 2018)</ref>. In contrast, personnel selection requires individual and flexible decision-making capabilities, as well as ethical considerations. These are capabilities that people more likely ascribe to humans <ref type="bibr" target="#b2">(Bigman &amp; Gray, 2018;</ref><ref type="bibr" target="#b39">M. K. Lee, 2018;</ref><ref type="bibr" target="#b48">Newman et al., 2020)</ref>. In other words, people might believe that humans are better able to complete tasks where ethical issues, as well as individuals' unique characteristics need to be considered <ref type="bibr" target="#b40">(Longoni et al., 2019;</ref><ref type="bibr" target="#b48">Newman et al., 2020)</ref>. In line with this and with further previous research <ref type="bibr" target="#b31">(Höddinghaus et al., 2020;</ref><ref type="bibr" target="#b39">M. K. Lee, 2018)</ref>, our participants perceived the automated system to be less able and less flexible, but also perceived the integrity (associated with systems being less biased) of systems to be comparably stronger. Overall, these findings indicate that people have specific attitudes towards automated systems that affect their evaluations of trustworthiness of automated systems differently for different tasks <ref type="bibr" target="#b19">(Elsbach &amp; Stigliani, 2019)</ref>. Those attitudes seem to lead to high overall levels of initial trustworthiness assessments for classical automation tasks (e.g., monitoring) that afford mechanical skills and where primary performance measures are effectivity and efficiency, and to comparably low initial trustworthiness assessments for tasks that involve consideration of ethical issues and decisions about individuals (e.g., personnel selection, performance evaluation; see also <ref type="bibr" target="#b47">Nagtegaal, 2021)</ref>.</p><p>In sum, this resulted in participants evaluating initial levels of trustworthiness to be stronger for human trustees, which also translated to stronger intentions to trust the human. This usually indicates that humans would also be more likely to rely on a recommendation provided by another human. However, overall effects regarding reliance were negligible. An explanation for this might be that differences in trustworthiness and trust were not strong enough which might have resulted from the fact that certain facets of trustworthiness would increase the likelihood of relying on a human trustee (i.e., assessments of ability and flexibility), whereas others would increase the likelihood of relying on an automated system (i.e., assessments of integrity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust Violations and Repair Interventions More Strongly Affect Human Trustworthiness</head><p>Contrary to what could have been expected based on classical trust in automation research <ref type="bibr" target="#b18">(Dzindolet et al., 2003;</ref><ref type="bibr" target="#b42">Madhavan &amp; Wiegmann, 2007)</ref>, our study showed comparably weaker negative reactions to trust violations by the automated system. On the one hand, this could be due to the fact that initial trustworthiness and trust evaluations of automated systems were comparably lower, thus they could not suffer as much from the trust violation as those perceptions for human trustees. However, after the trust violation, trustworthiness assessments for the human trustee fell below the level of the automated system as trustee which might imply stronger negative reactions to the ethical trust violation for human trustees. Furthermore, this trust violation affected all facets of trustworthiness for the human trustee and not just integrity as the facet most closely associated with the given trust violation <ref type="bibr" target="#b35">(Kim et al., 2006)</ref>.</p><p>Our findings regarding high integrity assessments for automated systems additionally support previous research indicating that people believe that systems are more consistent and less biased than humans <ref type="bibr" target="#b31">(Höddinghaus et al., 2020;</ref><ref type="bibr" target="#b37">Langer et al., 2019)</ref>. Our findings also imply that high integrity, consistency, or lack of bias that people ascribe to automated systems even holds when system outputs repeatedly indicate unfair biases. If ethical violations less strongly affect integrity assessment of automated systems, this could imply that people are less likely to realize such violations. This could be the case because people might be less aware that system outputs can reflect unfair bias. Alternatively, people might interpret reasons for ethical violations differently compared to when they result from a human trustee. In other words, automated systems producing biased outputs might result in less negative perceptions because people ascribe more discrimination intention to human beings <ref type="bibr" target="#b4">(Bigman et al., 2020;</ref><ref type="bibr" target="#b2">Bigman &amp; Gray, 2018)</ref>. Beyond various management tasks, this could also apply to the use of automated systems in other management task, or to tasks in medicine and jurisdiction <ref type="bibr" target="#b55">(Raghavan et al., 2020)</ref>. However, this interpretation and the fact that there were no effects on reliance, call for future research as we can only tentatively conclude that attitudes of high integrity and consistency regarding automated systems might mitigate negative reactions to ethical trust violations by automated systems.</p><p>Although such trust violations caused stronger negative effects for human trustees, our participants were also more forgiving of human trustees. Plausibly, stronger effects of trust repair interventions could result from the fact that trust violation effects for humans were also stronger, so there was also more to gain by trust repair interventions. However, and more coherent with the other results, findings for the trust repair intervention point to differences in perceptions of flexibility. People might perceive humans to be better able to learn from mistakes <ref type="bibr" target="#b13">(de Visser et al., 2018)</ref>. For automated systems, humans might believe that systems are deployed with a given set of attributes that cannot be easily changed.</p><p>Yet, although the trust repair intervention significantly affected reactions thus having the intended effect, we emphasize that the results regarding the trust repair intervention should be interpreted cautiously. Specifically, it is unclear how participants interpreted this intervention.</p><p>More precisely, what conclusions did participants draw when they received the information that "the error has been solved"? In the case of the human trustee, they might have interpreted that the human trustee received additional training. In the case of the automated system, they might have interpreted that developers have solved the issue. Future research investigating more specific trust repair interventions is needed, especially research that precisely highlights causes for trust violations <ref type="bibr">(Tomlinson &amp; Mayer, 2009)</ref>. For instance, emphasizing that there was a programming error might have different implications compared to explaining that an AI-based system may pick up existing bias in training data, thus producing biased outputs. In the first case, people might rebuild trust in a system if they believe that the programming error has been solved. In the second case, people might conclude that this is a problem that reflects systemic issues that cannot be easily solved, so future trust in the system will remain low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlighting Imperfection Reduces Assumptions of Consistency?</head><p>Emphasizing imperfection of the automated system had several implications. Specifically, trust violations effects and trust repair intervention effects were stronger when information regarding imperfection was presented for the automated system. Stronger trust repair effects are in line with prior research <ref type="bibr" target="#b11">(de Visser et al., 2016)</ref>. However, instead of buffering trust violation effects, emphasizing imperfection led to stronger negative reactions. Apparently, making salient imperfection of automated systems can influence what people expect from automated systems and how they perceive or interpret trust violations and trust repair interventions <ref type="bibr" target="#b0">(Bahner et al., 2008)</ref>.</p><p>In our case, highlighting imperfection might have reduced assumptions of consistency, thus trust violations but also repair interventions had stronger effects because people more likely believed that system performance can vary. This interpretation will hopefully stimulate future research regarding the way in which humans perceive automated systems and uncover human attitudes, expectations or heuristics regarding automated systems <ref type="bibr" target="#b19">(Elsbach &amp; Stigliani, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Practical Implications</head><p>Trust processes in relation to human and automated trustees differ <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>. This also means that managers interacting with systems might draw different conclusions when systems compared to humans produce potentially biased outputs. Specifically, our results imply that managers might less likely believe that there is a problem with the system.</p><p>In line with this, providers of AI-based personnel selection solutions commonly market their systems in a way that highlights the potential for less bias in personnel selection when organizations use their systems <ref type="bibr" target="#b55">(Raghavan et al., 2020)</ref>. If people believe that systems are less biased than humans, those marketing campaigns might actually convince organizations and managers to use such systems. However, evidence that such systems can prevent instead of amplify bias is still needed. It might thus be necessary to train decision-makers regarding what challenges or problems to expect when relying on automated systems <ref type="bibr" target="#b50">(Oswald et al., 2020)</ref>. For instance, it might be possible to make decision-makers aware of what they should scrutinize when using an AI-based automated system: What constitutes the underlying training data? How did the developer ensure that the system will work as intended? Is there validity evidence and evidence regarding adverse impact?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In addition to the lack of clarity of the trust repair intervention there is one more main limitation of our study: our sample did not consist of hiring managers. Thus, reactions of actual hiring managers may differ. Although hiring managers in organizations might currently not be better trained in working with automated systems than our sample <ref type="bibr" target="#b50">(Oswald et al., 2020)</ref>, they would be more aware of possible negative consequences. We therefore might expect stronger negative reactions to trust violations as unfair bias and adverse impact could lead to serious negative outcomes (e.g., lawsuits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>With the implementation of AI-based automated systems in management, work processes will change <ref type="bibr" target="#b54">(Parker &amp; Grote, 2020)</ref>. However, regardless of whether organizations delegate tasks to automated systems or human beings, trust processes remain central <ref type="bibr" target="#b24">(Glikson &amp; Woolley, 2020)</ref>.</p><p>Our study shows that previous research on interpersonal trust <ref type="bibr" target="#b44">(Mayer et al., 1995)</ref> and trust in automation <ref type="bibr" target="#b32">(Hoff &amp; Bashir, 2015</ref>; J. D. <ref type="bibr" target="#b38">Lee &amp; See, 2004)</ref> offers valuable starting points for research on the use of systems for managerial purposes as the main concepts (trustworthiness, trust) and drivers of trust dynamics (trust violations and trust repair interventions) remain the same.</p><p>Additionally, similar to previous research <ref type="bibr" target="#b42">(Madhavan &amp; Wiegmann, 2007)</ref>, having a human or a Tomlinson, E. C., <ref type="bibr">&amp; Mayer, R. C. (2009)</ref>. The role of causal attribution dimensions in trust repair. Academy of Management <ref type="bibr">Review, 34(1), 85-104. https://doi.org/10.5465/amr.2009</ref><ref type="bibr">.35713291 Wang, W., &amp; Benbasat, I. (2005</ref>. Trust in and adoption of online recommendation agents. <ref type="bibr">Systems,</ref><ref type="bibr">6(3)</ref>, 72-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal of the Association for Information</head><p>https://doi.org/10.17705/1jais.00065     Notes. Coding of Trustee: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding imperfection. N = 121. Notes. Coding of Trustee: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding imperfection. N = 121.   Notes.TV = trust violation, TR = trust repair. Coding of Trustee: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding imperfection. For the coding of the effects over time (i.e., Trust Violation and Trust Repair) we used backward difference coding. This means the row Trust Violation reflects the comparison of the mean of all measurements within the trust violation phase and the mean of all measures in the initial phase. The row Trust Repair reflects the comparison of the mean of all measurements within the trust repair phase and the mean of all measures in the trust violation phase. N = 121.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>The Trust Process Investigated in the Current Study ( <ref type="figure">Figure Based</ref> on <ref type="bibr" target="#b44">Mayer et al., 1995)</ref> Trust Reliance</p><p>Outcomes, behavior, and information associated with the trustee (results of the preselection, trust repair interventions) Ability Flexibility Benevolence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrity</head><p>Measured facets of trustworthiness</p><p>The trustee (human or system) and information about the trustee (imperfection information) affect initial trustworthiness assessments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1, RQ2, H3</head><p>The trustee (human or system) and information about the trustee (imperfection information) moderate the relation between outcomes, behavior, and information associated with the trustee (results of the preselection, trust repair interventions) and trustworthiness assessments H1, H2, H4, RQ2, RQ3</p><p>Notes. EH = exploratory hypothesis, H = hypothesis, RQ = research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>Line Graphs for the Mean Values of the Dependent Variables Measured at <ref type="bibr">Tasks 1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">9,</ref><ref type="bibr">and 12</ref> Notes. T = Task. Error bars indicate standard errors. why our products are tailored to fit our customers. Customer satisfaction is a central goal of the company.</p><p>-Goal Innovation: The company is also committed to new technologies and innovation and thus offers customers a wide range of digital solutions.</p><p>-Goal Diversity: The company is a is dedicated to the Diversity Charter to promote active diversity management and gender equality. Our values define the working environment we want to create -diversity, openness and equal opportunities</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(2020) who adapted the items fromWang and Benbasat (2005). A sample item was "I believe that the colleague/the automated system makes unbiased decisions." Benevolence was measured with two items taken from Wang and Benbasat (2005). A sample item was "The colleague/the automated system would consider my interests." Trustworthiness was calculated as the mean of the items for Ability, Flexibility, Integrity and Benevolence. Trust was measured with two items taken from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Notes.</head><label></label><figDesc>Coding of Trustee: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding imperfection. For the coding of the effects over time (i.e., Trust Violation and Trust Repair) we used backward difference coding. This means the row Trust Violation reflects the comparison of the mean of all measurements within the trust violation phase and the mean of all measures in the previous phase. The row Trust Repair reflects the comparison of the mean of all measurements within the trust repair phase and the mean of all measures in the trust violation phase. N = 121.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Notes.</head><label></label><figDesc>Coding of Trustee: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding imperfection. For the coding of the effects over time (i.e., Trust Violation and Trust Repair) we used backward difference coding. This means the row Trust Violation reflects the comparison of the mean of all measurements within the trust violation phase and the mean of all measures in the initial phase. The row Trust Repair reflects the comparison of the mean of all measurements within the trust repair phase and the mean of all measures in the trust violation phase. N = 121.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 Line</head><label>3</label><figDesc>Graphs for the Mean Values of the Dependent Variables Measured for all Tasks Notes. T = Task. Error bars indicate standard errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Means, Standard Deviations, and Correlations Over all Measurement Points</figDesc><table><row><cell>Variable</cell><cell>M</cell><cell>SD</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>1. Ability</cell><cell cols="2">4.36 0.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. Flexibility</cell><cell cols="2">4.11 0.95</cell><cell>.62 **</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. Integrity</cell><cell cols="2">4.18 1.11</cell><cell>.21 *</cell><cell>-.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4. Benevolence</cell><cell cols="2">3.81 0.96</cell><cell>.29 **</cell><cell>.29 **</cell><cell>.28 **</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5. Trustworthiness</cell><cell cols="2">4.11 0.66</cell><cell>.77 **</cell><cell>.67 **</cell><cell>.58 **</cell><cell>.69 **</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6. Trust</cell><cell cols="2">3.66 0.98</cell><cell>.70 **</cell><cell>.55 **</cell><cell>.17</cell><cell>.32 **</cell><cell>.63 **</cell><cell></cell><cell></cell><cell></cell></row><row><cell>7. Reliance</cell><cell cols="2">1.57 0.21</cell><cell>.41 **</cell><cell>.27 **</cell><cell>.18 *</cell><cell>.23 *</cell><cell>.40 **</cell><cell>.41 **</cell><cell></cell><cell></cell></row><row><cell>8. Imperfection</cell><cell>-</cell><cell>-</cell><cell>.02</cell><cell>.01</cell><cell>.04</cell><cell>-.06</cell><cell>.01</cell><cell>.06</cell><cell>-.06</cell><cell></cell></row><row><cell>9. Human vs. system</cell><cell>-</cell><cell>-</cell><cell>-.34 **</cell><cell>-.56 **</cell><cell>.43 **</cell><cell>.00</cell><cell>-.14</cell><cell>-.41 **</cell><cell>.05</cell><cell>.04</cell></row><row><cell cols="11">Note. Coding of Human vs. System: 0 = human, 1 = system. Coding of Imperfection: 0 = no information regarding imperfection, 1 = information regarding</cell></row><row><cell>imperfection. N = 121</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* p &lt; .05.** p &lt; .01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Percentage of Participants Who Accepted and Rejected the Preselection in Each Task</figDesc><table><row><cell>T1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>HLM Results for Fairness and the Diversity Goal</figDesc><table><row><cell>Fairness</cell><cell>Diversity Goal</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Regression Results for Overall Trustworthiness and Trust for the First Task</figDesc><table><row><cell></cell><cell></cell><cell>Trustworthiness</cell><cell></cell><cell></cell><cell>Trust</cell><cell></cell></row><row><cell>Predictors</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell></row><row><cell>(Intercept)</cell><cell>4.59</cell><cell cols="2">4.40, 5.02 &lt; .01</cell><cell>4.43</cell><cell>3.99, 4.87</cell><cell>&lt; .01</cell></row><row><cell>Trustee</cell><cell cols="3">-0.52 -1.15, -0.25 .02</cell><cell>-1.03</cell><cell>-1.67, -0.39</cell><cell>&lt; .01</cell></row><row><cell>Imperfection</cell><cell cols="2">-0.10 -0.57, 0.27</cell><cell>.61</cell><cell>0.21</cell><cell>-0.39, 0.81</cell><cell>.49</cell></row><row><cell cols="3">Trustee×Imperfection 0.18 -0.27, 0.97</cell><cell>.52</cell><cell>0.09</cell><cell>-0.76, 0.95</cell><cell>.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Regression Results for the Facets of Trustworthiness for the First Task</figDesc><table><row><cell></cell><cell></cell><cell>Ability</cell><cell></cell><cell></cell><cell>Flexibility</cell><cell></cell><cell></cell><cell>Integrity</cell><cell></cell><cell></cell><cell>Benevolence</cell><cell></cell></row><row><cell>Predictors</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell></row><row><cell>(Intercept)</cell><cell>5.34</cell><cell>4.90, 5.78</cell><cell>&lt; .01</cell><cell>5.29</cell><cell>4.91, 5.67</cell><cell>&lt; .01</cell><cell>3.79</cell><cell>3.34, 4.23</cell><cell>&lt; .01</cell><cell>3.95</cell><cell>3.55, 4.35</cell><cell>&lt; .01</cell></row><row><cell>Trustee</cell><cell cols="3">-1.40 -2.04, -0.76 &lt; .01</cell><cell cols="2">-1.89 -2.44, -1.33</cell><cell>&lt; .01</cell><cell>0.97</cell><cell>0.32, 1.62</cell><cell>&lt; .01</cell><cell>0.21</cell><cell>-0.37, 0.80</cell><cell>.47</cell></row><row><cell>Imperfection</cell><cell cols="2">-0.26 -0.86, 0.34</cell><cell>.39</cell><cell cols="2">-0.21 -0.73, 0.31</cell><cell>.42</cell><cell>-0.21</cell><cell>-0.82, 0.40</cell><cell>.49</cell><cell>0.28</cell><cell>-0.26, 0.82</cell><cell>.31</cell></row><row><cell>Trustee×Imperfection</cell><cell cols="2">0.38 -0.47, 1.24</cell><cell>.38</cell><cell cols="2">0.44 -0.30, 1.18</cell><cell>.24</cell><cell>0.39</cell><cell>-0.47, 1.26</cell><cell>.37</cell><cell>-0.48</cell><cell>-1.26, 0.29</cell><cell>.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>HLM Results for Trustworthiness and Trust</figDesc><table><row><cell>Trustworthiness</cell><cell>Trust</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>HLM Results for the Facets of Trustworthiness</figDesc><table><row><cell></cell><cell></cell><cell>Ability</cell><cell></cell><cell></cell><cell>Flexibility</cell><cell></cell><cell></cell><cell>Integrity</cell><cell></cell><cell></cell><cell>Benevolence</cell><cell></cell></row><row><cell>Predictors</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell><cell>B</cell><cell>95%CI</cell><cell>p</cell></row><row><cell>(Intercept)</cell><cell>4.74</cell><cell>4.42, 5.07</cell><cell>&lt; .01</cell><cell>4.67</cell><cell>4.38, 4.96</cell><cell>&lt; .01</cell><cell>3.76</cell><cell cols="2">3.39, 4.14 &lt; .01</cell><cell>3.69</cell><cell>3.34, 4.04</cell><cell>&lt; .01</cell></row><row><cell>TV</cell><cell>-1.51</cell><cell cols="2">-1.89, -1.13 &lt; .01</cell><cell>-1.54</cell><cell cols="2">-1.88, -1.19 &lt; .01</cell><cell cols="3">-0.96 -1.30, -0.63 &lt; .01</cell><cell>-0.74</cell><cell>-1.05, -0.44</cell><cell>&lt; .01</cell></row><row><cell>TR</cell><cell>1.25</cell><cell>0.87, 1.63</cell><cell>&lt; .01</cell><cell>1.36</cell><cell>1.01, 1.70</cell><cell>&lt; .01</cell><cell>0.96</cell><cell cols="2">0.62, 1.29 &lt; .01</cell><cell>0.50</cell><cell>0.19, 0.81</cell><cell>&lt; .01</cell></row><row><cell>Trustee</cell><cell>-0.86</cell><cell cols="2">-1.34, -0.39 &lt; .01</cell><cell>-1.20</cell><cell cols="2">-1.63, -0.77 &lt; .01</cell><cell>0.77</cell><cell>0.22, 1.31</cell><cell>.01</cell><cell>0.38</cell><cell>-0.13, 0.90</cell><cell>.14</cell></row><row><cell>Imperfection</cell><cell>-0.14</cell><cell>-0.58, 0.30</cell><cell>.53</cell><cell>-0.07</cell><cell>-0.47, 0.33</cell><cell>.73</cell><cell>-0.11</cell><cell>-0.61, 0.40</cell><cell>.68</cell><cell>0.21</cell><cell>-0.27, 0.69</cell><cell>.38</cell></row><row><cell>TV×Trustee</cell><cell>1.27</cell><cell>0.71, 1.82</cell><cell>&lt; .01</cell><cell>1.51</cell><cell>1.00, 2.01</cell><cell>&lt; .01</cell><cell>0.55</cell><cell>0.07, 1.04</cell><cell>.03</cell><cell>0.59</cell><cell>0.15, 1.04</cell><cell>.01</cell></row><row><cell>TR×Trustee</cell><cell>-1.14</cell><cell cols="2">-1.70, -0.58 &lt; .01</cell><cell>-1.33</cell><cell cols="2">-1.83, -0.82 &lt; .01</cell><cell cols="3">-0.83 -1.31, -0.34 &lt; .01</cell><cell>-0.46</cell><cell>-0.90, -0.02</cell><cell>.04</cell></row><row><cell>TV×Imperfection</cell><cell>0.49</cell><cell>-0.03, 1.00</cell><cell>.07</cell><cell>0.61</cell><cell>0.14, 1.08</cell><cell>.01</cell><cell>0.57</cell><cell>0.11, 1.03</cell><cell>.01</cell><cell>0.23</cell><cell>-0.19, 0.64</cell><cell>.29</cell></row><row><cell>TR×Imperfection</cell><cell>-0.50</cell><cell>-1.02, 0.02</cell><cell>.06</cell><cell>-0.90</cell><cell cols="2">-1.37, -0.42 &lt; .01</cell><cell cols="2">-0.49 -0.94, -0.03</cell><cell>.04</cell><cell>-0.14</cell><cell>-0.56, 0.27</cell><cell>.50</cell></row><row><cell>Trustee×Imperfection</cell><cell>0.43</cell><cell>-0.20, 1.06</cell><cell>.18</cell><cell>0.26</cell><cell>-0.31, 0.83</cell><cell>.37</cell><cell>0.33</cell><cell>-0.40, 1.05</cell><cell>.38</cell><cell>-0.67</cell><cell>-1.35, 0.02</cell><cell>.06</cell></row><row><cell>TV×Trustee×Imperfection</cell><cell>-0.77</cell><cell>-1.52, -0.03</cell><cell>.04</cell><cell>-0.95</cell><cell>-1.63, -0.28</cell><cell>.01</cell><cell>-0.50</cell><cell>-1.15, 0.15</cell><cell>.13</cell><cell>-0.70</cell><cell>-1.29, -0.10</cell><cell>.02</cell></row><row><cell>TR×Trustee×Imperfection</cell><cell>0.98</cell><cell>0.24, 1.72</cell><cell>.01</cell><cell>1.24</cell><cell>0.57, 1.92</cell><cell>&lt; .01</cell><cell>0.46</cell><cell>-0.19, 1.11</cell><cell>.16</cell><cell>0.38</cell><cell>-0.22, 0.97</cell><cell>.21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When referring to AI, we explicitly include classical approaches such as rule-based systems but also novel approaches from the area of machine learning and deep learning(Cheng &amp; Hackett, 2019).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Research Questions and Hypotheses were preregistered under https://aspredicted.org/blind.php?x=gh8xa4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In the preregistration we mentioned moral reasoning as dependent variable but, for reasons of readability, we do not include results for this variable. Additionally, we measured perceived transparency but received feedback on previous versions of this manuscript questioning the value of measuring transparency in our study design. Data and results can be made available upon request. In the preregistration, we mentioned the following research question: "Will participants realize trust violations by the automated agent later?" but omitted those analyses as we realized they do not provide insights beyond the other hypotheses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If we include participants who did not respond correctly to the manipulation checks regarding whether the advice came from a human or a system, whether there were information regarding imperfection, and whether there were rounds where there were predominantly male applicants (resulting in N = 188), the results remain stable (i.e., regarding significances and directions of effects).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Including propensity to trust in the analyses did not change the results in a way that would have changed the interpretation of the results. Those results can be made available upon request.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">system as trustee seems to affect initial trustworthiness assessments and moderate the effects of trust violations and trust repair interventionsbut partly different than could have been expected.Thus, our results emphasize that we cannot assume that effects from classical application areas of automated systems, where the main drivers of trustworthiness assessments are effectiveness and efficiency, translate to the use of automated systems for decisions that affect the fate of individuals and where trustworthiness assessments also depend on ethical considerations. We thus hope that this study can inspire future work investigating trust processes in the emerging context of AI-based systems in ethically sensitive domains.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Presentation of the preselection in the initial and in the final phase after the trust repair intervention.</p><p>The number of male and female applicants is similar. Information for applicants are their name, age, job experience, final grades, and strengths.</p><p>Presentation of the preselection in the trust violation phase. There are predominantly male applicants.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Misuse of diagnostic aids in process control: The effects of automation misses on complacency and automation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Bahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Elepfandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/154193120805201906</idno>
		<ptr target="https://doi.org/10.1177/154193120805201906" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">People are averse to machines making moral decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<idno type="DOI">10.1016/j.cognition.2018.08.003</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.08.003" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="21" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithmic discrimination causes less moral outrage than human discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arnestad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Psyarxiv</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/m3nrp</idno>
		<ptr target="https://doi.org/10.31234/osf.io/m3nrp" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advice taking and decision-making: An integrative literature review, and implications for the organizational sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonaccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Dalal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="151" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.obhdp.2006.07.001</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2006.07.001" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A critical review of algorithms in HRM: Definition, theory, and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hackett</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.hrmr.2019.100698</idno>
		<ptr target="https://doi.org/10.1016/j.hrmr.2019.100698" />
	</analytic>
	<monogr>
		<title level="j">Human Resource Management Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100698</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Amazon scraps secret AI recruiting tool that showed bias against women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dastin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reuters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Com</surname></persName>
		</author>
		<ptr target="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Almost human: Anthropomorphism increases trust resilience in cognitive agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>De Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mckendrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/xap0000092</idno>
		<ptr target="https://doi.org/10.1037/xap0000092" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From &apos;automation&apos; to &apos;autonomy&apos;: The importance of trust repair in human-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>De Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Shaw</surname></persName>
		</author>
		<idno type="DOI">10.1080/00140139.2018.1457725</idno>
		<ptr target="https://doi.org/10.1080/00140139.2018.1457725" />
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1409" to="1427" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empowering behaviour and leader fairness and integrity: Studying perceptions of ethical leader behaviour from a levels-of-analysis perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Den Hartog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H B</forename><surname>De Hoogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Work and Organizational Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1080/13594320802362688</idno>
		<ptr target="https://doi.org/10.1080/13594320802362688" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">People Reject Algorithms in Uncertain Decision Domains Because They Have Diminishing Sensitivity to Forecasting Error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bharti</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797620948841</idno>
		<ptr target="https://doi.org/10.1177/0956797620948841" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1302" to="1314" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The role of trust in automation reliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Dzindolet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Pomranky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Beck</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1071-5819(03</idno>
		<ptr target="https://doi.org/10.1016/S1071-5819(03" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">New information technology and implicit bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Elsbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stigliani</surname></persName>
		</author>
		<idno type="DOI">10.5465/amp.2017.0079</idno>
		<ptr target="https://doi.org/10.5465/amp.2017.0079" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="185" to="206" />
		</imprint>
	</monogr>
	<note>Academy of Management Perspectives</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From here to autonomy: Lessons learned from human-automation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720816681350</idno>
		<ptr target="https://doi.org/10.1177/0018720816681350" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="27" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.3758/brm.41.4.1149</idno>
		<ptr target="https://doi.org/10.3758/brm.41.4.1149" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1149" to="1160" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The perceived fairness of selection systems: An organizational justice perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Gilliland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of Management Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="694" to="734" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.2307/258595</idno>
		<ptr target="https://doi.org/10.2307/258595" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Human trust in artificial intelligence: Review of empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Glikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Woolley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Academy of Management Annals</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="627" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<idno type="DOI">10.5465/annals.2018.0057</idno>
		<ptr target="https://doi.org/10.5465/annals.2018.0057" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human decision making with machine assistance: An experiment on bailing and jailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grgić-Hlača</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359280</idno>
		<ptr target="https://doi.org/10.1145/3359280" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical-statistical controversy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Meehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology, Public Policy, and Law</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="323" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1037/1076-8971.2.2.293</idno>
		<ptr target="https://doi.org/10.1037/1076-8971.2.2.293" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated video interview personality assessments: Reliability, validity, and generalizability investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Advance Online Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.31234/osf.io/a62jv</idno>
		<ptr target="https://doi.org/10.31234/osf.io/a62jv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The automation of leadership functions: Would people trust decision algorithms? Computers in Human Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höddinghaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sondern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hertel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2020.106635</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2020.106635" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">106635</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Trust in automation: Integrating empirical evidence on factors that influence trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bashir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0018720814547570</idno>
		<ptr target="https://doi.org/10.1177/0018720814547570" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The global landscape of AI ethics guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ienca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vayena</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0088-2</idno>
		<ptr target="https://doi.org/10.1038/s42256-019-0088-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">When more blame is better than less: The implications of internal vs. External attributions for the repair of trust after a competence-vs. Integrity-based trust violation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Dirks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ferrin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2005.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2005.07.002" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="49" to="65" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Changing the means of managerial work: Effects of automated decision-support systems on personnel selection tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Busch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10869-020-09711-</idno>
		<ptr target="https://doi.org/10.1007/s10869-020-09711-" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Psychology</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Advance Online Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Highly automated interviews: Applicant reactions and the organizational context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-P</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1108/JMP-09-2018-0402</idno>
		<ptr target="https://doi.org/10.1108/JMP-09-2018-0402" />
	</analytic>
	<monogr>
		<title level="j">Journal of Managerial Psychology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="314" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Trust in automation: Designing for appropriate reliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>See</surname></persName>
		</author>
		<idno type="DOI">10.1518/hfes.46.1.50.30392</idno>
		<ptr target="https://doi.org/10.1518/hfes.46.1.50.30392" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951718756684</idno>
		<ptr target="https://doi.org/10.1177/2053951718756684" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">205395171875668</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Resistance to medical artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/jcr/ucz013</idno>
		<ptr target="https://doi.org/10.1093/jcr/ucz013" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Similarities and differences between human-human and human-automation trust: An integrative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wiegmann</surname></persName>
		</author>
		<idno type="DOI">10.1080/14639220500337708</idno>
		<ptr target="https://doi.org/10.1080/14639220500337708" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="301" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ethical implications and accountability of algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10551-018-3921-3</idno>
		<ptr target="https://doi.org/10.1007/s10551-018-3921-3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Ethics</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An integrative model of organizational trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Schoorman</surname></persName>
		</author>
		<idno type="DOI">10.2307/258792</idno>
		<ptr target="https://doi.org/10.2307/258792" />
	</analytic>
	<monogr>
		<title level="j">Academy of Management Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="709" to="726" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Trust in Management Information Systems (MIS): A theoretical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Meeßen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Thielsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hertel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>Zeitschrift Für Arbeits-Und Organisationspsychologie</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<idno type="DOI">10.1026/0932-4089/a000306</idno>
		<ptr target="https://doi.org/10.1026/0932-4089/a000306" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The impact of using algorithms for managerial decisions on public employees&apos; procedural justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nagtegaal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.giq.2020.101536</idno>
		<ptr target="https://doi.org/10.1016/j.giq.2020.101536" />
	</analytic>
	<monogr>
		<title level="j">Government Information Quarterly</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101536</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When eliminating bias isn&apos;t fair: Algorithmic reductionism and procedural justice in human resource decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Harmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="149" to="167" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.obhdp.2020.03.008</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2020.03.008" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Big data in industrialorganizational psychology and human resource management: Forward progress for organizational research and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Behrend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Putka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sinar</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-orgpsych-032117-104553</idno>
		<ptr target="https://doi.org/10.1146/annurev-orgpsych-032117-104553" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Organizational Psychology and Organizational Behavior</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="505" to="533" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Complacency and bias in human use of automation: An attentional integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Manzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="410" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0018720810376055</idno>
		<ptr target="https://doi.org/10.1177/0018720810376055" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Humans and automation: Use, misuse, disuse, abuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Riley</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872097778543886</idno>
		<ptr target="https://doi.org/10.1518/001872097778543886" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="253" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automation, algorithms, and beyond: Why work design matters more than ever in a digital world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grote</surname></persName>
		</author>
		<idno type="DOI">10.1111/apps.12241</idno>
		<ptr target="https://doi.org/10.1111/apps.12241" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychology</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Advance Online Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mitigating bias in algorithmic hiring: Evaluating claims and practices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372828</idno>
		<ptr target="https://doi.org/10.1145/3351095.3372828" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 FAT* Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 FAT* Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Artificial intelligence and management: The automationaugmentation paradox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krakoswki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Academy of Management Review</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<idno type="DOI">10.5465/amr.2018.0072</idno>
		<ptr target="https://doi.org/10.5465/amr.2018.0072" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Juni 2019 [Proportion of women and men in different occupational groups in Germany on the 30th of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rudnicka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Anteil von Frauen und Männern in verschiedenen Berufsgruppen in Deutschland am 30. Statista</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Artificial intelligence in human resources management: Challenges and a path forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yakubovich</surname></persName>
		</author>
		<idno type="DOI">10.1177/0008125619867910</idno>
		<ptr target="https://doi.org/10.1177/0008125619867910" />
	</analytic>
	<monogr>
		<title level="j">California Management Review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="15" to="42" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Trust and distrust in information systems at the workplace. PeerJ, 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Thielsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Meeßen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hertel</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.5483</idno>
		<ptr target="https://doi.org/10.7717/peerj.5483" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Notes</surname></persName>
		</author>
		<title level="m">NI = no information, I = information, H = human, S = system, T = total. n NI_H = 33, n NI_S = 35</title>
		<imprint/>
	</monogr>
	<note>n I_H =,28, n I_S = 25. T1 until T4 represent the initial phase, T5 until T8 represent the trust violation phase, T9 until T12 represent the trust repair phase</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
