<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from rewards and social information in naturalistic strategic behavior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionatan</forename><surname>Kuperwajs</surname></persName>
							<email>ikuperwajs@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Opheusden</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">M</forename><surname>Russek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from rewards and social information in naturalistic strategic behavior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Acting intelligently in complex environments poses a challenging learning problem: faced with many different situations and possible actions, how do people learn which action to take in each situation? While traditional laboratory-based experiments have been used to study specific learning mechanisms, these experiments often employ relatively simple tasks conducted over a short period of time. Thus, it is unclear to what extent these mechanisms are used in the significantly more complex and temporally extended environments people encounter in their everyday lives. To understand the processes by which people learn policies to guide their decisions, we investigate the opening strategies of novice online chess players over their first months of play. We use a large online data set consisting of 2, 499, 783 games, providing us with the necessary scale to explore learning mechanisms in a complex setting. In particular, we focus on two types of learning: reinforcement learning, or learning from rewards given repeated experiences, and social learning, or learning from the actions of others. We show that players&apos; choices are modulated by both game outcomes and observing their opponents&apos; actions, and that they exhibit important hallmarks of adaptive decision-making such as exploration and expertise. Our results provide evidence that people use sophisticated learning algorithms in naturalistic strategic behavior.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The ability to learn adaptive decision policies over the course of our lifetime is a core aspect of human cognition. Much progress has been made by studying learning using computational theories derived from constrained laboratory studies. Tasks employed in such studies aim to elicit specific behaviors that are amenable to precise analysis and modeling. The ultimate goal of such a research program is to gradually build towards a complete understanding of how people learn by understanding the various components that comprise this cognitive process. However, this approach does not capture the complexity of learning in the real world, as it is limited to relatively simple tasks that are not intrinsically motivating and can be learned in short periods of time.</p><p>Traditional experiments also result in small data sets, limiting the potential to analyze complex behavior. To address these limitations, cognitive scientists have gradually begun transitioning towards large-scale online experiments to study learning, using them to test theories over many more data points in naturalistic environments <ref type="bibr" target="#b32">(Stafford and Dewar, 2014;</ref><ref type="bibr" target="#b33">Steyvers and Schafer, 2020</ref>; <ref type="bibr" target="#b23">Kuperwajs and Ma, 2022;</ref><ref type="bibr" target="#b1">Allen et al., 2023;</ref><ref type="bibr">Wise et al., 2023)</ref>.</p><p>Faced with complex learning problems, people seem to learn in two ways: from their environment and from each other <ref type="bibr">(Yoeli and Hoffman, 2022)</ref>. The former is known as reinforcement learning, which can be broadly defined as learning from one's own experience with a task. More specifically, reinforcement learning describes the process of learning to select actions that maximize long-term reward, and has emerged as a dominant framework for explaining human learning and decision-making <ref type="bibr" target="#b34">(Sutton and Barto, 2018;</ref><ref type="bibr" target="#b11">Dayan and Daw, 2008;</ref><ref type="bibr" target="#b10">Daw and Tobler, 2014)</ref>. Despite the vast number of different reinforcement learning algorithms, a key signature of reinforcement learning is that past actions and outcomes influence future choices. A second method by which people learn is through observing or interacting with others. Social learning is supported by a large body of work in psychology and anthropology, including the idea that it enables cultural evolution at the population level <ref type="bibr" target="#b20">(Henrich, 2016;</ref><ref type="bibr" target="#b24">Laland, 2017;</ref><ref type="bibr">Thompson et al., 2022)</ref>. At the individual level, people likely integrate the actions and outcomes of others into their LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 4 own reinforcement learning algorithm <ref type="bibr" target="#b27">(Olsson et al., 2020)</ref>. To truly ascertain how people make use of such sophisticated learning strategies outside of the laboratory, it is necessary to empirically investigate whether prior results scale to more complex tasks.</p><p>The ideal paradigm for studying the interplay between reinforcement and social learning should be complex enough that there is opportunity for disparate types of learning to occur simultaneously. Additionally, it should allow for interaction between participants, be highly engaging, and produce substantial amounts of behavioral data collected over a long period of time. Given these desiderata, one of the few tasks that has been historically used to characterize complex decision-making becomes a promising candidate: chess (De Groot, 2014; <ref type="bibr" target="#b7">Chase and Simon, 1973)</ref>. Improving one's play requires learned strategies, the two-player nature of the game provides a social component, and players are intrinsically motivated to solve the challenging problem of selecting good moves. Further, the recent popularity of online chess platforms has generated billions of matches played across multiple years that are available to the public, already leading to newfound insights regarding the cognitive processes underlying chess play <ref type="bibr" target="#b2">(Anderson et al., 2017;</ref><ref type="bibr" target="#b21">Holdaway and Vul, 2021;</ref><ref type="bibr" target="#b28">Russek et al., 2022)</ref>. For our use case, a massive data set allows us to validate the mechanisms by which people learn strategic behaviors in a naturally incentivized setting.</p><p>To summarize our results, we leveraged large-scale chess gameplay data to find signatures of reinforcement and social learning. Importantly, we constrained our analysis in two ways. First, we investigated key positions in different opening lines in order to mirror traditional laboratory-based learning experiments with states that repeat across trials. Second, we focused on novice players in their first several months of experience with each opening in our data set where they are likely to learn the most and still be developing strategies. We begin by using the Queen's Gambit as a case study, and show across 480, 932 games that players' choices are influenced by both prior outcomes as predicted by the reinforcement learning literature and moves made by LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 5 their opponents as predicted by the social learning literature. We then highlight various additional cognitive factors that interact with these mechanisms for learning, and show that our results generalize to 2, 018, 851 games consisting of other opening sequences such as the Sicilian Defense, the King's Knight, and the Caro-Kann Defense. Taken together, our findings suggest that people learn strategies via sophisticated mechanisms even in complex, naturalistic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We looked for signatures of reinforcement and social learning in a large-scale data set of human decisions from the online chess platform Lichess (https://lichess.org). Specifically, we investigated key moves from different openings lines. Previous studies have similarly focused on openings to quantify chess performance and expertise effects <ref type="bibr" target="#b8">(Chowdhary et al., 2023;</ref><ref type="bibr" target="#b14">De Marzo and Servedio, 2023)</ref>. Since our goal is to study long-term learning of strategies, we selected players within a lower playing strength range as measured by their Elo rating <ref type="bibr" target="#b15">(Elo, 1978)</ref> and examined games between October 2020 and March 2021. Additionally, we filtered for players who had never encountered the particular opening we were analyzing between March 2020 and September 2020 to control for prior learning of the opening via experience (see Materials and methods for more details). In the following sections, we show that people learn which move to make in a single opening through the outcomes associated with their own decisions as well as those of their opponents. We then turn to other openings to validate that these results generalize.</p><p>The Queen's Gambit as a case study in strategic learning The Queen's Gambit provides an intriguing case study in learning strategic behavior. This is in part due to the famous Netflix miniseries of the same name released in October 2020, which caused the opening to suddenly increase in popularity and forced players who had not previously encountered it to quickly learn a response. For reference, the Queen's Gambit jumped from being part of 1 in 24.32 of all games played in October 2020 to 1 in 23.00 of all games played just one month later. A gambit is also a specific kind of chess opening in which a player attempts to LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR sacrifice material with the aim of achieving a positional advantage. This means that the response to the Queen's Gambit reduces to a binary choice between accepting or declining the gambit, simplifying analyses and mirroring traditional laboratory-based investigations of learning within the broader context of a complex task. In other words, we isolate a single decision embedded in a larger sequence of decisions which mimics a two-alternative forced choice. <ref type="figure" target="#fig_0">Figure 1A</ref> shows this binary decision, where black can choose to accept by taking white's pawn on c4 or decline by defending their own pawn on d5.</p><p>To get a sense for which of the options presented in the Queen's Gambit is better for the black LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 7 player, we computed the win rate for each choice in our data set. We valued wins, draws, and losses at 1, 0.5, and 0 points respectively. Empirically, declining the gambit has a higher win rate compared to accepting it (t(480, 930) = 15.96, p &lt; 0.0001, <ref type="figure" target="#fig_0">Figure 1B</ref>). For this analysis, we only considered declining the gambit as including the two most commonly played responses. We note that, despite the empirical win rate among players in our data set, accepting the gambit is a known line of play that can be studied such that it becomes a good strategy and is used in high-level play.</p><p>In <ref type="figure" target="#fig_0">Figure 1C</ref>, we calculated the win rates for 28 different choices available to black when selecting a move, showing that defending the pawn on d5 is the decision with the highest overall win rate. Considering the broader distribution of available moves, there are a number of alternatives that lead to higher win rates than accepting the gambit, as well as many moves that lead to lower win rates than accepting the gambit. In all future analyses, we utilize responses reduced to the binary choice of accepting or declining the gambit. Additionally, when using the term games, we always specifically refer to Queen's Gambit games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signatures of reinforcement learning</head><p>Given this binary decision problem, our goal was to investigate whether people make decisions in consecutive Queen's Gambit games, regardless of the number of other games that occur between them, that align with prior results from reinforcement learning. The simplest form this can take is a win-stay, lose-shift policy, where repeating the choice to accept or decline the gambit is more likely if the outcome of the game associated with that choice was a win as opposed to a loss.   game, we enumerated every combination of choice and outcome for two games and computed the probability of repeating the second choice as a function of this history ( <ref type="figure" target="#fig_2">Figure 2B</ref>). An immediately striking result was the difference between stay and shift sequences, where the LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 9 probability of repetition was always much higher if the two choices were congruent regardless of outcomes. This indicates that people who have already repeated their choice will likely continue to do so, a concept often referred to as choice perseveration in reinforcement learning <ref type="bibr" target="#b0">(Aarts et al., 1998;</ref><ref type="bibr" target="#b26">Miller et al., 2019)</ref>. Among shift sequences, the probability of repetition was higher overall when choosing to accept after declining compared to the opposite, confirming that players have a baseline preference for accepting the gambit. We also found that given the same sequence of choice-outcome-choice, the probability of repetition was consistently higher if the outcome of the second game was a win rather than a loss. In other words, people's choices seem to be modulated most strongly by the outcome of their most recent game as predicted by reinforcement</p><p>learning, suggesting that choice values reflect a recency weighted average of experienced reward.</p><p>Finally, the largest difference in repetition probability between a loss and win in the second game occurred in the shift sequences in which the first game resulted in a loss. This indicates that people's behavior is most impacted by receiving a negative reward signal, changing their decision, and then receiving a positive reward signal. Importantly, sequences in which players transitioned from accepting to declining the gambit given the above scenario resulted in the largest overall difference in repetition probability, highlighting that reinforcement learning plays a critical role in discovering the better decision. The statistics and significance values for this analysis are available in <ref type="table" target="#tab_4">Tables S1 and S2</ref>.</p><p>Ultimately, we were interested in the long-term learning of strategies via reinforcement learning beyond one or two game histories. To investigate this, we correlated previous game outcomes and current choice up to 30 Queen's Gambit games ago across all players in the data set. Theoretical and experimental findings regarding reward prediction errors hypothesize that the effects of previous rewards on current associative strength effectively decline as an exponential function of number of experiences <ref type="bibr" target="#b25">(Lau and Glimcher, 2005;</ref><ref type="bibr" target="#b19">Glimcher, 2011)</ref>. monotonically decreasing impact on current decisions, and are qualitatively well fit by an exponential function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signatures of social learning and adaptive decision-making</head><p>Shifting from reinforcement to social learning, we wanted to confirm that players were more likely to decline the gambit the more games they played. Indeed, people exhibited baseline learning as they were gradually more likely to make the choice that led to a higher empirical win rate as a function of total experience ( <ref type="figure" target="#fig_3">Figure 3A</ref>). Then, we investigated situations in which a player encounters positions, in this case the Queen's Gambit, that their opponent has already played against them. Many players use the same opening repertoire with white and black, and these positions occur naturally in our data set. To do so, we plotted the same learning curve as before, but only for games played as white. This analysis revealed that players learned to decline the gambit at a high rate more quickly when observing their opponent decide whether to accept or decline. This suggests that learning from other people's actions is a key factor in making good LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 11 decisions.</p><p>We then characterized two additional mechanisms of social learning. The first is a copy effect, where players are more likely to decline the gambit themselves if they observed their opponent doing so in their most recent game as white (t(74, 785) = 3.27, p &lt; 0.01, <ref type="figure" target="#fig_3">Figure 3B</ref>). This occurs irrespective of game outcome. Second, we were interested in the interaction between reinforcement and social learning. A relatively straightforward social learning strategy would be to incorporate color-reversed data as additional data in one's own reinforcement learning algorithm and treat it similarly to firsthand experience. This would predict correlations between the outcomes of past games and the tendency for players to repeat their opponent's moves in future games. <ref type="figure" target="#fig_3">Figure 3C</ref> demonstrates that people were more likely to repeat their opponent's decision to accept or decline the gambit if the outcome of that game led to their own loss as opposed to their own win, although the effect was not statistically significant (t(71, 299) = 1.26, p = 0.21). Regardless, people may take the outcomes of their opponent's actions into account, incorporating them into their own reinforcement learning policy.</p><p>In the next part of our analysis, we wanted to test whether people adhere to general psychological principles of adaptive decision-making in this naturalistic context. The first is exploration: do players always make the best move according to their current beliefs, or do they strategically test alternatives in order to gain broader experience? Navigating this exploration-exploitation tradeoff has been extensively studied in multi-armed bandit tasks <ref type="bibr" target="#b9">(Cohen et al., 2007;</ref><ref type="bibr" target="#b17">Gershman, 2018)</ref> as well as in real-world settings <ref type="bibr" target="#b29">(Schulz et al., 2019;</ref><ref type="bibr" target="#b4">Br√§ndle et al., 2023)</ref>, with various reinforcement learning algorithms developed to solve this problem <ref type="bibr" target="#b18">(Gittins, 1979;</ref><ref type="bibr" target="#b3">Auer et al., 2002)</ref>. Perhaps the simplest indicator of exploration is whether people change their choices from one instance of the gambit to the next. Exploring less as the values associated with actions are better known is a defining property of uncertainty-driven exploration <ref type="bibr">(Wilson et al., 2014)</ref>. In <ref type="figure" target="#fig_5">Figure 4A</ref>, we show that players are more likely to shift their decision to accept or decline when  they have not played many games. This slowly decreases with experience, indicating that players eventually settle on a preferred strategy.</p><p>Response times are a common measure of cognitive resources spent in a task, where executing additional computations incurs a cost but can be beneficial in discovering better decisions.</p><p>Following this, we measured the probability of declining the gambit as a function of time taken to make a move ( <ref type="figure" target="#fig_5">Figure 4B</ref>). In general, longer response times led to a higher probability of declining the gambit, suggesting that thinking for more time allowed players to select a better move more often. The exception to this is at very short response times, which we interpret as players also being more likely to decline if they have a preferred move in mind before their turn begins. One possible mechanism driving this result is planning, or the process of evaluating the future consequences of currently available options. Deeper forward search is typically correlated with better action selection <ref type="bibr" target="#b30">(van Opheusden et al., 2023)</ref>, and response times have previously LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 13 been utilized as a proxy for number of planning computations <ref type="bibr" target="#b6">(Chabris and Hearst, 2003;</ref><ref type="bibr" target="#b28">Russek et al., 2022;</ref><ref type="bibr" target="#b22">Kuperwajs et al., 2023)</ref>. As such, players may be more likely to decline the gambit because they plan further into the future and evaluate the outcomes associated with doing so.</p><p>Since people can decide to play games at-will on online chess platforms, another important influence on decision-making is the delay between games that contain the Queen's Gambit.</p><p>Learning mechanisms have been shown to be dependent on timescale, an example being that shorter delays between trials lead to faster learning due to the interaction between working memory and reward-based learning (van de Vijver and Ligneul, 2020; Wimmer and Poldrack, 2022). In our case, we found that players were less likely to decline the gambit the more days elapsed between games in which they encountered the position ( <ref type="figure" target="#fig_5">Figure 4C</ref>). Taken in the context of our prior results on reinforcement learning, it is reasonable to assume that these effects are more pronounced in games that are played in close proximity to each other.</p><p>Lastly, we were interested in expertise effects. A growing body of literature has investigated how expert chess players differ from their less-skilled counterparts, explaining their superior performance due to contributions from both improved pattern recognition and deeper search <ref type="bibr" target="#b16">(Ericsson and Smith, 1991;</ref><ref type="bibr" target="#b5">Campitelli and Gobet, 2004)</ref>. In our data set, we computed the probability of declining the gambit for different Elo ratings, finding that stronger players were more likely to decline ( <ref type="figure" target="#fig_5">Figure 4D</ref>). This result also includes a social learning component:</p><p>stronger players are themselves paired with stronger opponents ( <ref type="figure" target="#fig_0">Figure S1</ref>), meaning that they can selectively learn from players with higher Elo ratings. Since our data set is composed of novice players, even stronger players are still developing towards expertise. Importantly, though, they likely exhibit higher expertise than the majority of subjects performing completely novel tasks in traditional laboratory studies.  Defense. As before, when using the term games we always refer to games of each opening respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR</head><p>In the Sicilian Defense, players most often choose to play closed by moving one of their knights to c3 or play open by moving their other knight to f3 ( <ref type="figure" target="#fig_6">Figure 5A</ref>). When we computed the empirical win rates for these alternatives, playing open had a higher win rate compared to playing closed (t(714, 216) = 12.68, p &lt; 0.0001). A broader distribution of available moves is shown in <ref type="figure" target="#fig_2">Figure S2A</ref>, with playing open only having the second highest win rate and many options between playing closed and open and below playing closed. We replicated our main reinforcement learning finding, where recent outcomes are more highly weighted in their correlation with current choices when compared to outcomes further in the past that have an exponentially decreasing effect on decisions in the present. In <ref type="figure" target="#fig_3">Figure S3A</ref> we show the reinforcement learning effect for a single game, and in <ref type="figure" target="#fig_5">Figure S4</ref> the reinforcement learning effect for a two game history. The main effects all trend in the same direction as in the Queen's Gambit, and the two game history analysis shows that the probability of repetition was higher  <ref type="figure" target="#fig_6">Figure 5B</ref>). The win rates for a broader distribution of available moves are shown in <ref type="figure" target="#fig_2">Figure S2B</ref>, with only one move having a higher win rate than the two moves we considered. We then replicate our previous reinforcement learning findings, again showing a strong correlation between current choices and past outcomes. The single and two game history analyses are available in <ref type="figure" target="#fig_3">Figures S3B and S5</ref> respectively, with the statistics and significance values for the two game history analysis available in <ref type="table">Tables S5 and S6</ref>. While we find the same copy effect in the King's Knight (t(773, 510) = 10.84, p &lt; 0.0001), the opponent reinforcement learning effect is actually reversed (t(745, 746) = 4.25, p &lt; 0.0001). That is to say, players in the King's Knight are more likely to repeat their opponent's decision if it leads to their own win rather than their own loss. Overall, while the Sicilian Defense captured aspects of both reinforcement and social learning equally, the King's Knight more strongly displays reinforcement learning effects.</p><p>The last opening we analyzed is the Caro-Kann Defense, where players most often choose to exchange by capturing the pawn on d5 or advance by moving their pawn to e5. Similarly to the Queen's Gambit, the Caro-Kann Defense has gained in popularity in recent years because one of the most prolific chess content creators, Levy Rozman, advertised the opening often while streaming. For this opening, advancing has a higher empirical win rate when compared to exchanging (t(78, 721) = 9.65, p &lt; 0.0001, <ref type="figure" target="#fig_6">Figure 5C</ref>). The win rates for a broader distribution of available moves are shown in <ref type="figure" target="#fig_2">Figure S2B</ref>, with several moves having higher and lower win rates than the two moves we consider. We then replicate our previous reinforcement learning findings, again showing a correlation between current choices and past outcomes. Note that for the Caro-Kann Defense, this correlation decreases much more rapidly when compared to other openings, suggesting that only the most recent outcomes impact decision-making. The single and two game history analyses are available in <ref type="figure" target="#fig_3">Figures S3C and S6</ref> respectively, with the statistics and significance values for the two game history analysis available in Tables S7 and S8. We found that both the copy (t(5, 349) = 0.80, p = 0.42) and opponent reinforcement learning (t(5, 174) = 2.95, p &lt; 0.01) effects replicated, although the former was not significant. In contrast to the King's Knight, the Caro-Kann Defense more strongly displays social as opposed to reinforcement learning effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Humans have a remarkable ability to learn flexible decision policies. In this work, we tested whether two mechanisms underlying this hallmark of cognition, reinforcement and social learning, apply in the context of naturalistic strategic behavior. Specifically, we showed that theories developed from constrained laboratory studies of learning scale to complex environments, namely selecting which move to make in the opening of a chess match. We used the Queen's Gambit as a case study to show that the decisions made by novice players are influenced by the associations between prior choices and their outcomes. Then, we found that observing an opponent's behavior also affected decision-making. Finally, we examined additional cognitive processes that impact learning, and validated that signatures of reinforcement and social learning generalized to other openings. Ultimately, our results help advance our understanding of the algorithms that people employ during strategic behavior in complex settings.</p><p>While we uncovered clear evidence for reinforcement learning across multiple opening lines, there are several remaining questions to be addressed in this domain. Despite evidence that game outcome is a reward signal, it is certainly not the only rewarding signal present during gameplay.</p><p>For example, another signal might be the estimated value of the board position immediately following the decision or at any point until the end of the game. This type of signal represents an opposing extreme of temporal difference learning <ref type="bibr" target="#b35">(Tesauro et al., 1995</ref>  <ref type="bibr">Dayan, 1992)</ref>.</p><p>In terms of social learning, our analysis was limited to the individual level. However, social learning also enables new ideas to spread throughout a community. Studying this kind of cultural transmission requires identifying a new idea or strategy and tracking the mechanisms that permit it to propagate through people's play. One promising source of new ideas in chess is artificial intelligence, and in particular AlphaZero <ref type="bibr" target="#b31">(Silver et al., 2018)</ref>. AlphaZero was trained entirely from self-play and developed a novel understanding of the game. Thus, one promising research direction is to identify instances where individual players used a theme derived from AlphaZero and then examining whether social learning mechanisms can account for the spread of these themes throughout the community. This approach has already been successfully applied to Go, another complex task where strategic behavior is essential <ref type="bibr" target="#b30">(Shin et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 19</head><p>To conclude, we hope that our findings serve to highlight the necessity of moving beyond traditional laboratory-based studies for understanding how people learn. The increasing availability of large-scale, naturalistic data sets such as the one used here will pave the way for exciting advances in characterizing human cognition. If we ultimately seek to construct detailed, process-level models that are capable of describing and predicting behavior in complex environments such as chess, then gaining insight into the mechanisms by which people reason in such tasks is a valuable step towards that goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>The raw data consists of standard rated games played on the online chess platform Lichess (https://lichess.org) stored as Portable Game Notation (PGN) files. We downloaded all games between October 2020 and March 2021, filtering for games that contained the moves corresponding to the opening line we were investigating. We used all games in which the sum of the time control setting and the increment multiplied by 60 was between 120 and 300.</p><p>Additionally, we limited our analysis to players with an Elo rating between 800 and 1200 as computed on Lichess using the Glicko 2 system. To enable this preprocessing, we used Scoutfish, a program that allows for high-speed, flexible queries from very large chess databases. We then extracted the desired information from each game: player names for white and black, the Coordinated Universal Time for the game, the move played in response to the board position we were analyzing along with its corresponding response time, the outcome of the game, Elo ratings for white and black, and number of previously played games as white and black. Note that to compute response times, we take the difference in clock between a player's successive moves. We also downloaded all games between March 2020 and September 2020, repeating the process above to extract the set of unique player names who had played each particular opening line. This allowed us to remove all users who had previously encountered the opening from our analysis.</p><p>Finally, we reduced the data set further by only analyzing moves that were included in our defined van Opheusden, B., <ref type="bibr">Kuperwajs, I., Galbiati, G., Bnaya, Z., Li, Y., &amp; Ma, W. J. (2023)</ref>. Expertise increases planning depth in human gameplay. Nature, 618 <ref type="formula">7967</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional analyses</head><p>We conducted a variety of additional analyses to support the claims in the main text. To validate that stronger players tend to play against, and therefore selectively learn from, stronger opponents, we correlated player and opponent Elo ratings in each game we used in the data set ( <ref type="figure" target="#fig_0">Figure S1</ref>). Lichess pairs players based on their ratings, but we test this empirically nonetheless.</p><p>As expected, ratings were correlated in Queen's Gambit (r = 0.796), Sicilian Defense (r = 0.800), King's Knight (r = 0.785), and Caro-Kann Defense (r = 0.797) matches.</p><p>Next, we further characterized behavior in the Sicilian Defense, King's Knight, and Caro-Kann</p><p>Defense. We first computed the empirical win rates across all different choices that were present in the data set for each opening ( <ref type="figure" target="#fig_2">Figure S2</ref>). In the Sicilian Defense, playing open is the most common response for white but has only the second highest win rate. There are then a number of</p><p>alternatives between playing open and closed, and many moves that lead to lower win rates than playing closed. In the King's Knight, playing an Italian Game or the Ruy Lopez have the second and third highest win rates while being the two most common responses for white. In the Caro-Kann Defense, advancing is the most common response for white despite having the third highest win rate. There are then a number of alternatives between advancing and exchanging, and many moves that lead to lower win rates than exchanging.</p><p>In terms of reinforcement learning, we investigated the simplest win-stay, lose-shift policy that players exhibited in the Queen's Gambit in these three openings. <ref type="figure" target="#fig_3">Figure S3 shows</ref>   learning ones. As a whole, these results confirm that game outcome generalizes as a reward signal that drives learning.</p><p>Similarly to the Queen's Gambit, we extended our reinforcement learning analysis to a two game history of all possible choices and outcomes. In the Sicilian Defense, we found relatively similar results ( <ref type="figure" target="#fig_5">Figure S4)</ref>. Specifically, the difference between stay and shift sequences, preference for shifting from closed to open compared to the opposite, and effect of the second game resulting in a win as opposed to a loss all replicated. The exceptions to this were smaller differences between stay and shift sequences when the first choice was to play closed, and an additional insignificant value in the second game outcome analysis. We replicated this in the King's Knight as well, where the effects of stay versus shift sequences and the second game resulting in a win as opposed to a loss were even stronger than in the Queen's Gambit ( <ref type="figure" target="#fig_6">Figure S5)</ref>. Here, the baseline preference seemed to be for playing an Italian Game due to the probability of repetition being higher overall when choosing to play an Italian Game after playing the Ruy Lopez compared to  <ref type="figure" target="#fig_5">Figure S4</ref>. the opposite. In the Caro-Kann Defense, the effects of stay versus shift sequences were present as well as a baseline preference for advancing as opposed to exchanging ( <ref type="figure">Figure S6</ref>). Additionally, we found almost no significant effects in the second game outcome analysis, although this could be due to the size of the data set since the effects themselves trend in the same direction. Overall, this set of analyses shows that gameplay history modulates decision-making in consistent ways.</p><formula xml:id="formula_0">LEARNING</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics for the two game history analyses</head><p>In almost all of our analyses, we provide the necessary statistics and significant values along with the results. The exception to this is the two game history analysis, as there are simply too many combinations to include in the main text. Thus, we provide these for the stay versus shift sequences and second game losses versus wins results here. <ref type="table" target="#tab_4">Tables S1 and S2 correspond</ref>   <ref type="figure" target="#fig_6">Figure S5</ref>. Probability of repeating the most recent choice to play an Italian Game or the Ruy Lopez in the King's Knight as a function of every combination of choice and game outcome for a two game history.</p><p>Data are displayed as the mean across all players and error bars indicate the standard error of the mean.</p><p>Queen's Gambit <ref type="table" target="#tab_10">, Tables S3 and S4 correspond to the Sicilian Defense, Tables S5 and S6</ref> correspond to the King's Knight, and <ref type="table">Tables S7 and S8</ref>  accept-win-accept-loss accept-win-decline-loss 65.13 &lt;0.0001 33,454</p><p>accept-win-accept-win accept-win-decline-win 62.62 &lt;0.0001 27,278</p><p>decline-loss-accept-loss decline-loss-decline-loss 51.37 &lt;0.0001 <ref type="bibr">29,</ref><ref type="bibr">198</ref> decline-loss-accept-win decline-loss-decline-win 44.65 &lt;0.0001 25,067</p><p>decline-win-accept-loss decline-win-decline-loss 48.66 &lt;0.0001 25,072</p><p>decline-win-accept-win decline-win-decline-win 44.78 &lt;0.0001 22,098 <ref type="table" target="#tab_4">Table S1</ref>. Statistics and significance values for stay versus shift sequences in the two game history analysis for the Queen's Gambit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence 1 Sequence 2 t p df</head><p>accept-loss-accept-loss accept-loss-accept-win 1.91 0.06 64,178 accept-loss-decline-loss accept-loss-decline-win 3.47 &lt;0.01 12,551 accept-win-accept-loss accept-win-accept-win 2.41 &lt;0.05 51,423</p><p>accept-win-decline-loss accept-win-decline-win 0.44 0.66 9,309 decline-loss-accept-loss decline-loss-accept-win 3.05 &lt;0.01 11,693</p><p>decline-loss-decline-loss decline-loss-decline-win 2.96 &lt;0.01 42,572 decline-win-accept-loss decline-win-accept-win 1.45 0.15 9,493 decline-win-decline-loss decline-win-decline-win 2.84 &lt;0.01 37,677   <ref type="table">Table S4</ref>. Statistics and significance values for second game losses versus wins in the two game history analysis for the Sicilian Defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence 1 Sequence 2 t p df</head><p>exchange-loss-exchange-loss exchange-loss-advance-loss 9.67 &lt;0.0001 1,757 exchange-loss-exchange-win exchange-loss-advance-win 7.44 &lt;0.0001 1,745 exchange-win-exchange-loss exchange-win-advance-loss 10.34 &lt;0.0001 1,717</p><p>exchange-win-exchange-win exchange-win-advance-win 7.86 &lt;0.0001 1,703</p><p>advance-loss-exchange-loss advance-loss-advance-loss 31.04 &lt;0.0001 3,780</p><p>advance-loss-exchange-win advance-loss-advance-win 29.23 &lt;0.0001 3,910</p><p>advance-win-exchange-loss advance-win-advance-loss 29.22 &lt;0.0001 3,900</p><p>advance-win-exchange-win advance-win-advance-win 28.71 &lt;0.0001 4,293 <ref type="table">Table S7</ref>. Statistics and significance values for stay versus shift sequences in the two game history analysis for the Caro-Kann Defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence 1 Sequence 2 t p df</head><p>exchange-loss-exchange-loss exchange-loss-advance-loss 1.11 0.27 2,629 exchange-loss-exchange-win exchange-loss-advance-win 0.97 0.33 873 exchange-win-exchange-loss exchange-win-advance-loss 2.64 &lt;0.01 2,552 exchange-win-exchange-win exchange-win-advance-win 3.39 &lt;0.001 868 advance-loss-exchange-loss advance-loss-advance-loss 1.15 0.25 811 advance-loss-exchange-win advance-loss-advance-win 0.63 0.53 6,879 advance-win-exchange-loss advance-win-advance-loss 0.64 0.52 820 advance-win-exchange-win advance-win-advance-win 0.07 0.94 7,373 <ref type="table">Table S8</ref>. Statistics and significance values for second game losses versus wins in the two game history analysis for the Caro-Kann Defense.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Queen's Gambit. (A) Board position for the Queen's Gambit, which follows the opening moves d4, d5, and c4. Black can choose to accept the gambit by capturing the pawn on c4, or decline by protecting the pawn on d5. (B) Empirical win rates for black when accepting and declining the gambit, where declining includes the two moves shown in (A). Here and in (C) data are displayed as the mean across all players and error bars indicate the standard error of the mean. (C) Empirical win rates for 28 different moves available to black when faced with the Queen's Gambit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2A demonstrates that players' choices exhibit this behavioral signature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Reinforcement learning in the Queen's Gambit. (A) Probability of repeating the choice to accept or decline the gambit as a function of game outcome. Here and in (B) data are displayed as the mean across all players and error bars indicate the standard error of the mean. (B) Probability of repeating the most recent choice to accept or decline the gambit as a function of every combination of choice and game outcome for a two game history. (C) Correlation between previous game outcomes and current choice to accept or decline up to 30 Queen's Gambit games in the past. The shading indicates an exponential fit to the correlation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LEARNINGFigure 3 .</head><label>3</label><figDesc>Human behavior in the Queen's Gambit is consistent with this framework, as recent outcomes are the most highly weighted in terms of current choice (Figure 2C). Meanwhile, outcomes further in the past have a Social learning in the Queen's Gambit. (A) Probability of declining the gambit as a function of number of all games played or games played as white. Here and in all subsequent panels, the data are displayed as the mean across all players, with shading or error bars indicating the standard error of the mean. (B) Probability of declining the gambit as a function of whether the opponent chose to accept or decline in the previous game. (C) Probability of repeating the opponent's choice to accept or decline the gambit as a function of game outcome.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>LEARNING</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Adaptive decision-making in the Queen's Gambit. (A) Probability of shifting from a decision to accept the gambit to a decision to decline the gambit or vice versa as a function of total games played. Here and in all subsequent panels the data are displayed as the mean across all players and shading indicates the standard error of the mean. (B) Probability of declining the gambit as a function of the time taken to make a move measured in seconds. Here and in all subsequent plots, results are binned into 10 bins. (C) Probability of declining the gambit as a function of the time in days that has elapsed between games. (D) Probability of declining the gambit as a function of the player's current playing strength, as measured by their Elo rating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Generalization of reinforcement and social learning. (A) The Sicilian Defense, which follows the opening moves e4 and c5. White can choose to play closed by moving their knight to c3 or play open by moving their other knight to f3. The remainder of the column contains the empirical win rates for white when playing closed or open, the correlation between previous game outcomes and current choice to play closed or open up to 30 Sicilian Defense games in the past, the probability of playing open as a function of whether the opponent chose to play closed or open in the previous game, and the probability of repeating the opponent's choice to play closed or open as a function of game outcome. In all panels the data are displayed as the mean across all players with error bars indicating the standard error of the mean. (B) The same as in (A), but for the King's Knight, which follows the opening moves e4, e5, Nf3, and Nc6. White can choose to play an Italian Game by moving their bishop to c4 or play the Ruy Lopez by moving their bishop to b5. (C) The same as in (A), but for the Caro-Kann Defense, which follows the opening moves e4, c6, d4, and d5. White can choose to exchange by capturing the pawn on d5 or advance by moving their pawn to e5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>, 1000-1005.Watkins, C. J.,&amp; Dayan, P. (1992).Q-learning. Machine Learning, 8, 279-292.   Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., &amp; Cohen, J. D. (2014). Humans use directed and random exploration to solve the explore-exploit dilemma. Journal ofExperimental Psychology: General, 143(6), 2074. Wimmer, G. E., &amp; Poldrack, R. A. (2022). Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period. Memory &amp; Cognition, 50(2), 312-324. Wise, T., Emery, K., &amp; Radulescu, A. (2023). Naturalistic reinforcement learning. Trends in Cognitive Sciences. Yoeli, E., &amp; Hoffman, M. (2022). Hidden games: The surprising power of game theory to explain irrational human behavior. Basic Books.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S1 .</head><label>S1</label><figDesc>Validating that players are paired with opponents of similar playing strength. (A) Correlation between player and opponent Elo ratings in all Queen's Gambit games. (B) The same as in (A), but for all Sicilian Defense games. (C) The same as in (A), but for all King's Knight games. (D) The same as in (A), but for all Caro-Kann Defense games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S2 .Figure S3 .</head><label>S2S3</label><figDesc>Empirical win rates for different moves available to white when faced with the (A) Sicilian Defense, (B) King's Knight, and (C) Caro-Kann Defense. Data are displayed as the mean across all players and error bars indicate the standard error of the mean. Win-stay, lose-shift in the Sicilian Defense, King's Knight, and Caro-Kann Defense. (A) Probability of repeating the choice to play open or closed as a function of game outcome. Data are displayed as the mean across all players and error bars indicate the standard error of the mean. (B) The same as in (A), but for playing an Italian Game or the Ruy Lopez. (C) The same as in (A), but for exchanging or advancing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Probability of repeating the most recent choice to play open or closed in the Sicilian Defense as a function of every combination of choice and game outcome for a two game history. Data are displayed as the mean across all players and error bars indicate the standard error of the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 15Generalization of learning strategies across openingsUntil now, all of our results have been limited to the Queen's Gambit. While we believe that this is an illustrative case study in how people learn naturalistic strategic behavior, it is crucial to know whether these signatures of reinforcement and social learning generalize to other openings. When selecting new opening lines to test, we aimed to maintain the reduction to a binary choice exhibited by the Queen's Gambit while focusing on openings that were sufficiently popular to ensure enough games were available to allow us to test our hypotheses. Thus, we investigated strategies in three new openings: the Sicilian Defense, the King's Knight, and the Caro-Kann</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>when choosing to play open after playing closed compared to the opposite. This indicates that people have a baseline preference for playing open. The statistics and significance values for this We found that players LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 16 were more likely to play open if they observed their opponent doing so in their most recent game as black (t(200, 228) = 8.08, p &lt; 0.0001), and more likely to repeat their opponent's decision if the outcome of that game led to their own loss as opposed to their own win (t(192, 489) = 6.63, p &lt; 0.0001). Thus, game outcomes associated with decisions people observe their opponents make seem to be another reward signal.In the King's Knight, players most often choose to play an Italian Game by moving one of their bishops to c4 or play the Ruy Lopez by moving the same bishop to b5. This is a classic opening</figDesc><table /><note>analysis are available in Tables S3 and S4. For social learning, we replicated both the copy and the color-reversed win-stay, lose shift analyses in the Sicilian Defense.line in chess, cited as perhaps the oldest first move in the modern version of the game. Both alternatives we consider are well known openings that see regular play at all levels. For this opening, playing the Ruy Lopez has a higher empirical win rate when compared to playing an Italian Game, although this difference is not statistically significant (t(1, 225, 908) = 1.92, p = 0.05,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR 20 binary choice response for each opening. The number of unique players and games included in the subset of the data corresponding to each opening is provided inTable 1. Number of unique players and games included in the analyses corresponding to each opening.Source code for all analysis in the paper as well as the filtered data is available at htttps://github.com/ionatankuperwajs/learning-openings. The raw data is available from the Lichess open database at https://database.lichess.org, while source code for preprocessing the raw data is available at https://github.com/mcostalba/scoutfish.Thompson, B., van Opheusden, B., Sumers, T., &amp; Griffiths, T. (2022). Complex cognitive algorithms preserved by selective social learning in experimental populations.</figDesc><table><row><cell>Opening</cell><cell cols="2">Number of players Number of games</cell></row><row><cell>Queen's Gambit</cell><cell>146,721</cell><cell>480,932</cell></row><row><cell>Sicilian Defense</cell><cell>165,698</cell><cell>714,218</cell></row><row><cell>King's Knight</cell><cell>167,331</cell><cell>1,225,910</cell></row><row><cell>Caro-Kann Defense</cell><cell>39,825</cell><cell>78,723</cell></row><row><cell>Code and data availability</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Probability of repeating the most recent choice to exchange or advance in the Caro-Kann Defense as a function of every combination of choice and game outcome for a two game history. Data are displayed as the mean across all players and error bars indicate the standard error of the mean.</figDesc><table><row><cell>correspond to the Caro-Kann Defense. LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR Outcome 1 6 Choice 2 Choice 1 Outcome 2 Exchange Loss Win Exchange Advance Exchange Advance Loss Win Loss Win Loss Win Loss Win Advance Loss Win Exchange Advance Exchange Advance Loss Win Loss Win Loss Win Loss Win Caro-Kann Defense 7 Sequence 1 Sequence 2 t p df accept-loss-accept-loss accept-loss-decline-loss 77.96 &lt;0.0001 42,336 Figure S6. LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR accept-loss-accept-win accept-loss-decline-win 67.60 &lt;0.0001 34,393</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 .</head><label>S2</label><figDesc>Statistics and significance values for second game losses versus wins in the two game history analysis for the Queen's Gambit.</figDesc><table><row><cell>Sequence 1</cell><cell>Sequence 2</cell><cell>t</cell><cell>p</cell><cell>df</cell></row><row><cell cols="5">closed-loss-closed-loss closed-loss-open-loss 13.42 &lt;0.0001 19,750</cell></row><row><cell cols="5">closed-loss-closed-win closed-loss-open-win 13.14 &lt;0.0001 19,915</cell></row><row><cell cols="5">closed-win-closed-loss closed-win-open-loss 15.06 &lt;0.0001 19,442</cell></row><row><cell cols="5">closed-win-closed-win closed-win-open-win 15.97 &lt;0.0001 19,867</cell></row><row><cell>open-loss-closed-loss</cell><cell cols="4">open-loss-open-loss 166.23 &lt;0.0001 84,506</cell></row><row><cell>open-loss-closed-win</cell><cell cols="4">open-loss-open-win 165.24 &lt;0.0001 86,156</cell></row><row><cell>open-win-closed-loss</cell><cell cols="4">open-win-open-loss 166.13 &lt;0.0001 84,472</cell></row><row><cell>open-win-closed-win</cell><cell cols="4">open-win-open-win 168.10 &lt;0.0001 90,598</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 .</head><label>S3</label><figDesc>Statistics and significance values for stay versus shift sequences in the two game history analysis for the Sicilian Defense.</figDesc><table><row><cell>Sequence 1</cell><cell>Sequence 2</cell><cell>t</cell><cell>p</cell><cell>df</cell></row><row><cell cols="4">closed-loss-closed-loss closed-loss-closed-win 0.06 0.95</cell><cell>25,684</cell></row><row><cell>closed-loss-open-loss</cell><cell cols="3">closed-loss-open-win 0.36 0.72</cell><cell>13,981</cell></row><row><cell cols="5">closed-win-closed-loss closed-win-closed-win 2.91 &lt;0.01 25,889</cell></row><row><cell>closed-win-open-loss</cell><cell cols="3">closed-win-open-win 1.47 0.14</cell><cell>13,420</cell></row><row><cell>open-loss-closed-loss</cell><cell cols="3">open-loss-closed-win 1.75 0.08</cell><cell>12,852</cell></row><row><cell>open-loss-open-loss</cell><cell>open-loss-open-win</cell><cell cols="3">2.06 &lt;0.05 157,810</cell></row><row><cell>open-win-closed-loss</cell><cell cols="4">open-win-closed-win 2.14 &lt;0.05 12,570</cell></row><row><cell>open-win-open-loss</cell><cell>open-win-open-win</cell><cell cols="3">3.00 &lt;0.01 162,500</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was made possible by grants from the National Science Foundation (number IIS-2312373) and the NOMIS Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEARNING IN NATURALISTIC STRATEGIC BEHAVIOR</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting behavior from actions in the past: Repeated decision making or a matter of habit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Verplanken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Knippenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Social Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1355" to="1374" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Br√§ndle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hartshorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Leeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
	<note>Using games to understand the mind</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Assessing human error against a benchmark of perfection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empowerment contributes to exploration behaviour in a creative video game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Br√§ndle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Stocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1481" to="1489" />
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive expert decision making: Skilled chess players search more and deeper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Campitelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gobet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICGA Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="209" to="216" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualization, pattern recognition, and forward search: Effects of playing speed and sight of the position on grandmaster chess errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chabris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="648" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perception in chess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantifying human performance in chess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Iacopini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battiston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2113</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Should i stay or should i go? how the human brain manages the trade-off between exploitation and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="933" to="942" />
			<date type="published" when="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Value learning through reinforcement: The basics of dopamine and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neuroeconomics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decision theory, reinforcement learning, and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>De Groot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thought and choice in chess</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2014" />
			<publisher>Walter de Gruyter GmbH &amp; Co KG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantifying the complexity and similarity of chess openings using online chess community data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Marzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5327</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The rating of chessplayers, past and present</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Elo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Arco Pub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Toward a general theory of expertise: Prospects and limits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deconstructing the human algorithms for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="164" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">supplement_3</biblScope>
			<biblScope unit="page" from="15647" to="15654" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The secret of our success: How culture is driving human evolution, domesticating our species, and making us smarter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Risk-taking in adversarial games: What can 1 billion online chess games tell us?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holdaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Heuristics for meta-planning from a normative model of information search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuperwajs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A joint analysis of dropout and learning functions in human decision-making with massive online data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuperwajs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 44th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Darwin&apos;s unfinished symphony: How culture made the human mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Laland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic response-by-response models of matching behavior in rhesus monkeys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Experimental Analysis of Behavior</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="579" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Habits without values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">292</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The neural and computational systems of social learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Knapska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lindstr√∂m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="197" to="212" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Time spent thinking in online chess reflects the value of computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Russek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acosta-Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Opheusden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured, uncertainty-driven exploration in real-world consumer choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="13903" to="13908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superhuman artificial intelligence can improve human decision-making by increasing novelty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Opheusden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2214840120</biblScope>
			<date type="published" when="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tracing the trajectory of skill learning with a very large sample of online game players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dewar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inferring latent learning factors in large-scale cognitive training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1145" to="1155" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal difference learning and td-gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
