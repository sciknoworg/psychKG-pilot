<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The unpredictable Buridan&apos;s ass: Failure to predict decisions in a trivial decision-making task Kodi B. Arfer (http://arfer.net)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-12">12 Jul 2021, 22 Aug 2021, 5 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">The unpredictable Buridan&apos;s ass: Failure to predict decisions in a trivial decision-making task Kodi B. Arfer (http://arfer.net)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-12">12 Jul 2021, 22 Aug 2021, 5 Oct 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In order to better examine seemingly unpredictable variation that appears in decision-making studies, I had people choose between two options that had no features or consequences to distinguish them. 100 users of Mechanical Turk completed 200 binary choices, and I examined the accuracy with which statistical models could predict the choices. Across three different conceptualizations of the prediction problem and a variety of models ranging from logistic regression to neural networks, I obtained at best modest predictive accuracy. Predicting trivial choices may actually be more difficult than predicting meaningful choices. These strongly negative results appear to place limits on the predictability of human behavior. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>behavior of real consequence. Perhaps if we learned to predict that leftover variation that appears even in simple decisions, we could learn something about human decision-making with which we could tackle these applied problems.</p><p>In the pursuit of simplicity, consider trivial decisions: situations where you can make a decision, but your choice doesn't matter. A classic example is the version of the thought experiment called Buridan's ass in which the donkey is hungry and is midway between two identical bales of hay. The donkey, not being an intellectual, has no trouble deciding which way to go, but how can we predict its choice? In typical decision-making studies, such as studies of intertemporal choice or risky choice, there are meaningful features of the options, such as the size of a reward, that decision-makers use to make their choice. In a trivial decision, there are no such features, so all variation in choices must be unrelated to features-and perhaps of the same kind that led to the 85% ceiling in <ref type="bibr" target="#b1">Arfer and Luhmann (2015)</ref>. We must resort to superficial aspects of the scenario, such as the order in which options are presented, in order to predict decisions without consulting out-of-task data (such as the subject's gender, or a personality test).</p><p>The question of predicting trivial decisions is closely related to the question of how capable humans are of behaving randomly. The latter has gotten a fair amount of attention. <ref type="bibr" target="#b8">Hagelbarger (1956)</ref> is an early example of a computer to predict binary choices one at a time in a competitive context. A human makes choices and sees the computer's predictions, while trying to pick the opposite of what it will guess. Merill (2018) is a modern implementation of a similar idea. In a noncompetitive version of this method, in which predictve models are trained and tested after all the data was collected, <ref type="bibr" target="#b15">Shteingart and Loewenstein (2016)</ref> obtained about 60% accuracy with logistic regression. Broadly, the literature indicates that people fail to produce sequences that pass stringent tests of randomness <ref type="bibr" target="#b13">(Nickerson, 2002;</ref><ref type="bibr" target="#b14">Nickerson &amp; Butler, 2009)</ref>, although the tests and instructions used matter <ref type="bibr" target="#b13">(Nickerson, 2002)</ref> and with practice, people (e.g., <ref type="bibr" target="#b6">Delogu et al., 2020)</ref> as well as animals (e.g., <ref type="bibr" target="#b12">Nergaard &amp; Holth, 2020)</ref> can learn to produce better random numbers.</p><p>Similar to these experiments on random-number generation, I sought in this study to predict trivial decisions. I didn't ask subjects to try to make random choices; instead, I just told them to "choose whichever you like". I had each subject complete 200 trials, with the idea of using the first 100 for training predictive models and the second 100 for testing. I hoped to achieve something like 95% or 100% accuracy in this maximally simple situation in which there are no substantive features to model. I considered more complex models than are typically applied to experiments of random-number generation, with the idea that they might succeed where previous models fell short of high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data collection</head><p>The data was collected from 101 users of Mechanical Turk in 2019. One subject, the first, is omitted from all analyses because I accidentally gave them a pilot version of the task; all other subjects are included.</p><p>The task was described to prospective subjects thus: "Make some simple decisions. The task takes about 3 minutes." Subjects were required to live in the US and have at least 90% of their previous Mechanical Turk assignments approved. They were given up to 10 minutes to complete the task and were paid 0.75 USD. Given the actual time subjects took for the entire task, including the consent form, this resulted in a median hourly wage of $12.86 (range $4.56 to $45.76). The consent form described the procedure thus: "If you decide to be in this study, your part will involve pressing keys on your device's keyboard to indicate which of various options you prefer."</p><p>During the task, subjects saw these instructions on screen:</p><p>To complete this task, just make a bunch of decisions.</p><p>There are two options to choose from. Nothing special happens when you make a choice, so choose whichever you like.</p><p>Press the "f" key to choose the first option, or the "j" key to choose the second. I chose the "f" and "j" keys for their centrality on QWERTY keyboards. The subjects' choices were encoded such that the first option is regarded as true or the number 1, and the second option as false or the number 0.</p><p>There were 200 trials; that is, the subject had to press one of the two keys 200 times. A counter labeled "Trials left" was displayed. There was no visible log of which choices the subject had made. There was also no delay imposed between trials. But the task was programmed such that subjects had to make a separate keypress for each trial, instead of just holding down a key. I didn't collect any demographic information about the subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis</head><p>Deidentified raw data, task code, analysis code, and a research notebook can be found at http://arfer.net/projects/donkey. <ref type="figure">Figure 1</ref> shows the distribution of choices. Overall, subjects chose truly in 10,240 out of 20,000 trials (51%). This base rate near 50% is helpful for the purposes of this study, since it provides a lot of room for improvement by predictive methods. <ref type="figure">Figure 2</ref> shows the distribution of per-subject sums of response times, omitting the first trial, on which subjects presumably read the instructions. The mean is 58.8 s, so the mean of the individual per-trial response times, again omitting the first trial, is 296 ms; the median is 179 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptive information</head><p>Thus, overall, subjects decided quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Half splits</head><p>My first models used a within-subjects approach, in which models were trained and tested on one subject at a time. Each model was trained on the first 100 trials, and its predictions on the latter 100 trials were assessed. The accuracy for each subject is defined as the number of test trials for which the predicted choice equals the observed choice. Throughout this paper, I focus on traditional classification-style measures of accuracy instead of probabilistic classification so that pure classifiers (such as nearest-neighbors classifiers) can be directly compared to models capable of predicting probabilities (such as logistic regression). <ref type="table">Table 1</ref> shows the quartiles of per-subject accuracies for each model. For comparison, it also includes the quartiles of per-subject base rates, which are computed for each subject as simply the number of true choices or the number of false choices, whichever is greater. In general, the accuracies are about equal to or worse than the base rates, indicating very poor predictive accuracy. Three models are shown:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The first section of</head><p>TrialNumbers is based on logistic regression with a ridge penalty. There are eight predictors, defined as (n,n2,n3,n mod 2,n mod 3,n mod 5,sinn,cosn) and then standardized, where n is the trial number (counting the first training trial as 0 and the first test trial as 100), and a mod b represents the remainder after division of a by b. The penalty size is chosen per subject with tenfold cross-validation (CV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>RunLengths is a regression model of the same type, but with different predictors, namely (j, k, j 2 , k 2 , p), where j is the number of true choices made by this subject so far, k is the number of false choices, and p is 1 if the last choice was true and 0 otherwise (including on trial 0). j and k are reset whenever a choice of the opposite kind is made, so the predictors are determined entirely by the previous trial and how many times that same choice was made in consecutive previous trials. When the previous trial was not observed, j, k, and p are set based on the model's own prediction.</p><p>SuperMarkov resembles a Markov chain, as its name suggests. It assumes that each choice is a probabilistic function of the n-tuple formed of the previous n choices, where n is a hyperparameter varying from 2 to 5. The prediction is true when the observed choices for the n-tuple were more often true than false, false when they were more often false than true, and the modal choice in case of ties. For each subject, I report the greatest accuracy across values of n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolation</head><p>There are two ways in which the half-split form of this problem may be especially difficult.</p><p>If a model is based on trial numbers, it's trained on inputs 0 to 99 and then tested on inputs greater than 99. Thus, the training process places few constraints on the model's behavior for the inputs on which it's tested, and it's forced to extrapolate. On the other hand, an autoregressive kind of model, in which the choice for one trial depends on the choices for previous trials, is fragile, because a bad prediction for trial 102 can impair performance for all 97 remaining trials.</p><p>To avoid these issues, I assessed another batch of models in an interpolative fashion, with tenfold CV. The CV folds are chosen randomly for different subjects, but for a given subject, all models use the same folds. This setup resembles <ref type="bibr" target="#b1">Arfer and Luhmann (2015)</ref>.</p><p>The second section of <ref type="table">Table 1</ref> shows the resulting accuracies; each is divided by 2 for easier reading, since accuracy in predicting all 200 test trials is now checked. Here, comparison numbers are provided by the model Trivial, which is assessed in the same way as the other • • models, but works by simply predicting the modal choice in its training set. There is some improvement over Trivial, but not much. Four nontrivial models are shown:</p><p>Cubic is an unpenalized logistic-regression model with predictors computed as (n, n 2 , n 3 ) and then standardized, where n is the trial number.</p><p>CubicMod resembles TrialNumbers. It's a logistic-regression model with a ridge penalty and predictors (n, n 2 , n 3 , n mod 2, n mod 3). The penalty size is chosen per subject with nested tenfold CV.</p><p>KNN is a k-nearest-neighbors model with one predictor, n. k ranges from 1 to 10 and is chosen with nested CV.</p><p>XGBoost is an extreme gradient-boosting model <ref type="bibr" target="#b5">(Chen &amp; Guestrin, 2016)</ref> with n as the only predictor. The model uses a hinge objective and 250 trees. Five hyperparameters (learning_rate, gamma, reg_lambda, reg_alpha, max_depth) are allowed to vary; I randomly selected a set of 50 hyperparameter vectors, then tested them per subject with nested CV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy</head><p>The interpolative form of the problem, while not as difficult as the half-split approach, still doesn't allow models to learn things about human behavior from one subject and apply that knowledge to other subjects. With each new subject, the model starts from square one. For a final batch of models, I switched to a between-subjects CV approach. Here I treat each subject's first 100 trials as predictors for that subject and the latter 100 trials as dependent variables, modeling each of these test trials separately. (For simplicity, I only actually test 3 of the 100 test trials.) In some models, I also provide the response times (RTs) for the first 100 trials as additional predictors, although I neither test nor train on RTs for the test trials. The models use no predictors beyond the 100 training choices and 100 training RTs. Hence, while these between-subjects models may have some predictive ability, there remains quite a lot of unpredictable variation. Six models are shown:</p><p>OneCol is a simplistic model that finds the single most predictive trial of the 100 training trials for the given test trial, and uses only that trial to make its predictions.</p><p>Regression is a logistic-regression model with a lasso penalty. Penalty sizes were set by hand per test trial.</p><p>KNN is a 3-nearest-neighbors model. I set k = 3 by hand.</p><p>NeuralNetwork is a neural network with one layer of hidden nodes. There are 100 hidden nodes for the models without RTs and 200 hidden nodes for the models with them.</p><p>The hidden nodes use a hyperbolic-tangent activation function, regularized with a manually sized lasso penalty. The output node uses a sigmoid activation function. The network was fit with TensorFlow's RMSprop optimizer and cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this task, with this population, choices are not entirely unpredictable. Nevertheless, it's striking how far short I fell of the goal of 95% or 100% accuracy, considering both small, simple models and big, fancy models. Even the performances I obtained are in some cases likely overestimated because I was sloppy with hyperparameters, setting them manually instead of with nested CV or another method to avoid overfitting; by that point, my goal was merely to get a ceiling and confirm my hunch that the kind of accuracy I was looking for wasn't going to happen.</p><p>The models of Arfer and Luhmann (2015) did much better, suggesting that decisions without meaningful features are actually harder, not easier, to predict. The basic idea behind this study was sound, but my sample wasn't big enough. After all, in the hierarchy case, I had only 100 subjects for a dataset with 100 or 200 predictors. The gap between the kind of accuracy I was looking for and the accuracy I got makes me think that a twofold or even tenfold increase in sample size probably wouldn't have solved the problem. Much bigger samples, say tens of thousands of people, could more feasibly have made the difference, but they'd be expensive, even at the very modest rate I paid these subjects.</p><p>These choices are predictable given more information about the subjects, such as their gender, their complete history of choices in similar situations throughout their life, or a high-resolution scan of their brain. I'm skeptical that the more feasibly collected information of this kind would be helpful considering some of my previous studies <ref type="bibr" target="#b2">(Arfer &amp; Luhmann, 2017;</ref><ref type="bibr" target="#b0">Arfer, 2016)</ref>. The best predictors of behavior seems to generally be other behavior of the same kind. It's just that, as in this study, sometimes the best predictors aren't good enough.</p><p>People are simply capable of behaving unpredictably. This brings to mind the old question of whether the universe itself is deterministic or probabilistic, but this metaphysical issue isn't as important as it might sound, since probabilistic systems can be highly predictable whereas deterministic systems can defy predictability with e.g. chaos <ref type="bibr" target="#b9">(Hoefer, 2016)</ref>.</p><p>Some behaviorists have argued that variation itself is a reinforcable dimension of behavior <ref type="bibr" target="#b12">(Nergaard &amp; Holth, 2020)</ref>. Perhaps humans as well as other animals have a specifically evolved ability to generate random numbers, to help with things like choosing a foraging location while minimizing competition or predictability by predators, even if this ability is imperfect. This is an ugly possibility to consider for any scientist who's interested in the prediction of behavior, but here we are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• • •</head><p>People would behave differently, and more predictably, in a more Buridan's ass-like situation in which, although the options are still equivalent, the outcome is consequential (avoiding starvation, in the donkey's case). I didn't specifically ask subjects not to use a randomization device, like a coin. This was probably a mistake. Some may have used a randomization device. This said, the quick response times suggest that it wasn't common. 179 ms doesn't seem like enough time to flip a coin and read and input the result.</p><p>Incidentally, it's perhaps surprising that the left-hand option was more popular overall. Past research suggests that people associate goodness with the direction of their dominant hand and badness with the direction of their off-hand <ref type="bibr" target="#b3">(Casasanto &amp; Chrysikou, 2011;</ref><ref type="bibr" target="#b4">Casasanto &amp; Henetz, 2012)</ref>, an idea applied specifically to QWERTY keyboards by <ref type="bibr" target="#b10">Jasmin and Casasanto (2012)</ref>, and most people are right-handed.</p><p>• <ref type="table">Tables   Table 1.</ref> Quartiles of per-subject accuracies under the split-half and interpolation approaches.  <ref type="table" target="#tab_0">Table 2</ref>.</p><p>Accuracies in predicting trials 100, 101, and 150 under the between-subjects approach. The "RTs"</p><p>column indicates whether the model was given training-trial response times as predictors as well as the training-trial choices.  <ref type="figure">Figure 1</ref>. The number of true choices made by each subject. <ref type="figure">Figure 2</ref>. The distribution of completion times by subject, counting in seconds from the beginning of the second trial to the end of the last. A single subject who took almost 5 minutes is omitted from this plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>not sure what to make of this. Here are some possibilities, not all of which are mutually exclusive:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>shows base rates and accuracies for 3 of the test trials. For trials 100 and 101, the first two test trials, we see some improvement over the base rate. Still, the accuracies are far short 98, topping out at 73 against a base rate of 50. For trial 150, which is 50 trials after the last training trial, performance is less impressive yet, topping out at 56 against a base rate of 55.</figDesc><table><row><cell>•</cell></row><row><cell>•</cell></row><row><cell>•</cell></row><row><cell>•</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Predicting outcomes of interventions to increase social competence in children and adolescents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Arfer</surname></persName>
		</author>
		<ptr target="http://arfer.net/projects/pelt/paper" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>State University of New York at Stony Brook</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The predictive accuracy of intertemporal-choice models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Arfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Luhmann</surname></persName>
		</author>
		<idno type="DOI">10.1111/bmsp.12049</idno>
		<idno>doi:10.1111/ bmsp.12049</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="326" to="341" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Time-preference tests fail to predict behavior related to self-control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Arfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Luhmann</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2017.00150</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When left is &quot;right&quot;: Motor fluency shapes abstract concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casasanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Chrysikou</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611401755</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="419" to="422" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Handedness shapes children&apos;s abstract concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casasanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henetz</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2011.01199.x</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1603.02754" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Morra game as a naturalistic test bed for investigating automatic and voluntary processes in random sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Delogu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnewold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Toffalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fanari</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2020.551126</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2545</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Predictive inference: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geisser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SEER, a SEquence Extrapolating Robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Hagelbarger</surname></persName>
		</author>
		<idno type="DOI">10.1109/TEC.1956.5219783</idno>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Electronic Computers, EC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal determinism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoefer</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu/archives/spr2016/entries/determinism-causal" />
	</analytic>
	<monogr>
		<title level="m">Stanford Encyclopedia of Philosophy</title>
		<editor>E. N. Zalta</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The QWERTY effect: How typing shapes the meanings of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jasmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casasanto</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-012-0229-7</idno>
		<idno>doi:10.3758/ s13423-012-0229-7</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="499" to="504" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merill</surname></persName>
		</author>
		<ptr target="https://github.com/elsehow/aaronson-oracle" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>aaronson-oracle [Software</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A critical review of the support for variability as an operant dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nergaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holth</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40614-020-00262-y</idno>
		<idno>doi:10.1007/ s40614-020-00262-y</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Behavior Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="579" to="603" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The production and perception of randomness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Nickerson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.109.2.330</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="330" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On producing random binary sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Nickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Heterogeneous suppression of sequential effects in random sequence generation, but not in operant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shteingart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0157643</idno>
		<idno>doi:10.1371/ journal.pone.0157643</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
