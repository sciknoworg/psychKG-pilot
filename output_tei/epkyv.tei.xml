<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving measurements of similarity judgments with machine-learning algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">R</forename><surname>Stevens</surname></persName>
							<email>jeffrey.r.stevens@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Center for Brain, Biology &amp; Behavior</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln, Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><forename type="middle">Polzkill</forename><surname>Saltzman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Center for Brain, Biology &amp; Behavior</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln, Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln, Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Rasmussen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Center for Brain, Biology &amp; Behavior</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln, Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leen-Kiat</forename><surname>Soh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln, Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">B83 East Stadium</orgName>
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<addrLine>Lin-coln, 68588. ORCID 0000-0003-2375-1360</addrLine>
									<settlement>Lincoln</settlement>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving measurements of similarity judgments with machine-learning algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>classification</term>
					<term>intertemporal choice</term>
					<term>judgment</term>
					<term>machine learning</term>
					<term>similarity</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Intertemporal choices involve assessing options with different reward amounts available at different time delays. The similarity approach to intertemporal choice focuses on judging how similar amounts and delays are. Yet we do not fully understand the cognitive process of how these judgments are made. Here, we use machine-learning algorithms to predict similarity judgments to (1) investigate which algorithms best predict these judgments, (2) assess which predictors are most useful in predicting participants&apos; judgments, and (3) determine the minimum number of judgments required to accurately predict future judgments. We applied eight algorithms to similarity judgments for reward amount and time delay made by participants in two data sets. We found that neural network, random forest, and support vector machine algorithms generated the highest out-of-sample accuracy. Though neural networks and support vector machines offer little clarity in terms of a possible process for making similarity judgments, random forest algorithms generate decision trees that can mimic the cognitive computations of human judgment-making. We also found that the numerical difference between amount values or delay values was the most important predictor of these judgments, replicating previous work. Finally, the best performing algorithms such as random forest can make highly accurate predictions of judgments with relatively small sample sizes (~15), which will help minimize the numbers of judgments required to extrapolate to new value pairs. In summary, machinelearning algorithms provide both theoretical improvements to our understanding of the cognitive computations involved in similarity judgments and intertemporal choices as well as practical improvements in designing better ways of collecting data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Intertemporal choices are a critical class of decisions that involve choosing between rewards available at different times <ref type="bibr" target="#b0">[1]</ref>. We all face these decisions on a daily basis. Would you prefer to buy the latest gadget or put that money away for retirement? Would you prefer to consume a decadent dessert or avoid the calories for a slimmer waistline? Researchers of intertemporal choice typically probe people's preferences by providing a series of choices between smaller amounts of money available after a short or no delay and a larger amount available later (e.g., Would you prefer to receive $10 today or $12 in one week?).</p><p>Though temporal discounting is the dominant approach to intertemporal choices <ref type="bibr" target="#b1">[2]</ref>, an alternative heuristic model asserts that similarity judgments can account for these choices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. For example, if people find the reward amounts to be similar (e.g., $10 vs. $12) but the time delays to be dissimilar (e.g., today vs. one week), they may ignore the similar attribute and choose based on the dissimilar attribute (e.g., choose the immediate option). This approach predicts intertemporal choices well when it can make predictions <ref type="bibr" target="#b4">[5]</ref>, but it raises the question of what drives similarity judgments.</p><p>Previously, we used machine-learning algorithms to assess similarity judgments <ref type="bibr" target="#b5">[6]</ref>. Machine learning is a powerful set of tools that "sift through data looking for patterns" <ref type="bibr" target="#b6">[7]</ref>. Researchers can input predictors to evaluate if machine-learning algorithms can predict responses <ref type="bibr" target="#b7">[8]</ref>. In our case, we were interested in which features of the amount and delay values predicted people's similarity judgments. We proposed a particular type of machine-learning algorithm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> as both a potential predictor of choice and a reasonable approximation of the cognitive process that people could use to make the similarity judgments. We found that these decision trees accurately predicted choice (about 86% out-of-sample accuracy) and that the numerical difference between the large and small amounts and delays (large − small) and the numerical ratio between them (small / large) were the best features for predicting similarity judgments.</p><p>The aim of that study was to investigate a decision tree called Classification and Regression Tree or CART <ref type="bibr" target="#b10">[11]</ref>. This algorithm was chosen because it was a fairly simple decision tree algorithm that is well-studied and could provide a relatively straightforward cognitive process model of decision making. Yet there are many potential machine-learning algorithms that could be used to classify similarity judgments based on the numerical values of the small and large amounts and delays. One key aim of the current study is to test a range of algorithms on our data to determine which algorithms best predict similarity judgments. In addition to accuracy (number of correct predictions / total number of predictions), machine learning uses other performance metrics of classification <ref type="bibr" target="#b11">[12]</ref>. Precision (or positive predictive value) is the proportion of instances predicted to be positive that are actually positive (number of correct positive predictions / number of positive predictions). Recall (or sensitivity, hit rate, true positive rate) is the proportion of actual positives that are correctly classified (number of correct positive predictions / number of positive instances). For our purposes, we can think of "similar" judgments as positive. So precision is the proportion of similar predictions that the algorithms correctly classify as similar, and recall is the proportion of actual similar judgments that the algorithms correctly classify as similar <ref type="table">(Table 1)</ref>.</p><p>To calculate these performance metrics, we must have predictors. In Stevens and Soh <ref type="bibr" target="#b5">[6]</ref>, we mathematically arranged the small and large values to generate 11 predictors that may predict similarity judgments <ref type="table">(Table S1</ref>). A second aim of the current study is to reassess which predictors are most useful in predicting similarity judgments using the wider range of algorithms. Further, the previous analysis only found the single best predictor for each person by extracting the predictor used as the first node in the decision tree. Here, we assess predictor importance <ref type="bibr" target="#b7">[8]</ref> for each algorithm that allows this calculation. Therefore, we compute importance measures across a range of algorithms and for each predictor.</p><p>Finally, assessing similarity judgments requires asking for pairwise binary judgments of similar or dissimilar from participants. It would be useful to be able to predict an individual's similarity judgments with as few questions as possible. Therefore, our final aim is to evaluate prediction accuracy at different sample sizes to determine the minimum number of questions required to accurately predict similarity judgments using a learning-curve analysis <ref type="bibr" target="#b12">[13]</ref>. Further, we assess whether the ordering of the questions influences prediction accuracy. Typically, when assessing the effects of sample size on accuracy, machine-learning analyses randomly select the instances within the training sets. Though this is fine for overall analyses of sample size, our aim requires a different approach. Because we are interested in minimizing the number of questions asked, we must consider the questions in the order in which they were asked in case judgments change over time. Therefore, we assess the effect of sample size on accuracy for questions that are randomly selected as well as those that are selected in the order experienced by the participants.</p><p>To address the aims of this study, we reanalyzed the two similarity judgment data sets used in Stevens and Soh <ref type="bibr" target="#b5">[6]</ref>. We repeatedly split the data from each individual into a training set and testing set. We fit each algorithm to the training set and then used the fitted model to predict the testing set <ref type="bibr" target="#b13">[14]</ref>. We calculated accuracy, precision, and recall on this out-ofsample testing set. With this method, we investigated (1) which algorithms performed best, (2) which predictors best predicted judgments, and (3) how sample size and question order influenced predictive accuracy for similarity judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets</head><p>We tested the different machine-learning algorithms on two data sets used by Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> (available at https: //doi.org/10.17605/OSF.IO/EW8DC). In both data sets, participants with inattentive choice (e.g., judged 10 vs. 10 to be dissimilar or 1 vs. 90 to be similar), inconsistent choice (in a step-wise increase of large values, switching judgments more than three times), or near uniform choice (≥ 95% choice for similar or dissimilar) were removed. This eliminated 32 of the 155 participants, leaving 123 for our current analysis. </p><formula xml:id="formula_0">Recall = T S T S +FD Accuracy = T S +T D T S +FS +FD+T D</formula><p>choices used the same value pairs as the similarity judgments and were included first to expose participants to the range of amount and delay magnitudes and to provide the overall decision context before they made similarity judgments. This research was approved by the University of Nebraska-Lincoln Internal Review Board (IRB Approval # 20130313118EP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis</head><p>We used R <ref type="bibr" target="#b14">[15]</ref> and the R-packages C50 <ref type="bibr" target="#b15">[16]</ref>, caret <ref type="bibr" target="#b16">[17]</ref>, e1071 <ref type="bibr" target="#b17">[18]</ref>, foreach <ref type="bibr" target="#b18">[19]</ref>, GGally <ref type="bibr" target="#b19">[20]</ref>, here <ref type="bibr" target="#b20">[21]</ref>, kernlab <ref type="bibr" target="#b21">[22]</ref>, naivebayes <ref type="bibr" target="#b22">[23]</ref>, nnet <ref type="bibr" target="#b23">[24]</ref>, papaja <ref type="bibr" target="#b24">[25]</ref>, patchwork <ref type="bibr" target="#b25">[26]</ref>, ran-domForest <ref type="bibr" target="#b26">[27]</ref>, rpart <ref type="bibr" target="#b27">[28]</ref>, tidytext <ref type="bibr" target="#b28">[29]</ref>, and tidyverse <ref type="bibr" target="#b29">[30]</ref> for all our analyses (package usage described in the R script found in Supplementary Materials). The manuscript was created using rmarkdown <ref type="bibr" target="#b30">[31]</ref>. Data, analysis scripts, supplementary tables and figures, and the reproducible research materials are available in Supplementary Materials and at the Open Science Framework (https://doi.org/10.17605/OSF.IO/EDQ39). Predictors. We adapted predictors used in <ref type="bibr" target="#b5">[6]</ref> for our investigation in this paper. In the original study, Stevens and Soh tested 11 predictors: small value, large value, difference, ratio, mean ratio, log ratio, relative difference, disparity ratio, salience, discriminability, and logistic <ref type="table">(Table S1</ref>). However, we observed that a number of these predictors are very similar functions and thus may suffer from multicollinearity, which can be a problem for some machine-learning algorithms <ref type="bibr" target="#b6">[7]</ref>. Therefore, we computed pairwise correlations for all predictors ( <ref type="figure" target="#fig_0">Figures S1 &amp; S2)</ref>. Correlation coefficients for ratio, mean ratio, log ratio, relative difference, disparity ratio, salience, and discriminability all exceeded 0.81. Therefore, we removed mean ratio, relative difference, disparity ratio, and salience from the analyses. We kept ratio, log ratio, and discriminability as predictors because ratio was a key predictor in <ref type="bibr" target="#b5">[6]</ref> and log ratio and discriminability both have curvilinear relationships with ratio and therefore may provide additional information for classification. Thus, the following analyses include small value, large value, difference, ratio, log ratio, discriminability, and logistic. Algorithms. We used a set of commonly used algorithms, including tree-based models C5.0 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">32]</ref> and random forest <ref type="bibr" target="#b33">[33]</ref>, k-nearest neighbor <ref type="bibr" target="#b34">[34]</ref>, naive Bayes <ref type="bibr" target="#b35">[35]</ref>, neural networks <ref type="bibr" target="#b36">[36]</ref>, and support vector machines <ref type="bibr" target="#b37">[37]</ref>. We combined these with those used in <ref type="bibr" target="#b5">[6]</ref>: CART <ref type="bibr" target="#b10">[11]</ref> and logistic regression. Accuracy, precision, and recall. All analyses were conducted at the level of the individual participant for each judgment type (amount and delay). We conducted analyses for two different orderings: random and sequential. For random ordering, we first partitioned the data using a stratified random sample based on similarity judgments, so the training and testing sets had comparable distributions of similarity judgments (i.e., approximately the same proportion of "similar" vs. "dissimilar" judgments in both sets). For sequential ordering, we created the training set by drawing the judgments in the order in which each participant made their similarity judgments. Once the training sets were drawn, for both orderings, we generated testing sets by randomly drawing 10 samples from the non-training judgments. This ensured that all testing sets included the same number of judgments, regardless of training set size.</p><p>Because one of our research aims involved exploring how sample size influenced algorithm predictive accuracy, we analyzed accuracy over a range of training set sizes. The two data sets included 50 and 43 judgments of each type, and we analyzed training set sizes of 15, 20, 25, and 30 samples for both data sets. For data set 1, this is equivalent to 30%, 40%, 50%, and 60% of the total data, and, for data set 2, this maps to 36%, 48%, 59%, and 71% of the total data.</p><p>We fit models on each training set for each algorithm using the train function in the caret package <ref type="bibr" target="#b16">[17]</ref>, which uses bootstrapping to resample the data and fit the model repeatedly <ref type="bibr" target="#b6">[7]</ref>. We applied each model to the training set and calculated accuracy, precision and recall for the training data <ref type="table">(Table S2</ref>; <ref type="figure">Figures S3 and S4)</ref>. We then used the models to predict the testing data to calculate out-of-sample accuracy, precision, and recall. This process was repeated 100 times for each data set, judgment type, subject, algorithm, and training set size. We then calculated the mean accuracy, precision, and recall over the 100 repetitions (see <ref type="table">Table S2</ref> for other performance metrics). Predictor importance. All algorithms except support vector machines provide a measure of predictor importance. We calculated predictor importance on the full data set (no training and testing sets) for each participant, data set, judgment type, algorithm (except support vector machine), and predictor using the varImp function in the caret package <ref type="bibr" target="#b16">[17]</ref>. While each model type has a different metric of importance <ref type="table">(Table  S3)</ref>, we scaled importance values, with the most important variable importance set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm performance</head><p>To determine which algorithms best predict similarity judgments, we measured accuracy, precision, and recall on out-ofsample predictions from the aforementioned eight algorithms. We calculated these measures on the largest sample size (30 samples) and with random ordering for each participant. The confusion matrix, accuracy, precision, and recall percentages for training and testing sets and each algorithm are available in <ref type="table">Table S2</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> presents testing set accuracy, precision, and recall rates for each algorithm summarized over data set and judgment type (training set results available in <ref type="figure">Figures  S3 and S4</ref>). For accuracy (number of correct predictions / all predictions), neural network, random forest, and support vector machine algorithms yielded the highest accuracy rates at 90%, with naive Bayes and C5.0 performing slightly worse, followed by CART, logistic regression, and kNN. Precision (correct similar predictions / all similar predictions) shows a comparable ordering. For recall (correct similar predictions / actual similar judgments), naive Bayes tops the list, followed closely by random forest, neural networks, C5.0, and support vector machines. Similar rankings of the algorithms' performance were observed across both data sets and between amount and delay similarity judgments ( <ref type="figure">Figure S5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictor importance</head><p>Different algorithms use predictors differently, so the predictors can vary in their contribution to the model performance. To assess which predictors were most useful in predicting similarity judgments, we calculated predictor importance for each participant, data set, judgment type, algorithm, and predictor using the full data set. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the importance of each predictor summarized over data set, judgment type, and algorithm. The numerical difference between large and small values was the most important predictor, followed by logistic, ratio and discriminability, log ratio, large value, and small value. Similar rankings of the predictors' performance were observed across both data sets and between amount and delay similarity judgments ( <ref type="figure">Figure S6</ref>). While CART, kNN, naive Bayes, and random forest algorithms generate these rankings of predictor importance, C5.0, neural networks, and logistic regression generated different rankings ( <ref type="figure">Figure S7</ref>). C5.0 was somewhat similar to the others, logistic regression showed little differentiation between predictors, and neural networks generated completely different rankings than the other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size and order</head><p>Developing small but predictive sets of judgment questions can allow us to predict judgments of value pairs that participants have not made. To investigate the effect of sample size on algorithm performance, we randomly sampled different training set sizes and repeatedly assessed each algorithm's accuracy in predicting a fixed, out-of-sample testing set. <ref type="figure">Figure  3</ref> (left panel) shows out-of-sample accuracy for each algorithm at each sample size. Accuracy clearly increases with larger samples, but the rate of increase differs across algo- <ref type="figure">Figure 3</ref>. Out-of-sample accuracy for each sample size for each algorithm. Sample size refers to number of questions per participant used to train the algorithms. Random refers to a random sample of training questions used to predict a random sample of 10 testing questions. Sequential refers to a sample of training questions drawn in order of presentation to each participant that was used to predict a random sample of 10 testing questions. Dots represent means, and error bars represent between-subjects 95% confidence intervals (within-subject confidence intervals were not used because excessive missing data for small sample sizes caused too many participants to be removed from the calculations).</p><p>rithms. Remarkably, random forest and support vector machines have about 88% accuracy at the smallest sample size of 15 (out of 43-50 judgments). Naive Bayes, C5.0, and neural networks yield only slightly lower accuracy rates of 86%. The remaining algorithms perform substantially worse at the lowest sample size but increase their performances with larger sizes. CART, in particular, performs very poorly at the lowest sample size but dramatically improves its performance at the next size, where it surpasses kNN and logistic regression. These rank orderings of algorithm performance hold across data sets and judgments types, with slightly lower accuracy rates in data set 2 ( <ref type="figure">Figure S8A</ref>).</p><p>Though most assessments of sample size effects on algorithm performance randomly draw instances from data sets, the order in which participants experience questions can influence their responses. Given that the aim of this analysis is to determine how well small samples can predict judgments more generally, we must account for the sequential order in which participants make judgments. To investigate how well early questions can predict later ones, we fit the algorithms on training sets of various sizes, but, rather than randomly drawing the instances, we selected instances in the order in which participants experienced the questions. <ref type="figure">Figure 3</ref> (right panel) shows out-of-sample accuracy for each algorithm at each sample size for the sequentially ordered data. The pattern of results is qualitatively similar to those from the randomly selected data but with lower accuracy rates. Again, random forest and support vector machines top the algorithm rankings with only slightly lower accuracy than the random order (86%). And the algorithm rankings hold across data sets and judgment types ( <ref type="figure">Figure S8B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our analysis of algorithm performance found comparable levels of performance in accuracy, precision, and recall, but the algorithms differed in their performance across these three measures. Similarly, the different predictors varied in their contributions to algorithm performance, some of which matched previous findings, but others differed. Finally, as is typically the case in machine learning, algorithm performance improved with larger sample sizes, and the algorithms performed better predicting randomly selected samples than samples entered in the order experience by participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm performance</head><p>Neural network, random forest, and support vector machine algorithms generated the highest out-of-sample accuracy and precision for both data sets and judgment types. Naive Bayes and C5.0 joined all of these algorithms in showing the highest levels of recall. These analyses illustrate interesting differences across algorithms. First, this analysis contrasts with Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> showing better accuracy rates in CART than logistic regression. In the current analysis, CART and logistic regression have comparable levels of accuracy. This improvement in performance for logistic regression is likely due to removal of collinear predictors in the current analysis. Regression models are particularly susceptible to problems associated with multicollinearity <ref type="bibr" target="#b6">[7]</ref>.</p><p>The current analysis suggests that both CART and logistic regression are outperformed by a number of other machinelearning models, including C5.0, naive Bayes, neural networks, random forest, and support vector machines. Therefore, even higher levels of out-of-sample accuracy than those observed by Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> can be achieved by testing a wider range of models. A key reason that Stevens and Soh used CART was to test the possibility that decision trees capture the actual cognitive computations of decision making. That is, similarity judgments may actually be made in decision-tree-like ways. Thus, it is important to see that two other tree-based algorithms (C5.0 and random forest) outperform CART. While we do not directly test predictions about the computational process on C5.0 and random forest here, this provides a fruitful area of future research.</p><p>Decision trees are not the only class of algorithms that perform well. Neural networks and support vector machines perform as well as random forest. These algorithms, however, are "black box" algorithms in the sense that their process of converting predictors into predictions for the outcomes is not straightforward. Whereas random forest produces decision trees which can, in principle, mimic the cognitive computations of how judgments are made, neural networks produce a series of layers of nodes with weights connecting them <ref type="bibr" target="#b38">[38]</ref>, and support vector machines calculate multidimensional hyperplanes <ref type="bibr" target="#b39">[39]</ref>. Therefore, though neural networks mimic neural computations, these algorithms do not resemble a cognitive process.</p><p>The three performance measures were quite consistent across data sets and judgment types. Consistency across data sets indicates robustness of these analyses within the area of similarity judgments. Although there were only two data sets analyzed, the actual similarity value pairs differed between the data sets, and, perhaps more importantly, the study sample population differed with Germans being sampled in data set 1 and Americans in data set 2. Nevertheless, both populations were relatively similar in age and educational level, with the Germans being slightly older. Both participant groups were drawn from predominantly white, educated, industrialized, rich, and democratic (WEIRD) populations <ref type="bibr" target="#b40">[40]</ref>. The narrow scope of the questions and the similarity of the study populations make it difficult to generalize our findings beyond similarity judgments in WEIRD populations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictor importance</head><p>A key feature of many algorithms is that they can offer a metric of how much each predictor contributed to the predictions. This predictor importance offers insight into which predictors are most useful. Across all algorithms, our analysis showed that the numerical difference predictor contributed the most to predictive performance, followed by logistic, discriminability, and ratio. Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> also found difference to be the primary predictor used as the first node in 62-71% of participants' decision trees. In fact, difference was the most important predictor in the current analysis for all algorithms except logistic regression and neural networks. This provides robust evidence that one of the simplest predictors (large value − small value) is also the most important in making similarity judgments.</p><p>One key difference between the current analysis and Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> is the next most important predictors. Stevens and Soh found that ratio was the second most used primary node predictor for CART (27-33% of participants), with relative difference and logistic following (1-2%). The current analysis showed logistic followed by discriminability and ratio. This is a surprising contradiction of Stevens and Soh's findings because logistic and discriminability are more mathematically complicated combinations of small value and large value compared to ratio <ref type="table">(Table S1</ref>). Though a simple predictor is the most important predictor, the next most important predictor could be a more complex combination of small and large values.</p><p>The discrepancy with Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> could arise because of two reasons. First, the measure of predictor importance in the current analysis is based on different types of metrics across algorithms <ref type="table">(Table S3</ref>) that are scaled similarly for comparison. Because different algorithms use different metrics, the scaling (apart from the most and least important predictor) may not be comparable across algorithms. Therefore, the predictors of intermediate importance may be compressed or expanded differently across algorithms. Nevertheless, logistic was the second most important predictor across all but two of the algorithms. Second, the set of predictors in the two analyses differed. Stevens and Soh included all 11 predictors, and the current analysis used a limited set of predictors to reduce multicollinearity. The multicollinearity of many of the predictors with ratio could have somehow boosted its performance, whereas without multicollinearity, ratio's contribution could have been reduced. This finding speaks to the importance of feature selection in investigating predictor importance <ref type="bibr" target="#b41">[41]</ref>.</p><p>Finally, for this analysis, we eliminated a number of predictors from our analysis because of their multicollinearity with other predictors. Some of these predictors play key roles in psychological and economic models of intertemporal and risky choice (e.g., relative difference, salience). While these models may yield insights into decision making, it is important to consider that they are highly correlated with simpler predictors of these choices (e.g., ratio). Thus, parsimony requires substantial additional predictive value for these models to be favored over simpler ratio-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size and order</head><p>Sample size is a key aspect of algorithm performance <ref type="bibr" target="#b12">[13]</ref>. As expected, we found that accuracy increased with sample size of randomly selected data. Some algorithms (notably random forest and support vector machines) showed high outof-sample accuracy even at the smallest size (15 instances or 30-36% of the total number of instances). Therefore, choosing the appropriate algorithm can result in high out-of-sample accuracy even with small samples.</p><p>Analyses of randomly selected data, however, do not capture the potential effects of the order of experiencing questions on participants' judgments. That is, participants may get tired or change their judgment criteria over time. So judgments made early during testing may not match those made later in testing. To explore this, we analyzed the data by entering the instances in the order experienced by participants and examining accuracy across a range of sample sizes. Including the sequentially ordered instances reduced accuracy, but random forest and support vector machines still outperformed other algorithms, especially at small sample sizes.</p><p>While other algorithms dropped in accuracy substantially, random forest and support vector machines maintained very high accuracy for the sequentially ordered data. At the smallest sample size, these two algorithms correctly predicted 86% of the judgments. This level of accuracy with such small samples sizes is remarkable and bodes well for being able to collect rather small samples and extrapolate more generally.</p><p>In summary, we have evidence that machine-learning algorithms can take as input small amounts of data and make robust out-of-sample predictions. Leveraging these algorithms can influence experimental designs by requiring fewer questions. By reducing numbers of questions, we can minimize the burden on participants, which can either improve data quality by not tiring participants or allow the opportunity to add other experimental procedures when participant time is limited. Either way, employing machine-learning algorithms can enhance experimental design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and future directions</head><p>This article expands the application of machine learning to similarity judgments compared to Stevens and Soh <ref type="bibr" target="#b5">[6]</ref> by investigating more algorithms, more measures of performance, more sophisticated measures of predictor importance, and a more nuanced approach to sample size. However, the tools available in machine learning are many, and they are increasing in number and sophistication. We limited our analysis to eight algorithms, chosen based on suitability for our data and previous frequency of use in the machine-learning literature. Of course, there are other algorithms that we could have tested, some of which might have outperformed our top models. Nevertheless, we used a standard set of models, many of which had equally high performance. It seems unlikely additional models would provide substantial new insights or contradictory information.</p><p>A great deal of effort has focused on developing methods to optimize model parameters to improve fit <ref type="bibr" target="#b6">[7]</ref>. We took a relatively basic approach to tuning model parameters, primarily using default options in our analysis software. It is possible that more sophisticated parameter tuning could yield different results. However, more sophisticated tuning often comes at the price of longer computation times. We have opted to minimize computation time by using the default tuning methods. Finally, optimizing parameters can result in models overfitting data. We used standard cross-validation techniques to reduce overfitting by both calculating predictive performance measures on out-of-sample data fitted on training data and fitting models to the training data using resampling techniques <ref type="bibr" target="#b6">[7]</ref>.</p><p>In general, machine-learning models perform best with many instances to work with. This allows for large training sets that include representative instances from the population of possible instances. Though we have a large number of total instances (over 11,000), we conducted the analysis at the level of the participant and judgment type (amount or delay judgment) because we were interested in being able to predict individual participant judgments. This resulted in only 40-50 instances per analysis, which is rather small for machinelearning analyses that use cross-validation. This is apparent with the poor performance of CART at sample sizes of 15 samples but rapid improvement at 20 samples <ref type="figure">(Figure 3)</ref>. The other algorithms, however, show a more gradual increase in performance with sample size, suggesting that sample sizes used here are not too small to allow reasonable performance. From a logistical perspective, having participants answer more than 50 questions for each judgment is already rather tiring, and increasing the number of questions could result in poor data quality. So, though more instances could be better for the model performance, the models perform well with these sample sizes, and increasing them could produce more problematic data.</p><p>This article has focused on similarity judgments of monetary amounts and time delays because they are the attributes that are relevant to intertemporal choice. But the similarity approach also applies to risky and strategic choice <ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref>. Thus, this approach can be expanded beyond amounts and delays to probabilities of receiving rewards, an attribute of risky choice. Probabilities, however, are bounded, which could result in different algorithms and predictor importance compared to amounts and delays. Though the similarity approach has not been formally applied to multiattribute choice (e.g., choosing an apartment based on rent, size, distance from work, etc.), this is another area to which it could be applied. The scale and boundedness of the attribute values could influence how similarity is assessed, but these methods should be able to apply to most quantitative attributes. Yet research on similarity is not limited to quantitative attributes <ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref><ref type="bibr" target="#b47">[47]</ref>, and machine learning has broad application to understanding both quantitative and non-quantitative components of similarity <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Machine learning comprises a powerful set of tools to classify outcomes. While some areas of psychology have been fruitfully using machine learning for a while <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51]</ref>, the field has not leveraged these tools fully <ref type="bibr" target="#b13">[14]</ref>. Judgment and decision making, in particular, is an area ripe for applying machine learning, and some have taken advantage of these tools <ref type="bibr" target="#b52">[52]</ref><ref type="bibr" target="#b53">[53]</ref><ref type="bibr" target="#b55">[54]</ref>. Here, we used machine learning to achieve multiple goals. First, we assessed the performance of several algorithms in predicting similarity judgments from participant data. Though evaluating algorithm performance is not typically a psychological question, in our case, we investigated whether decision tree algorithms performed well, since they could offer cogni-tive process-based models of actual decision making. Indeed, previously, we found that CART outperformed regression we found that the random forest algorithm-one that is based on decision trees-topped the list of best-performing algorithms. We can further probe this algorithm because, not only does it accurately predict similarity judgments, it also may give us a window into the process of classification by generating measures of predictor importance and allowing the extraction of a step-by-step set of rules used to generate the predictions. Testing a broad range of machine-learning algorithms allowed us to pinpoint a highly accurate model that may also approximate the actual judgment process. Future research will integrate these algorithms into the similarity model to predict the intertemporal choices.</p><p>Second, our analysis provided the opportunity to examine which predictors were most important in making the judgment classifications. While regression alone can provide information about predictor performance, it is only a single model, and its predictions depend on its assumptions and methods. Our analysis produced predictor importance measures across a range of algorithms, which can provide information about the robustness of importance across models. For instance, we found rather consistent rankings of predictor importance across four very different types of algorithms ( <ref type="figure" target="#fig_1">Figure 2</ref>). But differences across algorithms are interesting as well. For example, while it has above average importance in most algorithms, the predictor discriminability is ranked most important by neural networks. This could inspire further investigations, as assessing predictor importance across a range of algorithms can be useful in drawing inferences about those predictors.</p><p>Finally, in addition to answering theoretical questions about models and predictors, machine learning can inform the logistics of data collection. We evaluated algorithm accuracy across a range of training set sizes to see how robust they are to sample size. Moreover, we used samples ordered by how they were experienced by participants to see how predictive different numbers of questions were to judgments more generally. Our analysis showed that some algorithms could predict judgments with quite high accuracy at rather small sample sizes. This finding is useful for designing future studies, where we can trim the number of questions that we ask participants, which can reduce participant fatigue or allow time to ask other questions. Thus, using machine-learning algorithms can help us both understand our data in more depth and design better ways of collecting those data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations Funding</head><p>This research was funded by the National Science Foundation (SES-1658837).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Out-of-sample accuracy, precision, and recall for each algorithm based on random ordering of a sample size of 30 instances and a testing set size of 10 instances. For each performance measure, algorithms are ordered by mean score. Dots represent means, error bars represent within-subjects 95% confidence intervals, boxplot horizontal lines represent medians, boxes represent interquartile range (25-75th percentile), whiskers represent 1.5 × interquartile range. Outliers are not shown. Note the y-axis is truncated at 0.5 to enlarge the presentation of the means and confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Importance of each predictor for each algorithm. Predictor importance refers to the relative contribution of each predictor to predicting the response. Predictors are ordered by mean importance. Dots represent means, error bars represent within-subjects 95% confidence intervals, boxplot horizontal lines represent medians, boxes represent interquartile range, whiskers represent 1.5 × interquartile range. Outliers are not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>recruited from the Adaptive Behavior and Cognition Web Panel at the Max Planck Institute for Human Devel- opment in Berlin, Germany in August 2011. Participants received a flat fee of €3 for completing the survey. Web panel participants made similarity judgments between 50 pairs</head><label></label><figDesc>of amount values (e.g., €6 vs. €8) and 49 pairs of delay values (e.g., 6 days vs. 8 days): "Please decide whether the numbers are similar". This research was approved by the Max Planck Institute for Human Development's Ethics Committee. Confusion matrix for true vs. predicted judgments with accuracy, precision, and recall</figDesc><table><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">True judgment</cell><cell></cell></row><row><cell>Predicted judgment</cell><cell>Judged similar</cell><cell>Judged dissimilar</cell><cell></cell></row><row><cell>Predicted similar</cell><cell>True Similar (T S )</cell><cell>False Similar (FS )</cell><cell>Precision = T S T S +FS</cell></row><row><cell cols="3">Predicted dissimilar False Dissimilar (FD) True Dissimilar (T D)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">The first data set was collected from 50 participants (25 males</cell></row><row><cell></cell><cell></cell><cell cols="2">and 25 females) with a mean±SD age of 28.6±3.8 (range 24-</cell></row><row><cell></cell><cell></cell><cell cols="2">42) years The second data set was collected from 73 participants (25</cell></row><row><cell></cell><cell></cell><cell cols="2">males and 48 females) with a mean±SD age of 19.9±1.6</cell></row><row><cell></cell><cell></cell><cell cols="2">(range 18-26) years recruited from the University of Nebraska-</cell></row><row><cell></cell><cell></cell><cell cols="2">Lincoln Department of Psychology undergraduate participant</cell></row><row><cell></cell><cell></cell><cell cols="2">pool in December 2014. Participants received course credit</cell></row><row><cell></cell><cell></cell><cell cols="2">for their participation. Participants started by making 20 in-</cell></row><row><cell></cell><cell></cell><cell cols="2">tertemporal choices before rating the similarity of 41 reward</cell></row><row><cell></cell><cell></cell><cell cols="2">amount values and 42 time delay values: "Do you consider</cell></row><row><cell></cell><cell></cell><cell cols="2">receiving [small amount] and [large amount] to be similar or</cell></row><row><cell></cell><cell></cell><cell cols="2">dissimilar?" and "Do you consider waiting [short delay] and</cell></row><row><cell></cell><cell></cell><cell cols="2">[long delay] to be similar or dissimilar?". The intertemporal</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of interest</head><p>The authors declare no known conflicts of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and material</head><p>All data files and supplementary materials (tables, figures) are available at https://doi.org/10.17605/OSF.IO/EDQ39.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>All data analysis scripts are available at https://doi.org/10.176 05/OSF.IO/EDQ39. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Blackwell Handbook of Judgment and Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Read</surname></persName>
		</author>
		<editor>D. Koehler &amp; N. Harvey</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Blackwell</publisher>
			<biblScope unit="page" from="424" to="443" />
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
	<note>Intertemporal choice</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Survey of time preference, delay discounting models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="135" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity judgments and anomalies in intertemporal choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Leland</surname></persName>
		</author>
		<idno type="DOI">10.1093/ei/40.4.574</idno>
		<ptr target="https://doi.org/10.1093/ei/40.4.574" />
	</analytic>
	<monogr>
		<title level="j">Economic Inquiry</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="574" to="581" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Economics and psychology&quot;? The case of hyperbolic discounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="DOI">10.1111/1468-2354.t01-1-00106</idno>
		<ptr target="https://doi.org/10.1111/1468-2354.t01-1-00106" />
	</analytic>
	<monogr>
		<title level="j">International Economic Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1207" to="1216" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intertemporal similarity: Discounting as a last resort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Stevens</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.1870</idno>
		<ptr target="https://doi.org/10.1002/bdm.1870" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="24" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting similarity judgments in intertemporal choice with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-K</forename><surname>Soh</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-017-1398-1</idno>
		<ptr target="https://doi.org/10.3758/s13423-017-1398-1" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="627" to="635" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Applied Predictive Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic construction of decision trees from data: A multi-disciplinary survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1009744630224</idno>
		<ptr target="https://doi.org/10.1023/A:1009744630224" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="389" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_204</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-30164-8_204" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<editor>C. Sammut &amp; G. I. Webb</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="263" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Confusion matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_157</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-30164-8_157" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<editor>C. Sammut &amp; G. I. Webb</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="209" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tree induction versus logistic regression: A learning-curve analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Perlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Simonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="211" to="255" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Choosing prediction over explanation in psychology: Lessons from machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Westfall</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691617693393</idno>
		<ptr target="https://doi.org/10.1177/1745691617693393" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1100" to="1122" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Core Team</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
	<note>: R Foundation for Statistical Computing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">C50: C5.0 decision trees and rule-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quinlan</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=C50" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">caret: Classification and regression training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=caret" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weingessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">;</forename><surname>Leisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu Wien</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=e1071" />
		<title level="m">e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">foreach: Provides foreach looping construct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=foreach" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">GGally: Extension to &quot;ggplot2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schloerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Briatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Larmarange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=GGally" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">here: A simpler way to find your files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=here" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernlab -an S4 package for kernel methods in R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<ptr target="http://www.jstatsoft.org/v11/i09/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">naivebayes: High performance implementation of the naive Bayes algorithm in R. Re</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Majka</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=naivebayes" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Venables</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<ptr target="http://www.stats.ox.ac.uk/pub/MASS4" />
		<title level="m">Modern Applied Statistics with S</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>4th ed.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">papaja: Create APA manuscripts with R Markdown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barth</surname></persName>
		</author>
		<ptr target="https://github.com/crsh/papaja" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">patchwork: The composer of plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Pedersen</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=patchwork" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification and regression by randomForest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/doc/Rnews/" />
	</analytic>
	<monogr>
		<title level="j">R News</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">rpart: Recursive partitioning and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Therneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Atkinson</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=rpart" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">tidytext: Text mining and analysis using tidy data principles in R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00037</idno>
		<ptr target="https://doi.org/10.21105/joss.00037" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">tidyverse: Easily install and load the &quot;tidyverse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=tidyverse" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">R Markdown: The Definitive Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Allaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grolemund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Chapman</publisher>
			<pubPlace>Boca Raton, Florida</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crc</surname></persName>
		</author>
		<ptr target="https://bookdown.org/yihui/rmarkdown" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">C4.5: Programs for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Random forests. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1967.1053964</idno>
		<ptr target="https://doi.org/10.1109/TIT.1967.1053964" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic indexing: An experimental inquiry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Maron</surname></persName>
		</author>
		<idno type="DOI">10.1145/321075.321084</idno>
		<ptr target="https://doi.org/10.1145/321075.321084" />
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="417" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02478259</idno>
		<ptr target="https://doi.org/10.1007/BF02478259" />
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1145/130385.130401</idno>
		<ptr target="https://doi.org/10.1145/130385.130401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory<address><addrLine>Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Computer Science</title>
		<editor>A. Ralston, E. D. Reilley, &amp; D. Hemmendinger</editor>
		<imprint>
			<publisher>John Wiley and Sons Ltd</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_804</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-30164-8_804" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<editor>C. Sammut &amp; G. I. Webb</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="941" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The weirdest people in the world?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norenzayan</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X0999152X</idno>
		<ptr target="https://doi.org/10.1017/S0140525X0999152X" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="61" to="83" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Feature Engineering and Selection: A Practical Approach for Predictive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Similarity and decision-making under risk (Is there a utility theory resolution to the Allais paradox?)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-0531(88)90154-8</idno>
		<ptr target="https://doi.org/10.1016/0022-0531" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90154" to="90162" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized similarity judgments: An alternative explanation for choice anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Leland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Uncertainty</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Equilibrium selection, similarity judgments, and the &quot;nothing to gain/nothing to lose&quot; effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Leland</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.1772</idno>
		<ptr target="https://doi.org/10.1002/bdm.1772" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Features of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.84.4.327</idno>
		<ptr target="https://doi.org/10.1037/00" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Toward a universal law of generalization for psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shepard</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.3629243</idno>
		<ptr target="https://doi.org/10.1126/science.3629243" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4820</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cambridge Handbook of Thinking and Reasoning</title>
		<editor>K. J. Holyoak &amp; R. Morrison</editor>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Instancebased learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00153759</idno>
		<ptr target="https://doi.org/10.1007/BF00153759" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Similarity and rules: Distinct? Exhaustive? Empirically distinguishable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0010-0277(97)00044-9</idno>
		<ptr target="https://doi.org/10.1016/S0010-0277" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integrating theory and data in category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Categorization by Humans and Machines: Advances in Research and Theory</title>
		<meeting><address><addrLine>San Diego, CA, US</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="189" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comparison of machine learning with human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Parks</surname></persName>
		</author>
		<idno type="DOI">10.1080/07421222.1993.11517977</idno>
		<ptr target="https://doi.org/10.1080/07421222.1993.11517977" />
	</analytic>
	<monogr>
		<title level="j">Journal of Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Combining psychological models with machine learning to better predict people&apos;s decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/s11229-012-0182-z</idno>
		<ptr target="https://doi.org/10.1007/s11229-012-0182-z" />
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The bias bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2015.01.061</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2015.01.061" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1772" to="1784" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
