<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">It Seems Smart, but It Acts Stupid: Development of Trust in AI Advice in a Repeated Legal Decision-Making Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><forename type="middle">C</forename><surname>Willemsen</surname></persName>
							<email>m.c.willemsen@tue.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">C P</forename><surname>Snijders</surname></persName>
							<email>c.c.p.snijders@tue.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">K</forename><surname>Kahr</surname></persName>
							<email>p.k.kahr@tue.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerrit</forename><surname>Rooks</surname></persName>
							<email>g.rooks@tue.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">of Technology</orgName>
								<orgName type="institution">The Gerrit Rooks</orgName>
								<address>
									<country>The</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The of Data</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">It Seems Smart, but It Acts Stupid: Development of Trust in AI Advice in a Repeated Legal Decision-Making Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3581641.3584058</idno>
					<note type="submission">Received 14 October 2022; revised 3 February 2023; accepted 11 February 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-AI Interaction</term>
					<term>Trustworthy AI</term>
					<term>Trust Development</term>
					<term>Collaborative Decision-Making</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Humans increasingly interact with AI systems, and successful interactions rely on individuals trusting such systems (when appropriate). Considering that trust is fragile and often cannot be restored quickly, we focus on how trust develops over time in a human-AI-interaction scenario. In a 2x2 between-subject experiment, we test how model accuracy (high vs. low) and type of explanation (human-like vs. not) affect trust in AI over time. We study a complex decision-making task in which individuals estimate jail time for 20 criminal law cases with AI advice. Results show that trust is significantly higher for high-accuracy models. Also, behavioral trust does not decline, and subjective trust even increases significantly with high accuracy. Human-like explanations did not generally affect trust but boosted trust in high-accuracy models. CCS CONCEPTS • Human-centered computing → User studies; HCI theory, concepts and models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It might not be the exception but the rule that in only a few years mixed teams consisting of humans and artificially intelligent (AI) systems will interact with each other on a daily basis. However, collaboration with AI is not yet widely accepted, and how to best shape the interaction between humans and AI systems is not clear. The aforementioned must be confident that the AI system can help with the decision despite having a limited understanding of the AI's underlying mechanisms. Such confidence may require specific knowledge or training to become accustomed to a system. Trust in AI can be challenged even more when individuals experience failures of AI systems. One way to increase trust in model-based AI decision-making is to open the black box in a way that individuals feel empowered to work with it and develop a well-calibrated trust relationship. That is, they should not over-trust a failing system (for instance, because of automation or positivity bias), nor should they under-trust recommendations (for instance, because of tech skepticism, status quo bias, or general overconfidence in their own ability compared to the system).</p><p>Research has recognized the importance of knowing what influences human-AI interaction. Earlier studies regarding trust in AI have focused on the influence of system properties (e.g., XAI performance), characteristics of users (e.g., skills, attitudes), the interaction of the latter with intelligent systems, or the overall context of the decision-making task. Despite the volume and variety of existing research, trust-in-automation studies' results are inconclusive and leave lots of open issues. A large part of the trust in AI research is conducted in laboratory studies with rather abstract tasks, which offer limited options for testing complex decision-making processes, making it difficult to draw conclusions for real-world applications. In addition, few studies have investigated human-AI interaction tasks over a more extended period of time, cf. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b72">73]</ref>. However, research in trust over time is essential to understanding and developing healthy long-term relationships between humans and AI.</p><p>With the underlying study, we follow up on previous work that has examined how different explanation types and performance levels affect trust <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b57">58]</ref>. We do so by analyzing trust on the basis of an applied scenario (the assessment of jail times), emphasizing trust development over time. We consider two factors in our experiment. First, we distinguish between abstract and human-like explanations. We argue that the latter adds value in the form of a contextual, reasonable explanation that helps individuals to trust AI decisions more when compared with displaying a list of keywords. Furthermore, we consider whether trust develops differently for different model accuracies: our rationale is that AI models that perform poorly will not be able to build up the same level of trust as high-performing models. With this setup, we not only hope to contribute to the current HCI literature but can address two practical challenges simultaneously: first, measuring trust in an applied decision-making scenario by letting individuals deal with jail time decisions of real criminal law cases. Second, measuring the trust based on repeated interactions. The following research questions guide our work:</p><p>• RQ1: Is trust in AI different for different model accuracies and explanation types? • RQ2: How does trust develop over time depending on model accuracy and type of explanation?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Trust in intelligent systems is a prerequisite for successful humancomputer interactions. It is defined as "the attitude that an [AI] agent will help achieve an individual's goal in a situation characterized by uncertainty and vulnerability" <ref type="bibr" target="#b43">[44]</ref>. In many scenarios, AI advice has been shown to lead to better outcomes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b59">60]</ref>. This, however, is neither a consistent finding nor does it always lead to outcomes that are as good as they can be. The aim is for humans to calibrate trust in AI correctly: they should neither place too much trust in a failing system nor put too little trust in systems for the wrong reason. Thus, we have to find ways to avoid biases in behavior, for example, blindly following AI (automation bias, positivity bias) or rejecting it on principle.</p><p>The scientific community has done considerable work to uncover processes and factors that influence trust in AI systems. Trust can be affected by the properties of the system: Transparency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52]</ref>, high system accuracy or reliability <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b71">72]</ref> increase trust, and also visual representations or human-like appearing systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. In addition, communication between system and humans affects trust, for example, explaining mistakes <ref type="bibr" target="#b21">[22]</ref>, repair strategies like AI providing justifications or apologies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref>, or written explanations -versus visuals - <ref type="bibr" target="#b67">[68]</ref> increase AI trust. The same accounts for "human in the loop" interaction approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b26">27]</ref>. Research also found evidence that individuals' skills and character traits influence trust: for example, confident people are found to be less trustworthy regarding AI <ref type="bibr" target="#b44">[45]</ref>, experts also tend to under-trust AI performance <ref type="bibr" target="#b48">[49]</ref>, and (political) conservatism and age are associated with low comfort of trusting AI systems <ref type="bibr" target="#b10">[11]</ref>. Lastly, trust may depend on the context in which a task occurs: certain emotional states mediate trust in AI <ref type="bibr" target="#b23">[24]</ref>, and uncertainty and complexity hinder trust in AI <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Trust in AI over time</head><p>Research on trust in AI appears to be gaining recognition in terms of quantity and diversity of research themes. What still lacks attention is research of human-AI interactions in a long-term or repeatedscenario context <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b66">67]</ref>. Measuring trust in AI over time adds additional complexity as we need to consider the factors just described (system properties, user characteristics, interaction modes, context) and the dynamics in the relationship between humans and AI systems. As individuals familiarize themselves with a system and understand it better, trust will grow but not necessarily in a linear fashion. Currently, research offers no clear answer as to whether trust in AI tends to decrease or increase over time. Yang et al. <ref type="bibr" target="#b70">[71]</ref> found that trust increases with automation successes but decreases even more when seeing it err, thus summarizing that automation failures loom larger than their successes. Similar studies from Chacon et al. <ref type="bibr" target="#b11">[12]</ref> and Nourani et al. <ref type="bibr" target="#b53">[54]</ref> show a sharp decline in trust after early AI errors, and it was not recovering to the same levels when experiencing good AI performance afterward. Nourani and colleagues as well as Desai et al. <ref type="bibr" target="#b17">[19]</ref> conclude a primary-recency effect: initial (and late) interactions affected trust the most when trust is measured after exposure to the system.</p><p>At the same time, some comparable studies show that trust grows over time: Chiou et al. <ref type="bibr" target="#b14">[15]</ref> found that individuals learn to trust an intelligent (robotic) system as operators understand how to work with it over the course of several interactions successfully. Similarly, Manchon et al. <ref type="bibr" target="#b49">[50]</ref> assessed trust in an automated driving system over time (three assessments in 4 months). In contrast to earlier studies <ref type="bibr" target="#b40">[41]</ref>, trust increased over time for both trustful and distrustful drivers. Manchon and colleagues posit that several positive interactions in the early phase of the study supported even distrustful participants to gain trust fast. It can be agreed upon that initial trust is crucial for longer interactions, especially motivating individuals to start using automated systems in the first place <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref>. Tolmeijer et al. <ref type="bibr" target="#b66">[67]</ref> add that initial trust is important to accept better later mistakes, strengthening long-term interactions. Still, caution is warranted because not everyone can judge AI advice appropriately. Laypeople over-relied on AI recommendations when experiencing correct outputs at the beginning of an interaction. In contrast, experts used AI advice less, even after their performance decreased over time <ref type="bibr" target="#b53">[54]</ref>. Beggiato and Krems <ref type="bibr" target="#b4">[5]</ref> conclude that initial information about a system is appreciated and helps people to trust and accept AI advice. Being able to form a mental model about a system's capabilities must match with the experiences individuals make, otherwise, trust and acceptance can decrease.</p><p>Lastly, Yu et al. <ref type="bibr" target="#b72">[73]</ref> studied trust dynamics based on different levels of model accuracy. Based on participants simulating a binary quality control task (assessing whether drinking glasses were produced correctly or not, with false positive or false negative AI advice), they suggested that trust trajectories are different based on accuracy levels: trust increased over time when system accuracy was 80% or higher. However, it decreased with 70% accuracy. They also showed that AI failures at different time stages along the interaction demonstrated different implications in the change of trust: participants that had time to become familiar with the system did not decrease in trust. Trust over time stabilized, especially at the end of each task block. Over time, individuals formed a stable mental model for themselves, also described as the inertia of trust. Overall, they propose a phase model in which individuals learn to what extent a system can be trusted (phase 1), adjust this learned trust (phase 2), and then fine-tune procedures (phase 3).</p><p>Taken together, previous research suggests that the initial phase in human-AI interactions dictates how trust in a system is developing. It is when users still get familiar with a system and form their mental model that stabilizes only after some interaction with a system. Trust increases with positive first impressions but does not recover easily from early mistakes. However, this scheme could lead to unwarranted mistrust in a system: Early failures are not always indicative of overall poor performance, and late failures do not lose severity just because people have already consolidated their own image of an AI. We now turn to how model accuracy and explanations influence trust in AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Trust in AI: System Accuracy</head><p>In many (though not all) prediction tasks, intelligent systems can match or outperform their human counterparts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. In general, a system's accuracy is an essential criterion for finding a helpful AI tool and is a prominent determinant of trust <ref type="bibr" target="#b63">[64]</ref>. Despite its capabilities, systems can fail at certain points, and users must remain alert to detect faulty systems and wrong decisions from (in principle) decent systems. Studies show that consistently low accuracy is indeed observed and acted upon: Yin et al. <ref type="bibr" target="#b71">[72]</ref> found that trust was significantly affected by the actual level of accuracy of a system; in comparison, the effect of stated accuracy only has a negligible impact on trust. Similarly, information about a system's (high) reliability increased trust and performance <ref type="bibr" target="#b24">[25]</ref>. Moreover, trust was sustainably damaged and recovered only slowly after experiencing errors in performance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b72">73]</ref>. Yu et al. <ref type="bibr" target="#b72">[73]</ref> manipulated system accuracy on four levels (70%, 80%, 90%, and 100%), giving false positive/negative advice accordingly: trust decreased only in the 70 percent condition. Papenmeier et al. <ref type="bibr" target="#b57">[58]</ref> compared trust levels of participants that interacted with a high, medium, and low ("antagonistic") accuracy and found that participants indeed showed adequate levels of trust, in line with Yu and colleagues. To summarize, individuals can distinguish between appropriate and poor AI advice based on its accuracy, which is a crucial prerequisite for optimal trust calibration. It is vital that individuals only trust sufficiently accurate models (that is, more accurate than they are themselves). We will apply different levels of accuracy to compare how trust develops. Like Yu or Papenmeier, model accuracy is not explicitly stated and can, therefore, only be anticipated by the recommendation itself. It will be interesting to see whether study participants can distinguish between higher and lower-performing models, and, more so, to what extent this is reflected in their trust trajectories over the course of repeated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Trust in AI: Explaining AI Output</head><p>With the increase in computing power, AI systems have also increased in complexity and opacity. The general consensus is that explanations, specifically explainable AI (XAI), enable individuals to understand AI systems and their operations better <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47]</ref>, which is especially true for decision-making in complex, risky domains. Explanations help people to understand how, when, and why models make predictions <ref type="bibr" target="#b33">[34]</ref>. XAI can be in textual or visual form, to complement a recommendation (e.g. counterfactual examples, probability values), as interventions (warnings, nudges), or post-decision arguments (apologies, promises, justifications) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref>. Explanations (in comparison to no explanations) have also been shown to be a success in increasing trust over time <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Although the majority of studies found positive effects of AI explanations on trust, some studies observed feelings of manipulation by AI <ref type="bibr" target="#b8">[9]</ref> or subsequent overconfidence in AI <ref type="bibr" target="#b68">[69]</ref>. Papenmeier et al. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> find evidence that not all explanations are helpful, and some might even be harmful: they discovered that adding nonsensical or random explanations hurt trust (as one would hope). Furthermore, their results show that explanations do not improve trust when individuals interact with a sufficiently accurate system. They argue that the type of explanations that were used, highlighting words in a text document, did not improve decision-making as it did not add any value for participants: explanations were of statistical rather than causal nature, which only partially supports human understanding. Lim and Dey <ref type="bibr" target="#b45">[46]</ref> found that understanding and trust in a system are highest when explanations are provided. Recent studies attempt to adapt human knowledge and rationales for XAI to increase understanding and, thus, trust in AI <ref type="bibr" target="#b42">[43]</ref>  <ref type="bibr" target="#b65">[66]</ref>. Finally, Nourani et al. <ref type="bibr" target="#b52">[53]</ref> tested whether trust differs for meaningful versus meaningless explanations. They defined meaningfulness as the level to which explanations were perceived as meaningful in the human context. Their results show that participants significantly underestimated system accuracy when providing weak (less humanmeaningful) explanations, as they did not understood results. The study, which was conducted with non-expert participants, claims the need for "human-interpretable" explanations. As a conclusion from these research results, we hypothesize that explanations offer added value if they are human-like and contextual rather than abstract explanations that are limited in both content and form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STUDY DESIGN</head><p>The underlying study builds on the knowledge of previous work: We analyze trust development using a decision-making task based on real (criminal law) data, in which participants are being supported by an AI system. Our study follows a 2x2 between-subjects design. Participants are randomly assigned to one of the four groups, in which we manipulate system accuracy (high vs. low) and explanation type (human-like vs. abstract). Their task is to estimate jail time for 20 legal cases. For this, participants are supported by an AI system that provides a numerical jail time estimation and an additional textual explanation (see <ref type="figure" target="#fig_0">Figure 1</ref>). All 20 criminal cases (from 2022) were selected from the Dutch database de Rechtspraak [17] and thus the final verdicts (our baseline truth for the experiment) were known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measurements and Hypotheses</head><p>System Accuracy. To test whether different model accuracies resulted in different trust levels, we applied two accuracy levels: one model representing an AI model with high accuracy and one model with low accuracy. Accuracy was defined as the extent to how close the jail time estimate from the AI system was to the actual jail time (ground-truth). As we wanted the deviation error to occur as natural as possible, we calculated the AI estimates by adding The highaccuracy model deviated +/-10 percent (95% interval) from the correct jail time, and +/-50 percent (95% interval) in the low-accuracy model. All estimates were weighed against each other to avoid systematic bias, such that AI estimates were either higher or lower than actual jail time. Overall, we wanted participants to perceive the high-accurate model as more competent as the low-accurate model. Still, there were a few cases where the low-accuracy model performed equally well or better than the high-accuracy model (see <ref type="table">Table 1</ref>) due to chance and the normal distribution. Unlike Yu et al. <ref type="bibr" target="#b72">[73]</ref>, we applied continuous measurements (estimates of jail time in months) and only manipulated two different accuracy levels. Finally, estimates were pre-calculated and fixed to be the same for all participants <ref type="table">(Table 1)</ref>. Explanation Type. We expect that trust develops differently with different explanation types of a system. To prepare explanations for each case, we identified a set of case-specific keywords. For this, we first read the law case reports from which we created the shorter case descriptions. Based on the latter, we picked keywords that best represented the cases in terms of their specific analysability. The selection of case-specific keywords was then applied to two different styles of explanations: human-like explanations (condition 1) which applied full-sentence text and, thus, added a sense of human-like intelligence by reasoning more contextually, while for the second style, the abstract explanations (condition 2), the selected keywords were presented as a simple series of words. Following the AI design proposals of Knijenburg and Willemsen <ref type="bibr" target="#b37">[38]</ref> and Nourani et al. 's <ref type="bibr" target="#b52">[53]</ref> findings, we argue that displaying full-sentenced explanations elicits comprehensibility and allows the feeling of interacting with a rather intelligent, human-like system. Following this line of thought, we highlighted the different (explanation) capabilities also by introducing the two systems to the study participants with different system names (human-like: AI legal case analysis system, abstract: jail time calculator program) and providing details to the inner workings of each. Both, the explanations and the supporting cues as described above could lead individuals to perceive the human-like system as more capable and potentially reliable, thus enhancing trust in its recommendations.</p><p>We expect that trust is different for our four conditions: We hypothesize that participants will place more trust in a high-accuracy model versus a low-accuracy one. We furthermore believe that trust increases more (or decreases less) when explanations have a human-like portrayal. We, therefore, assume the following hypotheses, which were also pre-registered under the Open Science Framework 1 .</p><p>• H1: Trust is higher in the high accuracy model than in the low accuracy model. • H2: Trust increases more / decreases less over time in the high accuracy model than in the low accuracy model ("high accuracy protects trust better"). • H3: Trust is higher with human-like explanations than with abstract explanations. • H4: Trust increases more / decreases less over time with human-like explanations than with abstract explanations ("Human-like explanations protect trust better").</p><p>Trust. Measuring trust development based on the presented hypotheses, we apply a two-fold approach: First, we measure behavioral trust, which Lee and See <ref type="bibr" target="#b43">[44]</ref> define as an attitude that is directly observable and more prone to show objective outcomes. Specifically, we measure participants' Weight on Advice (WoA), which tells us how much an individual relies on the AI's advice.</p><p>Sniezek and van Swol's <ref type="bibr" target="#b64">[65]</ref> Judge-Advisor System paradigm calculates the degree to which people change their behavior -it is calculated by weighing two consecutive estimations, the first estimate before seeing the estimate of the automated system, and the second estimate after seeing the estimate of the automated system. The continuous outcome ranges from 0 (people completely ignored the AI estimate and stayed with their own estimate) to 1 (people completely relied on the AI advice and adopted it as their own estimate). Applying this measurement, we follow other studies in the trust in AI literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49]</ref>.</p><formula xml:id="formula_0">ℎ = 2 − 1 − 1</formula><p>As a second measurement, we track self-reported trust. Subjective measures are used to capture inherently subjective trust data, reflecting an individual's perspective <ref type="bibr" target="#b60">[61]</ref>. These psychological constructs are usually measured with survey scales. We ask participants to indicate their level of trust after every legal case. This is done with a single-question item asking participants to indicate their current level of trust in the AI system (1 = no trust at all, 10 = full trust). Scharowski and colleagues point out that one reason the overall results of the AI literature on trust are inconclusive is that there are no standardized measures, partly because definitions of trust are ambiguous. In addition, most studies measure trust either in a selfreported or behavioral manner. To be able to compare both trust types, we use both metrics. They are measuring behavioral trust as the advantage to quantify trust somewhat objectively. However, a potential pitfall could be that not being close to the AI estimate does not necessarily mean that trust is low but that individuals are more confident in solving a task on their own. Since confidence in behavior is calculated, the result also depends on the result itself: if the user's initial estimate and the AI estimate are close from the <ref type="table">Table 1</ref>: Overview of jail time per case (from left to right): the actual jail time of each legal case, the calculated jail time estimate in the high-accuracy condition (+/-10%), the calculated jail time estimate in the low-accuracy condition (+/-50%) Another reason for measuring trust in the explained manner is that we want to see how trust develops over time, a dimension that is often neglected. To not overburden people with too many question items, we do not use a multi-item trust checklist as recommended by Jian et al. <ref type="bibr" target="#b32">[33]</ref> that can capture several trust dimensions (i.e., reliability, integrity, familiarity). We decided to ask for subjective trust with a single item immediately after every task similar to Yu et. al. <ref type="bibr" target="#b72">[73]</ref>.</p><p>In addition, we measure several other variables that we consider potential covariates. We ask participants to indicate their level of law expertise (10-point Likert scale: 1 = no expertise at all; 10 = very high). Logg et al. <ref type="bibr" target="#b48">[49]</ref> showed that experience could lead individuals to over-trusting their own skills and, therefore, to mistrust or ignore an intelligent system's recommendations. Following the assumption that personality traits affect trust behavior, as done before <ref type="bibr" target="#b61">[62]</ref>, we propose to measure whether prosocialness influences trust in AI, we apply the Prosocial Behavioral Intentions Scale [4] ( = .81). Participants have to rate their willingness to get involved in social situations on a 7-point Likert scale (1 = I would definitely not do this; 7 = I would definitely do this). Finally, we ask study participants to indicate their affinity for technology, using the 4-item Affinity for Technology Interaction Short Scale (ATI-S, = .87) from Wessel et al. <ref type="bibr" target="#b69">[70]</ref>. Participants have to indicate to what extent they agree with the four statements on a 6-Point Likert scale (1 = completely disagree; 6 = completely agree).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Study Procedure</head><p>At the beginning of the experiment, participants were randomly allocated to one of the four groups. After being introduced to the terms of the experiment and accepting the consent form, participants were introduced to the automated system -either the system with human-like explanations, called AI Legal Case Analysis System, or the system with simple explanations, called Basic Jail Time Calculator program. To support active engagement with the system, we asked participants to confirm that they had read the introduction carefully by ticking the corresponding box. As a second introduction part, participants learned about the task procedure ( <ref type="figure" target="#fig_1">Figure  2)</ref>: Participants read the case and indicated their initial jail time estimate (1). After seeing the system calculate a result (interactive visual element) (2), they were automatically directed to the AI output, the numeric estimate, and the explanation (3). Participants then adjusted or confirmed their second estimate <ref type="bibr" target="#b3">(4)</ref>. They learned about the correct verdict of the case <ref type="bibr" target="#b4">(5)</ref>, and finally indicated their current trust in the AI system with a slider going from 1 = no trust at all until 10 = full trust (6).</p><p>The task procedure was repeated for 20 legal cases. To avoid order effects, cases were presented in random order. The last part of the study covered the following topics as a questionnaire: perceived level of intelligence of the AI system, perceived level of accuracy of the AI system, participant's relationship towards technological systems (affinity to technology), their willingness to participate in social situations (prosocialness), their level of law expertise, and their demographics (age, biological sex). The study closed with the debriefing and the remuneration of participants. Participants took approximately 20 minutes to finish.</p><p>We used identical AI estimates for all participants: Depending on the factors (accuracy, type of explanation) in the four conditions, all estimates and explanations were pre-formulated and pre-calculated (see <ref type="table">Table 1</ref>) without using an actual AI system. Even though this was part of the study's cover story, the intelligent systems were purely fictitious and did not perform any actual calculations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participants</head><p>We calculated the sample size with an a-priori power analysis. The result was a total sample of n=171 participants, based on an ANOVA fixed effects, main effects and interactions calculation (effect size: 0.25, power: 0.9) <ref type="bibr" target="#b25">[26]</ref>. We base our calculation on similar studies conducted by Yu et al. <ref type="bibr" target="#b72">[73]</ref>, Tolmeijer et al. <ref type="bibr" target="#b66">[67]</ref>, and Papenmeier et al. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, which reported repeated tasks with medium effect sizes (0.25).</p><p>We recruited participants via Prolific <ref type="bibr" target="#b58">[59]</ref> based on the following features: British citizenship, aged 18 and over, with experience in the field of law (the latter characteristic only applied to half of all participants). We recruited 10 additional participants to pretest our study. Each participant received on average £5.87 as compensation. Both pre-test and study were conducted between May 24 and June 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>204 participants registered for the study from which 171 participants (84%) fully completed it. Participants were excluded due to being under-age, not finishing all questions, or indicating unrealistic or pattern-like estimates throughout the legal task. On average, participants were 36.6 years old (Min: 19, Max: 69), 71% were female, their mean legal expertise was 4.2 (10-point Likert scale), their prosocial level average 6.0 (7-point Likert scale), and their level of tech affinity was 3.7 on average (6-point Likert scale).</p><p>Following Harvey and Fischer <ref type="bibr" target="#b30">[31]</ref>, we defined margins for behavioral trust (0 &lt; WoA &lt; 1). With this, we mainly excluded extreme outliers or values following uncommon behavior (e.g., always deviating away from AI advice). Trials where participants' initial estimate was identical to the AI estimate were excluded from WoA analysis as they would result in division by zero. Thus, of the 3,420 assessments made by the participants, we excluded 513 due to either WoA margins or matching estimates.</p><p>Testing whether manipulations of the conditions were recognized, we asked participants to estimate system intelligence (based on the different explanations) and system accuracy of the AI system on a 10-point Likert scale. We performed a two-sided t-test (equal variance) for both questions. Participants perceived the system with human-like explanations (M =6.39 , SD =2.45) significantly more intelligent than the system with abstract explanations (M =5.64 , SD =2.35), t (169) = -2.0 , p &lt; 0.04. Participants also perceived the high-accuracy models (M =7.39 , SD =1.67) to be significantly more accurate than low-accuracy models (M =4.28 , SD = 1.84), t (169) = -11.55, p &lt; 0.001. These results suggest our manipulations were successful in changing participants perception of the system.</p><p>To test our hypotheses, which were based on the effect of model accuracy and type of explanation on the development of selfreported and behavioral trust, we applied multilevel regression models with (repeated) trust measurements nested within participants. We discuss these models in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Trust is higher for high-accuracy models</head><p>To test the two hypotheses regarding the main effect of model accuracy, we analyze whether trust is higher for high-accuracy models, and whether it increases over time. We start comparing trust means by accuracy (see <ref type="table">Table 2</ref>) and observe that for both trust measures, trust is on average higher in the high accuracy conditions: 0.61 vs 0.31, 0.49 vs 0.32 when we compare WoA for the high and low accuracy condition, and 6.80 vs 4.13 and 6.24 vs 4.14 for the self-reported trust measure (we test the statistical significance of these differences in the regression models).</p><p>We model the effects of our conditions and position of the trial (for the effect over time), as well as relevant covariates in a set of multilevel regression models on behavioral and subjective trust, as showing in <ref type="table" target="#tab_2">Table 3</ref>. Results from regression models in <ref type="table" target="#tab_2">Table 3</ref> (behavioral trust, model 3: 2 = 0.14; self-reported trust, model 6: 2 = 0.21) support the difference in mean effects (see 2 that trust is higher for high model accuracy: both, behavioral trust (model 1: = 0.26 , p &lt; 0.001), and self-reported trust (model 4: = 2.6 , p &lt; 0.001) were significant with high accuracy (model 1 and 4 are without interaction effects of trial). In addition we see a positive interaction effect of trial and accuracy, for both trust measures. For WoA (model 4), the effect of the trial round is negative in the low accuracy condition: trust decreases over time ( = -0.005, p &lt; 0.001). The effect of the trial round is less negative in the high accuracy condition (interaction effect = 0.008, p &lt; 0.001). For selfreported trust, we also observe a positive interaction of trial and accuracy (model 6). In the low accuracy condition, the effect of the trial round is zero ( = -0.003, p = 0.25), whereas it is positive in the high accuracy condition (interaction effect = 0.057, p &lt; 0.001). The results hence support both hypotheses H1 and H2: Trust is higher for high-accuracy models in comparison to low-accuracy models for both behavioral and self-reported measures (H1). We also find that trust is decreasing less over time (trials) when model accuracy is high compared to when it is low (H2). <ref type="figure">Figure 3</ref> illustrates these effects graphically.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Trust is not affected by different explanation types</head><p>We moreover expected that trust will be higher for AI models with human-like explanations than for AI models with abstract (keyword) explanations (H3), and that human-like explanations protect trust better over time (H4). We first compared explanation types regarding their absolute trust levels. We find that both WoA (model 1: = 0.05, p = 0.17) and self-reported trust (model 4: = 0.35, p = 0.21) are not different for human-like and abstract explanations. Furthermore, we analyzed the effect of explanations over trial: in line with the previous results, there is no significant effect regarding explanation type over time on behavioral trust (interaction effect = -0.001, p = 0.55) as well as self-reported trust (interaction effect = -0.012, p = 0.28). Based on our findings, we reject H3 and H4: Human-like explanations do not affect trust differently than abstract explanations, they are furthermore not effectively protecting better over time based on our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human-like explanations elevate trust for high-accuracy models</head><p>In addition to our hypotheses, we tested whether any interaction effects were found for model accuracy and explanation type. As it can be seen in <ref type="figure">Figure 3</ref>, comparing the lines for the two high-accuracy conditions, trust is higher with human-like explanations, esp. for behavioral trust. Our final model shows a significant interaction for behavioral trust ( = 0.123 , p = 0.03), however, this effect is not seen for self-reported trust where results are non-significant ( = 0.54 , p = 0.27). We can summarize that trust is significantly different for highly accurate models with human-like explanations. In contrast to abstract keyword explanations, human-like explanations are able to boost individuals' willingness to adapt their own advice towards the AI advice (behavioral trust).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">WoA and self-reported trust only correlate to a limited extent</head><p>For our experiment, we measured trust two-fold -behavioral trust was measured with regards to the actual jail time estimates and to what extent they are influenced by the AI estimate, self-reported trust was measured after every trial to be able to retrace any change of trust immediately. Based on prior findings on trust in AI systems, we learned about the possibility of both measurements producing different outcomes. Running the empty regression models for both trust measurements to find potential differences in the overall quality of trust, we find that self-reported trust can be attributed to the individual ( = 0.55) whereas behavioral trust is more attributed to the study task ( = 0.32). This is also demonstrated in <ref type="figure">Figure   Figure 3</ref>: Development of Trust per Condition (fitted line graph)</p><p>3: trust trajectories are developing differently for the same task.</p><p>Connecting those findings to the main effects for accuracy (H1, H2), we again find that self-reported trust is significantly different in accuracy whereas behavioral trust only shows borderline significant outcomes. A similar discrepancy but in the opposite direction shows for interaction effects of accuracy and explanations (H4). In addition, a Pearson's correlation was run to assess the relationship between both measurements. We find a moderate correlation between behavioral and self-reported trust, r (3130) = 0.305, p &lt; 0.00. Concluding, we interpret that trust has different "qualities", and the question remains which measurement has greater power and impact regarding trusting an AI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Trust is influenced by age but not by gender, tech affinity, or legal expertise</head><p>Even though there was no focus on testing demographic or other personal characteristics of our participants, we included several covariates into one of our models (age, biological sex, legal expertise, and tech affinity) as we expected these to correlate with trust in AI systems. For example, contextual expertise was found to influence trust in AI negatively, e.g., <ref type="bibr" target="#b48">[49]</ref>, and affinity to tech promotes trust in systems <ref type="bibr" target="#b55">[56]</ref>. We find that for both measurements trust is not significantly different for any of those variables, with one exception: age negatively affected behavioral trust ( = -0.004, p = 0.002) as seen in our final model, but self-reported trust was not ( = 0.009, p = 0.38) . This finding follows earlier arguments, for example, from Knowles and Hanson <ref type="bibr" target="#b38">[39]</ref>, that AI (and tech) aversion increases with age; as individuals become older, they grow more resistant or are not capable anymore to control, and thus, trust technology.</p><p>Besides these significant results, our overall non-significant findings are in line with research from Papenmeier et al. <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>With our study, we gained new insights in (repeated) decisionmaking with AI support. Our results one the one hand confirms prior findings of the HCI literature and also propose new details about the interplay between model accuracy and explanation type. Trust increases over time, but only when model accuracy is high. Our results suggest that trust is generally higher for high-accuracy models, which makes intuitive sense (even though it is not always found empirically). Moreover, trust also increases over time when participants interact with a high-accuracy model. Apparently, participants pick up the competency of the intelligent support system over time and follow its recommendation more as time progresses. However, we want to annotate that trust effects were somewhat different for our two measurements: the results for behavioral trust were less strong. A potential reason for this difference might be that if participants' initial estimate was already fairly close to the AI estimate, with -thus -no reason to deviate from the system's recommendation. This will be especially the case for cases with low jail time (e.g., 4 months) as it is more likely that the AI prediction and the participants' estimate are similar or even overlap. More testing with higher (and more homogeneous) jail times regarding the ground truth would be needed to address the assumption. Comparing our task setup with Yu et al. <ref type="bibr" target="#b72">[73]</ref>, we find similar results to their 90% accuracy condition as trust increased over time in their design as well, although we cannot compare these findings directly as the task setup was rather different. Papenmeier et al. <ref type="bibr" target="#b56">[57]</ref> measured trust in AI for three different accuracy levels (high, medium, low) with a hate speech task and arrived at similar conclusions, adding that accuracy (vs. explanations) showed the most impact on trust overall. Their latest work <ref type="bibr" target="#b57">[58]</ref> also confirms that high accuracy increases trust, showing once again that participants are able to pick up on an algorithm's competence, at least when it is competent enough. The question is whether there is a specific accuracy level at which trust no longer increases but decreases over time. Or, the appropriate AI accuracy level could depend on the level of the participant, where those with higher expertise need higher AI accuracy levels to be convinced.</p><p>Trust is not higher with human-like explanations. Our study reveals no significant findings of trust based on the type of explanation: there was no difference for participants that received humanlike, explanations compared to participants that received abstract keyword explanations. Results were non-significant for absolute trust values and for trust development over the sequence of trials. We assumed that human-like explanations would be easier to understand and perceived as more helpful in adding value for a decision at hand, and thus are more trustworthy than just enumerating keywords. Our argumentation followed the majority of studies in the HCI literature that claims that explanations increase trust in computer models (cf. <ref type="bibr" target="#b45">[46]</ref>). Perhaps the design of explanations were not distinct or appropriate to increase trust for the task at hand, and another design (e.g., confidence levels, visualizations <ref type="bibr" target="#b34">[35]</ref>) could have been more helpful. Papenmeier et al. <ref type="bibr" target="#b56">[57]</ref> conclude that not giving an explanation resulted in better or equal trust compared to giving an explanation. They reason that their approach -highlighting words in the text -might not add any trustworthy "components" to enhance understanding and, thus, increase trust in an AI system. A special case is demonstrated by the random explanation condition, which even decreases trust for the moderately and highly accurate models. We welcome the fact that random explanations do not help to build trust, as misleading measures should not be used to support decision-making after all. Even though earlier work of Papenmeier et al. <ref type="bibr" target="#b56">[57]</ref> argues that trust was highest in the condition without any explanations, it remains to be analyzed whether trust in AI models can be improved by the right kind of explanations.</p><p>Human-like explanations boost trust for models with a high accuracy. Although different explanation types did not affect trust, we found a significant interaction effect for explanation type and model accuracy: trust was higher in high-accuracy models when combined with human-like explanations. We argue that the more sophisticated explanations corroborate the high accuracy of the model and enabled participants to understand its processes even better. Interestingly, this counter-argues findings of Papenmeier et al. <ref type="bibr" target="#b57">[58]</ref>: in their most accurate condition, faithful explanations + high accuracy model, trust was not significantly higher. They argue that "faithful explanations do not necessarily imply meaningfulness in the eyes of the user" (p. 26) as these are based on statistical relations rather than causal information. Our findings are consistent with that individuals considered the human-like explanations supportive enough, which might be achieved by the fact that explanations followed the standard assessment processes (selecting key topics, weighing, and concluding arguments for a final estimate). Users do not blindly follow AI advice though: Human-like explanations do not increase trust in the AI estimates with low accuracy. This is in line with the results of <ref type="bibr" target="#b37">[38]</ref>.</p><p>Behavioral trust differs from self-reported trust. We measured selfreported trust and behavioral trust (WoA) after every interaction. Our results in the high accuracy condition show that self-reported trust significantly increased over the course of 20 tasks whereas behavioral trust did not. A practical reason could lie in the capability of estimating correctly: an already good initial assessment is not improved by AI advice, and respectively, weight-on-advice scores for behavioral trust would be low, regardless of the trust in the algorithm. Thus, consistent with our results, self-reported trust is high because it would be a confirmation of interacting with a helpful AI ("I am pretty much in line with the AI's estimate"), but behavioral trust would naturally be lower. Independent of the latter, these findings raise questions on the explanatory power of trust measures and the quality of trust outcomes: are individuals able to express trust adequately or is a behavioral trust measure stronger in predicting trust? Is trusting an AI as similar to relying on AI advice, cf., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>Concluding, we have evidence to support the claim that model accuracy go with higher levels of trust (H1), and trust also increases over time (H2). We cannot conclude similarly for explanation types (H3, H4): trust is not affected, neither as absolute value nor over time. Some additional analysis showed that human-like explanations boost (self-reported) trust in high accuracy models. Therefore, we can partly answer our research question: yes, trust is affected by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE STUDIES</head><p>Our work uncovered new insights in trust development in AI systems, shedding light on model accuracy and explanation efforts in particular. Still, we want to discuss some limitations of our work. We found that model accuracy is affecting trust over time: an AI system with high accuracy, deviating 10% away from the correct resolution versus an obviously flawed system with low accuracy, deviating up to 50%. The question remains whether the same results are obtained when the accuracy design changes. What would be the accuracy level at which participants could still learn and adjust accuracy patterns whether the system is right or wrong? Manipulations in our study could have been too obvious and, thus, interpreting trust is too simple to that extent. Yu et al. <ref type="bibr" target="#b72">[73]</ref> applied four different levels of accuracy, which resulted into different trust trajectories. Future work could test different or more accuracy levels at once. In addition, we challenge the definition of accuracy and its effect on trust: individuals might perceive an overall predictable (steadily running) model higher in trust than an overall highly-accurate one that lacks predictability. Is a system more trusted when giving steady support but overall lower accuracy than a system that is very high in accuracy but with the tendency to include few outliers? A final suggestion would be to analyze trust with regards to the perception of a system being able to learn and improve over time, which we would define as adaptive accuracy. Would this AI behavior increase trust as this demonstrates a natural behavior for humans as well when improving over time? Human-like explanations did not result in higher AI trust (development) across both the low and high accuracy models. In hindsight, using an extra control condition in which no explanations are used, would have allowed additional and useful comparisons. Key to our finding is that although explanations do not work throughout, they do work for high accuracy AI models. Our study showed that users were distrustful when experiencing odd AI behavior -in our case receiving seemingly intelligent explanations for what actually was a poor recommendation. In contrast, human-like explanations led participants to trust a highly accurate AI model more than when explaining a case with only simple keywords. Obvious further lines of inquiry would be to explore which kinds of explanations work best, and which other communication efforts affect trust. One could think of semantic feedback after model mistakes (apologies, justification), or task framing (solving a task together vs. AI is support), both of which seem likely to be able to affect trust. Result H1 Trust is higher in the high accuracy model than in the low accuracy model. Supported</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2</head><p>Trust increases more / decreases less over time in the high accuracy model than in the low accuracy model ("high accuracy protects trust better"). Supported</p><p>H3 Trust is higher with human-like explanations than with abstract explanations Not supported</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H4</head><p>Trust increases more / decreases less over time with human-like explanations than with abstract explanations ("human-like explanations protect trust better").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not supported</head><p>Although we have endeavored to create a plausible study task based on real legal data, and in this sense have created an experiment that is much closer to real life than in abstract lab studies, the question remains as to whether this is close enough. One could argue that the setup of the task, especially resolving each case at the end (participants got feedback on the actual jail time after each trial), is not depicting a real-life procedure, but teaches especially inexperienced participants to focus on observing (and relying on) what the AI model suggested. In addition, actual jurisprudence involves a more comprehensive, iterative approach -a shortcoming that some study participants with a legal background commented on. Besides these experimental design considerations, there are also some other implications of the task: jail sentencing standards are not only dependent on the country and the legal expert who is deciding, but also does not provide a clear objective ground truth (other than that in the presented case, the jail time happened to be of a particular magnitude). In this sense, participants had to predict in a noisy environment, where each particular case can be seen as just one example of what could happen. An alternative would be to take as ground truth the mean (or median) jail time for similar cases in the database. In addition, a participant's potential belief that AI should not be involved in such a typically human decision-making task may have affected our results, although participant level variance is relatively small in our data. We obviously acknowledge the importance of discussing various ethical issues regarding the use of AI in a legal context, but want to emphasize that they exceed the scope (and focus) of this work, which was to observe behavioral change in response to receiving AI support in a complex decision-making task.</p><p>Lastly, with our repeated task scenario, we aimed to contribute to the study of trust development. It is unclear whether 20 successive interactions in a single trial of 20-30 minutes is sufficient for this purpose. For example, Tolmeijer and colleagues <ref type="bibr" target="#b66">[67]</ref> offer as a compromise to repeat interactions over a period of a few weeks. We hope to be able to collect longer-term observations of human-AI collaboration over several months to draw more robust conclusions about the dynamics and effects of trust over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Decision-making in various fields is becoming more and more a hybrid performance of humans and intelligent systems. There are several arguments in favor of combining human talent and AI capabilities to achieve the best results. However, there is still some guessing what we need to do to design applications trustworthy enough. With our study we were able to demonstrate that model accuracy has significant effects on trust development of individuals.</p><p>We found no conclusive differences in the way an outcome was explained, which we argued would support individuals' decisionmaking. However, we found that human-like explanations could elevate trust when model performance was high. With our results, we were able to contribute new insights related to the development of trust in the context of a real-life task. We want to continue exploring how system performance, individuals' expectations, and additional circumstances influence trust. Our ultimate goal is to enable people to successfully calibrate their trust when systems act outside of the expected frame, similar to how humans sometimes do.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Exemplary Legal Case with (human-like and abstract) Explanations a normal-distributed error around the actual jail time:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Experimental Study Procedure (with the main study task (Phase II) being repeated 20 times)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 :</head><label>2</label><figDesc>Comparison of condition outcomes by trust measurements (95% conf. interval) Behavioral Trust (WoA) Self-reported Trust High Accuracy (1) Low Accuracy (0) High Accuracy (1) Low Accuracy (0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Regression Models for Behavioral and Self-Reported Trust</figDesc><table><row><cell></cell><cell cols="3">Behavioral Trust (WoA)</cell><cell cols="3">Self-reported Trust</cell></row><row><cell></cell><cell>Model 1</cell><cell>Model 2</cell><cell>Model 3</cell><cell>Model 4</cell><cell>Model 5</cell><cell>Model 6</cell></row><row><cell></cell><cell>b/se</cell><cell>b/se</cell><cell>b/se</cell><cell>b/se</cell><cell>b/se</cell><cell>b/se</cell></row><row><cell>HumanExpl</cell><cell>0.048 (0.03)</cell><cell>-0.001 (0.05)</cell><cell>-0.007 (0.05)</cell><cell>0.195 (0.25)</cell><cell>-0.060 (0.38)</cell><cell>-0.092 (0.38)</cell></row><row><cell>HighAcc</cell><cell>0.257*** (0.03)</cell><cell>0.095 (0.05)</cell><cell>0.096 (0.05)</cell><cell>2.259*** (0.25)</cell><cell>1.294** (0.40)</cell><cell>1.313** (0.41)</cell></row><row><cell>Trial</cell><cell>-0.001 (0.00)</cell><cell>-0.004* (0.00)</cell><cell>-0.005* (0.00)</cell><cell>0.019*** (0.01)</cell><cell>-0.003 (0.01)</cell><cell>-0.003 (0.01)</cell></row><row><cell>HumanExpl_x_HighAcc</cell><cell></cell><cell>0.129* (0.06)</cell><cell>0.123* (0.06)</cell><cell></cell><cell>0.551 (0.50)</cell><cell>0.541 (0.51)</cell></row><row><cell>Trial_x_HighAcc</cell><cell></cell><cell>0.008*** (0.00)</cell><cell>0.008*** (0.00)</cell><cell></cell><cell>0.057*** (0.01)</cell><cell>0.057*** (0.01)</cell></row><row><cell>Trial_x_HumanExpl</cell><cell></cell><cell>-0.001 (0.00)</cell><cell>-0.001 (0.00)</cell><cell></cell><cell>-0.012 (0.01)</cell><cell>-0.012 (0.01)</cell></row><row><cell>Age</cell><cell></cell><cell></cell><cell>-0.04** (0.00)</cell><cell></cell><cell></cell><cell>0.009 (0.01)</cell></row><row><cell>Female</cell><cell></cell><cell></cell><cell>-0.049 (0.03)</cell><cell></cell><cell></cell><cell>-0.123 (0.28)</cell></row><row><cell>TechAffinity</cell><cell></cell><cell></cell><cell>-0.011 (0.02)</cell><cell></cell><cell></cell><cell>-0.027 (0.13)</cell></row><row><cell>LegalExpertise</cell><cell></cell><cell></cell><cell>-0.008 (0.01)</cell><cell></cell><cell></cell><cell>0.047 (0.05)</cell></row><row><cell>Constant</cell><cell>0.317*** (0.03)</cell><cell>0.386*** (0.04)</cell><cell>0.595*** (0.07)</cell><cell>4.072*** (0.23)</cell><cell>4.465*** (0.27)</cell><cell>3.987*** (0.59)</cell></row><row><cell>R-Square</cell><cell>0.1176</cell><cell>0.1284</cell><cell>0.1379</cell><cell>0.1986</cell><cell>0.2051</cell><cell>0.2075</cell></row><row><cell cols="2">*p &lt;0.05, ** p &lt;0.01, *** p &lt;0,001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(high) model accuracy, and it positively develops over time when ac-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">curacy is high. However, human-like explanations do not generally</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>affect trust (development) positively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Hypotheses Overview Hypothesis</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://osf.io/avux5/?view_only=69230d1656b14f8aaf9a6f34030bef7b</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Luc Siecker, Jane Deijnen, Milo Simons, Lorea Ros, and Ruben van der Werf for their help in conducting the study. In addition, we would like to express our gratitude to the European Supply Chain Forum (ESCF), the department Industrial Engineering and Innovation Sciences (IE&amp;IS), the Eindhoven Artificial Intelligence Systems Institute (EAISI), and the Logistics Community Brabant for sponsoring the research project AI Planner of the Future, which is funding the Ph.D. project "Trust in AI over time" and thus supporting this and future studies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The importance of the assurance that &quot;humans are still in the decision loop&quot; for public trust in artificial intelligence: Evidence from an online experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">106572</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Del</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is the most accurate ai the best teammate? optimizing ai for teamwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11405" to="11414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring prosociality: The development of a prosocial behavioral intentions scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Baumsteiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">T</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality assessment</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The evolution of mental model, trust and acceptance of adaptive cruise control in relation to initial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Beggiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><forename type="middle">F</forename><surname>Krems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation research part F: traffic psychology and behaviour</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Florian von Wangenheim, and Andrea Ferrario. 2022. The Value of Measuring Trust in AI-A Socio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Benk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Tolmeijer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13480</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Technical System Perspective. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Watch me improve-algorithm aversion and demonstrating the ability to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rühr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Benlian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business &amp; Information Systems Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zana</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Maja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof Z</forename><surname>Malaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gajos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An analysis of the interaction between intelligent software agents and human users. Minds and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ladyman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="735" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linking precursors of interpersonal trust to human-automation trust: An expanded typology and exploratory experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Christopher S Calhoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennie</forename><forename type="middle">J</forename><surname>Bobko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph B</forename><surname>Gallimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Trust Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="28" to="46" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conservatism predicts aversion to consequential Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">F</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">261467</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A longitudinal approach for understanding algorithm use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kausel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The interrelationship between intelligent agents&apos; characteristics and users&apos; intention in a search engine by making beliefs and perceived risks mediators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsai-Chu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Chun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Shun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-autonomy teaming and agent transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Jessie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Selkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lakhmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasdaglis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Publication of the 21st International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trust, shared understanding and locus of control in mixed-initiative robotic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faye</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markella</forename><surname>Grigoriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rustam</forename><surname>Stolkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 30th IEEE International Conference on Robot &amp; Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="684" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms in practice: Comparing web journalism and criminal justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angèle</forename><surname>Christin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2053951717718855</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrid intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Dellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Söllner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Marco</forename><surname>Leimeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business &amp; Information Systems Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="637" to="643" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Impact of robot failures and feedback on real-time trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munjal</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poornima</forename><surname>Kaniarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Medvedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Yanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soaham</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1302" to="1314" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithm aversion: people erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cade</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The influence of feedback on automation use, misuse, and disuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Dzindolet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hall</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hall</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications Sage CA</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="551" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do you still trust me? humanrobot trust repair strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Esterwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 30th IEEE International Conference on Robot &amp; Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Yusuf Albayram, and Emil Coman. 2021. Do integral emotions affect trust? The mediating effect of emotions on trust in the context of human-agent interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Fahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Maifi Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Designing Interactive Systems Conference 2021</title>
		<imprint>
			<biblScope unit="page" from="1492" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The influence of agent reliability on trust in human-agent collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sooyoung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mcneese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haydee</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Strater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mica</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European conference on Cognitive ergonomics: the ergonomics of cool interaction</title>
		<meeting>the 15th European conference on Cognitive ergonomics: the ergonomics of cool interaction</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-Georg</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1149" to="1160" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliana</forename><forename type="middle">Jansen</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateus</forename><surname>Monteiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05460</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do as AI say: susceptibility in deployment of clinical decision-aids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Gaube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lermer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">V</forename><surname>Coughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errol</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzyeh</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human trust in artificial intelligence: Review of empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Glikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><forename type="middle">Williams</forename><surname>Woolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Annals</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="627" to="660" />
			<date type="published" when="2020" />
			<publisher>Academy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical-statistical controversy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">293</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Taking advice: Accepting help, improving judgment, and sharing responsibility. Organizational behavior and human decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="117" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">User trust in intelligent systems: A journey over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on intelligent user interfaces</title>
		<meeting>the 21st international conference on intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="164" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Foundations for an empirically determined scale of trust in automated systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">M</forename><surname>Jiun-Yin Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">G</forename><surname>Bisantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cognitive ergonomics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="53" to="71" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Explainable Artificial Intelligence: An Introduction to Interpretable Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing for Confidence: The Impact of Visualizing Artificial Intelligence Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>John Karran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théophile</forename><surname>Demazure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Hudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Senecal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Majorique</forename><surname>Léger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attractive agents are more persuasive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatima</forename><surname>Rabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="142" to="150" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How should intelligent agents apologize to restore trust? Interaction effects between anthropomorphism and apology attribution on trust repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taenyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayeon</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telematics and Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101595</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inferring Capabilities of Intelligent Agents from Their External Traits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Knijnenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Willemsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2963106</idno>
		<ptr target="https://doi.org/10.1145/2963106" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Wisdom of Older Technology (Non)Users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bran</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><forename type="middle">L</forename><surname>Hanson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3179995</idno>
		<ptr target="https://doi.org/10.1145/3179995" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Trust repair strategies with self-driving vehicles: An exploratory study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewart J De</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler H</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human factors and ergonomics society annual meeting</title>
		<meeting>the human factors and ergonomics society annual meeting<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications Sage CA</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1108" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction matters: Manipulating trust in automation and reliance in automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Körber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Baseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Bengler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied ergonomics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Guiding user choice during discussion by silence, examples and justifications</title>
	</analytic>
	<monogr>
		<title level="m">ECAI 2012: 20th European Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page">330</biblScope>
		</imprint>
	</monogr>
	<note>Maier Fenster1and Inon Zuckerman2and Sarit Kraus</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Characteristics that influence perceived intelligence in AI design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Krening</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">M</forename><surname>Feigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human factors and ergonomics society annual meeting</title>
		<meeting>the human factors and ergonomics society annual meeting<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications Sage CA</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1637" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Trust in automation: Designing for appropriate reliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrina</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="50" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The dynamics of trust: comparing humans to automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Lewandowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Why and why not explanations improve the intelligibility of context-aware intelligent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avrahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Why these explanations? Selecting intelligibility types for explanation goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danding</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Artificial intelligence and surgical decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">C</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filiberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><forename type="middle">M</forename><surname>Brakenridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rashidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azra</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bihorac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA surgery</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="148" to="158" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Calibration of Trust in Automated Driving: A Matter of Initial Level of Trust and Automated Driving Style?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>Jb Manchon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="page">00187208211052804</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human performance consequences of automated decision aids: The impact of degree of automation and system experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Manzey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Onnasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Engineering and Decision Making</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hands on the wheel: Navigating algorithmic management and Uber drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Möhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Zalmanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomy&apos;, in proceedings of the international conference on information systems (ICIS)</title>
		<meeting><address><addrLine>Seoul South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsan</forename><surname>Nourani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samia</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">D</forename><surname>Ragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</title>
		<meeting>the AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The role of domain expertise in user trust and the impact of first impressions with intelligent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsan</forename><surname>Nourani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanie</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</title>
		<meeting>the AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive trust calibration for human-AI collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuo</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">229132</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Trust toward robots and artificial intelligence: An experimental approach to humantechnology interactions online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atte</forename><surname>Oksanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Savela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Latikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aki</forename><surname>Koivula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">568256</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How model accuracy and explanation fidelity influence user trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Papenmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwenn</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12652</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">It&apos;s Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Papenmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagmar</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwenn</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Prolific Research Platform</title>
		<idno>co. 2022</idno>
		<ptr target="https://www.prolific.co/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Artificial intelligence can improve decision-making in infection management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raheelah</forename><surname>Timothy M Rawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christofer</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pantelis</forename><surname>Toumazou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><forename type="middle">H</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="543" to="545" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Scharowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perrig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12318</idno>
		<title level="m">Nick von Felten, and Florian Brühlmann. 2022. Trust and Reliance in XAI-Distinguishing Between Attitudinal and Behavioral Measures</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The effects of personality and locus of control on trust in humans versus artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navya</forename><surname>Nishith Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><forename type="middle">Maria</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heliyon</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4572</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">102551</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Beyond user experience: What constitutes algorithmic experiences?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Biocca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Management</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">102061</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Trust, Confidence, and Expertise in a Judge-Advisor System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">A</forename><surname>Sniezek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyn</forename><forename type="middle">M</forename><surname>Van Swol</surname></persName>
		</author>
		<idno type="DOI">10.1006/obhd.2000.2926</idno>
		<ptr target="https://doi.org/10.1006/obhd.2000.2926" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="288" to="307" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tocchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Brambilla</surname></persName>
		</author>
		<title level="m">The Role of Human Knowledge in Explainable AI. Data</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Second chance for a first impression? Trust development in intelligent system interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Tolmeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujwal</forename><surname>Gadiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Ghantasala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM Conference on user modeling, adaptation and personalization</title>
		<meeting>the 29th ACM Conference on user modeling, adaptation and personalization</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Trust calibration within a human-robot team: Comparing automatically generated explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">G</forename><surname>Pynadath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Transparency: motivations and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="23" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ATI-S-an Ultra-Short scale for assessing affinity for technology interaction in user studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Attig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Mensch und Computer</title>
		<meeting>Mensch und Computer</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Toward quantifying trust dynamics: How people adjust their trust after moment-tomoment interaction with automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schemanske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Searle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07374</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Understanding the effect of accuracy on trust in machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 chi conference on human factors in computing systems</title>
		<meeting>the 2019 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">User trust dynamics: An investigation driven by differences in system performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Berkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><surname>Taib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on intelligent user interfaces</title>
		<meeting>the 22nd international conference on intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
