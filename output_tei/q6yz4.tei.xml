<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI-Assisted Decision-Making: A Cognitive Modeling Approach to Infer Latent Reliance Strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliodoro</forename><surname>Tejeda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakriti</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AI-Assisted Decision-Making: A Cognitive Modeling Approach to Infer Latent Reliance Strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>AI assistance is readily available to humans in a variety of decision-making applications. In order to fully understand the efficacy of such joint decision-making, it is important to first understand the human&apos;s reliance on AI. However, there is a disconnect between how joint decision-making is studied and how it is practiced in the real world. More often than not, researchers ask humans to provide independent decisions before they are shown AI assistance. This is done to make explicit the influence of AI assistance on the human&apos;s decision. We develop a cognitive model that allows us to infer the latent reliance strategy of humans on AI assistance without asking the human to make an independent decision. We validate the model&apos;s predictions through two behavioral experiments. The first experiment follows a concurrent paradigm where humans are shown AI assistance alongside the decision problem. The second experiment follows a sequential paradigm where humans provide an independent judgment on a decision problem before AI assistance is made available. The model&apos;s predicted reliance strategies closely track the strategies employed by humans in the two experimental paradigms. Our model provides a principled way to infer reliance on AI-assistance and may be used to expand the scope of investigation on human-AI collaboration.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Over the past decade, there has been an increase in domains where AI is used to assist humans by providing recommendations in the context of a prediction problem. Examples of these AI recommendation systems include: making bail decisions in a legal context <ref type="bibr" target="#b13">(Kleinberg et al., 2018)</ref>, detecting deception in consumer reviews <ref type="bibr" target="#b20">(Ott et al., 2011)</ref>, making medical decisions in diagnostic imaging <ref type="bibr" target="#b6">(Esteva et al., 2017;</ref><ref type="bibr" target="#b22">Patel et al., 2019;</ref><ref type="bibr" target="#b28">Rajpurkar et al., 2020)</ref>, recognizing faces in forensic analysis <ref type="bibr" target="#b24">(Phillips et al., 2018)</ref>, and classifying astronomical images <ref type="bibr" target="#b36">(Wright et al., 2017)</ref>. Such widespread adoption of AI decision aids has been accompanied by burgeoning interest in investigating the efficacy of AI assistance in collaborative decision-making settings <ref type="bibr" target="#b0">(Becker et al., 2022;</ref><ref type="bibr" target="#b1">Buçinca et al., 2021;</ref><ref type="bibr" target="#b3">Chong et al., 2022;</ref><ref type="bibr" target="#b14">Kumar et al., 2021;</ref><ref type="bibr" target="#b21">Park et al., 2019;</ref><ref type="bibr" target="#b26">Poursabzi-Sangdeh et al., 2021;</ref><ref type="bibr" target="#b37">Yin et al., 2019;</ref><ref type="bibr" target="#b38">G. Zhang et al., 2021)</ref>.</p><p>To investigate such AI-assisted decision-making, researchers have designed a variety of workflows. Some workflows require the human to provide an independent decision first, then display the AI's advice which the human can then use to update their final decision <ref type="bibr" target="#b3">(Chong et al., 2022;</ref><ref type="bibr" target="#b26">Poursabzi-Sangdeh et al., 2021;</ref><ref type="bibr" target="#b37">Yin et al., 2019)</ref>. Other workflows present AI advice alongside the prediction problem and the human can decide to follow the advice or ignore it <ref type="bibr" target="#b28">(Rajpurkar et al., 2020;</ref><ref type="bibr" target="#b31">Sayres et al., 2019)</ref>. Finally, a few studies force individuals to spend time thinking about the decision problem by artificially delaying the presentation of AI advice <ref type="bibr" target="#b1">(Buçinca et al., 2021;</ref><ref type="bibr" target="#b21">Park et al., 2019)</ref> or making AI advice available only when it is requested <ref type="bibr" target="#b14">(Kumar et al., 2021;</ref><ref type="bibr" target="#b17">Liang et al., 2022)</ref>.</p><p>In this work, we focus on two of the aforementioned workflows of AI-assisted decision-making and refer to them as paradigms; a detailed illustration can be found in <ref type="figure">Figure 1</ref>. We term the first as a sequential paradigm, where AI advice is displayed only after the human provides an independent judgment and the human can choose to revise their initial judgment. We term the second as a concurrent paradigm where AI advice is displayed concurrently with the prediction problem.</p><p>The sequential paradigm provides direct insights about the human's reliance on the AI based on two human judgments: the initial independent judgment and a final judgment after receiving the AI advice. This paradigm makes it easier for experimenters to disentangle the influence of AI advice on the human's decision. However, in many real-world applications, the human user does not independently make a decision before AI assistance is provided since providing the AI's recommendation immediately simplifies the workflow and can save time. The concurrent paradigm offers an alternative setting to study AI-assisted decision-making. One drawback of the concurrent paradigm is the fundamental ambiguity in data interpretation -it is unclear as to how one can assess the usefulness of the AI decision aid to the human user. Since there is no initial human judgment available before AI advice is offered, there is no direct empirical observation about any changes the human is making in their decision-making. Any observed agreement between the human and the AI, in the concurrent paradigm, could arise because the human changed their judgment and took the AI's advice or the human already arrived at the same judgment independent of the AI.</p><p>How, then, do we assess the impact of AI assistance on the human's decision?</p><p>Present Decision Problem &amp; Human makes initial decision AI Assistance: Sequential <ref type="figure">Figure 1</ref>: Illustration of the sequential and concurrent paradigms for AI-assisted decision-making (top two rows). The no-AI assistance paradigm (bottom row) is used as a control condition for the concurrent paradigm.</p><p>Our research has three main goals. First, we develop a computational cognitive model for AI-assisted decision-making in the concurrent paradigm. The cognitive model provides a principled way to infer the latent reliance of a human on the AI assistant in spite of the fact that there are no direct observations of switching behaviors when a person is presented with the AI advice. We empirically validate the computational model by collecting empirical data from a behavioral study using both the sequential and concurrent paradigms. The data from the sequential paradigm offers a comparison to the concurrent paradigm and provides a test to assess the merit of the computational framework. We demonstrate that the model's predictions of reliance behavior in the concurrent paradigm are qualitatively similar to the reliance behavior observed in the sequential paradigm.</p><p>In addition, we demonstrate that the model can generalize to held out trials in the concurrent paradigm.</p><p>In our second goal, we use the cognitive modeling approach to understand how a human's reliance policy depends on a number of factors related to the human and the AI. Previous research has shown that a human's confidence in their own decision influences the tendency to rely on AI assistance <ref type="bibr" target="#b19">(Lu &amp; Yin, 2021;</ref><ref type="bibr" target="#b23">Pescetelli et al., 2021;</ref><ref type="bibr" target="#b35">Wang et al., 2022)</ref>. In addition, reliance on the AI is also affected by the AI's confidence in its decision (Y. <ref type="bibr">Zhang et al., 2020)</ref>. Another contributing factor is the overall accuracy of the AI. In some previous research, only a single AI model with a fixed degree of accuracy was used; for example, an AI model with an accuracy comparable to human performance (Y. <ref type="bibr">Zhang et al., 2020)</ref> or above human performance <ref type="bibr" target="#b15">(Lai &amp; Tan, 2019;</ref><ref type="bibr" target="#b23">Pescetelli et al., 2021)</ref>. A few studies have investigated the effect of varying AI accuracy on reliance strategy <ref type="bibr" target="#b37">(Yin et al., 2019)</ref>. In our empirical paradigm, we investigate how human reliance varies across multiple levels of AI accuracy. This allows for a more nuanced understanding of the impact of the AI aid's accuracy on the human's reliance behavior. In addition, we investigate how participant confidence and AI confidence scores affect the trial-by-trial reliance strategy used by participants.</p><p>In our third goal, we use the computational model to quantify the effectiveness of the reliance strategies employed by the human. In some instances, people adopt sub-optimal reliance policies when working with an AI. For example, it has been found that people will prefer to use their own (less accurate) forecasts instead of an algorithm if they have seen the algorithm make mistakes <ref type="bibr" target="#b5">(Dietvorst et al., 2015)</ref>. In another study, people placed too much trust in an automated system <ref type="bibr" target="#b4">(Cummings, 2017)</ref>. Over-and under-reliance on AI advice may depend on particular task domains and methods of interaction <ref type="bibr" target="#b2">(Castelo et al., 2019;</ref><ref type="bibr" target="#b18">Logg, 2017;</ref><ref type="bibr" target="#b27">Promberger &amp; Baron, 2006)</ref>. Whereas in these previous studies, the reliance was assessed at the aggregate level, our cognitive modeling approach enables us to estimate the trial-by-trial variations in reliance depending on factors such as the confidence state of the participant and the level of confidence of the AI for particular problem instances. For particular combinations of self and AI confidence (e.g., low self confidence and high AI confidence) and particular combinations of human and AI overall accuracy, we can expect joint decision making accuracy to be better than the human or AI alone <ref type="bibr" target="#b34">(Steyvers et al., 2022)</ref>. An empirical question is whether participants are able to adopt such a policy. We compare the reliance policies adopted by participants to optimal policies and show that in our experiment, people were quite effective in their adoption of AI advice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cognitive Model</head><p>Before describing the computational model, we note some key aspects of the concurrent advice-taking paradigm in particular that motivate the design of the model. In the experiment, participants have to predict the classification label of a set of images and a confidence level associated with their decision. Each participant alternates between two experimental conditions. In the control (no assistance) condition, participants indicate their predictions without help from the AI. In the AI assistance condition, we follow the concurrent approach; the AI provides a recommended set of predictions by highlighting the class labels according to the AI's confidence scores. The participant can use these recommendations in any way they want to order to maximize their own accuracy (see <ref type="figure" target="#fig_0">Figure 2</ref> for an illustration of the user interface in the experiment). An important aspect of this condition is that the participant's prediction reflects a combination of their own independent decision-making (which is not observable in this paradigm) and the AI prediction. In other words, the policy used by the participant to rely on and integrate AI predictions with their own predictions is not directly observable from their behavior.</p><p>The main goal of the computational model is to draw inferences about the latent advice taking policies. The policy can be determined by a number of factors, such as the confidence state of the participant, the confidence scores of the AI as well as the overall accuracy of the AI. We develop a hierarchical Bayesian model to draw inferences about the policies not only at the population level but also at the level of individual participants. In the first part of the model, a Bayesian Item-Response model <ref type="bibr" target="#b8">(Fox, 2010)</ref> is applied to the no-assistance condition to infer individual differences in ability as well as differences in difficulty across items (i.e., prediction problems). In the AI-assistance part of the model, these latent person and item parameters are used to explain the observed prediction from a participant which depends on their (unobservable) unaided prediction and the advice-taking policy that determines the likelihood that a participant switches to the AI prediction or stays with their own prediction. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes the graphical model of the computational model that explains the human predictions with and without AI assistance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Human Decisions before Assistance</head><p>The computational model for human predictions without AI assistance is based on a Bayesian Item-Response model <ref type="bibr" target="#b8">(Fox, 2010)</ref>. The Item-Response model makes it convenient to model individual differences in accuracy as well as differences in item difficulty (where items refer to the individual images participants have to classify). To model the human predictions, we use a three-parameter IRT model to capture the probability θ i,j that a correct response is made by person i on item j:</p><formula xml:id="formula_0">log ( θ i,j 1 − θ i,j ) = s j a i − d j<label>(1)</label></formula><p>The person parameter a i is an ability parameter that determines the overall performance of the person across items. The item parameter d j captures differences in the item difficulty while the item parameter s j captures discrimination: the tendency of an item to discriminate between high and low ability individuals.</p><p>In a typical IRT model, the probability of making a correct response, θ, is used to sample the correctness of an answer. However, for our model, we code the responses from individuals in terms of the predicted label. Let x i,j represent the prediction by person i for item j in the absence of AI assistance. Each prediction involves a choice from a set of L labels, i.e. x ∈ {1, . . . , L}. Let z j represent the true label for item j. We assume that person i produces the correct label z j on item j with probability θ i,j and otherwise chooses uniformly from all other labels, as follows:</p><formula xml:id="formula_1">p(x i,j = m) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ θ i,j if z j = m (1 − θ i,j )/(L − 1) if z j ≠ m<label>(2)</label></formula><p>Various model extensions could be considered that allow for response biases such that some labels are preferred a priori over other.</p><p>Participants not only make a prediction but also express a confidence level, r i,j , associated with their prediction. In the experimental paradigm, confidence levels are chosen from a small set of labels, r i,j ∈ {low, medium, high}. In the model, we assume that predictions associated with higher accuracy on average lead to higher confidence levels, but that at the item level, the mapping from accuracy to confidence is noisy. To capture the noisy relationship between accuracy and confidence, we use a simple generative model based on an ordered probit model:</p><formula xml:id="formula_2">r i,j ∼ OrderedProbit(θ i,j , v i , σ i )<label>(3)</label></formula><p>In this generative model, normally distributed noise with standard deviation σ i is added to the probability of being correct θ i,j . The resulting value is then compared against a set of intervals defined by parameters v i , and the interval which contains the value determines the resulting confidence level. Changes in v i can lead the participant to different uses of the response scale (i.e., using one particular confidence level relatively often) while σ i determines (inversely) the degree to which accuracy and confidence are related. Note that the parameters σ and v are person-specific to allow for individual differences in the confidence generating process. Appendix A provides more detail on the ordered probit model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Human Decisions after Advice</head><p>In the model for human decisions in the presence of advice, let y i,j,k represent the observed prediction made by person i on item j after AI advice is considered from AI algorithm k. We include a dependence on the type of algorithm as our empirical paradigm will present AI advice from different algorithms. In the advice-taking model, we assume that the participant initially makes their own prediction x i,j independent of the AI advice but that their final decision y i,j,k can be influenced by the AI advice. Note that in the no-assistance condition, the independent predictions x i,j and associated confidence levels r i,j are directly observable, but they are latent in the AI assistance condition. However, we can use the IRT model in the previous section to simulate the counterfactual situation about the prediction and confidence level that a person would have made if AI advice was not provided. Specifically, we can use the generative model in Equations 1-3 to generate predictions for x i,j and r i,j on the basis of information about the participant's overall skill (a) as well as information about the difficulty of the particular item (d j ) 1 .</p><p>In the advice-taking model, we assume that the participant will stay with their original decision</p><p>x i,j if it agrees with the AI's recommendation, denoted by c j,k . However, when the original decision is not the same as the AI's recommendation, we assume the participant switches to the AI's recommendation with probability α i,j,k . Therefore, we can model the probability that the participant chooses label m for their final prediction as follows:</p><formula xml:id="formula_3">p(y i,j,k = m) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ α i,j,k if x i,j ≠ m ∧ c j,k = m 1 if x i,j = m ∧ c j,k = m 0 if x i,j ≠ m ∧ c j,k ≠ m<label>(4)</label></formula><p>The variable α i,j,k determines the tendency of participant i to trust the AI advice from algorithm k related to item j. In the next section, we describe how this latent variable can depend on factors such as the confidence state of the participant as well as the confidence score of the AI.</p><p>Note that in this model, when the participant is provided with AI assistance, the independent prediction x i,j is latent in our experimental paradigm. Instead of explicitly simulating the process of first sampling an independent prediction x i,j and then a final prediction y i,j,k , we can simplify the generative process by marginalizing out x i,j :</p><formula xml:id="formula_4">p(y i,j,k = m) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ θ i,j + (1 − θ i,j )α i,j,k if z j = m ∧ c j,k = m 1−θ i,j L−1 + (1 − 1−θ i,j L−1 ) α i,j,k if z j ≠ m ∧ c j,k = m 1−θ i,j L−1 (1 − α i,j,k ) if z j ≠ m ∧ c j,k ≠ m<label>(5)</label></formula><p>In this equation, the probability that the participant selects label m is split into three different cases.</p><p>The first case reflects the probability that the participant makes the correct decision independently (which happened to agree with the AI recommendation) or makes an incorrect decision initially but then adopts the correct AI advice. The second case reflects the probability that the participant initially selects an incorrect decision (which happened to agree with the AI recommendation) or makes another decision different from the AI but then adopts the incorrect AI advice. The third case reflects the probability that the participant makes an incorrect independent decision and decides not to switch to the AI's recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Individual Differences in Advice-Taking</head><p>The key latent variable of interest in the model is α i,j,k , which determines the willingness of the participant per item to switch to the AI's recommended prediction if it differs from their own prediction. Generally, α i,j,k can depend on many characteristics related to the person, item, and classifier. Here, we will consider functions where α depends on the confidence state of the participant for item j (r i,j ), the AI confidence score associated with item j (η j,k ), and the type of classifier k:</p><formula xml:id="formula_5">α i,j,k = f (r i,j , η j,k , k)<label>(6)</label></formula><p>One way to specify function f is based on a linear model that captures main effects as well as interaction between the two putative factors. However, to avoid specifying the exact functional form of f , we will instead simplify the model and treat function f as a lookup table that specifies the α values based on a small number of combinations of participant confidence, AI confidence, and classifier type. Specifically, we create 3 x 4 x 3 lookup table that specifies the α value based on 3 levels of participant confidence ("low", "medium", "high"), 4 levels of AI confidence, and 3 types of classifiers (k). We use a hierarchical Bayesian modeling approach to estimate individual differences in the policy α (See Appendix B for details). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To validate our cognitive model, we investigated human performance with and without AI assistance in two paradigms: the concurrent and sequential paradigm. We will apply the cognitive model to the concurrent paradigm to infer the AI reliance strategies by individual participants. The results from the sequential paradigm serve as a means to validate our cognitive model, as the sequential paradigm allow us to empirically analyze participant strategies when integrating AI assistance.</p><p>In both paradigms, participants have to classify noisy images into 16 different categories (see <ref type="figure" target="#fig_0">Figure 2</ref> for an example of the user interface). There were two experimental manipulations. First, the image noise was varied to produce substantial difference in classification difficulty <ref type="figure" target="#fig_2">(Figure 4)</ref>. Second, we varied the overall accuracy of the AI predictions across three conditions: classifier A, classifier B, and classifier C. Classifier A was designed to produce predictions that are, on average, less accurate than human performance. Classifiers B and C were designed to produce predictions that are, on average, as accurate and more accurate than human performance. Each participant was paired with one type of classifier.</p><p>The main difference between the two paradigms is that in the concurrent paradigm, participants alternated between blocks of trials where AI assistance was or was not provided. In the sequential paradigm, there were no alternating blocks. On each trial, the participant first made an independent prediction for a image classification problem and was then given an opportunity to revise their prediction after AI assistance was provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A total of 60 and participants were recruited using Amazon Mechanical Turk for the concurrent and sequential experiments respectively. To ensure that participants understood the task, they were given a set of instructions describing the experiment and what they would have to do. Upon reading all of the instructions, participants were then tasked with a comprehension quiz to ensure they fully understood the task. The quiz consisted of having participants classify five different noisy images with AI help turned off. In order to participate in the study, participants had to correctly classify four of the five images in the quiz. Participants were given two opportunities to pass the quiz. Successful participants were then allowed to proceed with the rest of the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images</head><p>All images used for this experiment come from the ImageNet Large Scale Visual Recognition Challenge (ILSRVR) 2012 validation dataset <ref type="bibr" target="#b30">(Russakovsky et al., 2015)</ref>. Following <ref type="bibr">Geirhos et al.,</ref> 2019, a subset of 256 images was selected divided equally among 16 classes (chair, oven, knife, bottle, keyboard, clock, boat, bicycle, airplane, truck, car, elephant, bear, dog, cat, and bird).</p><p>To manipulate the classification difficulty, images were distorted by phase noise at each spatial frequency, where the phase noise is uniformly distributed in the interval [−ω, ω] <ref type="bibr" target="#b9">(Geirhos et al., 2019)</ref>. Eight levels of phase noise, ω = {0, 80, 95, 110, 125, 140, 155, 170}, were applied to the images, a different noise level for each unique image, resulting in 2 unique images per category per noise level (see <ref type="figure" target="#fig_2">Figure 4</ref> for examples of the phase noise manipulation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Predictions</head><p>We used a convolutional neural network (CNN), based on the VGG-19 architecture (Simonyan Three different levels of classifier performance were created by differentially fine-tuning the VGG-19 architecture to the phase noise used in our experiment. All models were trained with all levels of phase noise. However, to generate these different levels of performance, the models were fine-tuned for different periods of time. We used a pilot experiment with 145 participants to assess human performance at the different noise levels. Classifier A was produced by fine-tuning for less than one epoch (10% of batches of the first epoch) and produced a performance level that was on average below human performance. Classifier B was produced by fine-tuning for the entirety of one epoch and produced a performance level that was on average near human performance. Classifier C was fine-tuned for 10 epochs and produced a performance level above average human performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>In both the concurrent and sequential paradigms, participants were instructed to classify images as best as possible and to leverage AI assistance, when provided, to optimize performance. Each participant was assigned to a single classifier level (A, B, or C) at the start of the experiment and each was only presented with AI assistance from that particular classifier; 20 participants were assigned to each classifier level in concurrent paradigm, and 25 participants to each classifier level in the sequential paradigm. Participants were given no information about the accuracy of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concurrent Paradigm</head><p>In the concurrent paradigm, there were 256 trials total. Each trial presented a unique image randomly selected from the set of 256 images. The classification trials were separated into 4 four blocks where each block consisted of 48 consecutive trials in which AI assistance was turned on, and 16 consecutive trials without AI assistance. The larger number of trials with AI assistance was used to better assess participants AI reliance strategies under different levels of AI confidence. Because of the random ordering of images across participants, a particular image was shown for some participants in the AI assistance condition and for other participants in the control condition without AI assistance. Each unique image was shown to a median of 15 participants in the control condition and 45 participants in the AI assistance condition.</p><p>On each trial, participants were shown an interface as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Participants classified images into 16 categories by pressing the response buttons that represented the categories with visual icons as well as labels (when the participant hovers the mouse over the button). For each classification, the participant provided a discrete confidence level (low, medium, and high). Finally, the rightmost column of the interface was used for AI assistance. When AI assistance was turned off, this column displayed nothing. However, when AI assistance was turned on, a grid of the 16 category options was shown with the same layout as the participant response options. Each of the 16 categories would be highlighted based on a gradient scale associated with the probability that the AI classifier assigned to the category. The darker the hue of the highlighted category, the more confident the classifier was in that selection. Instances in which the classifier was extremely confident in a single category, there would only be one category highlighted with an extremely dark hue. However, in instances where the classifier was not confident in a classification, there would be multiple categories highlighted with low hue levels. Participants were to use the AI assistance to aid their classification decision so as to optimize their own performance on the task. At the end of each trial, feedback was provided to enable the participant to develop an AI reliance strategy tailored to the particular AI algorithm they were paired with. In the feedback phase, the correct response option was highlighted in blue. If the participant was incorrect, the incorrect response was highlighted in red.</p><p>Sequential paradigm In the sequential paradigm, there were 192 trials total. Each trial presented a unique image randomly selected from the set of images. On each trial, participants were first tasked with classifying an image on their own and were shown the interface as displayed in <ref type="figure" target="#fig_0">Figure 2</ref> but without AI assistance (the third column showing AI assistance was completely blank).</p><p>After selecting their initial classification decision and submitting their response by selecting a confidence level, participants then were provided with AI assistance. The user interface at this stage looked exactly like <ref type="figure" target="#fig_0">Figure 2</ref> and the procedure for displaying AI confidence was the same as in the concurrent procedure. With AI assistance turned on, participants then made a final classification decision for the image shown and submitted their response by selecting their confidence level. Once a final classification was made, participants were provided feedback for 3 seconds. <ref type="figure" target="#fig_3">Figure 5</ref> shows the average accuracy across noise levels, AI classifier accuracy levels, AI assistance conditions and the concurrent and sequential advice-taking paradigms. In both the concurrent and sequential procedures, substantial performance differences are observed as the level of image noise varies, ranging from near ceiling performance at the zero noise level to close to chance-level performance (i.e., 1/16 = 0.0625) at the highest noise level. Across all classifier conditions, human performance improves with AI assistance, especially at intermediate levels of noise, as illustrated in <ref type="figure">Figure 6</ref>. For classifiers B and C, the AI assistance produces performance levels comparable to the AI alone. For classifier A, the AI assistance improves human performance even though the AI assistance's accuracy is below human performance, on average. Note that this result is possible when participants rely on AI assistance on select trials when participants are in a low confidence state and the classifier is in a relatively high confidence state (see Appendix E for an analysis of the relationship between human and AI confidence). Overall, these results show that participants are able to rely on AI assistance to produce complimentarity -the joint human-AI accuracy is equal to or better than either the human or the AI alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are very similar across the concurrent and sequential paradigms. The average human accuracy with AI assistance for classifiers A, B, and C is 57%, 62% and 68% respectively in the concurrent paradigm and 56%, 61%, and 65% respectively in the sequential paradigm. A Bayesian independent samples t-test showed no evidence for a difference in performance for any the classifiers (i.e., all Bayes Factors &lt; 1) 2 . That these results are consistent and very similar in both the concurrent and sequential experiments suggests that the experimental advice-taking paradigm does not produce important differences in how humans rely on and integrate AI assistance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-based Analysis</head><p>The empirical results showed that the concurrent and sequential advice-taking paradigms produce similar levels of accuracy across all experimental manipulations. In this section, we report the results of applying the cognitive model to the data from the concurrent paradigm.</p><p>We used a Markov Chain Monte Carlo (MCMC) procedure to infer model parameters for the graphical model as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> (see Appendix B for details). Generally, the model is able to capture all the qualitative trends in the concurrent paradigm (see Appendix D on an out-of-sample assessment of model fit). We focus our analysis on two key parameters estimated by the model: β, the advice-taking policy at the population level, and α, the advice-taking policy for individual participants. In the next sections, we illustrate the inferred policies and compare the results against the empirically observed strategies from the sequential advice-taking paradigm. In addition, we analyze how effective the policies are relative to the set of all possible policies that participants could have adopted, ranging from the worst to best policies.</p><p>Inferred Advice-Taking Policies <ref type="figure">Figure 7</ref>, top row, shows the inferred advice-taking policy β as a function of classifier confidence, participant confidence and classifier. These policies represent the behavior of an average participant at the population level of the model. <ref type="figure" target="#fig_4">Figure 8</ref> shows examples of inferred advice-taking policies (α) from a subset of individual participants. Overall, the probability of taking AI advice differs substantially across classifiers. Advice is more likely to be accepted when the participant is in a low confidence decision-state and the classifier provides high confidence recommendations. In addition, across the different levels of classifier accuracy, advice is more likely to be accepted from high accuracy classifiers. Overall, these results show that the advice-taking behavior depends on a number of factors and is not based on simple strategies that rely solely on the confidence level of the AI or the confidence level of the participant. In addition, the results show that the advice-taking <ref type="figure">Figure 6</ref>: Differences in accuracy with AI assistance relative to no AI assistance and AI only. Results are shown as a function of noise level (horizontal axis) and type of AI (columns) across the Concurrent and Sequential advice-taking paradigms. Error bars reflect the 95% confidence intervals.</p><p>behavior is adjusted when the AI assistance becomes more accurate, from classifier A to classifier C, showing that participants are sensitive to AI accuracy. <ref type="figure">Figure 7</ref>, bottom row, shows the empirically observed reliance strategies for the sequential paradigm. This analysis focuses on the subset of trials where the initial prediction from the participant differs from the AI prediction (which is not yet shown) and then calculating the proportion of trials where the participant switches to the AI prediction. Importantly, even though there are some quantitative differences that can be observed between the reliance strategies in the two paradigms, the qualitative patterns are the same. Thus, the results from the sequential paradigm provide a key validation of the cognitive model. The latent strategies uncovered by the cognitive model in the concurrent paradigm are very similar to those observed in the sequential paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the Advice-Taking Policies</head><p>We now address the question of how effective are the participants' advice-taking policies. How much better (or worse) could participants have performed if they changed their advice-taking strategy? <ref type="figure">Figure 9</ref> shows the range of all possible outcomes across different instantiations of the advice-taking policies. The accuracies of the worst and best possible advice-taking strategies were inferred by an analysis that optimizes performance conditional on the performance of the participants (Appendix C). Note that the worst to best accuracies span the range of all possible outcomes. To understand how effective the average participants' policies (β) are on this range, we used a Monte Carlo sampling procedure to derive the accuracy distribution over all strategies (see Appendix C for details) and <ref type="figure">Figure 7</ref>: Advice-taking policies inferred from the advice-taking behavior in the concurrent paradigm (top row) and observed in the sequential paradigm (bottom row). The policy determines the probability of taking the AI advice as a function of human confidence (colors), classifier confidence (horizontal axis), and type of classifier (columns). The colored areas in the top row show 95% posterior credible intervals. The colored areas in the bottom row reflect the 95% confidence interval of the mean based on a binomial model. The inferred advice taking parameters (β) are converted from log-odds to probabilities in this visualization. compute the percentile rank of the participant strategies in this distribution. These results show that the actual policies adopted by participants were highly effective, scoring in or near the top 10% of all possible strategies. <ref type="figure">Figure 10</ref> shows the percentile rank for all individual participants when the effectiveness analysis is applied to the individual participant data. While a small subset of participants used suboptimal reliance strategies, the majority of participants used highly effective strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Appropriate reliance on AI advice is critical to effective collaboration between humans and AI.</p><p>Most research on AI-assisted decision making has focused on gaining insight into the human's reliance on AI though empirical observations based on trust ratings and comparisons of observed accuracy and final decisions by humans and AI. For instance, in work that uses trust as a proxy for reliance, individuals are required to report their trust in the AI assistant <ref type="bibr" target="#b16">(Lee &amp; See, 2004)</ref>.</p><p>However, self-reported trust is not a reliable indicator of trust <ref type="bibr" target="#b32">(Schaffer et al., 2019)</ref>. Researchers have also compared the accuracy of the human-AI team when AI assistance is provided to the accuracy without assistance <ref type="bibr" target="#b15">(Lai &amp; Tan, 2019)</ref>. However, this difference in accuracy is directly correlated with the performance of the AI. Another method used to investigate reliance is based on analyzing the agreement between the human's final decision and the AI's prediction (Y. <ref type="bibr">Zhang et al., 2020</ref>). This approach is problematic when used in the concurrent paradigm -while agreement can occur because of an individual's trust in the AI, it can also occur because the individual might have arrived at the same prediction as the AI even without the AI's assistance. Finally, in experiments using the sequential paradigm, reliance can be assessed by the propensity of individuals to switch to the AI's recommendation for those cases where their initial independent decision differs from the AI <ref type="bibr" target="#b37">(Yin et al., 2019;</ref><ref type="bibr">Y. Zhang et al., 2020)</ref>. While this is a simple and straightforward procedure to gain insight into a reliance strategy, it cannot be applied to the concurrent paradigm as the individual's independent response is inherently unobservable.</p><p>Instead of using empirical measures to assess reliance, we developed a cognitive modeling approach that treats reliance as a latent construct. The modeling framework provides a principled way to reveal the latent reliance strategy of the individual by using a probabilistic model of the advice-taking behavior in the concurrent paradigm. It can be used to infer the likelihood that a human would have made a correct decision for a particular item independently even when their independent decision is not directly observed. The model is able to make this inference because it assumes that people, at the same levels of skill, will likely make the same prediction. The model allows us to investigate the difference between agreement with the AI and switching to AI advice (two metrics often used to assess trust) without explicitly asking the human to respond independently to each problem. In order to apply the model, empirical observations are needed that assess people's independent decisions without the assistance of an AI.</p><p>We showed that the AI reliance strategy inferred by the cognitive model on the basis of the concurrent paradigm is qualitatively similar to the AI reliance strategy observed in the sequential paradigm. Therefore, this demonstrates that a latent modeling approach can be used to investigate AI-assisted decision-making. The reliance strategy estimated by the model showed that participants discriminatively relied on the AI and varied their reliance from problem to problem. Participants were more likely to rely on the AI if they were less confident in their own decisions or when the AI was relatively confident. In addition, participants relied more heavily on AIs that were more accurate overall. This finding is consistent with Liang et al., 2022 who showed that people rely on AI assistance more when the task is difficult and when they were given feedback about their performance and the AI's performance.</p><p>The results also showed that participants were able to build very effective reliance strategies compared to the optimal reliance strategy. We believe that participants were able to achieve this because of the following reasons. First, this is a simple image classification task and most people are experts at identifying everyday objects from images. This enables people to have a good understanding of their own expertise and confidence on any presented image. Second, in our experiment, people received feedback after each trial, which gave them the opportunity to learn about the AI assistant's accuracy and confidence calibration. This feedback allowed people to build reasonable mental models of the AI assistant when paired with any of the three classifiers.</p><p>Finally, our results showed that the concurrent and sequential AI assistance paradigms led to comparable accuracy. Some researchers have argued that the sequential paradigm is superior to the concurrent paradigm because the initial unassisted prediction encourages independent reflection which could lead to retrieval of additional problem-relevant information <ref type="bibr" target="#b11">(Green &amp; Chen, 2019)</ref>.</p><p>However, consistent with our study, other studies have found no difference in overall performance between the concurrent and sequential paradigm <ref type="bibr" target="#b1">(Buçinca et al., 2021)</ref>. Another factor that could be relevant is the timing of AI assistance. The AI advice can be presented after some delay which provides the decision-maker additional time to reflect on the problem and improve their own decision-making accuracy <ref type="bibr" target="#b21">(Park et al., 2019)</ref>. Another possibility is to vary the amount of time available for people to process the AI prediction after it is shown making it more likely for people to detect AI errors <ref type="bibr" target="#b29">(Rastogi et al., 2022)</ref>. Overall, more research is needed to understand the effects of soliciting independent human predictions and varying the timing of the AI recommendation.</p><p>Our empirical and theoretical work comes with a number of limitations. First, we provided trial-by-trial feedback to help participants with the task of building a suitable mental model of AI performance. However, feedback is not always possible in real-world scenarios <ref type="bibr" target="#b19">(Lu &amp; Yin, 2021)</ref>. Future research should investigate modeling extensions that model the cognitive process when participants don't receive feedback at all or receive feedback after a delay. Second, while the cognitive model captured the general process of advice taking based on a latent reliance policy, it did not model the process of establishing the reliance policy over time. Therefore, one important model extension, which we leave for future research, is to model the trial-by-trial adjustments of the reliance policy as a function of beliefs held a priori by participants about the accuracy of AI algorithms, external signals of AI confidence and accuracy as well as internally generated confidence signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statements &amp; Declarations Funding</head><p>This work was supported by the National Science Foundation <ref type="bibr">(Grant Numbers: 1900644 and 1927245)</ref>. Authors P.S. and M.S. received the research support from the NSF. <ref type="bibr">Zhang, Y., Liao, Q. V., &amp; Bellamy, R. K. (2020)</ref>. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Proceedings of the 2020 Conference on <ref type="bibr">Fairness,</ref><ref type="bibr">Accountability,</ref><ref type="bibr">and Transparency,</ref><ref type="bibr">[295]</ref><ref type="bibr">[296]</ref><ref type="bibr">[297]</ref><ref type="bibr">[298]</ref><ref type="bibr">[299]</ref><ref type="bibr">[300]</ref><ref type="bibr">[301]</ref><ref type="bibr">[302]</ref><ref type="bibr">[303]</ref><ref type="bibr">[304]</ref><ref type="bibr">[305]</ref> subject to an order constraint identical to the order constraint imposed in the MCMC procedure: α should be monotonically increasing for higher levels of participant confidence. <ref type="figure" target="#fig_0">Figure 12</ref> shows the resulting optimized policies (β) at the average participant level. These policies only take on extreme values such that advice is either always taken (β = 1) or never taken (β=0) for particular combinations of participant and classifier confidence. Similar to the participant strategies, the results show that classifier advice should more readily be adopted when the participant is in a low confidence state and the classifier is in a high confidence state 3 .</p><p>Deriving the percentile rank of participants policies We conducted a Monte Carlo procedure to estimate the percentile rank of the accuracy of participants' policies relative to accuracy achieved by random strategies. In this Monte Carlo procedure, we sampled α i,j,k from a uniform (0,1) distribution separately for the 3 levels of DM confidence (r i,j ) and 4 levels of AI confidence (η j ). We computed the expected accuracy for each of the α i,j,k samples. We next computed the percentile rank of the actual participants' policy relative to this distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the behavioral experiment interface in the AI assistance condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Graphical model for the AI-assisted decision-making model. In the condition without assistance, r ij and x ij , and z j are observed. In the condition where AI assistance is provided, r ij and x ij are latent and y ijk , z j , c jk and η jk are observed. For visual clarity, plate notation is omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of three images under different levels of phase noise. Original images (left) were not used in experiments and are shown only for illustrative purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Human accuracy with and without AI assistance as well as AI accuracy as a function of noise level (horizontal axis) across the Concurrent and Sequential paradigms (rows). Columns show different types of AI classifiers: classifier A's accuracy is below average human accuracy, classifier B's accuracy is comparable to average human accuracy, and classifier C's accuracy is above average human accuracy. Error bars reflect the 95% confidence interval of the mean based on a binomial model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Inferred advice-taking policies for a subset of 7 individual participants in the concurrent paradigm. The policy determines the probability (α) of taking the classifier advice as a function of human confidence (colors), classifier confidence (horizontal axis), and type of classifier (rows). Colored areas show 95% posterior credible intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Accuracy of the advice-taking policy at the population level relative to the best and worst possible advice-taking policies. The distributions show the accuracy of randomly sampled advice-taking policies. To quantify the participants' performance levels, percentages show the percentile rank of their performance relative to the accuracy distribution over all possible policies. Individual differences in the effectiveness of advice-taking strategies as assessed by the percentile rank relative to the distribution of all possible advice-taking policies.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that in empirical paradigm, each image is presented in both the control condition as well as the AI assistance condition to allow for the estimation of item difficulty parameters for each image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">&amp; Zisserman, 2014), pretrained on the ImageNet dataset as the basis for the AI assistance. Our choice of VGG-19 was motivated by previous experiments<ref type="bibr" target="#b34">Steyvers et al., 2022</ref> that showed that the performance of the VGG-19 model could be manipulated to produce above-human performance for the challenging image noise conditions in the experiment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Bayes factors were computed using JASP (JASP Team, 2022) with the default priors that came with the software.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the optimal policy for classifier B shows that advice should be accepted more often than for classifier C even though classifier B performs worse on average than classifier C. This result can be attributed to between-group differences in classifiers B and C as well as differences in classifier calibration.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head><p>The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>M.S., H.T., and P.S. designed research; M.S. and H.T. performed research; M.S., H.T., and A.K. analyzed data; and M.S., A.K., H.T., and .P.S. wrote the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Approval</head><p>The experimental protocol was approved by University of California,Irvine Institutional Review Board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to Participate</head><p>Informed consent was obtained from all individual participants included in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to Publish</head><p>Consent to publish is not required as no individual subject information is presented in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>The original and preprocessed versions of the data can be accessed at: https://osf.io/9xbpu/ ?view only=f8d57ea4ec1940a3bc16bebc17a8ce51</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Availability</head><p>Code can be accessed at: https://osf.io/9xbpu/?view only=f8d57ea4ec1940a3bc16bebc17a8ce51 Appendices Appendix A: the ordered probit model The ordered probit model, r ∼ OrderedProbit(θ, v, σ) is a generative model that maps a (latent) value θ to one of R + 1 discrete scores r ∈ {0, . . . , R}. In this process, noise is added is added to the latent value resulting in a new latent value, θ ′ = θ + ϵ, where ϵ ∼ N(0, σ) and the resulting discrete score is determined by the interval where θ ′ lies:</p><p>The ordered vector</p><p>represents the transition points between different discrete scores. With this construction, the probability of producing a score r = m conditional on the latent value θ is:</p><p>where Φ is the cumulative standard normal distribution and v 0 = −∞.</p><p>For our empirical (concurrent) paradigm, we use the ordered probit model to map the latent probability correct, θ to three different levels of confidence, r ∈ {Low, M edium, High}. <ref type="figure">Figure 11</ref> shows an example of how the latent scores are mapped to confidence levels. Note that the higher value of the parameter σ (top panel) results in a noisier mapping of latent probabilities to discrete scores. We used Markov Chain Monte Carlo (MCMC) to infer model parameters and obtain samples from the posterior distribution, conditioned on the observed data. We chose JAGS for posterior inference <ref type="bibr">(Plummer et al., n.d.)</ref>. To facilitate posterior inference, the inference procedure was separated into two stages. In the first stage, the observed data x i,j , z j , and r i,j from the no AI assistance condition was used to infer all model parameters related to person and item differences (a i ,d j ,s j ) and confidence generating process (σ i , v i ). As a result of this stage, we computed posterior predictive distributions for the latent (independent) decisions x i,j and associated confidence levels r i,j for the AI assistance condition. In the second inference stage, the posterior modes of x i,j and r i,j were used as observed data, along with y i,j,k , c j,k , z j and η j,k to infer the advice-taking model parameters α i,j,k .</p><p>In theory, one does not need to separate the first and second stage of inference and model parameters can be estimated in one joint procedure. We followed this two-stage inference process to facilitate the comparison with the optimization experiments (described in the next section). For both the first and second stage inference process, we ran the sampler with 8 chains with a burn-in of 1000 iterations before taking 50 samples per chain. The chains mixed appropriately. For prior distributions, we used normal priors for the ability and discrimination IRT parameters, consistent with previous Bayesian IRT modeling <ref type="bibr" target="#b8">(Fox, 2010)</ref>:</p><p>, where I(0, ) denotes truncation a values below zero. Because of the large item differences in the classification task, we use a uniform prior spanning a large range of item differences, d j ∼ Uniform(−10, 10). For the generative process of the confidence levels, we used τ i ∼ Uniform(0, 15), σ i = 1/τ i . In addition, we used uniform priors on the two cutpoints needed to produce three levels of confidence</p><p>with the constraint that the cutpoints are ordered</p><p>Finally, for the advice-taking process, the AI reliance parameter α is treated as a 3 x 4 x 3 lookup table for each individual i where entries are determined by the three confidence levels of the participant ("low", "medium", and "high"), 4 classifier confidence levels (0.00-0.35, 0.35-0.57, 0.57-0.78, 0.78-1.00), and 3 AI classifiers (A, B, and C). The classifier confidence levels were chosen to evenly distribute the observations across bins. Changing notation, the AI reliance parameters can be represented by α i,r,η,k where r indexes the participant confidence level and η is the (discretized)</p><p>AI confidence level. We use a hierarchical Bayesian approach to estimate the individual differences in reliance policies by assuming that these are sampled from a normal distribution on the log-odds scale log (</p><p>The parameter β represents the advice taking policy at the population level, the tendency across participants to accept AI advice. The standard deviation ϕ captures the spread in individual differences. For priors, we use β r,η,k ∼ N (0, 3). In addition, because there are relatively few "medium" confidence levels, we imposed an order constraint,</p><p>., 4, and k = 1, ...3.</p><p>Appendix C: Optimality Analysis <ref type="figure">Figure 12</ref>: Advice-taking policies derived from an optimality analysis to maximize accuracy conditional on human confidence and accuracy. The policy determines the probability of taking the AI advice as a function of human confidence (colors), classifier confidence (horizontal axis), and type of classifier (columns)</p><p>The inferred advice-taking policies show that participants use a number of different factors in sensible ways. A natural question is what could participants have done differently in order to optimize their performance. We conducted an optimality analysis to identify the best possible policy to better understand the strategies that would have maximized accuracy in the AI-assistance condition. Importantly, in this analysis, the goal is to identify the most effective policy conditional on the performance of the participants and confidence states before AI assistance is provided. In other words, in the optimality analysis, we are not changing any assumptions about the ability of participants to classify images and express their confidence in their prediction -we are only considering scenarios where participants might have utilized the AI assistance in different ways. Also, we are not considering how participants have to learn about the effectiveness of their AI reliance policy over the course of the experiment.</p><p>Specifically, we start the analysis with the inferred confidence state of the participant (r) and accuracy (θ) of the predictions before AI assistance is considered. We then find out what strategy (at the individual level, α, or at the population level, β) would have maximized the accuracy of the final predictions (y) after AI assistance. The analysis is applied separately for each classifier A, B, and C.</p><p>The goal of the optimality analysis is to identify the best and worst possible advice-taking policy (α i,j,k ) conditional on inferred accuracy (θ i,j ), confidence state (r i,j ) of the participant for individual items before AI assistance is provided, and classifier type k. For the inferred accuracy θ i,j , we took the posterior mean for each item based on our MCMC procedure. For the confidence state r i,j , we took the posterior mode for each item.</p><p>The analysis was based on a brute-force search conducted separately for the three types of classifier. In this search, the parameter values α were discretized into 80 equally spaced values between 0 and 1, and then searching the space across 3 levels of DM confidence (r i,j ) and 4 levels of AI confidence (η j ). We then applied Equations 6-5 to identify the α policy that produced the highest as well as the lowest accuracy of predictions y i,j,k in Equation 5. The parameters were</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Out-of-sample Model Predictions</head><p>To assess model fit of the concurrent experiment, we used a 10-fold cross-validation approach to compute out-of-sample model predictions for the human decisions and confidence levels. For each individual, the observed data from the AI assistance and no AI Assistance condition was randomly partitioned into 10 disjoint test sets. For the no AI assistance condition, model parameters were inferred on the basis of observed predictions x and confidence levels r for each training fold. For the test set, we used the MCMC inference procedure described in Appendix B to infer the predictions x and confidence levels r for the test set. For the AI assistance condition, the model has to predict the withheld data on y, the decisions made with the aid of the AI. <ref type="figure">Figure 13</ref> shows the out-of-sample model predictions and observed data. One point of deviation is that the model somewhat underpredicts the size of the assistance effect (bottom row). However, the model captures all qualitative trends in the data.  <ref type="figure">Figure 13</ref>: Model predictions and observed data for human performance with and without AI assistance in the concurrent paradigm. Model predictions and data are show with lines and points respectively. Error bars reflect the 95% confidence interval of the mean of the observed data based on a binomial model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E: Relationship between human and AI confidence</head><p>Prior to AI assistance, human confidence levels are correlated with AI confidence scores, with Spearman rank correlations of 0.28, 0.43, and 0.47 for AI classifiers A, B, and C respectively (in this analysis, we are combining results across the sequential and concurrent conditions). Therefore, what is a difficult problem for the human participant (e.g., a high noise classification problem) tends to be challenging for the classifier as well. <ref type="figure">Figure 14</ref> provides more detailed information about the relationship broken down by classifier and degree of image noise. For ease of interpretation, AI confidence scores were discretized into three labels "Low", "Medium", and "High" where the cutoffs to define the labels were chosen such that the marginal frequencies of the labels matched the marginal frequencies of human confidence ratings (note that in the Experiment, the participants did not see these discrete AI confidence labels).</p><p>For low noise conditions (phase noise levels at 110 or lower), there is a stronger correspondence between human and AI confidence, such that there are few cases (fewer than 14% for Classifier C)</p><p>where the human is in a low confidence state and the AI is in a high confidence state (or vice versa).</p><p>However, for the more challenging high-noise classification problems (phase noise levels above 110), the correspondence between human and AI confidence is reduced and in roughly a third of cases, the human and AI are in opposite confidence states.  <ref type="figure">Figure 14</ref>: Relationship between AI and human confidence scores prior to AI assistance across AI classifiers and noise levels. Percentages in each row show the relative number of AI discretized confidence levels given a particular level of human confidence. AI confidence levels were discretized into three labels to match the marginal frequencies of the human label frequencies (34%, 25% and 41% for "Low", "Medium", and "High" ratings). The results are combined across the concurrent and sequential conditions without AI assistance. Low noise (top row) include the 0, 80, 95, and 110 phase noise levels whereas high noise (bottom row) include the 125, 140, 155, and 170 phase noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Confidence</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boosting human decision-making with AI-generated decision aids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skirzyński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Opheusden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02776</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Malaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">CSCW1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task-dependent algorithm aversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="809" to="825" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human confidence in artificial intelligence and in themselves: The evolution and impact of confidence on adoption of AI advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goucher-Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kotovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">107018</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automation bias in intelligent time critical decision support systems. Decision making in aviation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Cummings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="289" to="294" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian item response modeling: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Medina Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wichmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m">Generalisation in humans and deep neural networks. Thirty-second Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7549" to="7561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The principles and limits of algorithm-in-the-loop decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">JASP (Version 0.16.2)[Computer software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasp Team</surname></persName>
		</author>
		<ptr target="https://jasp-stats.org/" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human decisions and machine predictions. The quarterly journal of economics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="237" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining algorithm aversion with metacognitive bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On human predictions with explanations and predictions of machine learning models: A case study on deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trust in automation: Designing for appropriate reliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adapting to the algorithm: How accuracy comparisons promote the use of a decision aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive research: principles and implications</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Theory of machine: When do people rely on algorithms? Harvard Business School working paper series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human reliance on machine learning models when performance feedback is limited: Heuristics and risks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1107.4557</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A slow algorithm improves users&apos; assessments of the algorithm&apos;s accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-machine partnership with artificial intelligence for chest radiograph diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltaxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Confidence, advice seeking and changes of mind in decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pescetelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-K</forename><surname>Hauperich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeckeln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="6171" to="6176" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plummer</surname></persName>
			<affiliation>
				<orgName type="collaboration">n.d</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Manipulating and measuring model interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Wortman Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do patients trust computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Promberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="455" to="468" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CheXaid: Deep learning assistance for physician diagnosis of tuberculosis using chest x-rays in patients with HIV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maartens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Van Hoving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Griesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Boyles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-00322-2</idno>
		<ptr target="https://doi.org/10.1038/s41746-020-00322-2" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>npj Digital Medicine</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomsett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">CSCW1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahimy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rastegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="552" to="564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">I can do better than your AI: Expertise and explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
		<meeting>the 24th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="240" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian modeling of human-AI complementarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tejeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2111547119</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Will you accept the AI recommendation? predicting human behavior in AI-assisted decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A transient search using combined human and machine classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lintott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Smartt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fortson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Bouslog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">472</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1315" to="1323" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the effect of accuracy on trust in machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wortman Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 chi conference on human factors in computing systems</title>
		<meeting>the 2019 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A cautionary tale about the impact of AI on human design teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Design Studies</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">100990</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
