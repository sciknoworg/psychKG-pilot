<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does Explainable Artificial Intelligence Improve Human Decision-Making?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasmeen</forename><surname>Alufaisan</surname></persName>
							<email>yasmeen.alufaisan@aramco.com</email>
							<affiliation key="aff0">
								<orgName type="institution">EXPEC Computer Center at Saudi Aramco Dhahran 31311</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">R</forename><surname>Marusich</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">U.S. Army Combat Capabilities Development Command Army Research Laboratory South at the University of Texas at Arlington 3 U.S. Army Combat Capabilities Development Command Army Research Laboratory South</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Z</forename><surname>Bakdash</surname></persName>
							<email>jonathan.z.bakdash.civ@mail.mil</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Kantarcioglu</surname></persName>
							<email>muratk@utdallas.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Does Explainable Artificial Intelligence Improve Human Decision-Making?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has typically focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Explainable AI is touted as the key for users to "understand, appropriately trust, and effectively manage. . . <ref type="bibr">[AI systems]</ref>)" <ref type="bibr" target="#b19">(Gunning 2017)</ref> with parallel goals of achieving fairness, accountability, and transparency <ref type="bibr" target="#b43">(Sokol 2019</ref>). There are a multitude of reasons for explainable AI, but there is little empirical research for its impact on human decisionmaking <ref type="bibr" target="#b30">(Miller 2019;</ref><ref type="bibr" target="#b1">Adadi and Berrada 2018)</ref>. Prior behavioral research on explainable AI has primarily focused on human understanding/interpretability, trust, and usability for different types of explanations <ref type="bibr">(Doshi-Velez and Kim</ref> To fully achieve fairness and accountability, explainable AI should lead to better human decisions. Earlier research demonstrated that explainable AI can be understood by people <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>. Ideally, the combination of humans and machines will perform better than either alone <ref type="bibr" target="#b1">(Adadi and Berrada 2018)</ref>, such as computerassisted chess <ref type="bibr" target="#b8">(Cummings 2014)</ref>, but this combination may not necessarily improve the overall accuracy of AI systems. While (causal) explanation and prediction share commonalities, they are not interchangeable concepts <ref type="bibr" target="#b1">(Adadi and Berrada 2018;</ref><ref type="bibr" target="#b41">Shmueli et al. 2010;</ref><ref type="bibr" target="#b15">Edwards and Veale 2018)</ref>. Consequently, a "good" explanation, interpretable model predictions, may not be sufficient for improving actual human decisions <ref type="bibr" target="#b1">(Adadi and Berrada 2018;</ref><ref type="bibr" target="#b30">Miller 2019)</ref> because of heuristics and biases in human decisionmaking <ref type="bibr" target="#b22">(Kahneman 2011)</ref>. Therefore, it is important to demonstrate whether, and what types of, explainable AI can improve the decision-making performance of humans using that AI, relative to performance using the predictions of "black box" AI with no explanations and for human making decisions with no AI prediction.</p><p>In this work, we empirically investigate whether explainable AI improves human decision-making using a twochoice classification experiment with real-world data. Using human subject experiments, we compared three different settings where a user needs to make decision 1) No AI prediction (Control), 2) AI predictions but no explanation, and 3) AI predictions with explanations. Our results indicate that, while providing the AI predictions tends to help users, the why information provided in explainable AI does not specifically enhance user decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background and Related Work</head><p>Using Doshi-Velez and Kim's (2017) framework for interpretable machine learning, our current work focuses on: real humans, simplified tasks. Because our objective is on evaluating decision-making, we do not compare different types of explanations and instead used one of the best available explanations: anchor LIME <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>. We use real tasks here, although our tasks involve relatively simple decisions with two possible choices. Additionally, we use lay individuals rather than experts. Below, we discuss prior work that is related to our experimental approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explainable AI/Machine Learning</head><p>While machine learning models largely remain opaque and their decisions are difficult to explain, there is an urgent need for machine learning systems that can "explain" its reasoning. For example, European Union regulation requires "right to explanation" for any algorithms that make decisions significantly impacting users with user-level predictors (Parliament and Council of the European Union 2016). In response to the lack of consensus on the definition and evaluation of interpretability in machine learning, Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability focusing on the synergy among human, application, and functionality. They contrast interpretability with reliability and fairness, and discuss scenarios in which interpretability is needed. To unmask the incomprehensible reasoning made by these machine learning/AI models, researchers developed explainable models that are built on top of the machine learning model to explain their decisions. The most common forms of explainable models that provide explanations for the decisions made by machine learning models are feature-based and rule-based models. The feature-based models resemble feature selection where the model outputs the top features that explain the machine learning prediction and their associated weights <ref type="bibr" target="#b9">(Datta, Sen, and Zick 2016;</ref><ref type="bibr" target="#b37">Ribeiro, Singh, and Guestrin 2016)</ref>. The rule-based models provide simple if-then-else rules to explain predictions <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018;</ref><ref type="bibr" target="#b3">Alufaisan et al. 2017)</ref>. It has been shown that rule-based models provide higher human precision when compared to featurebased models <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>. <ref type="bibr" target="#b29">Lou et al. (2012)</ref> investigate the generalized additive models (GAMs) that combine single-feature models through a linear function. GAMs are more accurate than simple linear models, and can be easily interpreted by users. Their empirical study suggests that a shallow bagged-tree with gradient boosting is the best method on low to medium dimensional datasets. Anchor LIME is an example of the current state-of-the-art explainable rule-based model <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>. It is a model-agnostic system that can explain predictions generated by any machine learning model with high precision. The model provides rules, referred to as anchors, to explain the prediction for each instance. A rule is an anchor if it sufficiently explains the prediction locally such that any changes to the rest of the features, features not included in the anchor, do not effect the prediction. Anchors can be found in two different approaches: bottom-up approach and beam search. <ref type="bibr" target="#b44">Wang et al. (2017)</ref> present a machine learning algorithm that produces Bayesian rule sets (BRS) comprised of short rules in the disjunctive normal form. They develop two probabilistic models with prior parameters that allow the user to specify a desired size and shape and balance between accuracy and interpretability. They apply two priors-beta-binomials and Poisson distribution-to constrain the rule generation process and provide theoretical bounds for reducing computation by iteratively pruning the search space. In our experiments, we use anchor LIME to provide explanations for all our experimental evaluation due to the high human precision of anchor LIME as reported in <ref type="bibr" target="#b38">Ribeiro, Singh, and Guestrin (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Decision-Making and Human Experiments with Explainable AI</head><p>A common reason for providing explanation is to improve human predictions or decisions <ref type="bibr" target="#b23">(Keil 2006</ref>). People are not necessarily rational (i.e., maximizing an expected utility function). Instead, decisions are often driven by heuristics and biases <ref type="bibr" target="#b22">(Kahneman 2011)</ref>. Also, providing more information, even if relevant, does not necessarily lead people to making better decisions <ref type="bibr" target="#b16">(Gigerenzer and Brighton 2009)</ref>. Bounded rationality in human decision-making using satisfying with constraints <ref type="bibr" target="#b16">(Gigerenzer and Brighton 2009)</ref> is an alternative theory to heuristics and biases <ref type="bibr" target="#b22">(Kahneman 2011)</ref>. Regardless of the theoretical account for human decisionmaking, people, which can include experts <ref type="bibr" target="#b10">(Dawes, Faust, and Meehl 1989)</ref>, generally do not make fully optimal decisions.</p><p>At a minimum, explainable AI should not be detrimental to human decision-making. The literature on decision aids (a computational recommendation or prediction, typically without an explicit explanation) has mixed findings for human performance. Sometimes these aids are beneficial for human decision-making, whereas at other times they have negative effects on decisions <ref type="bibr" target="#b24">(Kleinmuntz and Schkade 1993;</ref><ref type="bibr" target="#b42">Skitka, Mosier, and Burdick 1999)</ref>. These mixed findings may be attributable to absence of explanations; this can be investigated through human experiments testing AI predictions with explanations compared with AI predictions alone.</p><p>Most prior human experiments with explainable AI have concentrated on interpretability, trust, and subjective measures of usability, such as preferences and satisfaction, with work on decision-making performance remaining somewhat limited <ref type="bibr" target="#b30">(Miller 2019;</ref><ref type="bibr" target="#b1">Adadi and Berrada 2018)</ref>. Earlier results suggest explainable AI can increase interpretability (e.g. <ref type="bibr" target="#b38">Ribeiro, Singh, and Guestrin 2018)</ref>, trust (e.g. <ref type="bibr" target="#b27">Lakkaraju and Bastani 2020;</ref><ref type="bibr" target="#b37">Ribeiro, Singh, and Guestrin 2016;</ref><ref type="bibr" target="#b40">Selvaraju et al. 2017)</ref>, and usability (e.g. <ref type="bibr" target="#b38">Ribeiro, Singh, and Guestrin 2018)</ref> to varying degrees, but this does not necessarily translate to better performance on real-world decisions about the underlying data, such as whether to actually use the AI's prediction, whether the AI has made an error, and the role of explanations. In fact, recent work has shown that subjective measures commonly assessed (e.g., preference and trust) do not predict actual human performance <ref type="bibr" target="#b6">(Buçinca et al. 2020;</ref><ref type="bibr" target="#b45">Zhang, Liao, and Bellamy 2020)</ref>; similarly, performance on common proxy tasks such as predicting the AI's decision also may not be indicative of actual decision-making performance <ref type="bibr" target="#b6">(Buçinca et al. 2020)</ref>. These findings highlight the need for more study of the impact of AI explanation on objective human performance, not just proxy or subjective measures.</p><p>In the limited studies that do examine the effect of explanation on human decision-making performance, there are mixed findings about whether the explanation provides an additional benefit over AI prediction alone. For example, some researchers found that human performance was better when an AI prediction was accompanied by explanation than performance with the prediction alone (Buçinca et al. 2020; Lai and Tan 2019) However, other studies did not show any additional benefit of explanation over AI prediction alone <ref type="bibr" target="#b18">(Green and Chen 2019)</ref>, with some even showing evidence of worse performance with explanation <ref type="bibr" target="#b34">(Poursabzi-Sangdeh et al. 2018;</ref><ref type="bibr" target="#b45">Zhang, Liao, and Bellamy 2020)</ref>.</p><p>The two papers finding a benefit for explanations consisted of a task in which users made decisions about the fat content in pictures of food <ref type="bibr" target="#b6">(Buçinca et al. 2020)</ref> and judgments about whether text from hotel reviews were genuine or deceptive <ref type="bibr" target="#b26">(Lai and Tan 2019)</ref>. They also both used a simple binary choice as the decision-making task. In contrast, the work finding no improvement in decision accuracy with explainable AI used datasets that comprised variables and outcomes, including probabilistic assessments for risks with recidivism and loan outcomes (Green and Chen 2019), decisions about real estate valuations <ref type="bibr" target="#b34">(Poursabzi-Sangdeh et al. 2018)</ref>, and predictions about income <ref type="bibr" target="#b45">(Zhang, Liao, and Bellamy 2020)</ref>. In addition, instead of simple binary choices, these studies used prediction of values along a continuum <ref type="bibr" target="#b18">(Green and Chen 2019;</ref><ref type="bibr" target="#b34">Poursabzi-Sangdeh et al. 2018)</ref>, and binary choice with the option to switch after seeing the model prediction (Zhang, Liao, and Bellamy 2020).</p><p>Besides dataset and task differences, there are two other distinctions among these papers. Only a single paper assessed decision-making under time pressure (Zhang, Liao, and Bellamy 2020) and only two papers informed users if their decisions were correct or incorrect (Green and Chen 2019; Zhang, Liao, and Bellamy 2020). Our study design uses datasets of multiple variables and outcomes and provides correct/incorrect feedback, but also uses a very straightforward binary choice task. This combination could potentially resolve the disparity in results from the studies above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we first describe the two datasets used in our experiments. We then provide the details of our experimental design and hypotheses, participant recruitment, and general demographics of our sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>To conduct our experiments, we choose two different datasets that have been heavily used in prior research that tries to understand algorithmic fairness and accountability issues. For example, the COMPAS dataset has been used to detect potential biases in criminal justice system (ProPublica 2016). The Census income dataset, which has been used to test many machine learning techniques, involves predictions of individuals' income status. This has been associated with potential biases in making decisions such as access to credit and job opportunities.</p><p>We choose these datasets primarily because they both involve real-world contexts that are understandable and engaging for human participants. Further, the two datasets differ widely in number of features and in the overall accuracy classifiers can achieve in their predictions. This allows us to explore the effects of these differences on human performance; in addition, it ensures that our findings are not limited only to a specific dataset. We briefly discuss each dataset in more detail below.</p><p>COMPAS stands for Correctional Offender Management Profiling for Alternative Sanctions (ProPublica 2016). It is a scoring system used to assign risk scores to criminal defendants to determine their likelihood of becoming a recidivist. The data has 6,479 instances and 7 features. These features are: gender, age, race, priors count, and charge degree risk score, and whether the defendants re-offended in two years or not. We let the binary re-offending feature be our class.</p><p>Census income (CI) data contains information used to predict individuals' income <ref type="bibr" target="#b14">(Dua and Graff 2017)</ref>. It has 32,561 instances and 14 features. These features are: age, workclass, education, marital status, occupation, relationship, race, sex, capital gain, capital loss, hours per week, and country. The class value is low income (less or equal to 50K) or high income (greater than 50K). We preprocessed the dataset to allow equal class distribution 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design</head><p>Prior results demonstrating people interpret, trust, and prefer explainable AI, suggesting it will improve the accuracy of human decisions. Hence, our primary hypotheses are that explainable AI would aid human decision-making. The hypotheses (H.) are as follows: To investigate these hypotheses, we used a 2 (Dataset: Census and COMPAS) x 3 (AI condition: Control, AI, and <ref type="figure">Figure 1</ref>: Example from the study demonstrating the information appearing in the three AI conditions for a trial from the COMPAS dataset condition.</p><formula xml:id="formula_0">H. 1</formula><p>AI with Explanation) between-participants experimental design. The three AI conditions were:</p><p>• Control: Participants were provided with no prediction or information from the AI.</p><p>• AI: Participants were provided with only an AI prediction.</p><p>• AI with Explanation: Participants received an AI prediction, as well as an explanation of the prediction using anchor LIME <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>.</p><p>To achieve more than 80% statistical power to detect a medium effect size for this design, we planned for a sample size of N = 300 (50 per condition).</p><p>In all conditions, each trial consists of a description of an individual and a two-alternative forced choice for the classification of that individual. Each choice was correct on 50% of the trials, thus chance performance for human decisionmaking accuracy was 50%. Additionally, an AI prediction and/or explanation may appear, depending on the AI condition (see <ref type="figure">Figure 1)</ref>. After a decision is made, participants are asked to enter their confidence in that choice, on a Likert scale of 1 (No Confidence) to 5 (Full Confidence). Feedback is then displayed, indicating whether or not the previous choice was correct.</p><p>We compared the prediction accuracy of Logistic Regression, Multi-layer Perceptron Neural Network with two layers of 50 units each, Random Forest, Support Vector Machine (SVM) with rbf kernel and selected the best classifier for each dataset. We chose a Multi-layer Perceptron Neural Network for Census income data where it resulted in an overall accuracy of 82% and SVM with rbf kernel for COM-PAS data with an overall accuracy of 68%. Census income accuracy closely matches the accuracy reported in the literature <ref type="bibr" target="#b28">(Lichman 2013;</ref><ref type="bibr">Kag 2018;</ref><ref type="bibr" target="#b2">Alufaisan, Kantarcioglu, and Zhou 2016)</ref> and COMPAS accuracy matches the results published by ProPublica (ProPublica 2016). We split the data to 60% for training and 40% for testing to allow enough instances for the explanations generated using anchor LIME <ref type="bibr" target="#b38">(Ribeiro, Singh, and Guestrin 2018)</ref>.</p><p>In our behavioral experiment, 50 instances were randomly sampled without replacement for each participant. Thus, AI accuracy was experimentally manipulated for participants (Census: mean AI accuracy = 83.85%, sd = 3.67%; COM-PAS: mean AI accuracy = 69.18%, sd = 4.65%). Because of the sample size and large number of repeated trials per participant, there was no meaningful difference in mean AI accuracy for participants in the AI condition vs. those in the AI explanation condition (p = 0.90).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant Recruitment and Procedure</head><p>We developed the experiment using jsPsych (De Leeuw 2015), and hosted it on the Volunteer Science platform (Radford et al. 2016) 2 . Participants were recruited using Amazon Mechanical Turk (AMT) and were compensated $4.00 each. We collected data from 50 participants in each of the six experimental conditions, for a total of 300 participants (57.67% male). Most participants were 18 to 44 years old (80.67%). This research was approved as exempt (19-176) by the Army Research Laboratory's Institutional Review Board.</p><p>Participants read and agreed to a consent form, then received instructions on the task, specific to the experimental condition they were assigned to. They completed 10 practice trials, followed by 50 test trials and a brief questionnaire assessing general demographic information and comments on strategies used during the task. The median time to complete the practice and test trials was 18 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>In this section we analyze and describe the effects of dataset, AI condition, and AI accuracy on the participants' decision-making accuracy, ability to outperform the AI, adherence to AI recommendations, confidence ratings, and reaction time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant Decision-Making Accuracy</head><p>We compared participants' mean accuracy in the experiment across conditions using a 2 (Dataset) x 3 (AI) factorial Analysis of Variance (ANOVA) (see <ref type="figure" target="#fig_0">Figure 2)</ref>. We found significant main effects, with a small effect size for AI condition (F (2, 294) = 8.19, p &lt; 0.001, η 2 = 0.04) and a nearly large effect for dataset condition <ref type="figure" target="#fig_0">(F (1, 294)</ref> = 46.51, p &lt; 0.001, η 2 = 0.12). In addition, there was a significant interaction with a small effect size (F (2, 294) = 8.38, p &lt; 0.001, η 2 = 0.05), indicating that the effect of AI condition depended on the dataset. Specifically, the large effect for increased accuracy with AI was driven by the Census dataset.</p><p>Contrary to H. 1, explainable AI did not substantially improve decision-making accuracy over AI alone. We followed up on significant ANOVA effects by performing pairwise comparisons using Tukey's Honestly Significant Difference. These post-hoc tests indicated that participants who viewed the Census dataset showed improved accuracy over control when given an AI prediction (p &lt; 0.01) and higher accuracy with AI explanation versus control (p &lt; 0.001), but there was no statistically significant difference in participant accuracy for AI compared to AI explanation (p = 0.28). Whereas the COMPAS dataset had no significant differences in participant accuracy across pairwise comparisons for the three AI conditions (ps &gt; 0.75). Also, the mean participant accuracy for the COMPAS control condition (mean = 63.7%, sd = 9.24%) was comparable to participant accuracy for prior decision-making research using the same dataset (mean = 62.8%, sd = 4.8%) <ref type="bibr" target="#b13">(Dressel and Farid 2018)</ref>.</p><p>There was strong evidence supporting H. 1.a, the vast majority of participants had mean accuracy exceeding guessing (50% accuracy). The overall participant accuracy across all conditions was 65.65% (sd = 10.92%), with 90% (or 270 out of 300) participants performing above chance on the classification task. This indicates that the task was challenging but feasible for almost all participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Accuracy and Participant Decision-Making Accuracy</head><p>We also evaluated the effect of the randomly varied AI accuracy for each participant on their decision-making accuracy. We used linear regression to analyze this relationship, specifying participant accuracy as the dependent variable and the following as independent variables: mean AI accuracy (per participant), AI condition, and dataset condition, see <ref type="figure" target="#fig_2">Figure 3</ref>. Regressions are represented by the solid lines with the shaded areas representing 95% confidence intervals. The control condition is not included in the analysis or figure, because the accuracy of the AI is not relevant if no AI prediction is presented to the participant. The overall regression model was significant with a large effect size, F (4, 195) = 21.23, p &lt; 0.001, R 2 adjusted = 0.29. Consistent with H. 2, there was a large main effect for AI accuracy (β = 0.70, p &lt; 0.001, R 2 = 0.28). Also, there was a small AI accuracy and dataset interaction (β = −0.07, p &lt; 0.01, R 2 = 0.03), reflecting the same interaction depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. There were no significant regression differences for dataset or AI versus AI Explanation, ps &gt; 0.60; there was no significant effect of dataset because it largely drove AI accuracy. Note it is not just that participants perform better with the higher mean AI accuracy of the Census dataset, both datasets had large positive relationships with participant accuracy and corresponding mean AI accuracy shown in <ref type="figure" target="#fig_2">Figure 3</ref>.  Outperforming the AI Accuracy An interesting question is whether the combination of AI and human decisionmaking can outperform either alone. The previous analyses showed that the addition of AI prediction information improved human performance over controls with humans alone. We also evaluated how often the human decisionmaking accuracy outperformed the accuracy of the corresponding mean AI prediction accuracy, which was experimentally manipulated. Although most participants per- Following AI Recommendations <ref type="figure">Figure 4</ref>: Mean proportion of participant choices matching AI prediction as a function of whether the AI correct/incorrect and the dataset condition. Error bars represent 95% confidence intervals. To simplify this figure, results were collapsed for the AI and AI Explanation conditions which did not have a significant main effect, p = 0.62.</p><p>formed well above chance, only a relatively small number of participants had decision accuracy exceeding their mean AI prediction (7% or 14 out of 200). This result largely supports H. 3 and also shown above in <ref type="figure" target="#fig_2">Figure 3</ref> where each dot represents an individual; and dots above the black dashed line show the participants that outperformed their mean AI prediction. The black dashed line shows equivalent performance for mean AI accuracy and mean participant accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset/Condition AI AI Explanation</head><p>Census 0 1 COMPAS 10 3 Adherence to AI Model Predictions Participants followed the AI predictions more often when the AI was correct versus when the AI was incorrect, indicating some recognition of when the AI makes bad predictions (see <ref type="figure">Figure 4</ref>, F (1, 196) = 36.15, p &lt; 0.001, η 2 p = 0.16). This was consistent with participant sensitivity to AI recommendations, evidence for H. 2. Also, participants were better able to recognize correct vs. incorrect AI predictions when they were in the Census condition, demonstrated in the significant interaction between AI correctness and dataset, F (1, 196) = 9.01, p &lt; 0.01, η 2 p = 0.04. None of the remaining ANOVA results were significant, ps &gt; 0.16. Thus, there was no evidence for higher adherence to recommendations with explainable AI, which rejected H. 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Ratings</head><p>We found that AI (without and with explanation) resulted in slightly increased mean confidence. There was a small effect of AI condition on mean confidence (see <ref type="figure" target="#fig_4">Figure 5</ref>, F (2, 294) = 3.58, p = 0.03, η 2 = 0.02). Post hoc tests indicated participants had significantly lower mean confidence in the control condition than AI, p &lt; 0.03, but there were no statistical differences for other pairwise comparisons, ps &gt; 0.25. This contradicted H. 6, and there was no evidence of a confidence increase with explanations. In addition, there was no evidence for a main effect of dataset condition or interaction, ps &gt; 0.84.</p><p>Confirming H. 7, we found a positive relationship for accuracy and confidence rating within individuals indicating that participants' confidence ratings were fairly wellcalibrated with their actual decision accuracy. We calculated each participant's mean accuracy at each confidence rating they used, and then conducted a repeated measures correlation (Bakdash and Marusich 2017) (r rm = 0.48, p &lt; 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>We also assessed reaction time and summarize self-reported decision-making strategies. These results are exploratory, there were no specific hypotheses. There was no significant main effect of AI condition on participants' reaction time (F (2, 294) = 2.13, p = 0.12, η 2 = 0.01). There was only a main effect of dataset condition (F (1, 294) = 28.52, p &lt; 0.001, η 2 = 0.09), where participants took an average of 1600 ms longer in the Census condition than the COMPAS condition (see <ref type="figure" target="#fig_6">Figure 6</ref>). This effect was most likely due to the Census dataset having more variables for each instance than the COMPAS dataset, and thus requiring more reading time on each trial. The addition of an explanation did not meaningfully increase reaction time over an AI prediction only.</p><p>Subjective measures, such as self-reported strategies and measures of usability, often diverge from objective measures of human performance <ref type="bibr" target="#b4">(Andre and Wickens 1995;</ref><ref type="bibr" target="#b32">Nisbett and Wilson 1977)</ref> such as actual decisions (Buçinca et al.  2020). Participants self-reported varying strategies to make their decisions, yet there was a clear benefit for AI prediction (without and with explanation). In the AI and AI explanation conditions: n = 80 indicated using the data without mentioning AI, n = 39 reported using a combination of the data and the AI, and only n = 16 said they primarily used, trusted, or followed the AI. Despite limited self-reported use of the AI in the two relevant conditions, decision accuracy was higher with AI ( <ref type="figure" target="#fig_0">Figure 2</ref>), strongly associated with AI accuracy <ref type="figure" target="#fig_2">(Figure 3)</ref>, and there was some sensitivity to whether the AI was followed when it was correct versus incorrect ( <ref type="figure">Figure 4</ref>). Nearly 80% of user comments could be coded, blank and nonsense responses could not be coded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our results show providing an AI prediction enhances human decision accuracy, but in opposition to the hypotheses adding an explanation positively impact decisions and increase the ability to outperform the AI. This finding is in line with some previous studies that also found no added benefit for explanation over AI prediction alone <ref type="bibr" target="#b18">(Green and Chen 2019;</ref><ref type="bibr" target="#b34">Poursabzi-Sangdeh et al. 2018;</ref><ref type="bibr" target="#b45">Zhang, Liao, and Bellamy 2020)</ref>. This suggests that it was not the simplicity of the decision-making task that accounts for the opposite findings in <ref type="bibr" target="#b6">Buçinca et al. (2020)</ref> and <ref type="bibr" target="#b26">Lai and Tan (2019)</ref>, since the current study also uses a relatively straightforward binary decision. Rather, it may be the case that tasks with highly intuitive datasets are required for explanation to improve performance. Future work may address this question directly.</p><p>One possible explanation for findings of no added benefit of explanation is that providing more information, even if task-relevant, does not necessarily improve human decisionmaking accuracy <ref type="bibr" target="#b16">(Gigerenzer and Brighton 2009;</ref><ref type="bibr" target="#b17">Goldstein and Gigerenzer 2002;</ref><ref type="bibr" target="#b31">Nadav-Greenberg and Joslyn 2009)</ref>. This phenomenon is attributed to cognitive limitations and people using near-optimal strategies, and corresponds with Poursabzi-Sangdeh et al.'s <ref type="bibr" target="#b21">(2018)</ref> findings that explanation caused information overload, reducing people's ability to detect AI mistakes. However, their study and most other papers did not use the speeded response paradigm we used here, suggesting this was not solely attributable to participants responding as quickly as possible.</p><p>The lack of a significant, practically-relevant effect for explainable AI was not due to lack of statistical power or ceiling performance -nearly all participants consistently performed above chance, but well below perfect accuracy. These findings also illustrate the need to compare decisionmaking with explainable AI to other conditions including no AI and AI prediction without explanation. If we did not have an AI only (decision aid) condition a reasonable but flawed inference would have been that explainable AI enhances decisions.</p><p>Limitations The present findings have limited generalizability to decision-making with other datasets and explainable AI techniques. For example, the effectiveness of explanations, or lack thereof, for human decision-making may depend on a variety of factors: the specific explanation technique, the properties of the dataset, and the task itself (such as probabilistic predictions vs two choice decisions). Nevertheless, this paper demonstrates that one cannot assume explainable AI will necessary improve human decisions and the need to evaluate objective measures, of human performance, in explainable AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>Much of the existing research on explainable AI focuses on the usability, trust, and interpretability of the explanation. In this paper, we fill in the research blank by investigating whether explainable AI can improve human decisionmaking. We design a behavioral experiment in which each participant recruited using Amazon Mechanical Turk is asked to complete 50 test trials in one of six experimental conditions. Our experiment is conducted on two real datasets to compare human decision with an AI prediction and an AI with explanation. Our experimental results demonstrate that AI predictions alone can generally improve human decision accuracy, while the advantage of explainable AI is not conclusive. We also show that users tend to follow AI predictions more often when the AI predictions are accurate. In addition, AI with or without explanation can increase the confidence of human users which, on average, was wellcalibrated to user decision accuracy.</p><p>In the future, we plan to investigate whether explainable AI can help improve fairness, safety, and ethics by increasing the transparency of AI models. Human decision-making is a key outcome measure, but is certainly not the only goal for explainable AI. We also plan to explore the difference of distributions in the error space between human decision and AI predictions, especially at decision boundaries. Also, whether human-machine collaboration is feasible through interactions in closed feedback loops. We will also expand our datasets to include other data format such as images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Impact</head><p>For many important critical decisions, depending on the AI model prediction may not be enough. Furthermore, many recent regulations such as GDPR (Parliament and Council of the European Union 2016) allow potential audit of AI prediction by a human. Therefore, it is critical to understand whether the explanations provided by explainable AI methods improve the overall prediction accuracy, and help human decision makers to detect errors. Our results indicate that although the existence of an AI model may improve human decision-making, the explanations provided may not automatically improve the accuracy. We believe that our results could help ignite the needed research to explore how to better integrate explanations, AI models and human operators to have better outcomes compared AI models or humans alone.</p><p>Another consideration is the data itself, models created using systematically biased data will simply mirror and reinforce patterns inherent to the data. For example, in the COM-PAS dataset a high risk score for re-offending has far lower accuracy for black defendants than white ones (ProPublica 2016). Furthermore, multiple AI systems for predicting criminal activity have relied on "dirty" data with racial bias due to flawed polices and procedures <ref type="bibr" target="#b39">(Richardson, Schultz, and Crawford 2019)</ref>. One solution may be to combine approaches for fair machine learning (Corbett-Davies and Goel 2018) with explainable AI, while considering dataset properties such as its provenance.</p><p>This was despite higher mean AI accuracy for the Census Income dataset over COMPAS.</p><p>We compared the accuracy of participants who selfreported different strategies, grouping them into those who mentioned using the AI (in the AI and AI Explanation conditions; Primarily AI and AI and data), those who didn't mention using the AI (Data and gut, Primarily guy, and Primarily data), and those who provided blank or nonsense strategies (Could not be coded, labelled as NA). As shown in <ref type="figure" target="#fig_0">Figure  2</ref>, mean participant accuracy was very similar for those who mentioned using the AI versus those who didn't. In the future, it may be useful to compare the types of decision errors for AI versus AI with explanation as well as errors based on self-reported strategies. Different types of errors could produce similar decisional accuracy if the overall error rate is similar. However, participants whose self-reported strategies were blank or did not make sense did show a lower mean accuracy on the task, perhaps indicating a lower level of effort or engagement with the task overall with accuracy nearing or even at chance performance (50%). The number of users with no self-reported strategy was comparable across all six conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speed-Accuracy Tradeoff</head><p>We used linear regression to explore the tradeoff between speed and accuracy across participants; that is, whether slower participants tended to be more accurate and vice versa. Speed/Accuracy <ref type="figure" target="#fig_2">Figure 3</ref>: Speed-accuracy tradeoff generated by participants in each dataset condition. Shaded areas indicate 95% confidence intervals.</p><p>We found overall a small but significant speed-accuracy tradeoff (β = 1.01, p &lt; 0.001, η 2 = 0.08), where each additional second of reaction time predicted a 1.01% increase in accuracy. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, there was a small interaction between the dataset condition and reaction time (F (1, 288) = 6.65, p &lt; 0.02, η 2 = 0.02), indicating that the speed-accuracy tradeoff was present for participants in the Census dataset condition (β = 1.02), but it was near zero for those in the COMPAS dataset condition (β = −0.02).</p><p>We did not find a significant effect of AI condition on the speed-accuracy tradeoff.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Mean participant accuracy in each AI and dataset condition. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Mean AI accuracy (per participant) and mean participant accuracy by AI and AI Explanation and the two datasets. The shaded areas represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Mean participant confidence ratings in each AI condition. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Mean reaction time in each AI and dataset condition. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Participant accuracy by self-reported strategy and AI condition. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of participants with decision accuracy exceeding their mean AI accuracy.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The CI dataset is from 1994. We adjusted for inflation by using a present value of 88k. From 1994 to January 2020 (when the experiment was run) inflation in the U.S. was 76.45%: https://www. wolframalpha.com/input/?i=inflation+from+1994+to+jan+2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://volunteerscience.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Copyright c 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Combat Capabilities Development Command Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation. M.K. and Y.Z. were supported by ARL under grant W911NF-17-1-0356. We thank Jason Radford for help with implementing the experiment on the Volunteer Science platform and Katelyn Morris for independently coding the comments. We also thank Dan Cassenti for input on the paper, and Jessica Schultheis and Alan Breacher for editing the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for "Does Explainable Artificial Intelligence Improve</head><p>Human Decision-Making?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Age Distribution</head><p>See <ref type="figure">Figure 1</ref> for the age distribution of participants. Most participants were in the 25-34 age group.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Reported Decision Strategies</head><p>We performed an exploratory analysis on the self-reported decision strategies, qualitatively described by users in their comments. We coded strategies using six categories, see the first column in <ref type="table">Table .</ref> First, comments were independently coded by two raters: The third author and a research assistant. Inter-rater reliability was good, κ = 0.70, p &lt; 0.001. Second, we reached consensus to resolve all discrepancies and finalize categorical coding of decision strategies. The majority of participants commented they primarily relied on the data to make decisions, especially in the control condition: n = 74. The dominant reported strategy in the two AI conditions was primarily using the data to make decisions without any mention of AI: n = 80. Primary use of AI (n = 16) was surprisingly low. Although partial use of AI, including the possibility of explanations in the explainable AI condition, was higher (n = 39) was still limited compared to the more widely used strategy of making decisions using the data. Whereas the number of participants using each strategy was generally comparable between the two datasets <ref type="table">(Table )</ref>. Minor differences, more frequent use of AI and data together for COMPAS with less use of primarily data for COMPAS.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">on UCI adult dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Peeking inside the blackbox: A survey on Explainable Artificial Intelligence (XAI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting Discrimination in a Black-Box Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alufaisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC)</title>
		<meeting>the 2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From Myths to Norms: Demystifying Data Mining Models with Instance-Based Transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alufaisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thuraisingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC)</title>
		<meeting>the 2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When users want what&apos;s not best for them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics in design</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="10" to="14" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Repeated measures correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Bakdash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Marusich</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2017.00456</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Buçinca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Glassman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377325.3377498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces, IUI &apos;20</title>
		<meeting>the 25th International Conference on Intelligent User Interfaces, IUI &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The measure and mismeasure of fairness: A critical review of fair machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00023</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Man versus machine or man+ machine?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cummings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="62" to="69" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Symposium on Security and Privacy (SP)</title>
		<meeting>the 2016 IEEE Symposium on Security and Privacy (SP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clinical versus actuarial judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Meehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="issue">4899</biblScope>
			<biblScope unit="page" from="1668" to="1674" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The accuracy, fairness, and limits of predicting recidivism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5580</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enslaving the algorithm: From a &quot;Right to an Explanation&quot; to a &quot;Right to Better Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="54" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Models of ecological rationality: the recognition heuristic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Principles and Limits of Algorithm-in-the-Loop Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359152</idno>
		<ptr target="https://doi.org/10.1145/3359152" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2019" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence (xai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<ptr target="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Litman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metrics</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04608</idno>
		<title level="m">Challenges and prospects</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explanation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Keil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="227" to="254" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information displays and decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Kleinmuntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Schkade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An evaluation of the human-interpretability of explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00006</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On human predictions with explanations and predictions of machine learning models: A case study on deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How do I fool you?&quot;: Manipulating User Trust via Misleading Black Box Explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3375627</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI</title>
		<meeting>the AAAI/ACM Conference on AI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intelligible Models for Classification and Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;12</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uncertainty forecasts improve decision making among nonexperts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nadav-Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joslyn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Engineering and Decision Making</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Telling more than we can know: verbal reports on mental processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Nisbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">231</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Parliament and Council of the European Union</title>
		<ptr target="https://eur-lex.europa.eu/eli/reg/2016/679/oj" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>General data protection regulation</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07810</idno>
		<title level="m">Manipulating and measuring model interpretability</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Propublica</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
		<title level="m">Machine Bias</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Volunteer science: An online laboratory for experiments in social psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reichelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Welles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ognyanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meleis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychology Quarterly</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="396" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NYUL Rev. Online</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">To explain or to predict?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="310" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Does automation bias decision-making?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Skitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="991" to="1006" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sokol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="541" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Bayesian Framework for Learning Rule Sets for Interpretable Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v18/16-003.html" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372852</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* &apos;20</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency, FAT* &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
