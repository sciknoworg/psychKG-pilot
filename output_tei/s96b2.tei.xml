<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the computational principles underlying human exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Fox</surname></persName>
							<email>lior.fox@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Edmond and Lily Safra Center for Brain Sciences</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<settlement>Jerusalem</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Dan</surname></persName>
							<email>ohad.dan@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution" key="instit1">Yale School of Medicine</orgName>
								<orgName type="institution" key="instit2">The Alexander Silberman Institute of Life Sciences</orgName>
								<address>
									<country>The Federmann</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the computational principles underlying human exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Adapting to new environments is a hallmark of animal and human cognition, and Reinforcement Learning (RL) models provide a powerful and general framework for studying such adaptation. A fundamental learning component identified by RL models is that in the absence of direct supervision, when learning is driven by trial-and-error, exploration is essential. The necessary ingredients of effective exploration have been studied extensively in machine learning. However, the relevance of some of these principles to humans&apos; exploration is still unknown. An important reason for this gap is the dominance of the Multi-Armed Bandit tasks in human exploration studies. In these tasks, the exploration component per se is simple, because local measures of uncertainty, most notably visitcounters, are sufficient to effectively direct exploration. By contrast, in more complex environments, actions have long-term exploratory consequences that should be accounted for when measuring their associated uncertainties. Here, we use a novel experimental task that goes beyond the bandit task to study human exploration. We show that when local measures of uncertainty are insufficient, humans use exploration strategies that propagate uncertainties over states and actions. Moreover, we show that the long-term exploration consequences are temporally-discounted, similar to the temporal discounting of rewards in standard RL tasks. Additionally, we show that human exploration is largely uncertaintydriven. Finally, we find that humans exhibit signatures of temporally-extended learning, rather than local, 1-step update rules which are commonly assumed in RL models. All these aspects of human exploration are well-captured by a computational model in which agents learn an exploration &quot;value-function&quot;, analogous to the standard (reward-based) value-function in RL.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>When encountered with a novel setting, animals and humans explore their environment. Such exploration is essential for learning which actions are beneficial for the organism and which should be avoided. The speed of learning, and even the learning outcome, crucially depends on that was chosen more often. By contrast, visit-counters alone might be a poor measure of uncertainty in complex environments, because they completely ignore future consequences of the actions <ref type="figure" target="#fig_0">(Figure 1a)</ref>. Indeed, the limitations of naive counter-based exploration in structured and complex environments have been discussed in the machine learning literature, and different exploration schemes that take into account the long-term exploratory consequences of actions have been proposed <ref type="bibr" target="#b46">(Storck et al., 1995;</ref><ref type="bibr" target="#b30">Meuleau and Bourgine, 1999;</ref><ref type="bibr">Osband et al., 2016a,b;</ref><ref type="bibr" target="#b7">Chen et al., 2017;</ref><ref type="bibr" target="#b13">Fox et al., 2018)</ref>.</p><p>Our goal here is to study the extent to which human exploration is sensitive to long-term consequences of actions, as opposed to counter-based exploration. Crucially, this question cannot be addressed in the common bandit problems paradigm, because general exploration algorithms are reduced to counter-based methods when they are faced with a bandit problem.</p><p>Thus, even if humans do (approximately) use some general, beyond visit-counters, directed exploration strategies, they will likely manifest as counter-based strategies in bandit tasks.</p><p>Therefore, we set out to study exploration in a novel task that addresses these issues. First, we</p><p>show that humans take into account the long-term exploratory consequences of their actions when exploring complex environments <ref type="bibr">(Experimental results)</ref>. Next, we model this exploration using an RL-like algorithm, in which agents learn exploratory "action-values" and use these values to guide their exploration (Computational modeling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to future consequences of actions</head><p>To test the hypothesis that human exploration is sensitive to the long-term consequences of actions, we conducted an experiment that formalizes the intuition presented in the Introduction (see <ref type="figure" target="#fig_0">Figure 1a</ref>). In the experiment (denoted as "Experiment 1"), participants were instructed to explore a novel environment, a maze of rooms, by navigating through the doors connecting those rooms ( <ref type="figure" target="#fig_0">Figure 1b)</ref>. Each room was identified by a unique background, a title, and the number of doors in that room. No reward was given in this task, but participants were instructed to "understand how the rooms are connected" (see Methods). Testing participants in a task devoid of clear goal and rewards is somewhat unorthodox. We go back to this point in the Discussion section.</p><p>Three groups of participants were tested, each in a different maze as is described in <ref type="figure" target="#fig_0">Figure 1c</ref> (top): In all mazes, there was a start room (S) with two doors, each leading to a different room.</p><p>One of these rooms, a multi-action room (M R ) was endowed with n R doors, while the other, denoted as M L , was endowed with n L doors. All three mazes were unbalanced, in the sense that n R &gt; n L . Between the different mazes, we varied n R − n L , while keeping n R + n L = 7 constant.</p><p>The locations of the doors leading to M R and M L were counterbalanced across participants.</p><p>For clarity of notation, we refer to them as "right" and "left", respectively. All other remaining rooms were endowed with only a single door. After going through these single-door rooms, a participant would reach a common terminal room (T ). There, they were informed that they reached the end of the maze and then they were transported back to S. Overall, each participant In complex environments (right), actions have long-term consequences as particular actions might lead, in the future, to different parts of the state-space. In this example, these parts (shaded areas) are of different size. As a result, the local visit-counters are no longer a good measure of uncertainty. In this example, a 2 should be, in general, chosen more often compared to a 1 in order to exhaust the larger uncertainty associated with it. (b) Participants were instructed to navigate through a maze of rooms. Each room was identified by a unique background image and a title. To move to the next room, participants chose between the available doors by mouse-clicking. Background images and room titles (Armenian letters) were randomized between participants, and were devoid of any clear semantic or spatial structure. (c) The three maze structures in Experiment 1 (Top) have a root state S (highlighted in yellow) with two doors. They differ in the imbalance between the number of doors available in future rooms M R and M L (n R : n L -4:3, 5:2, 6:1). Consistent with models of directed exploration that take into account long-term consequences of actions, and unlike counterbased models, participants exhibited bias towards room M R , deviating from a uniform policy (Bottom, bars and error-bars denote mean and 95% confidence interval of p R ; number of participants: n = 161; 120; 137. Statistical significance, here and in following figures: * : p &lt; 0.05, * * : p &lt; 0.01; * * * : p &lt; 0.001).</p><p>visited S (of the one particular environment they were assigned to) 20 times.</p><p>Since there was no reward, all choices in this task are exploratory. If participant's exploration is driven by visit-counters, then we expect that the frequencies in which they choose each of the doors in S, denoted p R and p L , would be equal. By contrast, if they take into consideration the long-term consequences of their actions, then we would expect them to choose the right door more often (resulting in p R &gt; p L ). In line with the hypothesis that participants are sensitive to the long-term consequences of their actions, we found that averaged over all participants in the three conditions, p R &gt; p L (p R = 0.54, 95% confidence interval:</p><formula xml:id="formula_0">p R ∈ [0.518, 0.563]).</formula><p>Considering each group of participants separately, significant bias in favor of p R was observed in the 6:1 (p R = 0.572, n = 137, 95% CI: [0.528, 0.617]) and the 5:2 groups (p R = 0.549, n = 120, 95% CI: [0.506, 0.592]), but not in the 4:3 group (p R = 0.507, n = 161, 95% CI:</p><formula xml:id="formula_1">[0.472, 0.541]).</formula><p>We hypothesized that the larger the imbalance (n R − n L ), the stronger will be the bias towards</p><formula xml:id="formula_2">M R (larger p R ).</formula><p>To test this hypothesis, we compared the biases of participants in the different groups ( <ref type="figure" target="#fig_0">Figure 1c</ref>). As expected, the average p R in the 5:2 and 6:1 groups was significantly larger than that of the 4:3 group (p &lt; 0.05 and p &lt; 0.01 respectively, permutation test, see Methods). The average p R in the 6:1 group was larger than that of the 5:2 group. However, this difference was not statistically significant (p = 0.17).</p><p>The results depicted in <ref type="figure" target="#fig_0">Figure 1c</ref> indicate that on average, human participants are sensitive to the exploratory long-term consequences of their actions. Considering individual participants, however, there was substantial heterogeneity in the biases exhibited by the different participants. While some chose the right door almost exclusively, others favored the left door. We next asked whether some of this heterogeneity across participants reflects more general individualdifferences in exploratory strategies, which would also manifest in their exploration in other states. To test this hypothesis, we focused on state M R . In this state, exploration is also required because there are n R different alternatives to choose from. However, unlike in state S, these alternatives do not, effectively, have long-term consequences. As such, choosing an action in M R is a bandit-like task. Thereofre, directed exploration in M R is expected to be driven by visit-counters, such that participants would equalize the number of times each door in M R is selected. Note that this is not a strong prediction, because random exploration will, on average, also equalize the number of choices of each door. Yet, directed and random exploration have diverging predictions with respect to the temporal pattern of choices in M R . Specifically, with pure directed exploration (that is driven by visit-counters), participants are expected to avoid choosing the same door that they chose the last time that they visited M R . Consequently, the probability of repeating the same choice in consecutive visits of M R , which we denote by p repeat , is expected to vanish. By contrast, random exploration predicts that p repeat = 1/n R .</p><p>Figure 2 (Top) depicts the histograms (over participants) of p repeat in the three experimental conditions, demonstrating that participants exhibited substantial variability in p repeat . While for some participants p repeat was close to 0, as predicted by pure directed exploration, for others it was similar to 1/n R , as predicted by random exploration. Many other participants exhibited p repeat that was even larger than 1/n R , indicating that, potentially, choice bias and / or momentum also influenced choices in the task. Based on the predictions of directed and random exploration, we divided participants into two groups, depending on the quality of exploration in M R : "good" directed explorers, in which p repeat &lt; 1/n R , and "poor" directed explorers, in which p repeat ≥ 1/n R <ref type="figure" target="#fig_1">(Figure 2</ref> Top, dots and diagonal stripes, respectively).</p><p>Is the quality of directed exploration in the bandit-like task of state M R informative about directed exploration in S? To address this question, we computed the histograms of p R separately for the "good" and "poor" directed explorers (Figure 2 Bottom). Averaging within each group we found that indeed, p R among the "poor" explorers was not significantly different from chance in any of the three conditions <ref type="figure">(Figure 3a)</ref>, consistent with the predictions of random exploration. By contrast, among "good" explorers, there was a significant bias in the 5:2 (p R = 0.597, n = 53, 95% CI: [0.537, 0.652]) and the 6:1 (p R = 0.612, n = 71, 95% CI: [0.544, 0.678]) groups <ref type="figure">(Figure 3b</ref>). These findings show that participants that avoid repetition in the bandit task are also more sensitive to the long-term exploratory consequences of their actions. We conclude that those participants who tend to perform good directed exploration in M R also perform good directed exploration in S. Crucially, the implementation of directed exploration in the two states is rather different. In M R , where different actions have no long-term consequences, </p><formula xml:id="formula_3">n R = 4, 5, 6</formula><p>). Dashed vertical line represents the value expected by chance, 1/n R . Based on their p repeat values, we divided participants into "good" and "poor" directed explorers (dotted and striped patterns, respectively; "good" explorers proportion: 40%, 44%, 51%). Bottom: Histograms of p R at state S (highlighted in yellow), for the "good" and "poor" directed explorers groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal discounting</head><p>In the previous section we showed that if the future exploratory consequences of the actions are one trial ahead, humans are sensitive to these consequences. It is well known that in humans and animals, the value of a reward is discounted with its delay <ref type="bibr" target="#b52">(Vanderveldt et al., 2016)</ref>.</p><p>We hypothesized that similar temporal discounting will manifest in evaluating the exploratory "usefulness" of actions. To test this prediction, we conducted Experiment 2 on a new set of participants. Similar to Experiment 1, Experiment 2 consisted of 3 different maze structures.</p><p>The imbalance between the number of possible outcomes was kept fixed across 3 mazes, at n R = 5 and n L = 2. However, the depth at which these outcomes occur, relative to the root state S, varied between 1 (as in Experiment 1) to 3 <ref type="figure">(Figure 4</ref>, Top). The depth of M R determines the delay between the choice made at S and its exploratory benefit. In the presence of temporal discounting of exploration, we therefore expect p R to decrease with the depth of</p><formula xml:id="formula_4">M R .</formula><p>To test this prediction, we divided participants to "good" and "poor" directed explorers, as in Experiment 1, based on the degree of p repeat in M R . As depicted in <ref type="figure">Figure 4</ref>, both the "poor"</p><p>and "good" explorers exhibited a bias in favor of "right" in S. For the "good" explorers, a larger delay was also associated with a smaller bias.  <ref type="figure">Figure 3</ref>: "Poor" and "good" directed explorers. Choice biases at state S (p R ) analyzed separately for "poor" and "good" explorers (striped and dotted patterns; divided based on their exploration in M R , see <ref type="figure" target="#fig_1">Figure 2</ref>) in the conditions of Experiment 1. While behavior of the "poor" explorers was not significantly different from chance (consistent with the prediction of random exploration), "good" explorers in the n R = 5, 6 conditions exhibited significant bias towards "right". Bars and error bars denote mean and 95% confidence interval of p R ; number of participants n = 95; 66 67; 53, 66; 71 ("poor"; "good").  <ref type="figure">Figure 4</ref>: Temporal discounting of exploratory consequences. The three mazes in Experiment 2 (Top) had the same imbalance (n R = 5, n L = 2), however we varied the depth of M R (and M L ) relative to the root state S (left to right: depth = 1, 2, 3). "Poor" and "good" directed explorers (striped and dotted patterns, respectively) were divided by their p repeat value at M R (same as in Experiment 1, see <ref type="figure" target="#fig_1">Figure 2</ref>). Bars and error-bars denote mean and 95% confidence interval of p R . Number of participants n = 99; 92, 121; 84, 153; 85 ("poor"; "good").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The dynamics of exploration</head><p>Insofar, we demonstrated that human participants exhibit directed exploration in which they take into their considerations the future exploratory consequences of their action. To better understand the computational principles underlying this directed exploration, we revisit the question of why explore in the first place. One possible answer to this question is that exploration is required for learning. According to this view, actions are favorable from an exploratory point of view when they are associated with, or lead to other actions associated with, high uncertainty, missing knowledge, and other related quantities <ref type="bibr" target="#b40">(Schmidhuber, 1991;</ref><ref type="bibr" target="#b45">Still and Precup, 2012;</ref><ref type="bibr" target="#b27">Little and Sommer, 2014;</ref><ref type="bibr" target="#b22">Houthooft et al., 2016;</ref><ref type="bibr" target="#b38">Pathak et al., 2017;</ref><ref type="bibr" target="#b6">Burda et al., 2019)</ref>. An alternative, that has received some attention in the machine learning literature, is that exploration could be driven by its own normative objective <ref type="bibr" target="#b28">(Machado and Bowling, 2016;</ref><ref type="bibr" target="#b21">Hazan et al., 2019;</ref><ref type="bibr" target="#b57">Zhang et al., 2020;</ref><ref type="bibr" target="#b56">Zahavy et al., 2021)</ref>. For example, such objective could be to maximize the entropy of the discounted distribution of visited states and chosen actions <ref type="bibr" target="#b21">(Hazan et al., 2019)</ref>. Experimentally, the difference between the two approaches will be particularly pronounced towards the end of a long experiment. When all states and actions had been visited sufficiently many times, everything that can be learned has already been learned. Thus, if the goal of exploration is to facilitate learning, then exploratory behavior is expected to fade over time. By contrast, if exploration is driven by a normative objective, then we generally expect behavior to converge to a one that (approximately) maximizing this objective, and hence maintaining asymptotic exploratory behavior.</p><p>Specifically considering Experiment 1 and 2, we do not expect any bias in S (p R = 0.5) in the beginning of the task, because participants are naive and are unaware of the different long-term consequences of the two actions. With time and learning, we expect participants to favor M R over M L (p R &gt; 0.5). This prediction holds either if participants are driven by the goal of reducing the (long-term) uncertainty associated with M R , or by the goal of optimizing some exploration objective, such as to match the choices per door in M R and M L . In other words, both approaches predict that with time, p R will increase. With more time elapsing, however, the predictions of the two approaches diverge. As uncertainty decreases, uncertainty-driven exploration predicts a decay of p R to its baseline value (p * R = 0.5). By contrast, the normative approach predicts that p R will converge to a p * R &gt; 0.5 steady-state. <ref type="figure" target="#fig_3">Figure 5</ref> depicts the temporal dynamics of p R (t), as a function of the number of times t that S was visited (defined as "episodes"). The learning curves are shown separately for the "poor" <ref type="figure" target="#fig_3">(Figure 5a</ref>) and "good" <ref type="figure" target="#fig_3">(Figure 5b</ref>) explorers, averaged over all 6 conditions of Experiments 1 and 2. As expected, there was no preference in the first episodes. However, with time, the participants developed a bias in favor of M R , which was more pronounced in the "good" directed explorers group. In this group, participants exhibited a significant bias, p R (t) &gt; 0.5 from the 3 rd episode. Notably, this increased bias was followed by a decrease to a steady state bias value (episodes 10 − 20). This steady state value was lower than its peak transient value (consistent with uncertainty-driven exploration), but was higher than baseline level before learning (consistent with a normative exploration objective). , averaged over participants in all 6 conditions (Experiments 1 &amp; 2), shown for the "poor" (a) and "good" (b) groups. The "good" explorers exhibited a transient peak in p R (t), consistent with models of uncertainty-driven exploration. However, the steady-state value p * R was still slightly larger than chance, consistent with an objective-driven exploration component. Dots and shaded areas denote mean and 95% confidence interval of p R (t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational modeling</head><p>The model Together, the two experiments of the previous sections provide us with the following insights:</p><p>(1) Humans exploration is affected by long-term consequences of actions ( In theorizing about effective exploration we have alluded to concepts such as "exploratory value" or "usefulness" of particular actions, but did not provide a precise working definition for it. In this section we consider a specific computational model for directed exploration, and test this model in view of these experimental findings. The model is a general-purpose algorithm for directed exploration, which formalizes the intuition that the challenge of exploration in complex environments is analogous to the standard credit-assignment problem in RL (in the reward-maximization sense). According to the model, the agent observes the current state of the environment s at each time-step and chooses an action a from the set of possible actions. In response to this action, the environment transfers the agent to the next state s , at which the agent chooses action a .</p><p>Each state-action pair (s, a) is associated with an exploration value, denoted E (s, a) <ref type="bibr" target="#b13">(Fox et al., 2018)</ref>. These exploration values represent a current estimate of "missing knowledge", such that a high value indicates that further exploration of that action is beneficial. At the beginning of the process, E-values are initialized to a positive constant (specifically E = 1), representing the largest possible missing knowledge. Each transition from s, a to s , a triggers an update to E (s, a) according to the following update rule:</p><formula xml:id="formula_5">E (s, a) ← E (s, a) + η (−E (s, a) + γE (s , a ))<label>(1)</label></formula><p>In words, the change in E (s, a) is a sum of two contributions. The first, −E (s, a), is the immediate reduction in the uncertainty regarding state s and action a due to the current visit of that state-action. The second, γE (s , a ) represents future uncertainty propagating back to (s, a). This second part is weighted by a discount-factor parameter, 0 ≤ γ ≤ 1. The overall update magnitude is controlled by a learning-rate parameter 0 &lt; η &lt; 1. In the particular case that s is a terminal state, its exploration value is always defined as 0.</p><p>To complete the model specification, we define the policy as derived directly from these exploration values. We use a standard softmax policy, in which the probability of choosing an action a in state s is given by: <ref type="bibr">s,a)</ref> a e βE(s,a )</p><formula xml:id="formula_6">π (a|s) = e βE(</formula><p>where β ≥ 0 is a gain parameter. A gain value of β = 0 corresponds to random exploration, with all actions chosen at equal probability, while a positive gain corresponds to (stochastically)</p><p>preferring actions associated with a larger E-value (and hence higher uncertainty).</p><p>Conceptually, this model is similar to standard RL algorithms (specifically the sarsa algroithm, <ref type="bibr" target="#b39">Rummery and Niranjan, 1994</ref>) that are used to account for operant learning in animals and humans. There, a similar update rule is used to learn the expected discounted sum of future rewards (and a similar rule is assumed for action-selection). Therefore, similar cognitive mechanisms that account for operant learning, can account for this type of directed exploration (at least to the extent that standard RL models are indeed a good descriptions of operant learning;</p><p>see <ref type="bibr" target="#b32">Mongillo et al., 2014;</ref><ref type="bibr" target="#b14">Fox et al., 2020)</ref>.</p><p>To gain insight into the properties of the E-values, we consider first the case of "infinite" discounting, namely γ = 0. In that case, the update rule of Equation 1 becomes:</p><formula xml:id="formula_8">E (s, a) ← (1 − η) E (s, a)<label>(3)</label></formula><p>and hence, after n visits of (s, a), the associated E-value is E (s, a) = (1 − η) n , such that − log E ∝ n. 1 In other words, when γ = 0, and long-term consequences are completely ignored, the E-value is effectively a visit-counter.</p><p>When γ &gt; 0, the change in the value of E (s, a) following a visit of (s, a) is more complex.</p><p>In addition to the decay term, a term that is proportional to E (s , a ) is added to E (s, a).</p><p>Notably, E (s , a ) depends on the number of past visits of (s , a ), (as well its own future states (s , a ) and so on). Consequently, the number of actual visits that is required to reduce the E-values by a given amount is larger in state-actions leading to many future states than in state-actions leading to fewer future states. In that sense, the E-values are a generalization of visit-counters.</p><p>Finally (and regardless of the value of γ), the softmax policy of Equation 2 favors actions associated with larger E-values. Because choosing these actions will generally lead to a reduction in their associated E-values, the result will be a policy that effectively attempts to equalize the E-values of all available actions (within a given state). In the case of γ = 0, this will result in a preference toward those actions that were chosen less often. In the case of γ &gt; 0, it will result in a preference that is also sensitive to (the number of) future potential states reachable through the different actions.</p><p>To conclude, the model therefore encapsulates the three principles identified in human behavior -it propagates information to track long-term uncertainties associated with individual state-actions, it temporally discounts future exploratory consequences, and it uses estimated uncertainties to derive a behavioral policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directed-exploration in the maze task</head><p>We now return to the maze task and study the behavior of the model there. In state M R ,</p><p>where the E-values correspond to visit-counters, the attempt to equalize the E-values will result in a bias against repeating the same action, yielding a low p repeat value and on average, a uniform policy. To demonstrate this, we simulated behavior of the model in the 3 conditions of Experiments 1. Indeed, as depicted in <ref type="figure" target="#fig_5">Figure 6a</ref>, the values of p repeat in the simulations were smaller than chance-level. Unlike the population of human participants, simulated agents are more homogeneous, as reflected in the narrower histograms of p repeat . This is due to the fact that the model is designed to perform directed exploration, that is, to model the behavior of the "good" directed explorers. Nevertheless, the model can also produce random exploration if the gain parameter is set to β = 0 (see also Discussion).</p><p>More interesting is the behavior of the model in state S. The larger n R , the smaller will be the decay of E (s = S, a = right) per a single visit of (s = S, a = right). Therefore, the model will tend to choose "right" more often (p R &gt; 0.5), a bias that is expected to increase with n R .</p><p>Indeed, similar to the behavior of the "good" human explorers, the simulated agents exhibited a preference towards "right" in S, a preference that increased with n R − n L <ref type="figure" target="#fig_5">(Figure 6b</ref>).</p><p>The model is sensitive to long-term consequences because it propagates future uncertainty, from the next visited state-action back to the current state-action. This future uncertainty, however, is weighted by γ &lt; 1, such that the effect of further away states on E (s, a) is expected to decrease with distance. In the environments of experiment 2, where we manipulated the depth of M R (relative to S), this will result in a decrease of the bias (p R ) at S, as demonstrated in <ref type="figure" target="#fig_5">Figure 6c</ref>.</p><p>Because the policy in the model is derived from the E-values, the temporal pattern of exploration is expected to be transient. In the first episodes, when E (s = S, a = right) = E (s = S, a = left), the result is p R = 0.5. With sufficient learning, exploration values of all visited state-actions decay to 0 and in this limit, p R = 0.5 as well. Therefore, we expect the learning dynamics to exhibit a transient increase in bias, followed by a decay back to chance level. This is demonstrated in <ref type="figure" target="#fig_5">Figure 6d</ref> where we plot p R (t), averaged over the simulations of the model in all six conditions of Experiments 1 and 2.</p><p>Qualitatively, the transient dynamics resemble the experimental results <ref type="figure" target="#fig_3">(Figure 5b</ref>). However, there are two important differences. First, while the human participants exhibited what seems like a steady-state bias even at the end of the experiment, p R in the model decays to chance level.</p><p>As discussed above, the decay to chance in the simulations is expected because exploration in the model is uncertainty-driven. In the framework of this model, steady-state exploration can be achieved if we assume that β is not stationary, but rather increases over episodes. However, we hypothesize that to capture this aspect of humans' exploration, we may need to go beyond this class of uncertainty-driven models. Second, the transient dynamics of the model are longer than that of the human participants. While the learning speed in the model is largely controlled by the learning-rate parameter η, the value of η cannot by itself explain this gap. This is because in the model η &lt; 1, and the dynamics cannot be arbitrarily fast. Particularly, in the simulations of <ref type="figure" target="#fig_5">Figure 6d</ref> we have used a large learning-rate of η = 0.9, but learning was still considerably slower compared to human participants. We further discuss the issue of learning speed in the next section.</p><p>Learning dynamics: 1-step updates and trajectory-based updates</p><p>To learn to prefer "right" in S, the agent needs to learn that this action leads, in the future, to M R , which from an exploratory point of view is superior to M L . This kind of learning of delayed outcomes is typical of RL problems, in which the agent needs to learn that the value of a particular action stems from its consequences, which can be delayed. For example, an action may valuable because it leads to a large reward, even if this reward is delayed. In the RL literature this is known as the credit assignment problem, because during learning, upon observing a desired outcome (in "standard" RL, getting a large reward; here, arriving at M R ), the agent needs to properly assign credit for past actions that have led to this outcome.</p><p>RL algorithms typically address the credit assignment problem by propagating information about the reward backwards through sequences of visited states and actions <ref type="bibr" target="#b47">(Sutton, 1988;</ref><ref type="bibr" target="#b53">Watkins and Dayan, 1992;</ref><ref type="bibr" target="#b9">Dayan, 1992)</ref>. According to some RL algorithms, the information about the reward propagates backwards one state at a time. By contrast, in other algorithms, a trace of the entire trajectory is maintained, allowing the information to "jump" backwards over a large number of states and actions. We refer to these alternatives as 1-step and trajectory-based updates, respectively.</p><p>The E-values model can be understood as an RL algorithm that propagates visitations information (rather than reward information). Specifically, it uses 1-step updates (Equation 1) such that with each observation (a transition of the form s, a, s , a ) only immediate information, from (s , a ), is used to update the exploration value of (s, a). With 1-step updates it takes time (episodes) for information from M R to reach back to S. We hypothesized that this reliance on 1-step updates might be an important source for the difference in learning speed between the model and humans, who might use more temporally-extended learning rules. To test this, we considered an extension to the exploration model in which E-values are learned using a trajectory-based update rule. Technically, this corresponds to changing the TD algorithm of Equation 1 to a TD (λ) algorithm (see Methods, Algorithm 1). Simulating this extended model we found that, similar to the original model, it reproduces the main experimental findings ( <ref type="figure" target="#fig_0">Figure S1</ref>, compare with <ref type="figure" target="#fig_5">Figure 6</ref>). Moreover, as predicted, learning is faster than that the learning in the original model ( <ref type="figure" target="#fig_0">Figure S1d</ref>, compare with <ref type="figure" target="#fig_5">Figure 6d</ref>). Nevertheless, even this faster learning is still slower than the rapid learning observed in human participants, suggesting further components of human learning that are not captured by either of the models (we get back to this point in the Discussion).</p><p>Another way of distinguishing between 1-step and trajectory-based updates is to consider the predictions they make in Experiment 2. Recall that the three conditions in Experiment 2 differ in the delay (in the sense of number of states) between S and M R . If information (about the exploratory "value" of M R ) propagates one step at a time, then the time it takes to learn that "right" is preferable in S will increase with the delay: it will be shortest in Condition 1, in which M R and M L are merely one step ahead of S, and longest in Condition 3, in which M R and M L are three steps away from S <ref type="figure" target="#fig_6">(Figure 7, top left)</ref>. By contrast, if information about M R and M L can "jump" directly to S within each episode, as in trajectory-based updates, learning speed will be comparable in all three conditions <ref type="figure" target="#fig_6">(Figure 7, top right)</ref>. A more thorough analysis of the model dependence on the parameters γ and λ is depicted in <ref type="figure" target="#fig_1">Figure S2</ref>. Finally, <ref type="figure" target="#fig_6">Figure 7</ref> (bottom) depicts the learning dynamics of the "good" human explorers, analyzed separately in the three conditions of Experiment 2. We did not find evidence supporting the hypothesis that learning time increases with depth. These results further support the hypothesis that human learning relies on more global, temporally-extended update rules in which information can "jump" backwards over several states and actions. : 1-step backups and trajectory-based updates. Learning dynamics simulated by the E-values model using the 1-step backup learning rule of TD (0) (Equation 1-2; top left) and the trajectory-based learning rule TD (λ) (Methods, Algorithm 1; top right) in the environments of Experiment 2. With TD (0), the depth of M R relative to S (depth = 1, 2, 3) affects both the peak value of p R (t) (due to temporal discounting) and the time it takes the model to learn (due to the longer sequence of states over which the information has to be propagated). By contrast, with TD (λ), different depths result in a different maximum bias (due to temporal discounting), but the learning time is comparable (because information is propagated over multiple steps in each update). For the same reason, learning is overall faster with TD (λ). In humans (bottom), peak bias decreased with depth (consistent with temporal-discounting), but there was no noticeable difference in learning speed (consistent with trajectory-based updates). Learning curves of human participants are shown with a moving-average of 3 episodes. Dots and shaded areas denote means and 70% confidence intervals of p R (t).</p><p>Model results are average over 30, 000 simulations; model parameters: η = 0.9, β = 5, γ = 0.6, and λ = 0.6 (for the TD (λ) model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Exploration is a wide phenomenon that has been linked to different aspects of behavior, including foraging <ref type="bibr" target="#b31">(Mobbs et al., 2018;</ref><ref type="bibr" target="#b23">Kolling and Akam, 2017)</ref>, curiosity <ref type="bibr" target="#b18">(Gottlieb and Oudeyer, 2018)</ref>, and creativity <ref type="bibr" target="#b20">(Hart et al., 2018)</ref>. In this study, we focused on exploration as part of learning. For that, we use the framework of RL, in which exploration is an essential component.</p><p>Particularly, we study the computational principles underlying human exploration in complex environments -sufficiently complex such that exploration per se requires learning, due to delayed and long-term consequences of actions. Our approach builds on the analogy between the challenges of learning to explore, and the challenges of learning to maximize reward -the latter being the standard RL scenario. In both cases, the agent needs to represent information, propagate it, and use it to choose actions. In the former case it is information about uncertainty and in the latter it is information about expected reward.</p><p>We found that while exploring in complex environments, humans are sensitive to long-term consequences of actions and not only to local measures of uncertainty. Moreover, such longterm exploratory consequences are temporally-discounted, similar to the discounting of future rewards. Finally, the dynamics of exploration is consistent with the predictions of uncertaintydriven exploration, in which directed exploratory behavior peaks transiently, and then decay to a more random exploration (supposedly when most of the uncertainty have been resolved).</p><p>To account for these experimental results, we introduce a computational model that uses a RL-like learning rule implementing the aforementioned principles. In the model, information about state-action visits, rather than about reward as in standard RL algorithms, is being propagated (and discounted) over sequences visited state-actions. This results in a set of "exploration values" (analogous to reward-based values) which are then used to choose actions.</p><p>Directed exploration beyond bandit tasks Previous studies have identified some components of directed exploration in human behavior using bandit tasks <ref type="bibr" target="#b55">(Wilson et al., 2014;</ref><ref type="bibr" target="#b15">Gershman, 2018</ref><ref type="bibr" target="#b16">Gershman, , 2019</ref>, particularly, the use of counter-based methods such as Upper Confidence Bounds <ref type="bibr">(UCB, Auer et al., 2002)</ref>. Going beyond the bandit, we were able to show that these counter-based strategies might be a special case implementation (appropriate for bandit tasks) of more general principles. To study and identify these principles, it is therefore necessary to test human exploration in environments that are more complex than the bandit task.</p><p>Indeed a more recent study have shown that more general principles might underlie human exploration, both random and directed, in sequential tasks <ref type="bibr" target="#b54">(Wilson et al., 2020)</ref>. However, unlike our experiments, in that study actions did not have long-term consequences in the sense of state transitions. Finally, the necessity of going beyond simple bandit tasks is not unique to the study of exploration alone. It is present also when studying other components of RL algorithms underlying operant learning. For example, it is impossible to distinguish in a bandit task between model-based and model-free RL, because there is no "model" to be learned in those tasks <ref type="bibr" target="#b8">(Daw et al., 2011)</ref>.</p><p>Non-stationary aspects of exploration While the analogy between learning to explore and learning to maximize rewards is a useful one, there are some important differences. One difference is that while in RL, rewards (more precisely, the distribution thereof) are typically assumed to be Markovian and stationary, exploration has a fundamental non-stationary nature.</p><p>This is due to the fact that if exploration is interpreted as part of the learning process, or is uncertainty driven, then the exploratory "reward" from a given state-action will decrease over time, because uncertainty will reduce with visits of that state-action. This non-stationarity poses a challenge for exploration algorithms. The E-values model circumvents that by assuming a stationary (and constant) zero fictitious "reward", combined with an optimism bias at initialization <ref type="bibr" target="#b13">(Fox et al., 2018)</ref>.</p><p>A different solution to the challenge of non-stationarity is to posit an exploration objective function which is by itself independent of learning. The predictions of the two classes of models differ with respect to the expected steady-state behavior. In the former, exploration will diminish over time while in the latter, it will be sustained. The observation that human participants maintain a preference (albeit relatively small) for "right" even at the end of the experiment suggests that human exploration is driven, at least in part, by more than just uncertainty. A more complete characterization of these two components will be an interesting topic for future work.</p><p>Pure-exploration and the role of reward It has been long argued that at least part of human and animal behavior is driven by intrinsic motivation, which is largely independent of external rewards <ref type="bibr" target="#b37">(Oudeyer and Kaplan, 2009;</ref><ref type="bibr" target="#b1">Barto, 2013)</ref>. Pure exploration tasks can be used to characterize aspects of such intrinsic motivation. In this study, the "desire" to visit less-visited states is one such intrinsic motivation factor. Additional factors that are based on information-theoretic quantities <ref type="bibr" target="#b45">(Still and Precup, 2012;</ref><ref type="bibr" target="#b27">Little and Sommer, 2014;</ref><ref type="bibr" target="#b22">Houthooft et al., 2016)</ref> or prediction errors of non-reward signals <ref type="bibr" target="#b38">(Pathak et al., 2017;</ref><ref type="bibr" target="#b6">Burda et al., 2019)</ref> have also been proposed in the literature. While many of these will, in general, be correlated, and hence difficult to identify experimentally, we believe that future studies of pure-exploration in complex environments will allow to better relate these concepts, mostly discussed in the theoretical and computational literature, to the learning and behavior of humans and animals.</p><p>To dissect the exploratory component of behavior, we focused on a pure-exploration, rewardfree task. This allowed us to neutralize the exploration-exploitation dilemma, focusing on the unique challenges for exploration itself. More generally, we expect the identified exploration principles to be relevant also in the reward maximization scenario. Indeed, it has been shown theoretically and empirically that the naive use of counter-based methods (or other "local" exploration techniques) can be highly sub-optimal for learning an optimal policy (in the reward maximization sense) in complex environments <ref type="bibr">(Osband et al., 2016a,b;</ref><ref type="bibr" target="#b7">Chen et al., 2017;</ref><ref type="bibr" target="#b13">Fox et al., 2018;</ref><ref type="bibr" target="#b33">Oh and Iyengar, 2018)</ref>. How humans deal with the exploration-exploitation dilemma in complex environments is an important open question.</p><p>Implications for neuroscience Algorithms such as TD-learning hold considerable sway in neuroscience. For example, it is generally believed that dopaminergic neurons encode reward prediction errors, which are used for learning the "values" of states and actions <ref type="bibr" target="#b41">(Schultz et al., 1997;</ref><ref type="bibr" target="#b17">Glimcher, 2011</ref>, but see also Elber-Dorozko and Loewenstein, 2018). More recent studies suggest that in fact, the brain maintains a separate representation of different reward dimensions <ref type="bibr" target="#b44">(Smith et al., 2011;</ref><ref type="bibr" target="#b19">Grove et al., 2022)</ref>. Given that our formalism of uncertainty (E-values) is identical to that of other types of value, it would be interesting to test whether the representation of uncertainty in the brain is similar to that of other reward types. For example, whether dopaminergic neurons also represent the equivalent of E-values TD-error.</p><p>Along the same lines, it would be interesting to check whether the finding that dopaminergic neurons encode what seems to be reward-independent features of the task (Engelhard et al., 2019) can be better understood assuming that uncertainty is a reward-like measure.</p><p>Heterogeneity There was a substantial heterogeneity among participants in both Experiments 1 and 2. We used this heterogeneity to divide participants into "good" and "poor" explorers in terms of the "directedness" of their exploration. However, this division is somewhat crude. For example, while bias in favor of M R was smaller in the "poor" explorers, it was still larger than the baseline level of 0.5 predicted by a true random exploration behavior <ref type="figure" target="#fig_3">(Figure 5a</ref>). This separation can be understood as a first approximation, highlighting the more prominent source of exploratory behavior at the individual subject basis. Moreover, even within the "good" explorers, there was considerable variability. Heterogeneity in the parameters of the computational model can, perhaps, explain some of the heterogeneity, but parameters variability alone (within the E-values model) certainly cannot explain all of the heterogeneity in participants' behavior. For example, consider again the division to "poor" and "good" directed explorers. In principle, such a division could be modeled through the gain parameter β, with random explorers having a value of β = 0 (and directed explorers a value of β &gt; 0). Even with random exploration, the model prediction for p repeat is 1/n R . By contrast, many participants exhibited values of p repeat larger than this chance-level, all the way up to p repeat = 1. Similarly, considering behavior at S as measured by p R , no combination of model parameters predict p R values which are smaller than 0.5. This is because even random exploration will result in p R = 0.5. Values of p R that are close to 1 are also impossible in the model, because they imply under-exploration of the left-hand-side of the maze. Yet some human participants exhibited extreme (close to 0 or 1) values of p R . Other factors, such as (task-independent) choice bias <ref type="bibr" target="#b2">(Baum, 1974;</ref><ref type="bibr" target="#b24">Laquitaine et al., 2013;</ref><ref type="bibr" target="#b26">Lebovich et al., 2019)</ref> and tendency to repeat actions <ref type="bibr" target="#b51">(Urai et al., 2019)</ref> are likely to contribute to participants' choices.</p><p>Learning speed Another limitation of the model is the gap between the learning speed of human participants and the learning speed of the model. Overall, humans learned considerably faster than the model, even with a large learning-rate. On average participants exhibited a bias as soon as the 3 rd episode, which is faster than the theoretical limit possible for the TD(0) model in this task. While some of this discrepancy can be attributed to the model's reliance on 1-step backups, it is noteworthy that even in comparison with TD(λ), humans' learning is faster than the that of the model. The rapid learning in humans suggest mechanisms that go beyond simple model-free learning as implemented in our models. In our model, the fact that "right" is favorable can only be learned implicitly, by actually visiting more unique states</p><formula xml:id="formula_9">following M R (compared to M L )</formula><p>. This is because the only information that is available to the agent is the identity of states and actions. By contrast, a single visit of both M R an M L is likely sufficient for humans to learn that the number of doors in M R is larger than in M L , a fact which can by itself bias their following choices in favor of "right". Indeed by using this (possibly salient) feature, of the number of doors, as an explicit part of the state representation, one could infer that M R is more favorable over M L already after 2 episodes even with model-free learning. While such strategy is not as general as the computational principles encapsulated by our models, in the specific task at hand it will be rather effective. The ability of humans to rapidly form and utilize such heuristics and generalizations is likely an important part of their ability to rapidly adapt and learn in novel situations. The interplay between basic, more general-purpose, computational principles, and heuristic, more ad-hoc, principles remains an important challenge for computational modeling in the cognitive sciences.</p><p>Generalization, priors, and "natural" exploration The goal of this study was to identify computational principles underlying exploration in a "general" setting. To that goal, we used a task in which the semantic content attached to states was minimal, with no a-priori indication of any structure (temporal, geometric, spatial, etc.) of the state-space. The motivation behind this design was to de-emphasize, as much as possible, behavior components stemming from participants' prior knowledge and generalization abilities, and focus on core exploratory strategies.</p><p>This also justified the models that we used: general-purpose, simplistic, learning models that operate on an abstract notion of states and actions. On the other hand, the abstract design of the task limits its applicability to more realistic tasks and natural behavior. Indeed in complex environments, it has been demonstrated that humans rely largely on both priors and generalizations to achieve efficient learning and exploration <ref type="bibr" target="#b10">(Dubey et al., 2018;</ref><ref type="bibr" target="#b42">Schulz et al., 2020)</ref>. How such priors, semantic knowledge, and generalization interact with more abstract and general principles of exploration and decision-making is an important open question. Notably, we have found that humans are capable of performing directed exploration of complex environments even in the absence of a readily-available semantic structure to guide their exploration. This is in contrast to the recent work of <ref type="bibr" target="#b5">Brändle et al. (2022)</ref>, that demonstrated directed exploration (interpreted as driven by the information-theoretic quantity of empowerment) in complex environments with available semantic structure, that was not observed in a structurally identical task where the semantic structure has been masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online experiments and data collection</head><p>The study was approved by the Hebrew University Committee for the Use of Human Subjects in Research. Participants were recruited using the Amazon MechanicalTurk online platform, and were randomly assigned to one of the conditions in each experiment. Participants were instructed to "understand how the rooms are connected", and were informed regarding the test phase: "At the end of the task, a test will check how quickly can you get from one specific room to a different one.". The training phase of the experiment consisted of 120 trials, corresponding to 20 episodes. Between 20% to 30% of participants (depending on the experiment and condition) performed a longer experiment of 250 trials corresponding to 42 episodes, but for these participants only the first 20 episodes were analyzed. The end of each episode (reaching the terminal state T ) was signaled by a message screen ("Youv'e reached a dead-end room, and will be moved back to the first room"). After the training episodes, there was a test phase in which participants were asked to navigate to a target room in the minimal number of steps possible, starting from a particular start room (which was not the initial state S). An online working copy of the experiment can be accessed at:</p><p>https://decision-making-lab.com/lf/eee_rep/Instructions.php .</p><p>For each participant, we recorded the sequence of visited rooms (states) and chosen doors (actions), in the train and test phases. No other details (including demographics details, questionnaire, or comments about the experiment) were collected from participants. Test performance was used as a criterion for filtering. Out of the total participants who finished the experiment (i.e., finished both training and test phases), we rejected those who did not finish the test phase in a number of steps smaller than expected by chance (e.g., the expected number of steps it would take to reach the target by random walk). We also rejected participants who, during training, did not choose both "right" and "left" at least twice. The test start and target rooms were identical for all participants, and were chosen as to maximize the difference between performance (i.e., number of steps) expected by chance to that of the optimal (shortest path)</p><p>policy. The number of participants in each experiment is given in <ref type="table" target="#tab_2">Table 1</ref>, and their division into "Good" and "Poor" explorers is given in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating policy from behavior</head><p>For the average results, we computed for each participant their p R value as the number of "right" choices divided by the total (and fixed) number of visits to S. Similarly, p repeat was calculated for individual participants as the number of visits to M R in which the chosen action was identical to the one chosen in their previous visit of M R , divided by the total visits of M R minus one. Note that the total number of visits to M R was different for different participants,</p><p>as it depended on their policy at S. We have used the same measurements for the results of the model simulations for consistency. Note that, in principle, the model allows to measure the policy of individual agents (at individual time-points) directly, without the need to estimate it from behavior (i.e., the generated stochastic choices). To estimate learning dynamics, we can no longer estimate p R (t) on an individual level, because each participant only made one binary choice at a given episode. Therefore, we computed p R (t) at the population level, as the number of participants who chose "right" in the t th episode divided by the total number of participants (possibly within a particular group, for example only "good" explorers). Alternatively, when considering specific experimental conditions, we have estimated p R (t) for individual participants using a moving-average over a window of 3 consecutive episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>Confidence Intervals (CI) for p R were computed using bootstrapping, by resampling participants and choices. Comparisons between different conditions were computed using a permutation test, by shuffling all participants of the two groups being compared, and resampling under the null hypothesis of no group difference. With this resampling we computed the distribution of p R (A) − p R (B) for two random shuffled groups of participants A and B. Reported p-value is the CDF of this distribution evaluated at the real (unshuffled) groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TD (λ) learning for E-values</head><p>We start by proving a short, non-technical description of the TD and TD (λ) value-learning algorithms. The value of a state-action (denoted Q (s, a)), is defined as the expected sum of (discounted) rewards achieved following that state-action. The goal of the algorithms is to learn these values. To that end, the agent maintains and updates estimatesQ (s, a) of the true state-action values Q (s, a). In TD-learning, Upon observing a transition <ref type="figure">(s, a, r, s , a )</ref>, the estimated value (Q (s, a)) is updated towards r + γQ (s , a ). Crucially,Q (s , a ) is also, on its own, an estimated value. This usage of (a part of) the current estimator to form the target for updating the same estimator is known as bootstrapping. TD learning therefore breaks the estimation of value -the sum of rewards -into two parts: the first reward, which is taken from the environment, and the rest of the sum, which is bootstrapped.</p><p>It is possible, however, to estimate the values while breaking the sum of rewards in other ways.</p><p>For example one could sum the first two rewards based on observations, and bootstrap the rest, that is, from time-step 3 on-wards. Importantly, this would result in information (about the rewards) propagating backwards 2-steps in a single update, rather than 1-step. More generally, breaking the sum after n steps will result in an n-step backup learning rule. It is also possible to average multiple n-step backups in a single update. The TD (λ) algorithm is a particular popular scheme to do that: it can be understood as combining all possible n-step backups, with a weighting function that decays exponentially with n (i.e., the weight given to the n-step backup is λ n−1 , where λ is a parameter). With λ = 0 the algorithm recovers the standard 1-step backup algorithm, or in other words, TD (0) is simply TD. A value of λ = 1 corresponds to no bootstrapping at all, relying instead on Monte Carlo estimates of the action value by collecting direct samples (sum of rewards over complete trajectories). 2</p><p>Equation 1 can be understood as a TD algorithm (specifically, using the sarsa algorithm <ref type="bibr" target="#b39">(Rummery and Niranjan, 1994;</ref><ref type="bibr" target="#b48">Sutton and Barto, 2018)</ref>) in the particular case that all the rewards signals are assumed to be r = 0, and estimates are initialized at 1. The extended model (Algorithm 1) is a direct generalization of that correspondence to the TD (λ) case.</p><p>Require: Parameters η, λ, γ Initialize E (s, a) = for all s, a for all episodes do set ε (s, a) = for all s, a eligibility-traces set τ = {} trajectory in this episode set s to the initial state and choose action a while s is not a terminal state do sample the next state and action s , a increment ε (s, a) ← ε (s, a) + 1, and concatenate (s, a) to τ for all (s t , a t ) in τ do E (s t , a t ) ← E (s t , a t ) + ηε (s t , a  <ref type="figure" target="#fig_0">Figure S1</ref>: Simulations results of TD (λ). Simulating behavior of the E-values model with the TD (λ) learning rule (Methods, Algorithm 1) reproduces the main findings of directed exploration in the maze task. (a) In M R , the model exhibits directed exploration which manifests in low values of p repeat (shown for the 3 conditions of Experiment 1; dashed line denote chance-level expected for random exploration, 1/n R ) (b) In the environments of Experiment 1, agents exhibited bias towards M R that increased with imbalance of n R : n L , reflecting the propagation of long-term uncertainties over states. (c) In the environments of Experiment 2, the bias decreased with depth, reflecting temporal discounting. (d) Bias towards M R peaks transiently, followed by a decay to baseline at steady-state, as expected from uncertainty-driven exploration (average results over all 6 environments). The learning dynamics is faster than that of the 1-step update model. Results are based on 3,000 simulations in each environment. Bars and histograms in (a)-(c) are shown for the first 20 episodes to match the behavioral experiments. Model parameters: η = 0.9, β = 5, γ = 0.6, λ = 0.6. <ref type="figure" target="#fig_1">Figure S2</ref>: Model parameters. Learning curves of the TD (λ) model in the 3 environments of Experiment for different values of γ,λ (with fixed η = 0.9, β = 5). With infinite discounting (γ = 0), future consequences are neglected, resulting in a uniform (counter-based like) policy with no bias. With no discounting (γ = 1), information from the terminal state T dominates, resulting in a bias towards "right" (since there are more routes to the terminal states via the "right" branch) that is not dependent of the depth of M R . For intermediate values of γ, transient exploration opportunities (i.e., in M R ) becomes important, resulting in a bias towards M R that decreases with depth, reflecting temporal-discounting. In this regime, one-step backup learning rule (λ = 0) results in difference learning speed for different depths, while for trajectory-based learning rules (λ &gt; 0) learning speed is comparable for the different depths. Each learning curve is the average of 30, 000 simulations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Directed exploration in complex environments. (a) In a bandit problem (left), actions have no long-term consequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"Figure 2 :</head><label>2</label><figDesc>good" explorers rely on visit-counters that are the relevant measure of uncertainty, resulting in an overall uniform choice. By contrast in S, actions do have long-term consequences, and "good" explorers go beyond the visit-counters, biasing their choices in favor of the action associated with more future uncertainty. Heterogeneity in exploration strategies. Top: Histograms of p repeat at state M R (highlighted in yellow) for participants in the three conditions of Experiment 1 (left to right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Learning dynamics. Bias towards M R as a function of training episode (p R (t))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 1c); (2) Both the number of future states and their depth affect this exploration(Figure 3andFigure 4); and finally, (3) Exploration dynamics peaks transiently and then decays, consistent with an uncertainty-driven exploration (Figure 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Simulations results. Simulating behavior of the E-values model (Equation 1-2) reproduces the main findings of directed exploration in the maze task. (a) In M R , the model exhibits directed exploration which manifests in low values of p repeat (shown for the 3 conditions of Experiment 1; dashed line denote chance-level expected for random exploration, 1/n R ) (b) In the environments of Experiment 1, agents exhibited bias towards M R that increased with imbalance of n R : n L , reflecting the propagation of long-term uncertainties over states. (c) In the environments of Experiment 2, the bias decreased with depth, reflecting temporal discounting. (d) Bias towards M R peaks transiently, followed by a decay to baseline at steady-state, as expected from uncertainty-driven exploration (average results over all 6 environments). Results are based on 3,000 simulations in each environment. Bars and histograms in (a)-(c) are shown for the first 20 episodes for comparison with the behavioral experiments. Error bars are negligible and therefore are not shown. Model parameters: η = 0.9, β = 5, γ = 0.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7: 1-step backups and trajectory-based updates. Learning dynamics simulated by the E-values model using the 1-step backup learning rule of TD (0) (Equation 1-2; top left) and the trajectory-based learning rule TD (λ) (Methods, Algorithm 1; top right) in the environments of Experiment 2. With TD (0), the depth of M R relative to S (depth = 1, 2, 3) affects both the peak value of p R (t) (due to temporal discounting) and the time it takes the model to learn (due to the longer sequence of states over which the information has to be propagated). By contrast, with TD (λ), different depths result in a different maximum bias (due to temporal discounting), but the learning time is comparable (because information is propagated over multiple steps in each update). For the same reason, learning is overall faster with TD (λ). In humans (bottom), peak bias decreased with depth (consistent with temporal-discounting), but there was no noticeable difference in learning speed (consistent with trajectory-based updates). Learning curves of human participants are shown with a moving-average of 3 episodes. Dots and shaded areas denote means and 70% confidence intervals of p R (t). Model results are average over 30, 000 simulations; model parameters: η = 0.9, β = 5, γ = 0.6, and λ = 0.6 (for the TD (λ) model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>t ) (γE (s , a ) − E (s, a)) update E-value ε (s t , a t ) ← γλε (s t , a t ) decay eligibility-trace end for s ← s , a ← a end while E (s, a) ← (1 − η) E (s, a)update in terminal-state end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>.</cell><cell></cell></row><row><cell cols="4">Exp. Env. Completed Included</cell></row><row><cell></cell><cell>3 : 4</cell><cell>191</cell><cell>161</cell></row><row><cell>1</cell><cell>2 : 5</cell><cell>174</cell><cell>120</cell></row><row><cell></cell><cell>: 6</cell><cell>176</cell><cell>137</cell></row><row><cell></cell><cell>d = 1</cell><cell>244</cell><cell>191</cell></row><row><cell>2</cell><cell>d = 2</cell><cell>269</cell><cell>205</cell></row><row><cell></cell><cell>d = 3</cell><cell>282</cell><cell>238</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Number of participants in Experiments 1 and 2.</figDesc><table><row><cell cols="4">Exp. Env. "good" explorers "poor" explorers</cell></row><row><cell></cell><cell>3 : 4</cell><cell>66</cell><cell>95</cell></row><row><cell>1</cell><cell>2 : 5</cell><cell>53</cell><cell>67</cell></row><row><cell></cell><cell>1 : 6</cell><cell>71</cell><cell></cell></row><row><cell></cell><cell>d = 1</cell><cell>92</cell><cell>99</cell></row><row><cell></cell><cell>d = 2</cell><cell>84</cell><cell></cell></row><row><cell></cell><cell>d = 3</cell><cell>85</cell><cell>153</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Participant groups in Experiments 1 and 2</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">because η &lt; 1, we have that log (1 − η) &lt; 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Implicitly assuming an episodic setting, in which every episode terminates after a finite number of steps.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Lea Kaplan for technical support, and Gal Yarden for discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intrinsic motivation and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intrinsically motivated learning in natural and artificial systems</title>
		<editor>Baldassarre, G. and Mirolli, M.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On two types of deviation from the matching law: Bias and undermatching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Experimental Analysis of Behavior</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="242" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brändle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Stocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Intrinsically motivated exploration as empowerment</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploration by random network distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01502</idno>
		<title level="m">Ucb exploration via q-ensembles</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-based influences on humans&apos; choices and striatal prediction errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1204" to="1215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The convergence of td (λ) for general λ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating human priors for playing video games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elber-Dorozko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
		<title level="m">Striatal action-value neurons reconsidered. eLife</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">34248</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Specialized coding of sensory, motor and cognitive variables in vta dopamine neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ornelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Koay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Thiberge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">570</biblScope>
			<biblScope unit="issue">7762</biblScope>
			<biblScope unit="page" from="509" to="513" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DORA the explorer: Directed outreaching reinforcement action-selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploration: from machines to humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elber-Dorozko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Curiosity (Explore vs Exploit</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deconstructing the human algorithms for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Uncertainty and exploration. Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">277</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="15647" to="15654" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards a neuroscience of active sampling and curiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="758" to="770" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dopamine subsystems that track internal states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>La Santa Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Corpuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Berke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kreitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">608</biblScope>
			<biblScope unit="issue">7922</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Z. A.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Creative exploration as a scale-invariant search on a meaning landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Striem-Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Provably efficient maximum entropy exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Soest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vime: Variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">(reinforcement?) learning to forage optimally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computational Neuroscience</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complex population response of dorsal putamen neurons predicts the ability to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laquitaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abellanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boraud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bandit Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Idiosyncratic choice bias naturally emerges from intrinsic stochasticity in neuronal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lebovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Darshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hansel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1190" to="1202" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning and exploration in action-perception loops. Closing the Loop Around Neural Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Sommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">295</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07700</idno>
		<title level="m">Learning purposeful behaviour in the absence of rewards</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unpacking the exploration-exploitation tradeoff: A synthesis of human and animal literatures. Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Braithwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hausmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">191</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploration of multi-state environments: Local measures and back-propagation of uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bourgine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="117" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Foraging for foundations in decision neuroscience: insights from ethology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Trimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Blumstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="419" to="427" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The misbehavior of reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mongillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shteingart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="528" to="541" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Directed exploration in pac model-free reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10552</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Balcan, M. F. and Weinberger, K. Q.</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01310</idno>
		<title level="m">Count-based exploration with neural density models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? a typology of computational approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Curious model-building control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="1458" to="1463" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A neural substrate of prediction and reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5306</biblScope>
			<biblScope unit="page" from="1593" to="1599" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Finding structure in multi-armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">101261</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of first impression in operant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shteingart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">476</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Disentangling pleasure from incentive salience and learning signals in brain reward circuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Berridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Aldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="255" to="264" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to curiosity-driven reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Still</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory in Biosciences</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reinforcement driven information acquisition in non-deterministic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Storck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on artificial neural networks</title>
		<meeting>the international conference on artificial neural networks<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reinforcement learning : an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main"># exploration: A study of count-based exploration for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deturck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2753" to="2762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Efficient exploration in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Choice history biases subsequent evidence accumulation. eLife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>De Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">46331</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Delay discounting: pigeon, rat, human-does it matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vanderveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Animal learning and cognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep exploration as a unifying account of explore-exploit behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sadeghiyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>PsyArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Humans use directed and random exploration to solve the explore-exploit dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ludvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2074</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Reward is enough for convex mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O' Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Ranzato, M.,</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25746" to="25759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational policy gradient method for reinforcement learning with general utilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4572" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
