<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Representations by Humans, for Humans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Hilgard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Rosenfeld</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahzarin</forename><forename type="middle">R</forename><surname>Banaji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Representations by Humans, for Humans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new, complementary approach to interpretability, in which machines are not considered as experts whose role it is to suggest what should be done and why, but rather as advisers. The objective of these models is to communicate to a human decision-maker not what to decide but how to decide. In this way, we propose that machine learning pipelines will be more readily adopted, since they allow a decision-maker to retain agency. Specifically, we develop a framework for learning representations by humans, for humans, in which we learn representations of inputs (&apos;advice&apos;) that are effective for human decision-making. Representationgenerating models are trained with humans-in-the-loop, implicitly incorporating the human decision-making model. We show that optimizing for human decisionmaking rather than accuracy is effective in promoting good decisions in various classification tasks while inherently maintaining a sense of interpretability. * Equal contribution, alphabetical order. Preprint. Under review.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Across many important domains, machine learning algorithms have become unparalleled in their predictive capabilities. The accuracy and consistency of these algorithms has made them highly appealing as tools for supporting human decision-making <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46]</ref>. However, these criteria are far from comprehensive <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5]</ref>. Our continued reliance on humans as the final arbiters of these decisions suggests an awareness that incorporating higher-level concepts, such as risk aversion, safety, or justification, requires the exercise of human reasoning, planning, and judgment.</p><p>The field of interpretable machine learning has developed as one answer to these issues. A common view of interpretable ML is that it provides explanations <ref type="bibr" target="#b40">[41]</ref>, thereby allowing integration into the human reasoning process, and verification as to whether or not auxiliary criteria are being met. Under this framework, the algorithm is an expert whose task is to suggest what should be done, and, from its own perspective, why. The human role is reduced to that of quality control: should the algorithm's work be accepted or rejected? This role of 'computer as expert' undermines a decision-maker's sense of agency and generates information that is difficult to integrate with existing intuition. Hence, users may be reluctant to accept algorithmic suggestions or even inclined to go against them <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref>, especially after seeing the algorithm make errors, which can lead to a degradation in performance over time <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. In any system in which humans make the final decisions, even highly-accurate machine outputs are only useful if and when humans make appropriate use of them; c.f. the use of risk assessment tools in the context of sentencing <ref type="bibr" target="#b59">[60]</ref>. <ref type="figure">Figure 1</ref>: Examples of visualized advice for various inputs: word highlighting for text data, customized plots for embedded data, and computerized avatars for structured data. Instead of explaining algorithmic predictions, we learn representations that directly aid in human decision-making.</p><p>Fortunately, advice that conveys how to decide (rather than what) can often be of great value <ref type="bibr" target="#b12">[13]</ref>. Advice of this form can be designed to augment the capabilities of human decision makers, rather than replace them, which many see as a more socially-optimal role for AI <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref>. This can be achieved, for example, by highlighting certain aspects of the problem, providing additional information, presenting tradeoffs in risks and returns, or outlining possible courses of action. There is ample empirical evidence suggesting that informative advice can, by acknowledging the central role decision makers play, both enhance performance and retain agency <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Motivated by the above, we advocate for a broader perspective on how machine learning can be used to support decision-making. Our work builds on a well-known observation in the social sciences, which is that the performance of humans on decision tasks depends on how problems are presented or framed <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref> To leverage this idea, we shift the algorithmic focus from learning to predict to learning to represent, and seek representations of inputs ('advice') that will lead to good decisions and thus good outcomes when presented to a human decision maker. Our framework is designed to use machine learning in a way that preserves autonomy and agency, and in this way builds trust-crucial aspects of decision-making that are easy to overlook <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>To successfully reframe difficult problems, we harness the main engine driving deep learning-the ability to learn useful representations. Just as deep neural networks learn representations under which classifiers predict well, we learn representations under which human decision makers perform well. Our model includes three main components: a "truncated" neural network that maps inputs into vector representations, a visualization module that maps vector representations into visual representations, and a human decision maker. Our main innovation is a human-in-the-loop training procedure that seeks to directly optimize human decision outcomes, thus promoting both accuracy and agency.</p><p>We demonstrate the approach on three experimental tasks, represented in <ref type="figure">Figure 1</ref>, that cover different types of decisions and different forms of computational advice, and in problems with increasing complexity. Both training and evaluation are done with the aid of real human subjects, which we argue is essential for learning credible human-supportive tools. Our results show that we can iteratively learn representations that lead to high human accuracy while not explicitly presenting a recommended action, providing users with means to reason about decisions. Together, these results demonstrate how deep learning can serve as an instrumental tool for human intelligence augmentation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Interpretability as decision support. There are several ways in which interpretability can be used to support decision-making. In general, interpretability can help in evaluating criteria that are important for decisions but hard to quantify, fairness or safety for example, and hence hard to optimize <ref type="bibr" target="#b16">[17]</ref>. Many methods do this by producing simplified <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref> or augmented <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38]</ref> versions of the input that aids users in understanding if the data is used in ways that align with their goals or not. While some methods exist for systematically iterating over models <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b35">36]</ref>, these give no guarantees as to whether models actually improve with respect to user criteria. Virtually all works in interpretability focus on predictive algorithms. Our work differs in that the focus is directed at the human-decision maker, directly optimizing for better decisions by learning useful human-centric representations.</p><p>Incorporating human feedback. Our use of human-in-the-loop methods is reminiscent of work in active learning, in that humans supply labels to reduce machine uncertainty <ref type="bibr" target="#b57">[58]</ref>, and in preferencebased reinforcement learning in that we implicitly encode human preferences in our evaluation <ref type="bibr" target="#b66">[67]</ref>.</p><p>However, in our work, learning a model that approximates human policy decisions is not the end goal but rather a tool to improve decisions by approximating 'decision gradients'. While this can be viewed as a form of black-box gradient estimation <ref type="bibr" target="#b26">[27]</ref>, current methods assume either inexpensive queries, noise-free gradients, or both, making them inadequate for modeling human responses.</p><p>Expertise, trust, and agency. Recent studies have shown that links between trust, accuracy, and explainability are quite nuanced <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref>. Users fail to consistently increase trust when model accuracy is superior to human accuracy and when models are more interpretable. Expertise has been identified as a potentially confounding factor <ref type="bibr" target="#b42">[43]</ref>, when human experts wrongly believe they are better than machines, or when they cannot incorporate domain-specific knowledge within the data-driven model estimate. Agency has also been shown to affect the rate at which people accept model predictions <ref type="bibr" target="#b15">[16]</ref>, supporting the hypothesis that active participation increases satisfaction, and that users value the ability to intervene when they perceive the model as incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Decision-Optimal Representations 2.1 Preliminaries</head><p>We consider a setting where users are given instances x ∈ X sampled from some distribution D, for which they must decide on an action a ∈ A. For example, if x are details of a loan application, then users can choose a ∈ {approve, deny}. We denote by h the human mapping from arbitrary inputs to decisions or actions (we use these terms interchangeably). We assume that users are seeking to choose a = h(x) to minimize an incurred loss (x, a), and our goal is to aid them in this task. To achieve this, we can present users with machine-generated advice γ(x), which we think of as a human-centric 'representation' of the input. To encourage better outcomes, we seek to learn the representation γ under which human decisions a = h(γ</p><formula xml:id="formula_0">(x)) entail low expected loss L(γ) = E D [ (x, h(γ(x)))].</formula><p>We will focus on tasks where actions are directly evaluated against some ground truth y ∈ Y associated with x and given at train time, and so the loss is of the form (y, h(γ(x))). In this way, we cover a large class of important decision problems called prediction policy problems, where the difficulty in decision-making is governed by a predictive component <ref type="bibr" target="#b34">[35]</ref>. For example, the loss from making a loan depends on whether or not a person will return a given loan, and thus on being able to make this conditional prediction with good accuracy. This setting is simpler to evaluate empirically, and allows for a natural comparison to interpretable predictive approaches where γ(x) includes a machine predictionỹ and some form of an explanation. In our experiments we have Y = {1, . . . , C}, and denote by ∆ C the C-dimensional simplex (allowing probabilistic machine predictionỹ ∈ ∆ C ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a train set</head><formula xml:id="formula_1">S = {(x i , y i )} m</formula><p>i=1 , we will be interested in minimizing the empirical loss:</p><formula xml:id="formula_2">min γ∈Γ m i=1 (y i , a i ) + λR(γ), a i = h(γ(x i ))<label>(1)</label></formula><p>where Γ is the advice class, R is a regularization term that can be task-specific and data-dependent, and λ is the regularization parameter. The main difficulty in solving Eq. (1) is that {a} m i=1 are actual human decisions that depend on the optimized function γ via an unknown decision mechanism h. We first describe our choice of Γ and propose an appropriate regularization R, and then present our method for solving Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning human-facing representations</head><p>Deep neural networks can be conceptualized as powerful tools for learning representations under which simple predictors (i.e., linear) perform well <ref type="bibr" target="#b5">[6]</ref>. By analogy, we leverage neural architectures for learning representations under which humans perform well. Consider a multi-layered neural network N (x). Splitting the network at some layer partitions it into a parameterized representation mapping φ θ : R d → R k and a predictor f :</p><formula xml:id="formula_3">R k → ∆ C such that N (x) = f (φ θ (x)).</formula><p>If we assume for simplicity that f is fixed, then learning is focused on φ. The challenge is that optimizing θ may improve the predictive performance of the algorithm, but may not facilitate good human decisionmaking. To support human decision makers, our key proposal is to remove f and instead plug in the human decision function h, therefore leveraging the optimization of θ to directly improve human performance. We refer to this optimization framework as "M•M", Man Composed with Machine, pronounced "mom" and illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>   The network learns a mapping φ from inputs x to representations z, such that when z is visualized through the visualization component ρ, the representation elicits good human decisions a. Right: The learning process. Users are queried for decisions on the current representations (A). These decisions are used to train a proxy networkĥ (B), that is then used to re-train representations (C). This process is repeated until convergence.</p><p>We also need to be precise about the way a human would perceive the output of φ. The outputs of φ are vectors z = φ θ (x) ∈ R k , and not likely to be helpful as human input. To make representations accessible to human users, we add a visualization component ρ : R k → V, mapping vector representations into meaningful visual representations v = ρ(z) in some class of visual objects V (e.g, scatter-plots, word lists, avatars). Choosing a proper visualization is crucial to the success of our approach, and should be chosen with care to utilize human cognition (and this is in itself a research question). Combined, these mappings provide what we mean by the 'algorithmic advice':</p><formula xml:id="formula_4">γ(x) = ρ(φ θ (x))<label>(2)</label></formula><p>In the remainder of the paper, we assume that the visualization component ρ is fixed, and focus on optimizing the advice by learning the mapping φ θ . It will be convenient to fold ρ into h, using the notation h (ρ) (z) = h(ρ(z)). Eq. (1) can now be rewritten as:</p><formula xml:id="formula_5">min θ∈Θ m i=1 (y i , a i ) + λR(θ), a i = h (ρ) (φ θ (x i ))<label>(3)</label></formula><p>By solving Eq. (3), we hope to learn a representation of inputs such that, when visualized, promote good decisions. In the remainder of the paper we will simply use h to mean h (ρ) (z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>The difficulty in optimizing Eq. (3) is that gradients of θ must pass through h. But these are actual human decisions! To handle this, we propose to replace h (ρ) with a differentiable proxyĥ η : R k → Y parameterized by η ∈ H (we refer to this proxy as "h-hat"). A naïve approach would be to trainĥ to mimic how h operates on inputs z, and use it in Eq. (3). This, however, introduces two difficulties. First, it is not clear what data should be used to fitĥ. To guarantee good generalization,ĥ should be trained on the distribution of z induced by the learned φ θ (x), but the final choice of θ depends onĥ itself. Second, precisely modeling h can be highly unrealistic (i.e., due to human prior knowledge, external information, or unknown considerations).</p><p>To circumvent these issues, we propose a human-in-the-loop training procedure alternating between fittingĥ η for a fixed θ and training φ θ for a fixedĥ η .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Alternating optimization algorithm</head><p>1: Initialize θ = θ 0 2: repeat 3:</p><formula xml:id="formula_6">x 1 , . . . , xn ∼ S Sample n train examples 4</formula><p>:</p><formula xml:id="formula_7">z i ← φ θ (x i ) ∀ i ∈ [n] Generate representations 5</formula><p>:</p><formula xml:id="formula_8">a i ← h(ρ(z i )) ∀ i ∈ [n] Query human decisions 6</formula><p>:</p><formula xml:id="formula_9">S = {(z i , a i )} n i=1 7: η ← argmin η E S [ (a,ĥη(z))]<label>Trainĥ 8</label></formula><p>: </p><formula xml:id="formula_10">θ ← argmin θ E S [ (y,ĥη(φ θ (x)))] Train φ 9: until convergence</formula><formula xml:id="formula_11">z i = φ θ0 (x i )</formula><p>for n ≤ m random training inputs x i with an initial θ 0 , and obtaining decisions a i for each z i generated in this way by querying human participants. Next, we take these representationdecision pairs and create an auxiliary sample set S = {(z i , a i )} n i=1 , which we use to fit the human modelĥ η by optimizing η. Fixing η, we then train φ θ by optimizing θ on the empirical <ref type="figure">Figure 3</ref>: Visualization of 2D projection task. Points in their original 3D representation give little visual indication of class (X or O). The initial 2D projection (round 1), set to the final layer representation of a fully accurate machine-only model, is similarly unintelligible to humans. However, as training progresses, feedback from human decisions improves the learned 2D projection until the class becomes visually apparent (round 5), achieving 100% human accuracy.</p><p>loss of the original sample set S. We repeat this alternating process until re-trainingĥ does not improve results. In our experiments, both φ andĥ are implemented through neural networks. In the Appendix, we discuss practical issues regarding initialization, convergence, early stopping, and working with human inputs.</p><p>The initial training ofĥ makes it match h as best as possible on the distribution of z induced by θ 0 . In the next step, however, optimizing θ causes the distribution of z to drift. As a result, forward passes push out-of-distribution samples intoĥ, andĥ may no longer be representative of h (and with no indication of failure). Fortunately, this discrepancy is corrected at the next iteration, when h is re-trained on fresh human-annotated samples drawn from the distribution induced by the new parameters θ. In this sense, our training procedure literally includes humans-in-the-loop.</p><p>In order for performance to improve, it suffices thatĥ induces gradients of the loss that approximate those of h. This is a weaker condition than requiringĥ to match h exactly. In the Appendix we show how even simpleĥ models that do not fit h well are still effective in the overall training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct a series of experiments on data-based decision-making tasks of increasing complexity. Each task uses the general algorithmic framework presented with a different, task-appropriate class of advice representations. Each experiment is also successively more sophisticated in the extent of human experimentation that is entailed. The appendix includes further details on each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decision-compatible 2D projections</head><p>High-dimensional data is notoriously difficult for humans to handle. One way to make it accessible is to project points down to a low dimension where they can be visualized (e.g., with plots). But neither standard dimensionality reduction methods nor the representation layer of neural networks are designed to produce visualizations that support human decision-making. PCA, for example, optimizes a statistical criterion that is agnostic to how humans visually interpret its output.</p><p>Our M•M framework suggests to learn an embedding that directly supports good decisions. We demonstrate this in a simple setting where the goal of users is to classify d-dimensional point clouds, where d &gt; 2. Let V be a linear 2D subspace of R d . Each point cloud is constructed such that, when orthogonally projected onto V , it forms one of two visual shapes-an 'X' or an 'O' -that determine its label. All other orthogonal directions contain similarly scaled random noise. We use M•M to train an orthogonal 2D projection (φ) that produces visual scatter-plots (ρ). Here, φ is a 3x3 linear model augmented with an orthogonality penalty φ T φ − I, andĥ is a small single-layer 3x3 convolutional network that takes as inputs a soft (differentiable) 6x6 histogram over the 2D projections.</p><p>In each task instance, users are presented with a 2D visualization of a point cloud and must determine its shape (i.e., label). Our goal is to learn a projection under which point clouds can be classified by humans accurately, immediately, and effortlessly. Initially, this is difficult, but as training progresses, user performance feedback gradually "rotates" the projection, revealing class shapes (see <ref type="figure">Fig. 3</ref>). Importantly, users are never given machine-generated predictions. Rather, progress is driven solely <ref type="figure">Figure 4</ref>: Examples of word sets selected by M•M and by LIME. Color indicates machine-perceived sentiment (green for positive, red for negative). The explanation generated by LIME includes many words with no intuitive sentiment (e.g., 'movie'). While LIME can be useful for identifying words that may not be desirable as predictive features (e.g., 'female'), M•M works in a different way, by directly adjusting itself to how humans make decisions.</p><p>by the performance of users on algorithmically "reframed" problem instances (i.e., projections), achieving 100% human accuracy in only 5 training rounds with at most 20 queries each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decision-compatible feature selection</head><p>In some applications, inputs are composed of many discrete elements, such as words or sentences in a document, or objects in an image. A useful form of advice in this setting is to 'summarize' inputs by highlighting a small subset of important elements or features. Consider, for example, a task of determining text sentiment, where the summary would be relevant words. The M•M framework suggests that models should be trained to choose summaries (representations) that are effective in helping humans make good decisions.</p><p>In this section, we consider the task of determining text sentiment using the IMDB Movie Review Dataset <ref type="bibr" target="#b43">[44]</ref>. We compare M•M with the LIME <ref type="bibr" target="#b53">[54]</ref> method, which learns a post hoc summarization to best explain the predictions of black-box predictive models. LIME chooses a subset of words for an input x by training a simpler model to match the black-box prediction in the neighborhood of x. The summarization selected by LIME may therefore give insight to the model's internal workings, but seems only likely to build trust to the extent that the "explanation" matches human intuition. And when it does not, the advice offered by LIME is unlikely to help users to form their own opinion.</p><p>In our experiment, we implement a subset-selection mechanism in φ as a Pointer Network <ref type="bibr" target="#b64">[65]</ref>, a neural architecture that is useful in learning mappings from sets to subsets. In particular, we model φ as a pair of "dueling" Pointer Network advisers, one for 'positive sentiment' and one for 'negative sentiment'. The learning objective is designed to encourage each adviser to give useful advice by competing for the user's attention, with the idea of giving the user a balanced list of "good reasons" for choosing the each of the possible alternatives (see Appendix for details). The visualizer ρ simply presents the chosen words to the user, and the goal of users is to determine the sentiment of the original text from its summary. In this experiment we trained using simulated human responses via queries to a word sentiment lexicon, which proved to be cost effective, but as in all other experiments, evaluation was done with real humans. For LIME we use a random forest black-box predictor and a linear 'explainable' model, as in the original LIME paper.</p><p>Results. The black-box random forest classifier is fairly accurate, achieving 78% accuracy on the test set when trained and evaluated on full text reviews. However, when LIME summaries composed of the top and bottom three words with highest coefficients were given as input to humans, their performance was only 65%. Meanwhile, when given summaries generated by M•M, human performance reached 76%, which almost matches machine performance but using summaries alone. Examples of summaries generated by M•M and LIME are given in <ref type="figure">Figure 4</ref>. M•M creates summaries that are more diverse and nuanced; LIME uses half the number of overall unique words, five of which account for 20% of all word appearances. Words chosen by LIME do not necessarily convey any sentiment-for instance, the word 'movie' is LIME's most frequent indication of negative sentiment (7.4%), and the word 'female' is chosen to convey negative sentiment. This artifact may be helpful in revealing spurious correlations used by the black-box algorithm to achieve high accuracy, but is uninformative as input as input to a human decision maker.  <ref type="figure">Figure 5</ref>: Right: different learned avatars conveying algorithmic advice through facial expressions (see Appendix for more examples). Left: Human accuracy in the algorithmic advice condition ('avatar advice') consistently increases over rounds. Performance quickly surpasses the 'data only' condition, and steadily approaches performance of users observing algorithmic predictions ('predictive advice'), which in itself is lower than machine-only performance. When faces are shuffled within predicted labels ofĥ, accuracy falls, suggesting that faces convey important multi-variate information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decision-compatible algorithmic avatars</head><p>Our main experiment focuses on the problem of approving loans using the Lending Club dataset. <ref type="bibr" target="#b1">2</ref> Given details of a loan application, the task of a decision maker is to decide whether to approve the loan or not. This can be done by first predicting the conditional outcome of giving a loan, and then determining an appropriate course of action. Predicting accurately is important but not sufficient, as in reality, decision makers must also justify their decisions. Our goal in this task is twofold: aid decision makers in making good decisions, and provide them with means to reason about their choices.</p><p>The standard algorithmic approach to assisting users would be to give them predictions or risk scores, perhaps along with an 'explanation'. This, however, reduces the rich data about an application to a single number. Instead, we propose to give a decision maker 'just right' high-dimensional advicecompressed enough to be managable, yet rich enough to preserve multi-variate aspects of the input -crucial for retaining users' ability to reason about their decisions <ref type="bibr" target="#b50">[51]</ref>.</p><p>For this task, we augment inputs with algorithmic advice in the form of an 'avatar' framed as conveying through its facial expression information that is relevant to the conditional outcome of giving a loan. Facial expressions have been used successfully to represent and augment multivariate data <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10]</ref>, but by manually mapping features to facial components (whereas we learn this mapping). We use realistic-looking faces, with the goal of harnessing innate human cognitive capabilities-immediate, effortless, and fairly consistent processing of facial signals <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b22">23]</ref> -to successfully convey complex high-dimensional information (see <ref type="figure">Fig. 5</ref> and Appendix for details).</p><p>Setup. We split the data 80:20 into a train set and a held-out test set, which is only used for the final evaluation. To properly assess human decisions we include only loans for which we know the resolution in the data (either repay in full or default), and accordingly set (y, a) = 1 {y=a} where y ∈ {0, 1} indicates the ground truth (1 = repay, 0 = default), and a ∈ {0, 1} indicates the decision (1 = approve, 0 = deny). Following M•M we use the train set to optimize the representation φ, and at each round, use the outputs of φ (parametrizations of faces) to fitĥ using real human decisions (i.e., approve or deny) gathered from mTurk. <ref type="bibr" target="#b2">3</ref> We set φ andĥ to be small fully connected networks with 1 25-hidden unit layer and 2 20-hidden unit layers, respectively. The visualizing unit ρ turns the vectorized outputs of φ into avatars by morphing seven 'facial dimensions' from various sources <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b61">62]</ref> using the Webmorph software <ref type="bibr" target="#b13">[14]</ref>. To prevent mode collapse, wherein faces "binarize" to two prototypical exemplars, we add a reconstruction regularization term R(x) = x − ψ(φ(x)) <ref type="bibr" target="#b1">2</ref> 2 to the objective, where ψ is a decoder implemented by an additional neural network. In the Appendix we give a detailed description of the learning setup, training procedure, mTurk experimental environment, and the unique challenges encountered when training with turkers in the loop.</p><p>Evaluation. We are interested in evaluating both predictive performance and the capacity of users for downstream reasoning. We compare between the following conditions: (1) no advice, (2) predictive advice: γ(x) =ỹ ∈ [0, 1] is a predictive probability by a pre-trained predictive model N (x), <ref type="bibr" target="#b2">(3)</ref> representational advice: γ(x) = v, where v = ρ(φ(x)) is an avatar, and (4) a 'shuffled' condition which we will soon describe. In all conditions, this advice is given to users in addition to the five most informative features of each example (given by the regularization path of a LASSO model). Since users in the experiment are non-experts, and because there is no clear incentive for them not to follow predictive advice, we expect the predictive advice condition to give an upper bound on human performance in the experiment; this artifact of the experimental environment should not necessarily hold in reality. We benchmark results with the accuracy of N (having architecture equal toĥ • φ).</p><p>Results. <ref type="figure">Fig. 5</ref> shows the training process and resulting test accuracies <ref type="bibr" target="#b3">4</ref> (the data is fairly balanced so chance ≈ 0.5). Initially, the learned representation φ produces arbitrary avatars, and performance in the avatar condition is lower than in the no advice condition. This indicates that users take into account the (initially uninformative) algorithmic advice. As learning progresses, user feedback accumulates, and accuracy steadily increases. After six training rounds, accuracy in the avatar condition reaches 94% of the accuracy in the predictive advice condition. Interestingly, performance in the predictive advice condition does not reach the machine accuracy benchmark, showing that even experimental subjects do not always follow predictive advice. This resonates well with our arguments from Sec. 1.</p><p>In addition to accuracy, our goal is to allow users to reason about their decisions. This is made possible by the added reconstruction penalty R, designed to facilitate arguments based on analogical reasoning: "x will likely be repaid because x is similar to x , and x was repaid" <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29]</ref>. Reconstruction serves two purposes. First, it ensures that reasoning in 'avatar-space' is anchored to the similarity structure in input space, therefore encouraging sound inference, as well as promoting fairness through similar treatment of similar people <ref type="bibr" target="#b69">[70]</ref>. Second, reconstruction ensures the high dimensionality of the avatar advice representation, conveying rich information. To demonstrate the importance of using high-dimensional advice, we add a condition where avatars are "shuffled" within predicted classes according toĥ (i.e., examples withŷ = 0 and withŷ = 1 are shuffled separately). Results show a drop in accuracy, confirming that avatars support decision-making by conveying more than unidimensional predictive information. Clearly, this cannot be said of scalar predictive advice, and in the Appendix we show how in this condition reasoning becomes impractical.</p><p>In regard to the gap between the avatar and predictive advice conditions, note that (1) R is a penalty term, and introduces a tradeoff between accuracy and reasoning capacity, and (2) users on mTurk have nothing at stake and are more likely to follow predictive advice where professionals would not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our paper presents a novel learning framework for supporting human decision-making. Rather than viewing algorithms as omniscient experts asked to explain their conclusions, we position algorithms as advisors whose goal is to help humans make better decisions while retaining agency. Our framework leverages the power of representation learning to find ways to provide advice promoting good decisions. By tapping into innate cognitive human strengths, learned representations can aid decision-making by prioritizing information, highlighting alternatives, and correcting biases.</p><p>The broader M•M framework is motivated by the many professional settings, such as health, education, justice, and business, in which people make data-dependent decisions. We also believe it applies to everyday decisions of a personal, social, or financial nature. Without access to professional decision makers, a challenge we have faced is that we've needed to limit our experimental focus to decision tasks that are governed by a prediction problem. But the framework itself is not limited to these tasks, and we hope to stimulate further discussion and motivate future research initiatives.</p><p>The idea of seeking to optimize for human decisions should not be considered lightly. In our work, the learning objective was designed to align with and support the goals of users. Ideally, by including humans directly in the optimization pipeline, we can augment human intelligence as well as facilitate autonomy, agency, and trust. It is our belief that a responsible and transparent deployment of models with "h-hat-like" components should encourage environments in which humans are aware of what information they provide about their thought processes. Unfortunately, this may not always be the case, and ethical, legal, and societal aspects of systems that are optimized to promote particular kinds of human decisions must be subject to scrutiny by both researchers and practitioners. Decision support methods can also be applied in a biased way to induce persuasion <ref type="bibr" target="#b27">[28]</ref>, and strategies for effecting influence that are learned in one realm may be transferable to others <ref type="bibr" target="#b18">[19]</ref>. Of course, these issues of algorithmic influence are not specific to our framework, consider news ranking, social content promotion, product recommendation, and targeted advertising, for example.</p><p>Looking forward, we think there is good reason to be optimistic about the future of algorithmic decision support. Systems designed specifically to provide users with the information and framing they need to make good decisions can seek to harness the strengths of both computer pattern recognition and human judgment and information synthesis. Through this, we can hope that the combination of man and machine can do better than either one by themselves. The ideas presented in this paper serve as a step toward this goal.</p><p>Convergence. As is true in general of gradient descent algorithms, our framework is not guaranteed to find a global optimum but rather is likely to end up at a local optimum dependent on both the initialization of φ andĥ. In our case, however, the path of gradient descent is also dependent on the inherently stochastic selection and behavior of human users. If users are inconsistent or user groups at different iterations are not drawn from the same behavior distribution, it is possible that learning at one step of the algorithm could result in convergence to a suboptimal distribution for future users. It remains future work to test how robust machine learning methods might be adapted to this situation to mitigate this issue.</p><p>Regularization/Early Stopping As mentioned in Section 2, training φ will in general shift the distribution of the representation space away from the region on which we have collected labels forĥ in the previous iterations, resulting in increasing uncertainty in the predicted outcomes. We test a variety of methods to account for this, but developing a consistent scheme for choosing how best to maximize the information in human labels remains future work.</p><p>• Regularization ofĥ: We test regularization ofĥ both with Dropout and L2 regularization, both of which help in preventing overfitting, especially in early stages of training, when the representation distribution is not yet refined. As training progresses and the distribution φ θ (x) becomes more tightly defined, decreasing these regularization parameters increases performance.</p><p>• Trainingĥ with samples from previous iterations: We also found it helpful in early training iterations to reuse samples from the previous human labeling round in trainingĥ, as inspired by <ref type="bibr" target="#b6">[7]</ref>. We weight these samples equally and use only the previous round, but it may be reasonable in other applications to alter the weighting scheme and number of rounds used.</p><p>• Early stopping based on Bayesian Linear Regression: In an attempt to quantify how the prediction uncertainly changes as θ changes, we also implement Bayesian Linear Regression, found in <ref type="bibr" target="#b54">[55]</ref> to be a simple but effective measure of uncertainty, over the last layer ofĥ(φ θ ) as we vary θ through training. We find that in early iterations of training, this can be an effective stopping criterion for training of φ. Again, as training progresses, we find that this mostly indicates only small changes in model uncertainty.</p><p>Human Input. Testing on mTurk presents additional challenges for our application:</p><p>• In some applications, such as loan approval, Mturk users are not experts. It is therefore difficult to convince them that anything is at stake (we found that bonuses did not meaningfully affect performance), It is also difficult to directly measure effort, agency, trust, or autonomy, all of which result in higher variance in responses.</p><p>• In many other applications, the ground truth is generated by humans to begin with (for example, sentiment analysis). Since we require ground truth for training, in these task it cannot be expected of humans to outperform machines.</p><p>• As the researchers found in <ref type="bibr" target="#b35">[36]</ref>, there can be large variance in the time users take to complete a given task. Researchers have found that around 25% of mTurk users complete several tasks at once or take breaks during HITs <ref type="bibr" target="#b44">[45]</ref>, making it difficult to determine how closely Turkers are paying attention to a given task. We use requirements of HIT approval rate greater than 98%, US only, and at least 5,000 HITs approved, as well as a simple comprehension check.</p><p>• Turker populations can vary over time and within time periods, again leading to highly variate responses, which can considerably effect the performance of learning.</p><p>• Recently, there have been concerns regarding the usage of automated bots within the mTurk communiy. Towards this end, we incorporated in the experimental survey a required reading comprehension task and a captcha task, and filtered users that did not succeed in these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Experimental Details B.1 Decision-compatible 2D projections</head><p>In the experiment, we generate 1000 examples of these point clouds in 3D. The class of φ is a 3x3 linear layer with no bias, where we add a penalization term on φ T φ − I during training to constrain the matrix to be orthogonal. Humans are shown the result of passing the points through this layer and projecting onto the first two dimensions. The class ofĥ is a small network with 1 3x3 convolutional layer creating 3 channels, 2x2 max pooling, and a sigmoid over a final linear layer. The input to this network is a soft (differentiable) 6x6 histogram over the 2D projection shown to the human user.</p><p>In an interactive command line query and response game we tested ourselves, φ was consistently able to find a representation that allowed for 100% accuracy. Many times this was the projection that appeared to be an 'x' and 'o' shown in <ref type="figure" target="#fig_3">Figure 6</ref>, but occasionally it was user-specific. For example, a user who associates straight lines with the 'x' may train the network to learn any projection for 'x' that includes many points along a straight line.</p><p>The architecture of φ andĥ are described in Section 3. For training, we use a fixed number of epochs (500 forĥ and 300 for φ) with base learning rates of .07 and .03, respectively, that increase with lower accuracy scores and decrease with each iteration. We have found these parameters to work well in practice, but observed that results were not sensitive to their selection. The interface allows the number of rounds and examples to be determined by the user, but generally 100% accuracy can be achieved after about 5 rounds of 10 examples each. The input to our pointer network is a sequence of 100-dimensional GloVe <ref type="bibr" target="#b49">[50]</ref> embeddings of words.</p><p>The outputs toĥ are one-hot vectors of the selected words' GloVe embeddings multiplied by the softmax probabilities output by the attention mechanism. This allows for differentiable subset selection. The outputs to the human user are subsets of words.</p><p>Hereĥ attempts to replicate the evaluation of the human user on each individual word selected by mapping the GloVe embedding for the word to the human value for that word. With real humans in the loops, we would allow this to be -1 (negative sentiment), 0 (neutral sentiment), or 1 (positive sentiment).</p><p>In this experiment, to isolate the performance of the Pointer Network with feedback fromĥ and because hand-labeling examples without access to a crowd is time-consuming, these evaluations were made by a simple simulation of how a human might make decisions. In our reference task of sentiment classification, the simulation assigns positive and negative weights to all words, with explicitly positive words receiving a weight w p ∼ U [. The positive and negative weights are fixed for any given word throughout a dataset, so for example "good" has the same value every time it appears in an example. While this represents a very rough approximation of human text evaluation, it has the clear benefit of being able to be queried many times , which allows us to test whether or not the Pointer Network can succeed in combination witĥ h before proceeding to tests with real human users. Note that while training was performed with a simulator, evaluation on the test set was done using real human queries and therefore represent human performance.</p><p>Datasets are generated by taking the first 40 alphanumeric non-stop words from the IMDB review dataset <ref type="bibr" target="#b43">[44]</ref> for examples with at least 40 such words.</p><p>We additionally use LIME to explain a Random Forest Classifier with 500 estimators and max depth 75 on the bag of words transformation of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Decision-compatible algorithmic avatars B.3.1 Data Preprocessing</head><p>We use the Lending Club dataset, which we filter to include only loans for which we know the resolution (either default or paid in full, not loans currently in progress) and to remove all features that would not have been available at funding time. We additionally drop loans that were paid off in a single lump sum payment of at least 5 times the normal installment. This results in a dataset that is 49% defaulted and 51% repaid loans. Categorical features are transformed to one-hot dummy variables. There are roughly 95,000 examples remaining in this dataset, of which we split 20% into the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 Learning architecture and pipeline</head><p>The network φ takes as input the standardized loan data. Although the number of output dimension are R 9 , φ outputs vectors in R 11 . This is because the some facial expressions do not naturally coexist as compound emotions, i.e., happiness and sadness <ref type="bibr" target="#b17">[18]</ref>. Hence, we must add some additional constraints to the output space, encoded in the extra dimensions. For example, happiness and sadness are split into two separate parameters (rather than using one dimension with positive for happiness and negative for sadness). The same is true of "happy surprise", which is only allowed to coincide These parameters are programmatically mapped to a series of Webmorph <ref type="bibr" target="#b13">[14]</ref> transformation text files, which are manually loaded into the batch transform/batch edit functions of Webmorph. We use base emotion images from the CFEE database <ref type="bibr" target="#b17">[18]</ref> and trait identities from <ref type="bibr" target="#b47">[48]</ref>. This forms ρ for this experiment.</p><p>The network φ is initialized with a WGAN to match a distribution of parameters chosen to output a fairly uniform distribution of feasible faces. To achieve this, each parameter was chosen to be distributed according to one of the follwowing: a clipped N (0, 4), U[0, 1] , or Beta <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref>. The choice  In the first experiment, we collect approximately 5 labels each (with minor variation due to a few mTurk users dropping out mid-experiment) for the LASSO feature subset of 400 training set x points and their φ 0 mappings (see <ref type="figure">Figure 10)</ref>. a is taken to be the percentage of users responding "approve" for each point.</p><p>To trainĥ, we generate 15 different training-test splits of the collected {z, a} pairs and compare the performance of variations ofĥ in which it is either initialized randomly or with theĥ from the previous iteration, trained with or without adding the samples from the previous iteration, and ranging over different regularization parameters. We choose the training parameters and number of training epochs which result in the lowest average error across the 15 random splits. In the case of random initialization, we choose the best out of 30 random seeds over the 15 splits.</p><p>To train φ, we fixĥ and use batches of 30,000 samples per epoch from the training set, which has 75,933 examples in total. In addition to the reconstruction regularization term x − ψ(φ(x)) 2 2 (see <ref type="figure" target="#fig_5">Figure 7</ref>) and the binary cross entropy accuracy loss, φ here also features a constraint penalty that prevents co-occurrence of incompatible emotions.</p><p>We train φ for 2,000 epochs with the Adam optimizer for a variety of values of α, where we use α to balance reconstruction and accuracy loss in the form L total = αL acc + (1 − α)L rec . We choose the value of α per round that optimally retains x information while promoting accuracy by inspecting the accuracy vs. reconstruction MSE curve. We then perform Bayesian Linear Regression over the final layer of the currentĥ for every 50th epoch of φ training and select the number of epochs to use by the minimum of either 2,000 epochs or the epoch at which accuracy uncertainty has doubled. In all but the first step, this resulted in using 2,000 epochs.</p><p>At each of the 2-5th epochs, we choose only 200 training points to query. In the 6th epoch we use 200 points from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Results by user type</head><p>In the end of the survey, we ask users to report their decision method from among the following choices:</p><p>• I primarily relied on the data available • I used the available data unless I had a strong feeling about the advice of the computer system</p><p>• I used both the available data and the advice of the computer system equally • I used the advice of the computer system unless I had a strong feeling about the available data</p><p>• I primarily relied on the advice of the computer system • Other</p><p>The percentage of users in each of these groups varied widely from round to round. We consider the first two conditions to be the 'Data' group, the third to be the 'Equal' group, and the next two to be the 'Computer Advice' group. While the groups are too small to draw many conclusions from this data, we find that users who report only or primarily using the data increase in mean accuracy from .51 in round 1 to .65 in round 6 (p &lt; .001).</p><p>This implies at least one of the following: users misreport their decision method; users believe they are not influenced by the advice but in fact are; as the algorithmic evidence becomes apparently better, only the population of users who are comparatively skilled at using the data continue to do so. PCA dimensionality reduction of their corresponding feature vectors z, along which a 'gradient' of facial changes can be observed. Top: Here avatars are grouped by human predictive probability. The figure shows how for the same human decisions, learning results in avatars of varied and complex facial expressions, conveying rich high-dimensional information. Interestingly, avatars corresponding to loan denial exhibit more variance, suggesting that there may be more 'reasons' for denying a loan than for approving one. Bottom: Here avatars are grouped by machine predictive probability. Since all examples in each group have the same predictive probability, they are equally similar, which does not facilitate a clear notion for reasoning. In contrast, avatars maintain richness in variation, and can be efficiently used for reasoning (e.g., via similarity arguments) and other downstream tasks.  We believe the additional dimensionality of the avatar representation relative to a numerical or binary prediction of default is useful for two reasons. Most importantly, high dimensionality allows users to retain an ability to reason about their decisions. In particular, avatars are useful because people likely have at least two inherent mental reference points for what they believe to be 'good' and 'bad' faces.</p><p>Moreover, users who have a more sophisticated mental reference space than this either inherently or because they have undergone training with the algorithm may be able to teach the advising system to match specific reasoning patterns to specific characteristics over time. Additionally, when the advising system does not have a strong conviction about a prediction, presenting neutral advice should encourage the user to revisit the data, whereas percentages above or below the either base rate of default or 50% may suffer from the anchoring effect <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>• Humans are capable of perceiving, processing, and inferring faces at almost effortlessly and with remarkable speed. Inferences are consistent and, to some extent, universal. This is made possible due to innate and dedicated neural circuitry for face perception found in human brains, playing the role of 'brain GPU' in our learning framework.</p><p>• There are many pre-existing tools for facial morphing and face recognition, which can be useful as reliable components in the training pipeline.</p><p>We emphasize that this is merely a convenient example of a broader space of potential representations and not an important component of our framework.</p><p>Moreover, the expressions of the facial avatar developed here are only intended to be used in the context of the present system, to provide a suitable representation of the data that is relevant to a given individual and helps with decision making. The facial avatar is not intended to be used to drive decision making in other contexts, and indeed, its very generation requires access to a particular set of covariates for an individual.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: The M•M framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 (</head><label>2</label><figDesc>right) illustrates this process, and pseudocode is given in Algorithm 1. The process begins by generating representations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Images of x-o interface B.2 Decision-compatible feature selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 , 1 ]</head><label>51</label><figDesc>, explicitly negative words receiving a weight w n ∼ U [−1, −.5], and all other neutral words receiving a weight w i ∼ U [−.1, .1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of reconstruction component with happiness, as opposed to "sad surprise". For parameters which have positive and negative versions, we use a tanh function as the final nonlinearity, and for parameters which are positive only, we use a sigmoid function as the final nonlinearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Loss in trainingĥ over 3 rounds (b) Validation Accuracy in training φ over 3 rounds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>ĥ does not necessarily have to match h well to lead to an increase in accuracy of distribution was based on inspection as to what would give reasonable coverage over the set of emotional representations we were interested in testing. In this initial version of φ, x values end up mapped randomly to representations, as the WGAN has no objective other than distribution matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Richness of avatar representation. A visualization of 200 avatars randomly sampled from the held-out test set, grouped by either human (top) or machine (bottom) predictive probability (0.2 in blue, 0.8 in orange, with a tolerance of 0.05). Avatars are positioned based on a 1D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Results by Reported User Type B.5 Diversity in avatar representation</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/wendykan/lending-club-loan-data<ref type="bibr" target="#b2">3</ref> All experiments were approved by the Harvard University IRB.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Results are statistically significant under a one-way ANOVA test, F(3, 97) = 9.8, p &lt; 1e − 5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Initialization to a desired distribution with a WGAN: In scenarios in which the initialization problem is to isolate a region of representation space into which to map all inputs, as in the avatar example, in which we wish to test a variety of expressions without creating expression combinations which will appear overly strange to participants, it can be useful to hand-design a starting distribution over representation space and initialize φ with a Wasserstein GAN<ref type="bibr" target="#b1">[2]</ref>. In this case, we use a Generator Network with the same architecture as φ but allow the Discriminator Network to be of any effective architecture. As with the previous example, this results in an φ in which the desired distribution is presented to users, but not necessarily in a way that reflects any human intuitive concept.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A General Optimization Issues</head><p>Initialization. Because acquiring human labels is expensive, it is important to initialize φ to map to a region of the representation space in which there is variation and consistency in human reports, such that gradients lead to progress in subsequent rounds. In some representation spaces, such as our 2D projections of noisy 3D rotated images, this is likely to be the case (almost any 3D slice will retain some signal from the original 2D image). However, in 4+ dimensions, as well as with the subset selection and avatar tasks, there are no such guarantees. To minimize non-informative queries, we adopt two initialization strategies:</p><p>1. Initialization with a computer-only model: In scenarios in which the representation space is a (possibly discrete) subset of input space, such as in subset selection, the initialization problem is to isolate the region of the input space that is important for decision-making. In this situation, it can be useful to initialize with a computer-only classifier. This classifier should share a representation-learning architecture with φ but can have any other classifying architecture appended (although simpler is likely better for this purpose). This should result in some φ which at least focuses on the features relevant for classification, if not necessarily in a human-interpretable format. For an example, see <ref type="table">Table 1</ref>, which shows how a machine-initialized Pointer Network selects relevant words but uses an idea of '1'(top set) and '0'(bottom set) which is not discernible to human users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Notes on Facial Avatars</head><p>We are aware of the many concerning ways in which faces can have been used in AI systems in discriminatory ways <ref type="bibr" target="#b65">[66]</ref>. Ours is not a paper about bias, and we have aimed to minimize these concerns to the extent possible, e.g., by restricting to variations on the image of a single person. Given current generative flow model technology, it is feasible that a similar experiment could be conducted using other abstract out-of-domain representation, such as landscapes, scenes, or even abstract color splashes generated according to latent parameters. Among these, we chose faces primarily for the following reasons:</p><p>• Humans have some pre-existing, shared representations in facial emotion space. This holds to a larger extent when for populations of higher homogeneity (i.e., our testing group, which included only Americans). This is convenient, as with the other representations we would have had to have workers undergo a training round so that they would have some shared conception of the representation space. • "I wasn't always looking at just happiness or sadness. Sometimes the expressions seemed disingenuously happy, and that also threw me off. I don't know if that was intentional but it definitely effected my gut feeling and how I chose."</p><p>• "In my opinion, the level of happiness or sadness, the degree of a smile or a frown, was used to represent applications who were likely to be payed back. The more happy one looks, the better the chances of the client paying the loan off (or at least what the survey information lead me to believe)."</p><p>• "I was more comfortable with facial expressions than numbers. I felt like a computer and I didn't feel human anymore. Didn't like it at all."</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning certifiably optimal rule lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaine</forename><surname>Angelino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Larus-Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margo</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human agency in social cognitive theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Bandura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1175</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-efficacy. The Corsini encyclopedia of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Bandura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interventions over predictions: Reframing the ethical debate for actuarial risk assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Barabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Dinakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joichi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madars</forename><surname>Virza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zittrain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08238</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adapting to continuously shifting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A theory of psychological reactance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brehm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Framing lifetime income</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">R</forename><surname>Jeffrey R Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendhil</forename><surname>Kling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><forename type="middle">V</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wrobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On chernoff faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical representation of multivariate data</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page" from="93" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistically inaccurate and morally unfair judgements via base rate intrusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahzarin R</forename><surname>Banaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">738</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cognitive adaptations for social exchange. The adapted mind: Evolutionary psychology and the generation of culture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leda</forename><surname>Cosmides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tooby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="163" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What types of advice do decision-makers prefer? Organizational Behavior and Human Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reeshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bonaccio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lm Debruine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bp Tiddeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webmorph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cade</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cade</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1155" to="1170" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichuan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1454" to="1462" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social responses in mobile messaging: influence strategies, self-disclosure, and source orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Thamrongrattanarit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Bastea-Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Fogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1651" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When suboptimal rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avshalom</forename><surname>Elmalech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sarne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><forename type="middle">Shalom</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Augmenting human intellect: A conceptual framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Engelbart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<pubPlace>Menlo Park, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">More than meets the eye: Split-second social perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerri L Johnson</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="362" to="374" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How to improve bayesian reasoning without instruction: frequency formats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Hoffrage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">684</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Innate and universal facial expressions: evidence from developmental and cross-cultural research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural network gradient-based learning of black-box function interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Hadash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03995</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Choice architecture for human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Jameson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bettina</forename><surname>Berendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Cena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Gena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabiana</forename><surname>Vernero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Reinecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="235" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Syllogistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip N Johnson-Laird</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="61" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Artificial intelligence -the revolution hasn&apos;t happened yet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medium</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noise: How to overcome the high, hidden cost of inconsistent decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Rosenfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blaser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard business review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prospect theory: An analysis of decision under risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of the fundamentals of financial decision making: Part I</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="99" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The fusiform face area: a module in human extrastriate cortex specialized for face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin M</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4302" to="4311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Human decisions and machine predictions. The quarterly journal of economics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="237" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prediction policy problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="491" to="95" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human-inthe-loop interpretability prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10159" to="10168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Rationalizing neural predictions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">How to make a.i. that&apos;s good for people. The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Man-computer symbiosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robnett</forename><surname>Licklider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE transactions on human factors in electronics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="11" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Polarity and analogy: two types of argumentation in early Greek thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">Ernest</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey Ernest Richard</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Hackett Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Theory of machine: When do people rely on algorithms?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Marie</forename><surname>Logg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How do most mturk workers work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Litman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Political campaigns and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Nickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="51" to="74" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An experimental evaluation of bidders&apos; behavior in ad auctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gali</forename><surname>Noti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="619" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The functional basis of face evaluation. Proceedings of the National Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nikolaas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Oosterhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Todorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="11087" to="11092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regulation of predictive analytics in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">6429</biblScope>
			<biblScope unit="page" from="810" to="812" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The elaboration likelihood model of persuasion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication and persuasion</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forough</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07810</idno>
		<title level="m">Manipulating and measuring model interpretability</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The algorithmic automation problem: Prediction, triage, and human effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katy</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12220</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09127</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03717</idno>
		<title level="m">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The constructive, destructive, and reconstructive power of social norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">M</forename><surname>Nolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Cialdini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladas</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griskevicius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="434" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Algorithmic risk assessment tools in the hands of humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Doleac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Margaret thatcher: a new illusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Understanding evaluation of faces on social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">P</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaas N</forename><surname>Engell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oosterhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="455" to="460" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The effects of including a patient&apos;s photograph to the radiographic examination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehonatan</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irith</forename><surname>Hadas-Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radiological Society of North America scientific assembly and annual meeting</title>
		<meeting><address><addrLine>Oak Brook, Ill</addrLine></address></meeting>
		<imprint>
			<publisher>Radiological Society of North America</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">576</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Judgment under uncertainty: Heuristics and biases. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Discriminating systems: Gender, race and power in ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whittaker</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey of preference-based reinforcement learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riad</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4945" to="4990" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Making sense of recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yeomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Understanding the effect of accuracy on trust in machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
