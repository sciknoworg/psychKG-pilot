<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Analysis of Processed Information in Decision Making Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillmann</forename><surname>Nett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cologne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Nett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cologne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Glöckner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cologne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernuniversität In</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cologne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Analysis of Processed Information in Decision Making Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In research on decision making, experiments are often analysed in terms of decision strategies. These decision strategies define both which information is used as well as how it is used. However, often it is desirable to identify the used information without any further assumptions about how it is used. We provide a mathematical framework that allows to analyse which information is used by identifying consistent patterns on the choice probabilities. This framework makes it possible to generate the most general model consistent with an information usage hypothesis and then to test this model against others. We test our approach in a recovery simulation to show that the used information may be reliably identified AU C ≥ .90. In addition, to further verify the correctnes we compare our approach with other approaches based on strategy fitting to show that both produce similar results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>During decision making humans rarely follow the rational methods that can be derived from probability theory and economics <ref type="bibr" target="#b25">(Kahneman, 2003)</ref>. Instead, they use bounded rationality (BR; <ref type="bibr" target="#b47">Simon, 1957)</ref>. BR is characterized as better suited to the constraints of the human cognitive system, namely limited time, limited knowledge and limited computational capacities <ref type="bibr" target="#b11">(Gigerenzer, 2002</ref>; but see also <ref type="bibr" target="#b2">Betsch &amp; Glöckner, 2010)</ref>. The simplest form of BR is to ignore some information (e. g., <ref type="bibr" target="#b12">Gigerenzer &amp; Goldstein, 1999)</ref>. While ignoring information could intuitively be assumed to lead to worse decision strategies, it can in fact protect from overfitting. In machine learning ignoring irrelevant information as part of the feature selection process is a widespread method to avoid overfitting <ref type="bibr" target="#b19">(Hall &amp; Holmes, 2003)</ref>, which shows that ignoring information can actually improve performance. Similarly, machine learning simulations on real world data have show that decision strategies, which ignore information, can perform at a similar level as more complex strategies <ref type="bibr" target="#b43">(Phillips, Neth, Woike, &amp; Gaissmaier, 2017)</ref>. In fact, <ref type="bibr" target="#b0">Allport (1987)</ref> reasoned that humans are incapable of performing any coherent goal directed action without the ability to ignore some information available to the perceptive system (selection-for-action). Thus, humans not only ignore information to cope with limited processing capacity, but rather the ability to ignore irrelevant information is an integral part of action control <ref type="bibr" target="#b24">(Hommel, 2010)</ref>. Since the ultimate goal taking some action, the same reasoning should be applied to human decision making.</p><p>Contrastingly, other research on decision making has shown that people sometimes still include irrelevant information, even if using this information may lead to worse decisions <ref type="bibr" target="#b7">(Dorrough, Glöckner, Betsch, &amp; Wille, 2017;</ref><ref type="bibr" target="#b35">N. Nett, Bröder, &amp; Frings, 2015</ref><ref type="bibr" target="#b30">, 2016</ref>. For example, people use stereotype information, even if this information is irrelevant to the task <ref type="bibr" target="#b4">(Correll, Wittenbrink, Park, Judd, &amp; Goyle, 2011;</ref><ref type="bibr" target="#b34">Moss-Racusin, Dovidio, Brescoll, Graham, &amp; Handelsman, 2012;</ref><ref type="bibr" target="#b49">Williams &amp; Ceci, 2015)</ref>. This leads to the discrimination of stereotyped groups. Our method, was inspired by the requirement to identify the processed information in a decision making task that includes stereotype information. We presented decision matrices to participants, similar to the one shown in <ref type="figure" target="#fig_0">Figure 1</ref>. These decision matrices contained recommendation from experts about two potential candidates for a position. Participants also were given the name of the candidates and the type of job for which these candidates applied above the matrix. Their task was to select the candidate, which they assumed to better suited for this job. Our question was, if participants would only use the recommendations provided by the experts, or if they would also include stereotype information corresponding to the gender of the candidates. The gender information can be used in two ways, either participants can show a bias towards male or female candidates in all cases, or they can speficically choose male candidates more often for some jobs and prefer male candidates for others. For the expert recommendation, the situation is even more complex, since participants may use any of the many strategies described in the literature on decision making (e. g., <ref type="bibr" target="#b11">Gigerenzer, 2002;</ref><ref type="bibr">Gigerenzer, Todd, &amp; ABC Research Group, 1999;</ref><ref type="bibr" target="#b15">Glöckner &amp; Betsch, 2008)</ref>. Furthermore, they could use any arbitrary combination of the gender, the gender in relation to the job type, and the cues integrated using any of the methods described in the literature.</p><p>To test a scientific theory it is necessary to derive predictions from this theory and then test if these predictions hold (e. g., <ref type="bibr" target="#b16">Glöckner &amp; Betsch, 2011)</ref>. With the previous example such an approach may seem almost impossible. In a task with two differently gendered candidates, if people prefer women over men, then they may choose one of the options. However, if they prefer men over women, they would choose the other other option. Therefore, from the usage of the gender information, both options can be predicted, because we do not know the direction of the bias. This leaves no room for falsification, because any of the possible choices is predicted. The more possibilities we include, such as integration of the expert recommendations, inclusion of match/mismatch between gender and job etc. the less specific our predictions for each trial in isolation must become, and thus less testable a hypothesis about information usage seems to become (e. g., <ref type="bibr" target="#b16">Glöckner &amp; Betsch, 2011)</ref>. The way out of this dilema can be found by not looking at each trial individually, but by investigating all trials together and their relationship with each other. Thus, we can use the choice in one trial to predict the choice in another related trial. Therefore, we assume that the behavior of the participant is consistent over the course of the experiment. If women were (uncoditionally) preferred over men in all other trials by a participant, the probability of choosing the women again in the current trial should be higher again, and this can be used to test if the gender is used or not. Hence, while we cannot predict the choice in a specific trial, we can still predict patterns that show consistency in the choices and then use these patterns as falsifiably hypotheses that can be tested.</p><p>Unlike our approach, most analysis methods in decision making research use decision strategies to analyse the data. These decision strategies describe both which information is used, as well as how this information is then integrated to arrive at the decision (e. g., <ref type="bibr" target="#b11">Gigerenzer, 2002;</ref><ref type="bibr" target="#b15">Glöckner &amp; Betsch, 2008)</ref>. These strategies are fit to the observed choices and then compared based on the goodness off the fit (e. g., <ref type="bibr" target="#b3">Bröder &amp; Schiffer, 2003;</ref><ref type="bibr" target="#b22">Hilbig &amp; Moshagen, 2014;</ref><ref type="bibr" target="#b30">Lee, 2016)</ref>. However, this method assumes that the correct strategy is among the ones tested and therefor, if applied without further tests of the dominant strategy, can be seen as a form of confirmatory testing <ref type="bibr" target="#b33">(Moshagen &amp; Hilbig, 2011)</ref>. However, even if a falsificationist approach is used, rejection of a strategy can lead to mis-interpretation when the rejection of a specific strategy is interpreted as a rejection of a more general theory. A rejection of a strategy can always only be interpreted as the rejection of this specific strategy including all of the assumptions underlying the strategy about which information is used and how this information is used, but never as the rejection of a part of these assumptions. For example, researchers might equate usage of only the most valid cue (information usage) with the take the best heuristic (TTB), that implies that only the most valid cue is considered and then the option indicated by that cue is taken (strategy). Then, if TTB is rejected in an experiment, this is incorrectly interpreted to indicate that more than the only the most valid cue is used. However, another possible usage of only the most valid cue would be a avoid the best (ATB) heuristic, in which only the most valid cue is considered and then the opposite option of the one indicated by that cue is taken. That is, usage of only the most valid cue allows for much more general hypotheses than those that can be derived from TTB. But to reject the hypothesis that only the most valid cue is used, we must reject all predicitions which are compatible with this hypothesis, not only a specific subset of these predictions. Thus, an analysis based on decision strategies may not identify information that is used, if the information is not used in an unpredicted way. This implies, that we must make as little assumptions about how information is used as possible when testing for information usage and that we always have to construct the most general prediction that is compatible with the tested information usage. In Section "Algebraic Operations on Perceptive Frames" we will show how an algebraic approach to hypothesis construction can be used to automatically construct the most general predictions compatible with a theory and then test these predictions.</p><p>Other approaches that attempt to identify the information that is used employ additional measurements, such as eye-tracking <ref type="bibr" target="#b39">(Norman &amp; Schulte-Mecklenbeck, 2009)</ref>. While these measures can produce sufficient conditions to reject the hypothesis that some information was used, failure to reject the hypothesis cannot be seen as support for the hypothesis <ref type="bibr" target="#b16">(Glöckner &amp; Betsch, 2011)</ref>. Thus, these methods may fail to reject some hypotheses that should be rejected. For example eye-tracking can only indicate that a participant has perceived or read some information, but it is impossible to conclude from there that this information was used, since it may have been ignored later on. The same argument can be made of other similar measures, such as EEG, or for methods that use changes in the experimental design, such as requiring participants to uncover hidden information <ref type="bibr" target="#b39">(Norman &amp; Schulte-Mecklenbeck, 2009)</ref>.</p><p>We propose a mathematical framework, which models the consequences of including or ignoring information in a binary choice task. This framework is based on a mathematical structure caleld a perceptive frame (p-frame). A p-frame describes the mental filter used to include information in decision making. The mathematical defintion allows the definition of statistical methods on the choice data, that allow identification of the p-frame that was used by a participant. Furthermore, we show that p-frames can be arranged in a partial ordering, which allows a researcher to identify the correct statistical comparisons for a given research question. Finally, we show that it is also possible to identify an algebraic operation on the p-frames, which correspond to the combination of the filtered information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal Treatment of Perceptive Frames</head><p>To show the definition of a term we will use := (=:), with the term to be defined written on the left (right) side. For elements of a set, we will use normal font lower case roman or greek letters (e. g., τ 1 , τ 2 , a, b, g). Sets of objects (e. g., sets of trials) will be denoted using upper case letters (e. g., T , S, V ). Let S = {s 1 , s 2 , . . . , s N } be any set, then we will use 2 S := { W | W ⊆ S } to denote the power set of S. To write the difference of two sets A and B, we will use the notation A \ B := { A | A ∈ A, A / ∈ B }. Furthermore, for any vector v ∈ R N we will use v T to denote the transpose of that vector. We will also use the word "iff" instead "if" to mean "if and only if". Proofs for all lemmas, theorems, and corrolaries are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric Interpretation of Hypotheses</head><p>The scope of the presented method is the analysis of single subject data <ref type="bibr" target="#b32">(Molenaar, 2004)</ref> from binary choice experiments, for example from experiments with multiple trials for the same subject (single subject analysis). Choice frequencies for a trial of an experiment of can be modeled as binomial distributions (see <ref type="bibr" target="#b21">Heck &amp; Davis-Stober, 2019</ref>, for a discussion of the more general case of two or more options modeled as a multinomial distribution). Any binomial distribution can be described by a single parameter θ giving the probability of one of the two possible outcomes (the probability of the other outcome is then 1 − θ). Furthermore, choice distributions for an experiment with N trials, may be described by giving one choice probability θ i for each trial and combining all the θ i into a single vector <ref type="bibr" target="#b21">(Heck &amp; Davis-Stober, 2019)</ref> with N components (i. e. θ ∈ [0, 1] N ). When we make predictions about behavior in this type of experiment, we usually leave some room for individual differences between participants. For example, participants may differ in how much they avoid errors, thus they may have different error rates. To include these individual differences, we identify a hypothesis about the behavior in such an experiment with a subset of points in the [0, 1] N hypercube, which are compatible with a hypothesis. For example, if we assume that the θ i always indicates the probability of choosing the option in line with some strategy, and we want to allow for an error rate of at most 30 % for each trial <ref type="bibr" target="#b14">(Glöckner, 2009)</ref>, then we would encode the predictions for this strategy as the set of all vectors θ ∈ [0, 1] N | θ i ≥ 0.7 . In addition, if error rates in one trial are not seen as independent from error rates in another trial, this can be encoded as subsets of points that correspond to these constraints. For example, <ref type="bibr" target="#b22">Hilbig and Moshagen (2014)</ref> reasoned that under the weighted additive strategy more consistent information should always lead to decreased error rates compared to trials with less consistent information (WADDprob strategy). Thus, if the information in trial i is more consistent than in trial j we can add the constraint that θ i ≥ θ j and identify the subset of the [0, 1] N that satisfies this constraint. Most importantly, when hypotheses are encoded as subsets of the [0, 1] N in this way, these subsets often do not consist of isolated points, but rather they create geometric structures. For example, any strategy, which corresponds to a multinomial model with inequality constraints, must correspond to a convex polytope of dimension N within the [0, 1] N <ref type="bibr" target="#b21">(Heck &amp; Davis-Stober, 2019;</ref><ref type="bibr">Regenwetter et al., 2014)</ref>. Similarly, any equality constraints reduce the dimensionality of the resulting geometric shape. The resulting geometric shapes can be directly used to infer statistical properties of the underlying hypthesis and to perform hypothesis testing. Lee (2016) uses a similar approach and identifies the volume spanned by the geometric shapes with the tendency of a model to perform overfitting, or equivalently as a measure of model complexit. Similarly, the maximul likelihood parameters for a model as a convex polytope can be performed by selecting the point on or within the polytope that best describes the observed data <ref type="bibr" target="#b6">(Davis-Stober, 2009)</ref>. Also, one can use a Baysian approach with models defined as convex polytope by defining a prior distribution over the convex polytope and then sampling from this distribution <ref type="bibr" target="#b21">(Heck &amp; Davis-Stober, 2019)</ref>. Different models may be compared using Bayesian model selection by integrating over the posterior for all points within the corresponding convex polytope and then comparing the resulting integrals. Because the resulting integrals correspond to the average posterior over the complete convex polytope, and models with larger volume (and therefore higher complexity) are automatically penalized <ref type="bibr" target="#b30">(Lee, 2016</ref>; see also <ref type="bibr" target="#b28">Kruschke, 2010)</ref>. Thus, geometric interpretations of models provide a versatile approach to define and use models.</p><p>Out methods applies a similar geometric approach. Because we focus on consistency, the models we define will correspond to multinomial models with equality constraints. Thus, the p-frames defined here correspond to lower dimensional slices of the N dimensional [0, 1] hypercube. In addition, we will show that it is crucial that all slices must contain the center of the [0, 1] N hypercube. Because forcing slices through the center of the hypercube would create unnecessary descriptions of the resulting structures, we will also employ the fact that chances can both be encoded as probabilities using values between 0 and 1, as odds using values between 0 and ∞, or as log-odds using values between −∞ and ∞. For binomial distributions, all these three descriptions are completely equivalent, and we can directly translate between log-odds and probabilities using the standard logistic function</p><formula xml:id="formula_0">sig(t) = 1 1 + e −t<label>(1)</label></formula><p>and its inverse sig −1 . Most importantly, when we use the logistic function to translate a vector of log-odds to probabilities or vice versa, the resulting mapping is a bijection that preserves connectedness, 1 that is geometric shapes in one interpretation must correspond to geometric shapes in the other interpretation. Through this transformation, the center of the hypercube is mapped to the origin of the R N and slices of the hypercube through this point are mapped onto lower dimensional subspaces of the R N . This makes a description of these slices much simpler. In the following section, we present a formal method describing the equality constraints on the trials and how to use these equality constraints to construct the corresponding subspaces. Thus, a hypothesis corresponds to a geometric shape in the R N . Single points within this geometric shape correspond to predictions compatible with this hypothesis. For a statistical interpretation of these geometric shapes we must thus derive the likelihood of a point ω ∈ R N . Let Y := (Y i ) i=1,...,N denote the random variable corresponding to the choices such that Y i = 1 if one of the options is chosen and Y i = −1 if the other option is chosen in trial τ i . 2 It does not matter which option is encoded as 1 and −1, as long as a consistent scheme is chosen. The probability for observing a 1 in τ i is</p><formula xml:id="formula_1">P (Y i = 1) = sig(ω i ) = 1 1 + e −ω i<label>(2)</label></formula><p>and likewise P (</p><formula xml:id="formula_2">Y i = −1) = 1 − sig(ω i ) = 1 − 1 1 + e −ω i = 1 1 + e ω i<label>(3)</label></formula><p>for observing a −1. Letȳ = (ȳ i ) i=1,...,N ∈ {−1, 1} N be the vector of observed choices. We combine this as P (ȳ i ) :</p><formula xml:id="formula_3">= P (Y i =ȳ i ) = 1 1+e −ȳ i ω i to get Lȳ(ω) = ln P (ȳ| ω) = − N i=1 ln(1 + e −ȳ i ω i )<label>(4)</label></formula><p>as the log-likelihood for a given vector ω of log-odds. Because a posterior distribution results directly from a prior and the likelihood, this also means we can use the same methods to integrate of shapes in the R N for model selection as in the [0, 1] N hypercube. 3</p><p>1 More precisely this mapping is only a bijection when constrained to the interior of the [0, 1] N hypercube, because points on the border of hypercube get mapped to points at infinity and therefore not within the R N . However, for the integrals we will later consider the points on the border must always lay on the integral bounds and only have a measure of zero, that is they do not influence the value of the integral. Also, we will translate back any posteriors before the actual integration steps in our calculations.</p><p>2 This encoding scheme deviates from the usual encoding of the choices, where one option is encoded as 1 and the other as 0. However, using this encoding allows us to simplify (2) and (3) into (4)</p><p>3 Because the mapping is only a bijection on the interior of the hypercube, however, maximum-likelihood methods are only possible if the maximum-likelihood parameters lie within the [0, 1] N hypercube. If they lie on the boundary, maximum-likelihood methods will fail to converge. This is akin to the case of perfect separation in logistic regression via maximum-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Examples trials with some possible p-frames on these trials</p><formula xml:id="formula_4">τ 1 τ 2 τ 3 v O 1 O 2 O 1 O 2 O 1 O 2 Cue 1 .80 + − + − − + Cue 2 .75 − + − + + − Cue 3</formula><p>.70</p><formula xml:id="formula_5">+ + − + + + Cue 4 .65 + − + − − + p-frame Representation Guessing 123, 123 → 123 0 0 0 Side 123, 123 → ∅ 1 1 1 First Cue 12|3, 12F oo3 1 1 −1 Third Cue 13|2, 13 → 13, 2 → ∅ 0 1 0 Count 1|2|3, 1F oo3, 2 → 2 1 0 −1 Ordered 1|2|3, 1F oo3, 2 → ∅ 1 2 −1 First Cue &amp; Side 12|3, 12 → ∅, 3 → ∅ 1 1 2 Third Cue &amp; Side 13|2, 13 → ∅, 2 → ∅ 1 1 2 Saturated 1|2|3, τ → ∅ 1 2 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intuitive Definition of Perceptive Frames</head><p>To show how we will generate equality constraints from hypotheses about information usages, we will first investigate and formally describe what it means to restrict the used information. <ref type="table">Table 1</ref> gives an example of a set of three trials, and we will now use this example to discuss all the cases that can appear when information is restricted. For all trials, we will use ω i to indicate the log-odds that O 1 is preferred over O 2 . We first note, that the first cue is the same for τ 1 and τ 2 . Thus, if someone were to ignore all cues besides the first one, and only uses the first cue, this participant would not be able to see any difference between these trials. Assuming that the behavior is consistent during the experiment, that would also mean that this participant must show the same error rate in τ 1 and τ 2 giving us the equality constraint ω 1 = ω 2 . Likewise we will say that τ 1 and τ 2 are equivalent under the p-frame corresponding only to the first cue. When we compare τ 1 and τ 3 restricted to the first cue, we note that the first cue has the same validity, but is reversed for τ 3 . Thus, someone who only uses the first cue and ignores all others must see these two trials as mirror images of each other. Again, assuming that the behavior is consistent during the experiment, we can use this to derive the equality constraint ω 3 = −ω 1 (recall that ω i gives the log-odds in τ i ). The log-odds in τ 3 must be opposite of those in τ 1 because any difference would require the usage of additional information, for example the side on which the + appears, which we explicitely excluded here. Likewise, we will say that τ 1 and τ 3 are opposites of each other under the p-frame corresponding only to the first cue. We also observe, that both τ 1 as well as τ 2 are opposites of τ 3 , and that these two trials are equivalent. In general, any two trials, which are both opposites of another trial must always be equivalent, or otherwise there must have been some information that we did not correctly reverse. Finally, we investigate how this experiment looks to someone who uses only the third cue and no additional information. For τ 1 and τ 3 , when only the third cue is used there is no valid information that allows discriminating the two options, as both are recommended by this cue. Thus, unless additional information, for example the presentation side of O 1 and O 2 is used, the only possible outcome is a random choice between O 1 and O 2 . This gives the equality constraint ω 1 = ω 3 = 0. We will say that τ 1 and τ 3 are ambivalent trials under the p-frame. In addition, under the p-frame corresponding to only the third cue τ 1 and τ 3 are also equivalent, because both have the same pattern and the same validity for the third cue. In general, any two trials, which are ambivalent under the same p-frame, must also be equivalent under this p-frame, because there can't be any distinguishable difference in the way no usable information is provided. For someone who uses only the third cue, τ 2 does not have any opposite trials, thus it is not necessary for each trial to always have an opposite. Thus, we have identified the three possible equality constraints that we may define. Two trials can be equivalent, opposites, or a trial may be ambivalent. Any additional equality constraints or inequality constraints would imply that we make some additional assumption, and therefore do not test the most general prediction in line with the theory. For example, if we assume a different set of trials where the first cue both recommends O 1 in τ 1 and in τ 2 , but has a higher validity for τ 1 , we might be tempted to also define the inequality constraint ω 1 &gt; ω 2 . However, this constraint assumes that the participant prefers cues with higher validity over those with lower validity, and thus when adding this constraint we are not testing the most general prediction anymore. Thus, the three equality constraints we presented are indeed the only three constraints we may put on our statistical models.</p><p>We will not directly define p-frames based on the equality constrains of over the logodds. Rather, we will define p-frames based directly on the relationship between trials. The reason is, that this method of defining p-frames provides a basis where p-frames are defined independently from the methods that we will later use for statistical testing. By defining p-frames based on the relationship between trials also allows derivation of additional properties that we will later use when we investigate relationships between different p-frames and when we define an algebra over the set of p-frames. Therefore, a p-frame is modelled as a set of subsets of trials, where all trials in the same subset are equivalent to each other when only a part of the information is used. Since each trial must appear in exactely one of these subsets, the resulting structure is a partition of the set of trials. We will follow the notations and definitions used by <ref type="bibr" target="#b18">Grätzer (1998)</ref> and <ref type="bibr" target="#b8">Ellerman (2010)</ref> when talking about partitions of sets. Let S be any set and π ⊆ 2 S . π is called a partition of S, iff it does not contain the empty set ∅ / ∈ π and covers the complete set S uniquely, that is the sets β ∈ π are disjunct ∀β, β ∈ π, β = β : β ∩ β = ∅ and can be combined to yield the complete set S = β∈π β. The set of all partitions over a set S is denoted as Part S. The elements β ∈ π of a partition π are called the blocks of that partition. For simplicity, we will separate the blocks by | when writing partitions. Thus, for any partition given as</p><formula xml:id="formula_6">π = {{a 1 , a 2 , . . . , a N }, {b 1 , b 2 , . . . , b M }, . . . , {c 1 , c 2 , . . . , c L }} we will write π = a 1 a 2 . . . a N |b 1 b 2 . . . b M | . . . |c 1 c 2 . . . c L</formula><p>and similarly for the blocks of a partition</p><formula xml:id="formula_7">π = {β 1 , β 2 , . . . , β M } we will write π = β 1 |β 2 | . . . |β M .</formula><p>If possible without ambiguity we will also leave out the names of the variables and just write partitions by using the index numbers in the underlying set, for example π = 12|34|5. Each partition π ∈ Part S is isomorphic to an equivalence relation over S, given by a ≡ π b ⇔ ∃β ∈ π : a, b ∈ β. Similarly, we will write a ≡ π iff no such β exists. The equivalence relation defines an equivalence class for each element a ∈ S, which is denoted as [a]</p><formula xml:id="formula_8">π := { b | b ≡ π a }.</formula><p>As is easy to see, for any β ∈ π we have [a] π = β iff a ∈ β, that is the equivalence class of a corresponds to the unique block that contains a. While equivalence classes describe which items in a partition are in the same block, it is sometimes simpler to look at items which are different. Therfore, we define the set of distinctions or dit set as</p><formula xml:id="formula_9">dit π := β,β ∈π β =β β × β = { (a, b) ∈ S × S | a ≡ π b } ,</formula><p>that is the dit set is the set of all pairs of items that are not equivalent under the partition. Both the equivalence relation and dit set completely and uniquely define the partition and therefore are isomorphic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Perceptive Frame</head><formula xml:id="formula_10">) = ∅ ⇒ Ω(Ω(β)) = β</formula><p>Definition 1 formally describes the three types of relations we have identified when we discussed our example. The Partition π groups all trials which are identical under the p-frame. The function Ω then formalizes what it means for two trials to be opposites of each other. Because all trials opposite to another trial must be equivalent to each other, it is sufficient to map blocks in the partition onto opposite blocks instead of mapping individual trials. However, because some block may not have any opposite trials, we cannot map the partition onto itself, but we must also allow the mapping to go to the empty set in the case we have a block that cannot be reversed. However, if an opposite block exists, then mapping the opposite block to its opposite must return to the original block. This is formalized in the second condition. Ambivalent trials are not formalized as a separate set, but are implicit in the definition. Because we allow Ω to map a block onto itself, we can have a case that some trials are opposites of themselves. While this may seem counter intuitive at first sight, this provides the exact condition for ambivalent trials. Reversing no available information always just leaves no information, thus reversing an ambivalent trial does not change the provided information. All that is left is to ensure that ambivalent trials are always equivalent to each other, which is provided by the first condition. The equivalence and opposite relationships defined by any p-frame are specific to the information which is considered and may differ arbitrarily for another p-frame. For our example, under the "First Cue" p-frames only the most valid cue is considered. Therefore, τ 1 and τ 2 are seen as the same, and both are opposites to τ 3 . For the "Count" p-frame, however, the number or cues in favor of either option is used. Thus, τ 1 , which has three cues in favor of O 1 and only two in favor of O 2 , must be opposite to τ 3 , so these two trials have the same relationship as in the "First Cue" p-frame. However, for τ 2 the relationship is different under the "First Cue" and the "Count" p-frame, with τ 2 only allowing for guessing behavior (two cues in favor of either option) when only the number of recommendations is used.</p><p>We use F i := (π i , Ω i ), where this is possible without ambigouity. In addition, we also overload the notation for partitions. For any p-frame F := (π, Ω) we write dit F := dit π, τ 1 ≡ F τ 2 ⇔ τ 1 ≡ π τ 2 etc. If possible without confusion, we leave out the index from <ref type="bibr">[τ ]</ref> and ≡. Furthermore, we write Ω(τ ) := Ω([τ ] F ) as the set of all opposite trials to τ and τ ≡ Ω(τ ) if τ and τ are opposites of each other.</p><p>There are two p-frames that can be defined for any set of trials. First, participants may ignore all information and resort to guessing in all trials. The p-frame for this is F guess := (T , Ω(T ) = T ). Second, participants may completely use all presented information, such that each trial is considered different from each other. This would give the saturated p-frame modeled as F sat := (τ 1 |τ 2 | . . . |τ N , Ω(τ ) = ∅). In addition, depending on the mode of presentation, many experiments allow participants to show a side bias by either prefering the left or the right option more often. This can be modeled by the p-frame F side := (T , Ω(T ) = ∅). To show how other types of information can be modelled using p-frames, we refer to our example in <ref type="table">Table 1</ref>. The "First Cue" p-frame is using only the information in the first cue and ignores all other cues. The "Count" p-frame only uses the number of recommendations for both options, but completely ignores the validity. A strategy that uses this type of information is Dawes' rule. The "Ordered" perceptive frame treats recommendations while also including the validity. Strategies that use this type of information are Franklins rule, but also the rational model based on Bayes theorem as well as more complex models of decision making such as PCS. The "Saturated" p-frame is able to completely distinguish all three trial types. This may seem surprising at first, because τ 1 and τ 3 are complete opposites of each other, with all recommendations exactely reversed. However, the "Saturated" p-frame takes into account all information available, including the side on which the recommendations are presented, thus making τ 1 and τ 3 fully distinguishable and not just opposites of each other.</p><p>To embedded p-frames into the methods that employ geometric approaches to hypothesis testing, we have to derive equality constraints over the log-odd space. Let the two options in each trial be denoted O 1 and O 2 as in the example. First, if two trials are equivalent using only part of the provided information, then the behavior of a participant using this information must be the same in both trials, that is, they must show the same probability of choosing O 1 . Similarly, if two trials are opposite of each other, then the probability of choosing O 1 in the first trial must be the same probability as the one for choosing O 2 in the second trial. Finally, if a trial requires guessing, the probability of choosing O 1 must be .5. Thus, a vector ω of log-odds must be consistent with a p-frame, to be considered a valid realization of that p-frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. Let F be a p-frame for a set of trials</head><formula xml:id="formula_11">T = {τ 1 , τ 2 , . . . , τ N }. Let ω = (ω i ) i=1,...,N ∈ R N an assignment of log-odds to each trial, such that τ i is assigned the log-odd ω i . Then ω is consistent with F, if • τ i ≡ τ i implies ω i = ω i • τ i ≡ Ω(τ i ) implies ω i = −ω i .</formula><p>Because we defined guessing trials as their own opposites, there is no reason to specifically deal with guessing trials in this definition (0 is the only valid solution to the equation x = −x). The above gives criteria to deterime when a given vector of log-odds is consistent, but it does not allow us to construct such consistent vectors or to provide a constructive description of all consistent assignments. Because a Bayesian model selection requires that we integrate over all consistent assignments, we need to find a way to construct all assignments. We will show how this can be done in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry of Perceptive Frames</head><p>In this section we will show, how a p-frame can be used to uniquely define a subspace within the R N . Therefore, we will show that each p-frame may be used to define a set of vectors that can than be reduced to a basis of the corresponding subspace. Most importantly, in the final theorem, we will show that these subspaces correspond exactely to the consistent assignments to the p-frame. Because we define the subspaces in terms of their basis vectors, and we can directly get these basis vectors from the p-frame, this provides a constructive method for the definition of the consistent assignments. Most importantly, when we later integrate over the subset of consistent assignment, we can instead do this by integrating over the basis vectors.</p><p>For</p><formula xml:id="formula_12">any v 1 , v 2 , . . . , v M ∈ R N</formula><p>let the linear subspace generated by these vectors be denoted by span{v</p><formula xml:id="formula_13">1 , v 2 , . . . , v M }.</formula><p>Definition 3 (Geometry of a Perceptive Frame). Let T := {τ 1 , τ 2 , . . . , τ N } and the vectors e 1 , . . . , e N be the cannonical basis of the R N . Also let F be any p-frame with π</p><formula xml:id="formula_14">= β 1 |β 2 | . . . |β M . For each β j ∈ π define the vector v j =   τ i ∈β j e i   −   τ i ∈Ω(β j ) e i   (5)</formula><p>The set of all vectors v j is denoted by Gen(F) :</p><formula xml:id="formula_15">= {v 1 , v 2 , . . . , v M }.</formula><p>The linear subspace F := span Gen(F) of the R N is called the geometry of the p-frame F and Gen(F) is called the generator of the geometry F .</p><p>The generator of a geometry is not necessarily a basis of F , since it can include linear dependent vectors and even the zero vector. However, it can be reduced to a basis B ⊆ Gen(F) such that span B = span Gen(F). Also, for two different set of basis vectors B and B of F by the Steinitz exchange lemma we know that |B| = |B |, thus the dimension of the geometry is independent of the choice of B. Furthermore, as we will now show all bases that are constructed as subsets of Gen(F) must be orthogonal and different choices of the basis only differ in the orientation of the basis vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Let F be any p-frame and let</head><formula xml:id="formula_16">v j , v j ∈ Gen(F). Then v j = ±v j or v T j • v j = 0.</formula><p>The previous lemma allows us to conclude that the basis B is an orthogonal (not orthonormal) basis of F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Let F be any p-frame and let</head><formula xml:id="formula_17">B = {b 1 , . . . , b l } ⊆ Gen(F) be a basis of F .</formula><p>Then B is an orthogonal basis. Also, for each i there is at most one b such that b ,i = 0.  <ref type="figure" target="#fig_16">Figure 2</ref> . Geometries of the p-frames given in <ref type="table">Table 1</ref>. Red arrows indicate the basis vectors of the geometries, a sphere in the origin indicates guessing behavior for some trials (green axes).</p><p>In <ref type="figure" target="#fig_16">Figure 2</ref> we show the geometries for some of the p-frames. We define the matrix</p><formula xml:id="formula_18">Φ B := (b 1 , b 2 , . . . , b L ) ∈ {−1, 0, 1} N ×L ,<label>(6)</label></formula><p>which allows us to translate any vector λ ∈ R L into an element of the geometry of the p-frame Φ B •λ ∈ F . This matrix defines a bijection between the R L and the geometry F . The matrix that corresponds to the inverse of this bijection is given by the Moore-Penrose Pseudoinverse Φ</p><formula xml:id="formula_19">+ := (Φ T Φ) −1 Φ T .</formula><p>The Moore-Penrose Pseudoinverse is a left-inverse to Φ (i. e. Φ + Φ is the identity matrix), but not necessarily a right inverse. Therefore, in general it only provides an inverse for v ∈ F but not for vectors that lie outside the geometry of the p-frame. Thus, we can use λ ∈ R L as a parametrization of the geometry. Most importantly, the values λ i directly define the entries of Φ B • λ.</p><p>Lemma 2. Let F be a p-frame and B ⊆ Gen(F) be a basis of F .</p><formula xml:id="formula_20">Let λ = (λ i ) i=1,...,L ∈ R L a vector and ω = Φ B • λ. Then for each ω i = 0 there exists a j ∈ {1, . . . , L} such that ω i = ±λ j .</formula><p>To proceed we will now show that the vectors in the geometry of a p-frames are exactly the consistent asignments of log-odds to the trials, and therefore λ can be used as a parametrization over the consistent assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. Let F be a p-frame and ω</head><formula xml:id="formula_21">∈ R N . Then ω is consistent with F iff ω ∈ F .</formula><p>Thus, the geometries do indeed provide a simple way to interpret p-frames as statistical models parametrized over R L for some L ∈ 0, . . . , N . In addition, the previous lemmas indicate that if we pick a parameter λ = (λ i ) i=1,...,L , then the elements of the vector directly translate to the log-odds in each of the blocks and their opposites. In addition, the choice of the basis B of F only determines the direction of the parametrization. Thus, for different choices of the basis B and B the corresponding translation matrices Φ B and Φ B are the same up to a permutation and a change of the signs, that is if we have a matrix M such that</p><formula xml:id="formula_22">Φ B = Φ B • M , then M ∈ {−1, 0, 1}</formula><p>L×L such that there is exactely one non-zero entry in each row and column. In other words, M is a signed permutation matrix. Similarly, for any two choices of the basis we can find a signed permutation matrix M that translates between the parametrizations for these two bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayes Factors of Perceptive Frames</head><p>The previous lemma shows, that we can use the geometry of a p-frame to identify consistent assignments and even interpret p-frames as statistical model parametrized over the vector space R L . This shows a great similarity to the method presented here and other methods of analysis (e. g., <ref type="bibr" target="#b6">Davis-Stober, 2009;</ref><ref type="bibr">Regenwetter et al., 2014)</ref>, which use geometric interpretations of strategies. Furthermore, given some choice of basis B, the simple parametrization allows us to rewrite the log-likelihood from (4) in terms of the model parameters as</p><formula xml:id="formula_23">L B y (λ) = Lȳ(Φ B • λ) = ln P (ȳ| Φ B • λ).<label>(7)</label></formula><p>Furthermore, because any consistent assignment can be written as Φ B •λ given some suitable prior p :</p><formula xml:id="formula_24">R L → R with R L p(λ) dλ = 1</formula><p>over the parameters and a prior over all possible p-frames, we can marginalize out the parametrization and derive a term that is proportional to the probability of the p-frame F</p><formula xml:id="formula_25">P (F|ȳ) ∝ P (F) • R L P (ȳ| Φ B • λ)p(λ) dλ =: W F .<label>(8)</label></formula><p>There are two constraints that we must put on our prior. First, we always want to test the most general model that is consistent with the usage of some information. Thus, our prior must not add any assumption about how this information is used. In addition we must ensure that the choice of our basis does not change the value of W F . Since for any two choices B and B of the basis for F we can translate between these using a suitable signed permutation matrix M with Φ B = Φ B • M , we want the prior to have the same result when we apply M to the parametrization. Thus, we require that p(λ) = p(M λ) for any signed permutation matrix M . To ensure that we do not introduce additional assumptions, we should treat all model parameters equivalently. Any difference in the prior over the model parameters would imply that some parts of the used information may be weighted more or less than others. Also we must ensure that our prior is symmetric, since we would otherwise make the assumption of a directional bias. A simple choice for p is any function that can be written as</p><formula xml:id="formula_26">p(λ) = L i=1 q(λ i ) where q : R → R is an even function, that is q(x) = q(−x)</formula><p>. The Bayes weights W F are proportional to the posterior probability of the p-frame given the data. In fact by Bayes theorem we have P (F|ȳ) = W F /P (ȳ). This allow us to derive the Bayes factor (BF <ref type="bibr" target="#b26">Kass &amp; Raftery, 1995)</ref> for two p-frames as</p><formula xml:id="formula_27">BF F 1 ,F 2 = W F 1 W F 2 .<label>(9)</label></formula><p>Also, since multiple p-frames are independent of each other, for any two disjunct sets of p-frames F 1 and F 2 with F 1 ∩ F 2 = ∅ we can define the BF for these sets as</p><formula xml:id="formula_28">BF F 1 ,F 2 = F ∈F 1 W F F ∈F 2 W F .<label>(10)</label></formula><p>This leaves the question how the integral in (8) may be calculated. While the integral could be estimated using monte-carlo integration, this can be very inefficient. <ref type="bibr" target="#b17">Gopalan and Berry (1998)</ref> propose a method that is very similar to the one that is present here. In their method, they also define the statistical model based on partitions over a set of experimental results. In addition, they show that under some conditions the Bayes weights may be calculated analytically. However, <ref type="bibr" target="#b17">Gopalan and Berry (1998)</ref> only consider equality constraints of the type θ i = θ j wheras p-frames may also require ω i = −ω j or ω i = 0. Thus, we must adapt their method to also include these types of constraints.</p><p>For this we will need an intermediate lemma, that allows us to re-write the log-likelihood for some special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3. Let F be a p-frame with</head><formula xml:id="formula_29">γ ∈ π such that Ω(γ) = γ. Let B = (b 1 , . . . , b L ) ⊆ Gen(F) be a basis for F . For any ω ∈ F let λ = Φ + B ω.</formula><p>Furthermore, for &gt; 0 let n denote the number of non-zero entries of the vector b and n 0 = |γ|. Then the log-likelihood of ω is</p><formula xml:id="formula_30">Lȳ(ω) = −n 0 ln 2 − 1 2 L =1 (n + b T •ȳ) ln(1 + e −λ ) + (n − b T •ȳ) ln(1 + e λ ).<label>(11)</label></formula><p>Equation (11) can be used to derive an analytic formula for the Bayes weight. Thus, let p : R L → R be a prior over the parametrization with p(λ) = L =1 q(λ ) for some even function q : R → R. Then we have</p><formula xml:id="formula_31">W F = P (F) 1 2 n 0 • • • ∞ −∞ L =1 sig(λ ) n +b T ȳ 2 (1 − sig(λ )) n −b T ȳ 2 p(λ) dλ 1 . . . dλ L (12)</formula><p>as the marginal posterior probability of the p-frame F. Because the integrand in the multivariate integral can be written as the product of univariate functions we can exchange the product and the integrals. Also, substitute θ = sig(λ) in each of the univariate integrals. For simplicity, take p(θ ) to indicate the prior p that we defined above over the parametrization in log-odd space after the substitution. Then we re-write</p><formula xml:id="formula_32">W F = P (F) 1 2 n 0 L =1 1 0 θ n +b T ȳ 2 (1 − θ) n −b T ȳ 2 p(θ) dθ.<label>(13)</label></formula><p>Since p was defined as the product of even function, the prior over the parameters θ must be symmetric around 0.5. To get an analytical formula for W F we chose p(θ) = (θ (1 − θ)) a−1 / B(a, a), where B(a, b) = Γ(a)Γ(b)/Γ(a + b) is the beta function. That is, θ is distributed symmetrical around θ = 0.5 according to θ ∼ Beta(a, a). The use of this prior over θ also has the benefit, that the integral in (13) has an analytical solutions as  Also, the single parameter a ∈ R &gt;0 has a simple interpretation, that can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>. When a is larger than 1 more weight of the distribution is accumulated to values of θ closer to 0.5. Thus, for a &gt; 1 values of θ are more likely that indicate more guessing, even if information is available under F. On the other hand for a &lt; 1 more weight of the distribution is accumulated towards the end points of the interval [0, 1], making values of θ more likely that indicate usage of the provided information.</p><formula xml:id="formula_33">W F = P (F) 1 2 n 0 L =1 B 1 2 n + b T ȳ + a, 1 2 n − b T ȳ + a B (a, a) .<label>(14)</label></formula><p>With these results in place, we can calculate the Bayes weight to compare any two p-frames or even two sets of perceptive frames. Also, since we do not require any sampling all calculations can be done using simple matrix algebra and therefore this analysis method is very fast. However, we still face the problem, that a p-frame corresponds to the hypothesis that some information, but no additional information is used. In most cases this may be to specific. Even if we want to test for the hypothesis that only a specific information and no other information is used, we still need to compare this hypothesis with alternative hypotheses when computing the BF. These hypotheses must both contain p-frames that include other information than the one to be tested as well as p-frames that include the tested information together with alternative information. Similarly, we may often want to test if some information is used, independently of other information (that may also be used or not). Thus, we need methods to identify all p-frames that correspond to information usage both in isolation as well as in combination and methods to generate more complex p-frames, such that we can actually test the most general prediction in line with a hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Partial Order of Full Containment</head><p>The previous section allows us to compute BFs corresponding to the comparision of two p-frames or two sets of p-frames. However, not all comparisons are meaningfull. For example, take a decision experiment given in matrix format and the p-frame, under which only the most valid cue is used, and the one, under which only the second most valid cue is used. Clearly, a comparison between these two p-frames is spurious. Even if the first cue is more likely to be used in isolation than the second cue in isolation, the possibility still remains, that both cues are used. However, this possibility is not included in the analysis, thus the results cannot be interpreted easily. Thus, for two p-frames to be compared directly, one of the p-frames should consider at least all the information considered under the other one. Similarly, if the hypothesis indicates that one particular piece of information is used, with the possibility that it is used in combination with other information, it is necessary to identify all p-frames, under which this information is considered. Thus, we must be able to order the p-frames by the information that is considered for each p-frame. Therefore, we introduce the relation of full containment between to p-frames. This relation models the fact that a p-frame may consider at most as much information as another frame, but no additional information.</p><p>To define the ordering, we will first have to introduce some additional definitions. We will mostly follow the notation given by <ref type="bibr" target="#b5">Davey and Priestley (2002)</ref>. Let S be any set S. A relation R ⊂ S × S, which is reflexiv ∀a ∈ S : (a, a) ∈ R, antisymmetric ∀a, b ∈ S : (a, b) ∈ R ∧ (b, a) ∈ R ⇒ a = b, and</p><formula xml:id="formula_34">transitive ∀a, b, c ∈ S : (a, b) ∈ R ∧ (b, c) ∈ R ⇒ (a, c) ∈ R,</formula><p>is called a partial ordering of S. We will use the more readable relation symbols ≤, ⊆, , and instead of the pair notation, such that we will for example write a ≤ b ⇔ (a, b) ∈ R for some relation R. The pair (S, ≤) is called a partial ordered set or poset. In the following, we will often simply refer to S as the poset instead of the pair (S, ≤), since this is usually possible without ambiguity. In addition, we will also use the relation symbols such as a &lt; b to indicate that a ≤ b and a = b (similarly for all other relation symbols).</p><p>The poset of partitions Part S over a set S is given by the ordering relations π π ⇔ ∀β ∈ π : ∃β ∈ π : β ⊇ β. We write π refines (is a refinment of) π or that π generalizes (is a generalization of) π. This means, that for any block in π there is a block in the generalized π which completely contains the block β. Note that the direction of ⊇ and is reversed here. While is often defined in reverse to follow the direction of the subset relation, we use the definition given by Ellerman (2010) here, because it will make latter definitions more intuitive. The ordering relation can also be defined based on the distinctions as π π ⇔ dit π ⊆ dit π, i. e. the more general partition can make at most the same distinctions as the more refined one <ref type="bibr" target="#b8">(Ellerman, 2010)</ref>. The atoms of the poset of partitions are partitions which have exactely two blocks <ref type="bibr" target="#b8">(Ellerman, 2010)</ref>.</p><p>Definition 4 (Full Containment). For two p-frames F 1 and F 2 we define a relation F 1 F 2 , and say that F 1 is fully contained in F 2 if we have π 1 π 2 and for any trial τ ∈ T the condition Ω 2 (τ ) ⊆ Ω 1 (τ ) holds.</p><p>We will further write F 1 F 2 to indicate that F 1 F 2 and F 1 = F 2 . And F 1 F 2 if we do not have F 1 F 2 . Also, we will write F 1 : F 2 if F 2 is a immediate successor of F 1 (F 1 is a immediate predecessor of F 2 ), i. e. if we have F 1 F 2 and for any F 3 we have</p><formula xml:id="formula_35">F 1 F 3 F 2 implies F 3 = F 1 or F 3 = F 2 .</formula><p>For two p-frames F 1 F 2 (F 1 F 2 ) we will call F 2 the (strictly) larger frame and F 1 the (strictly) smaller frame.</p><p>This definition has two important parts. Assume that F 1 F 2 . First by relating the partitions under each p-frame to each other, we require that the larger frame can make at least any distinction that could also be made under the smaller frame, i. e. dit F 1 ⊆ dit F 2 . Therefore, if the information considered under F 1 suffices to conclude that trial τ 1 and τ 2 are different, then this distinction cannot be removed by adding more information to get F 2 . Second, if two sets of trials are opposites to each other under F 2 (i. e. when more information is considered), then they must also be opposites under F 1 (i. e. when less information is considerd). Or the other way around, if two trials are considered distinct and not opposites to each other under F 1 , then the information that lead to this distinction must also be present under F 2 , such that they cannot be opposites. Similarly, if two trials have shown a common element, that is they are part of the same block and not opposites under F 1 , then this common element must also be present under F 2 . Furthermore, it is important to consider that we may actually have Ω 2 (β 2 ) = ∅ but Ω 1 (β 1 ) = ∅ for some β 1 ⊇ β 2 , since this trivially gives Ω(β 2 ) ⊆ Ω(β 1 ). In this case, information available under F 1 is also available under F 2 , since trials, which were seen as opposites under the smaller p-frame are now not opposites any more but rather completely unrelated. As an example consider the p-frame under which only the most valid cue is used, such that two trials τ 1 and τ 2 appear as opposites. Now if we have three cues for the left option and one cue for the right option in both τ 1 and τ 2 , then τ 1 and τ 2 do not appear to be opposites any more if this information is used as well. There are some additional facts that can be followed from this definition, which are less obvious. If the larger frame contains any guessing trials, that is there exists some γ 2 = Ω 2 (γ 2 ), then there must also be guessing trials under the smaller p-frame, and there may be no additional guessing trials under the larger frame (i. e. γ 2 ⊆ γ 1 ). This is consistent with all information available under F 1 also being available under F 2 . If a participant already had information available such that guessing was not necessary for any trial τ under F 1 , then this information remains under F 2 and no guessing is needed there either.</p><p>Second, we may actually have trials, which are opposites of each other under F 2 , but guessing trials under F 1 . Since no information is available for these trials under F 1 , the relationship may only be seen, if the information available F 2 is available. In fact the guessing trials may be arbitrarily partitioned, with any valid relationship between these trials. This is easy to see, since in the second condition of Definition 4 if γ 1 = Ω 1 (γ 1 ) exists, i. e. we have guessing under the smaller frame, then we only require the opposites of any block β 2 ⊆ γ 1 with β 2 ∈ π 2 to also be guessing trials under F 1 , i. e. Ω 2 (β 2 ) ⊆ γ 1 and no further restriction is imposed.</p><p>To further investigate the structure that is imposed by the relation of full containment, we will now show that the set of p-frames ordered under full containment is a poset.</p><p>Theorem 3. p-frames together with the relationship of full containment are a partially ordered set, that is the relation is</p><formula xml:id="formula_36">1. reflexiv: F F 2. antisymmetric: F 1 F 2 and F 2 F 1 imply F 1 = F 2 3. transitiv: F 1 F 2 and F 2 F 3 imply F 1 F 3 .</formula><p>The poset structure of p-frames shows that our definition intuitively makes sense. By just adding information, we can never get back to a state of less information, i. e. there are no circles in the structure of p-frames. Furthermore, the relation only defines a partial ordering, and not a full ordering. With the partial ordering we may find two p-frames, which are incommensurable, i. e. for which we have F 1 F 2 and F 2 F 1 . This is unlike the comparison for numbers, in which we can compare any two numbers to determine which one is greater and which is smaller. Also this shows a more complicated stucture of p-frames and therefore also decision strategies than the one which is usually used during analysis, in which the statistical model complexity of each strategy is taken into account. While measures of model complexity do not necessary define a full order on the set of all models, as two different models may nevertheless have the same complexity (e. g., if they have the same number of parameters), still just looking at measurements of complexity may lead to questionable conclusions. Assume for example two models, one with two parameters (low complexity) and one with three parameters (high complexity). Now the usual approach would be to compare these models in a one versus the other fashion, and then conclude which one of the two better describes the behavior. Thus, just by the analysis approach one must be chosen and the other rejected. Now assume also that the p-frames used by these models are actually incomensurable, such that neither model can be seen as a more complex version of the other. In that case, it could actually be the case, that both models accurately describe the behavior, if the behavior is determined by a p-frame which is larger than the one for either model (see also <ref type="bibr" target="#b33">Moshagen &amp; Hilbig, 2011)</ref>. Thus, by only testing these two models in a one versus the other fashion, we may loose insights, since we are comparing objects that have no direct relationship with each other. Instead, each of the two models should be tested in relation to the ones directly related to it by full containment, which may show that both models capture some part of the observed behavior. This example also shows, that it is not sufficient to only order some preceptive frames, which could be defined based on existing theories or models of decision making, but also to be able to combine two p-frames such that the resulting p-frame captures the information that can be observed under both p-frames but no additional information. In other words, we must be able to construct a least upper bound or join of two p-frames. We will define such a join and also show a way by which it can be computed from two p-frames in Section "Algebraic Operations on Perceptive Frames".</p><p>With the current definitions, however, the poset of p-frame is just defined based on a set of abstract criteria that define what is meant by one p-frame fully containing another. Most importantly, the current definition does not provide a way to check if one p-frame contains another or not. To provide an actual algorithm, we will need another way to represent p-frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representations of Perceptive Frames</head><p>Definition 1 gives an abstract mathematical structure, however, it does not show an easy way to implement p-frames in software. Similarly, Defintion 4 also is not constructive and therefore does provide an algorithm to check for full containment of two p-frames. To show how an algorithmic approach may be taken to these definitions, we will use some ideas from category theory. We give a very short introduction to category theory (see <ref type="bibr" target="#b1">Barr &amp; Wells, 1990;</ref><ref type="bibr" target="#b29">Lawvere &amp; Schanuel, 2009</ref>, for a more complete introduction). Category theory analyzes mathematical objects by their relation to each other, while abstracting as much as possible what these objects represent. For example, finite subsets of N 0 can be related to each other by defining injective functions between them. Similarly, numbers in N 0 can be related to each other by using the ≥ operator. Now, since there exists an injective function between two discrete sets A and B exactely if |A| ≥ |B| the relationship defined between numbers is the same as the one defined by injective functions on discrete sets. Thus in category theory, these two would be seen as the same and studied simultaneously. 4 When applied to computational problems, this way of thinking has the benefit, that we can easily translate between different implementations of the same category and define our algorithms in the implementation that is simplest. In general a category consists of a class of objects (e. g., {A, B, . . .}) of the category and a class of arrows (also called maps or morphisms) between these objects (e. g., {f, g, . . .}). For each arrow there exists one object that is called the domain of that arrow (written as dom f ) and one object that is called the codomain (written as cod f ). For example, in the category of finite subsets of N 0 defined above, the class of all subsets A ⊂ N 0 defines the object of this category and the functions define the arrows. Then for any function f : A → B we would have dom f = A and cod f = B. For any two objects A and B in a category, the set of all arrows between them is called the Hom-Set Hom(A, B) = { f | dom f = A, cod f = B }. Two arrows f and g in any category may be composed if their domain and codomain matches, that is g • f is an arrow if cod f = dom g and we have dom g • f = dom f as well as cod g • f = cod g. Arrows are often written by just joining their domain and codomain, for example as A → B or f : A → B. Also, for each object A in a category, there must be a special arrow 1 A : A → A and for any other arrow f : A → B we must have f • 1 A = f as well as for any other arrow g : B → A we must have 1 A • g = g. It is often common to present commutative diagrams showing relationships in a category. In these diagrams, if we have any two paths between to objects, the arrows that results from combining the arrows along these paths must be the same. For example, in the diagram</p><formula xml:id="formula_37">A B C D f 1 f 2 g 1 g 2 we must have f 2 • f 1 = g 2 • g 1 .</formula><p>For any two objects A and B in a category if there is a pair of arrows f : A → B and g : B → A such that g • f = 1 A and f • g = 1 B , then A and B are said to be isomorphic and we write A ∼ = B. In that case, for all intents and purposes A and B can be treated as the same object. Between two categories C 1 and C 2 there may be a functor F , that is made up of two functions F = (F 1 , F 2 ). The first one maps objects in C 1 to objects in C 2 , whereas the second one maps arrows in C 1 onto arrows in C 2 . When writing functors it is often usefull to treat the pair of functions as a single function and leave out the indices. This does not create ambiguity, because one of the functions maps objects in C 1 to objects in C 2 whereas the other maps arrows in C 1 to arrows C 2 . Any functor F must preserve the structure of the category C 1 when mapping to C 2 . That is for any arrow g in C 1 we must have F (dom g) = dom F (g) and F (cod g) = cod F (g). Furthermore, for any pair of arrows g 1 and g 2 such that</p><formula xml:id="formula_38">g 2 • g 1 is an arrow in C 1 we require F (g 2 • g 1 ) = F (g 2 ) • F (g 1 ).</formula><p>Finally, the identity arrows must be preserved, thus F (1 A ) = 1 F (A) . If a category can be embedded into another using a functor without losing or adding information, the two categories are said to be equivalent. More formally, two categories C 1 and C 2 are said to be equivalent, if there exits a functor F : C 1 → C 2 such that for each pair of objects A and B in C 1 the function F A,B : Hom(A, B) → Hom(F (A), F (B)) with F A,B (f ) = F (F ) is bijective and for every object D in C 2 there exists and object A such that F (A) ∼ = D <ref type="bibr">(Barr &amp; Wells, 1990, p. 86)</ref>. While this definition may be hard to grasp at first, what this essentially is saying that in an object in a category may have more than one corresponding object in another equivalent category. However, if an object corresponds to more than one object in the equivalent category, then all these objects must be isomorphic.</p><p>With these definitions, the poset of p-frames can be seen as defining a category. The objects in this category are the p-frames, and there is an arrow between two p-frames F 1 → F 2 iff F 2 F 1 . Thus, in this category we define the Hom-sets as Hom</p><formula xml:id="formula_39">(F 1 , F 2 ) = {(F 1 , F 2 )} if F 2</formula><p>F 1 and Hom(F 1 , F 2 ) = ∅ otherwise. Transitivity of the ordering relationship ensures that we may combine any two arrows between matching domains and codomains, while reflexivity ensures the existence of arrows of the type F → F. Also, categories like this one, which have at most one single arrow between two objects are also called simple categories.</p><p>We will now introduce another category where the objects are functions over Z. This is similar to the construction of partitions as functions over N as done by <ref type="bibr" target="#b8">Ellerman (2010)</ref>. Afterwards we will show that this category is equivalent to the category of p-frames, and thus we can treat these two categories as the same one.</p><p>Definition 5 (Category of Representations). For any set of trials T we define a category Ψ(T ). The objects of this category are all functions f : T → Z. We call the objects of Ψ(T ) the representations of p-frames. For any two objects f and g of Ψ(T ) define the set of odd functions</p><formula xml:id="formula_40">Υ f,g = { Y : Z → Z | Y (−z) = −Y (z), Y • f = g }. That is, Υ f,g</formula><p>is the set of all functions for which the diagram </p><formula xml:id="formula_41">T Z Z Z Z f g Y Y •(−1) •(−1) commutes. Also, let img f = { f (τ ) | τ ∈ T } ⊆ Z</formula><formula xml:id="formula_42">= { Y • i f : img f → Z | Y ∈ Υ f,g }.</formula><p>The objects of this categories assign a number to each trial in the experiment. However, these numbers are not required to be unique, that is different trials can be assigned the same number. Also, the objects in Ψ(T ) can assign both positive as well as negative numbers. The arrows allow to translate from the numbers assigned by one object f to the numbers assigned by another object g. Since numbers are translated and not just re-assigned, this means if f assigns the same number to τ i and τ j if there is an arrow from f to g, then g must also assign the same number to τ i and τ j . In addition, the requirement that Υ f,g must only contain functions such that Y (−z) = −Y (z) ensures that the relation between assigned positive and negative numbers must be retained. That is, if f assigns some number z to τ i and −z to τ j , then g must assign some number z to τ i and −z to τ j .</p><p>For any given two objects f and g the set Υ f,g may contain infinitely many functions. Thus, one could expect Ψ(T ) to have infinitely many arrows between any two objects. However, because the Hom-sets are restricted to the image of f there is in fact at most one arrow between any two objects in Ψ(T ).</p><p>Lemma 4. Let T be any set of trials. Then Ψ(T ) is a simple category, that is, there is at most one arrow between any two objects in Ψ(T ).</p><p>The previous lemma also indicates, that it is sufficient to provide values only on img f when defining an arrow. This can be used to define a simple algorithm to construct an arrow between to objects f and g or indicate that no such arrow exists (see Algorithm 1).</p><p>We now want to show that the category Ψ(T ) is equivalent to the category of p-frames. In that case, we can use Algorithm 1 to automatically detect for any two p-frames if F 1 F 2 . Thus, we need to show how a functor between these two categories may be defined. function</p><formula xml:id="formula_43">ConstructHomSet(f, g) returns Hom(f, g) Y ← { z → None | z ∈ Z } Y : Z → Z ∪ {None} with ⊥ ∈ Z Y (0) ← 0 may break Y (−z) = −Y (z) otherwise for all τ ∈ T do if Y (f (τ )) = None then Not yet assigned Y (f (τ )) ← g(τ ) Y (−f (τ )) ← −g(τ ) else if Y (f (τ )) = g(τ ) then return ∅ Cannot construct arrow end if end for return {Y • i f } Single arrow f → g end function 2. f (τ ) = −f (τ ) iff τ ≡ Ω(τ )</formula><p>We write ψ (f ) = F to indicate that f is a representation of F.</p><p>This definition provides some rules that relate objects in Ψ(T ) and p-frames to each other. However, this definition neither shows how a p-frame F that is represented by f may be constructed, nor that such a p-frame must exists.</p><p>Lemma 5. Let f be an object in Ψ(T ). Then there exists a unique p-frame F such that ψ (f ) = F.</p><p>The proof to the previous lemma not only shows the uniqueness and that we can use the concept of a representation to define a functor between the categories, but also gives a simple method that can be used to construct a p-frame from a representation. Second, we will now show that there is a bijection between the arrows in the category of p-frames and the ones in Ψ(T ).</p><p>Lemma 6. Let F 1 and F 2 be two p-frames with any two representations f and g in Ψ(T ) such that f = ψ (F 1 ) and g = ψ (F 2 ).</p><p>If F 1 F 2 then there is an arrow f ← g in Ψ(T ). Similarly, if there is an arrow f ← g in Ψ then F 1 F 2 .</p><p>The previous lemma plays a key part in showing that both categories are equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4. The category of p-frames with Hom</head><formula xml:id="formula_44">(F 1 , F 2 ) = {(F 1 , F 2 )} iff F 2 F 1 and the category Ψ(T ) are equivalent.</formula><p>What this theorem shows is, that although a p-frame can have infinitely many representations in Ψ(T ), still the representations carry the same relations to each other as the p-frames do. That is, any algorithm that we define over the representations can directly be turned into an algorithm over p-frames. The only crucial caveat is, that we cannot check for equivalence in Ψ(T ), but if we want to ensure equivalence in the category of p-frames we have to check for an isomorphism in Ψ(T ) instead. Fortunately, this can easily be done by running Algorithm 1 twice to construct Hom(f, g) and Hom(g, f ) and check that both sets are non-empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algebraic Operations on Perceptive Frames</head><p>The equivalent category of Ψ(T ) already allows us to use Algorithm 1 to check if F 1 F 2 for any two p-frames. Furthermore, we will now show that we can use the structure of Ψ(T ) to also combine any two p-frames to construct a new preceptive frame that allows to use to the information used by both original p-frames.</p><p>Posets often can be equipped with a pair of algebraic operations known as the join and the meet, which can be defined using the partial ordering of the poset (see <ref type="bibr" target="#b18">Grätzer, 1998</ref>, for a complete introduction). Since we currently only provide an implementation of the join, we will not discuss the meet operation here. For each non-empty subset W ⊆ S of a poset (S, ≤) there is an upper bound W u := { s ∈ S | ∀w ∈ W : w ≤ s }. If there exists a single element x ∈ W u such that ∀w ∈ W u : x ≤ w, that is x it is the smallest element in the upper bound, then x is called the least upper bound or supremum of W . We will write x = sup W to indicate the supremum of any non-empty subset W of a poset if a supremum exists. Let (S, ≤) be any poset, such that for all sup{x, y} is called the join of x and y. We write x ∨ y (also x ∪ y, x y, or x y) to denote the join of x and y. If a poset has a join for any pair of elements, then it is called a join semi-lattice. The join can be interpreted as an algebraic operator over the set S which is idempotent a ∨ a = a, commutative a ∨ b = b ∨ a, and</p><formula xml:id="formula_45">associative (a ∨ b) ∨ c = a ∨ (b ∨ c).</formula><p>The join of two elements of a join semi-lattice has a direct correspondence in category theory. In any category with objects a,b, and c, the object c is called the product a  </p><formula xml:id="formula_46">(f × c g)(τ ) = f (τ ) • (2 • c + 1) + g(τ ).<label>(15)</label></formula><p>Then f × c g is a representation of the join of the two p-frames, i. e. ψ (f × c g) = F 1 F 2 .</p><p>We would like to point out, that for any two objects f and g the object h := f × c g with h(τ ) = f (τ ) • (2 • c + 1) + g(τ ) is only one possible representation of F 1 F 2 . Most importantly, h = h := f × c g depends both on the choice of the constant c as well as the order in which f and g are combined, such that usually f × c g = g × c f . However, the equivalence of the two categories ensures that for any F 1 , F 2 with F 1 = ψ (f ), F 2 = ψ (g), and F 1 F 2 = ψ (h) the relation h ∼ = f × c g must hold for any valid choice of c. Still, we want to ensure that no additional information that was not present in F 1 and F 2 is created when joining these two p-frames. Corollary 1. Let F 1 and F 2 be two p-frames and F 3 = F 1 F 2 . Then for any two trials τ and τ</p><formula xml:id="formula_47">• τ ≡ F 3 τ iff τ ≡ F 1 τ and τ ≡ F 2 τ . • τ ≡ F 3 τ iff τ ≡ F 1 τ or τ ≡ F 2 τ . • τ ≡ F 3 Ω 3 (τ ) iff τ ≡ F 1 Ω 1 (τ ) and τ ≡ F 2 Ω 2 (τ ).</formula><p>Thus all distinctions that can be made by using a p-frame F 3 that is the join of F 1 and F 2 must already have been possible with either F 1 or F 2 . Also, no new opposites are created when combining two p-frames. Therefore, we can see that combining two p-frames does not add any further information that was not available under each p-frame which entered the combination. This has an important implication for the analysis of experimental data. In Section "Bayes Factors of Perceptive Frames" we provided a method to analyze data, such that we can compute the BF, which provide a measure of the plausibility of one hypothesis over the other. For two p-frame F 1 and F 2 if the BF for F 1 F 2 compared to F 1 shows support of F 1 F 2 , then we know that the additional support must come from the information that is added from F 2 and cannot be attributed to any other information. In addition, in any join semi-lattice, the join must be associative and commutative, such that we can compute the least upper bound of any finite set of p-frames. The upper bound of multiple p-frames is then the single p-frame that uses the information available under all other perceptive frames. That means, that is it sufficient to generate the p-frames corresponding to simple hypotheses, and all more complex p-frames can be automatically generated using equation <ref type="formula" target="#formula_46">15</ref>. This allows us to automatically generate the most general model in line with the hypothesis that some information is used or not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure of the Perceptive Frame Lattice</head><p>In the previous sections we have shown that p-frames induce a join semi-lattice. In addition, the definition of the category Ψ(T ) of the representations allows us to make some observations about the structure of this join semi-lattice.</p><p>If there exists a smallest element ⊥ = inf S of the complete poset S, then ⊥ is called the bottom element of the poset. Similarly, if there exists a largest element = sup S of the complete poset S, then is called the top element of the poset. The elements x following the bottom element ⊥ of a poset ⊥ &lt;: x are called the atoms of the poset. For p-frames the bottom and top elements are the guessing and the saturated p-frame, which consider no information at all, or all possible information respectively.</p><p>Corollary 2. For any p-frame F we have F guess F F sat where π guess := T and Ω guess (β) = T as well as π sat := τ 1 |τ 2 | . . . |τ N and Ω sat (β) = ∅.</p><p>The last corrolary shows that the guessing and saturated strategies, which are for example used in Bayesian strategy selection (e. g., <ref type="bibr" target="#b30">Lee, 2016)</ref> are indeed the most general and most complex strategy that can be defined. In accordance with the usual naming scheme of the minimal and maximal element of a poset, we will therefore also refer to the guessing strategy as ⊥ (read: "bottom") and the saturated strategy as (read: "top"). The geometry F guess of the guessing perceptive frame is the zero dimensional vector space containing only the origin. The geomtry F sat is the complete R N , showing that these two p-frames are also the smallest and largest possible object in the poset of linear subspaces. The atoms of the poset of p-frame have a similarly simple characterization.</p><p>Lemma 7. Let F be a p-frame. Then F is an atom in the poset of p-frames iff dim F = 1.</p><p>Thus, the atoms are all those posets, which correspond to one dimensional linear subspaces of the R N . It is important to mention that we only provide these atoms to show the structure of the poset. These atoms are not necessarily psychologically meaningfull, and may often represent information that is not useable by itself. For example, we could imagine a choice task where cues are presented on either a green or red background. While one could hypothesize that a red background could lead to more carefull inspection of the provided information, as red is often seen as a warning signal, this kind of information usage would represent the usage of the color in combination with other provided information. However, the atomic p-frame F color in which only the background color is processed would require a different probability of chosing the left option in trials with a red or a green background. In addition, because this p-frame excludes any information besides the background color, this change in probability must result only from the change in color, and cannot result from the background color in combination with any other presented information. Such a hypothesis seems unlikely. Still, it might make sense to define this kind of p-frame, since we can then include this perceptive frame when the complete lattice is generated by joining it with all other frames. Also, for example when testing if the background color is used, one should test the BF</p><formula xml:id="formula_48">BF F,F = F ∈F W F F ∈F W F</formula><p>with F = { F | F color F } and F = { F | F ∈ F } instead of comparing W F color with any of the other Bayes weights. Thus, all p-frames for which the background color is used in any combination with any of the other provided information should be seen as supporting the hypothesis that background color was used by the participants.</p><p>Also, knowing the structure allows us to calculate how many atoms there are for any given number of trials.</p><p>Lemma 8. Let T be a set of trials and N := |T | the number of trials. Then the number of atoms is O(3 N ), that is there exponentially many atoms.</p><p>Again this shows, that the p-frames used to test a hypothesis should be defined based on theoretical reasons. Even for the small experiment with just three trials we presented as an example in <ref type="table">Table 1</ref> there are already 13 atoms. When these 13 atoms are then joined to generate the complete lattice the combinatorial explosion is even higher. Fortunately, not all of these 13 atoms are psychologically meaningfull. For example, the p-frame with π = 1|23 and Ω(1) = ∅ and Ω(23) = 23 is a valid p-frame, but trial τ 1 and τ 3 were constructed by mirroring all cue information, so it would be hard to justify why participants should ignore the information in one of these trials and use it in the other.   <ref type="table">Table 1</ref>. Green borders mark all p-frames for which the side information is used, blue borders mark all p-frames which use the first cue, orange borders mark all p-frames which use the third cue, and red borders mark all p-frame which use the number of recommendations for either option while ignoring the order.</p><p>Before proceeding to understand the structure of the p-frame lattice, we will first return to the example experiment given in <ref type="table">Table 1</ref>. Lattice and posets can easily be visualized by using so called "Hasse diagrams" <ref type="bibr" target="#b20">(Hasse, 1952</ref><ref type="bibr" target="#b20">(Hasse, /1985</ref>, in two elements of a poset are connected by lines, if they follow each other. <ref type="figure" target="#fig_9">Figure 4</ref> shows the hasse diagram corresponding to the p-frames given in <ref type="table">Table 1</ref>. This diagram directly shows how the different p-frames are related. For example, this diagram shows that the order p-frames (corresponding for example to the weighted additive strategy) is also the join of the count p-frame and the first cue p-frames. Thus, participants could show the same behavior when using the recommendations in the order they given as when they only use the first cue in combination with the sum of recommendations for either options. Thus, in this experiment these two behaviors would become indistinguishable. The same behavior could also be achieved, if participants only used the first and third cue, but ignored all others. Further, by comparing the geometries presented in <ref type="figure" target="#fig_16">Figure 2</ref> with the Hasse diagram in <ref type="figure" target="#fig_9">Figure 4</ref>, one can observe that the geometries of p-frames, which are created by combining simpler p-frames, can also be seen as the combination of the simpler geometries. This is given by the following lemma. </p><formula xml:id="formula_49">:= { a + b | a ∈ A, b ∈ B }. Then F 1 F 2 ⊆ F 1 F 2 .</formula><p>This lemma provides a lower bound for the geometries of the join of two p-frames. However, by looking at <ref type="figure" target="#fig_16">Figure 2</ref> and 4, one can immediately see, that the Minkowski sum is only a subset of the complete geometry of the join. For example, the geometries of the side and the count p-frames both have dimension one, but when joined these yield the saturated p-frame, which has dimension three. Thus, when these two p-frames are joined, the Minkowski sum of each geometry does not produce the resulting geometry. Also, the previous lemma allows us to proof the the dimension must increase when p-frames contain each other.</p><p>Lemma 10. The number of dimensions increases strictly monotonical with the lattice of the p-frames. That is, let F 1 and F 2 be two p-frames with</p><formula xml:id="formula_50">F 1 F 2 then dim F 1 &lt; dim F 2 .</formula><p>Most importantly, this shows that p-frames define nested models, if they are contained within each other. Algorithm 1 then gives a simple method to check whether two perceptive frames are nested or not. Also, by showing that related p-frames define nested models, we confirm the previously stated intution that only related p-frames should be compared directly, whereas comparisons between different p-frames may be problematic and lead to errornous conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation and Evaluation</head><p>To test our method, we both run a simulation study as well as an analysis on real data collected from an experiment. The simulation study is used to determine the reliability of the described method in terms of specificity and sensitivity. Thus, we test how well we can recover the p-frames used to generate artificial data. In addition we will also test how well we can test hypotheses corresponding to sets of p-frames on the same data. Furhermore, we will also present some benchmarking results to indicate how efficient the analysis can be carried out. In the second experiment, we will show an example analysis based on data collected from real participants as part of another study. This section mainly serves as an example for future researchers using our method to show how the data may be presented and how hypotheses may be formulated based on the lattice structure of the p-frames. All code and the used datasets are provided on the open science framework (OSF) at https://osf.io/pajqk/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Recovery Simulation and Benchmarking</head><p>Experiment 1 is used to determine the overall reliabilty of the presented method using a recovery simulation. Thus, we will use artificially generated datasets to test how well the presented method may be used to recover the p-frames originally used during generation of the datasets.</p><p>Materials and Methods. To test how well we can retrieve a p-frame from simulated data, we used the same cue patterns as <ref type="bibr" target="#b3">Bröder and Schiffer (2003)</ref> and <ref type="bibr">Hilbig and Moshagen (2014, see</ref>  <ref type="table">Table 2</ref>). However, to also test for a possible side bias, each cue pattern was used twice, once in the original form and once in mirrored form. To keep the number of trials per simulated participant the same as Hilbig and Moshagen (2014) we only repeated each pattern 15 times instead of 30 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Trial patterns used in the recovery simulation taken from <ref type="bibr" target="#b22">Hilbig and Moshagen (2014)</ref>   <ref type="figure" target="#fig_7">Figure 5</ref> . Lattice of the p-frames used for the recovery simulation.</p><formula xml:id="formula_51">τ 1 τ 2 τ 3 O 1 O 2 O 1 O 2 O 1 O</formula><p>Procedure. We generated random choice data by first generating a random consistent assignment for a perceptive frame and then generating random choices based on the resulting probabilities in each trial. To test the influence of the guessing parameter we varied both the p-frame as well as the parameter a of the Beta(a, a) distribution that was used to generate the consistent assignments. The parameter a was taken from the set {0.5, 0.75, 1, 1.25, 1.5}. For each combination of a p-frame and a guessing parameter we generated 500 simulated paticipants. P-frames were created by first creating the p-frames F guess , F side , F F irstCue , F Count , and F Ordered (see <ref type="table">Table 2</ref> for example representations of these p-frames). Based on these four p-frames the complete lattice with 8 p-frames was generated (see <ref type="figure" target="#fig_7">Figure 5)</ref>. Data was generated for each of these 8 p-frames. To get a reliable estimate of the recovery rate and the required time for the analysis the complete process was repeated 100 times.</p><p>The analysis method was implemented in python using the scipy and numpy libraries (e. g., <ref type="bibr" target="#b40">Oliphant, 2007</ref><ref type="bibr" target="#b41">Oliphant, , 2015</ref> for matrix algebra and calculation of the beta function. In addition the preprocessing functions from scikit-learn <ref type="bibr" target="#b42">(Pedregosa et al., 2011)</ref> are used for generating the representations. We used a SQLite3 <ref type="bibr" target="#b23">(Hipp, Kennedy, &amp; Mistachkin, 2018)</ref> database as backend for data storage during analysis and for computing all summaries (e. g., sums of weights used for computation of the BF). The work laptop of one of the authors was used for simulation. Thus, the simulation was run on a computer using a 2.50 Ghz Intel i5-6500T CPU (4,993 BogoMIPS) running Gentoo Linux. The system was equipped with 16 GB of RAM. No multi-core parallelisation was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Benchmarking. We benchmarked the total time each simulation run took. These simulation runs consisted of the generation of the four base p-frames, completion of the lattice, generation of 500 simulated participants for each of the 8 p-frames and each of the 5 different values of the guessing parameter. Thus, in each run 20,000 random datasets were generated and analysed. For each simulated dataset Bayes weights for all p-frames and all different values of the guessing parameter were generated. Therefore, each run was equivalent to 800,000 evaluations of Equation (14). On average each of the 100 runs required 16,389ms <ref type="bibr">(SE = 16,</ref><ref type="bibr">389ms)</ref>. This shows, that this analysis is suitable even for large data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fout = Fin</head><p>Fout Fin Fin Fout Fin = Fout 0 . 5 0 0 . 7 5 1 . 0 0 1 . 2 5 1 . 5 0 0 . 5 0 0 . 7 5 1 . 0 0 1 . 2 5 1 . 5 0 0 . 5 0 0 . 7 5 1 . 0 0 1 . 2 5 1 . 5 0 0 . 5 0 0 . 7 5 1 . 0 0 1 .  To check the correctness of the results we used two different approaches. First, we checked if the perceptive frame that was used during simulation could be identified based on the Bayes weights, i. e. if the Bayes weight for that frame was higher than for all others. This test was performed for all possible values of a used during simulation (a (generative)) and as a parameter to the prior (a prior). The results for exact recovery are presented on the left side of <ref type="figure" target="#fig_5">Figure 6</ref>. Overall, the correct p-frame could be identified in at least 60 % of the cases, even for simulations where strong guessing was simulated. Recovery rates were only little influence by the choice of the prior parameter. While the recovery rates are somewhat worse than for methods used for strategy classification (e. g., <ref type="bibr" target="#b3">Bröder &amp; Schiffer, 2003;</ref><ref type="bibr" target="#b22">Hilbig &amp; Moshagen, 2014)</ref>, the number of possible classifications is also much larger. For example <ref type="bibr" target="#b3">Bröder and Schiffer (2003)</ref> used six strategies during their recovery simulation to achive a recovery rate of at least 83 % (chance level: 16 %), whereas the 75 % recovery rate was achieved using 20 different p-frames (chance level: 5 %). In addition, we also checked if the recovered p-frame was related to the one used during simulation. The second panel from the left in <ref type="figure" target="#fig_5">Figure 6</ref> shows the percentage of simulations, during which the recovered p-framecontained less information than the one used during generation. In a substantial number of the cases, during which the original p-frame was not exactly recovered, our method was at least able to recover a p-framewhich contained part of the information used during simulation. This was especially the case, if strong guessing was assumed. This is unsurprising, since a large amount of guessing means, that not all information available under the p-frame was used during generation, such that the unused information cannot be detected. Most importantly, hardly ever was a p-frame classified that contained more information (second panel from the right in <ref type="figure" target="#fig_5">Figure 6</ref>) and even less often a completely unrelated p-frame(right panel in <ref type="figure" target="#fig_5">Figure 6</ref>).  <ref type="figure">Figure 7</ref> . ROC curves and AUC for all basic p-frames except F guess . Strong lines show the average over all combinations of a used for generation and analysis. Grey lines show the ROC curves for individual combinations of a. Curves above the diagonal indicate that our method is able to correctly distinguish simulations in which a piece of information was used from those were it was not.</p><p>As a second analysis approach we tested how well the BFs can be used to identify if information is used during decision making. Thus, for each of the p-frame F given in <ref type="table">Table 2</ref> except for F guess we defined the set of p-frames larger than this p-frame, {F} u . This set includes all p-frames for which at least the information distinguished by F can be processed. We then computed the BFs for F F versus F F where F F contained all other generated p-frames not in F F . We then analysed the resulting BFs by choosing a cutoff value, and determined that the information defined by F was used if the BF was above that cutoff value. By choosing different cutoff values, we get a pair of true positive and false positive rates for each of the cutoff values, the so called receiver operating characteristic (ROC). <ref type="figure">Figure 7</ref> shows the ROC-curves for all p-frames that we tested. To summarize an ROC-curve the area under the curve (AUC) is often used. The AUC is a value in the range [0, 1] with AU C = .5 indicating guessing and AU C = 1 indicating perfect classification. The AUC for all perceptive frames we tested were above .9, showing that our method can be used for determining if a piece of information was used during processing with very high accuracy.</p><p>Discussion. Our simulation shows that the proposed analysis method can be used with high reliability and efficiency. The p-frames used to generate the artificial data were recovered in the majority of the cases. Some problems may be expected, though, if the participants tend to behave erratic and mostly resort to guessing, even if information is available. This does not necessary mean, that they ignore the information, but rather that the provided information only causes a very small bias of the probabilities. In this case, simpler p-frames may be recovered. However, this problem cannot be solved in general, since in this case the small bias simply creates only a very small signal, which may be impossible to detect in most cases. However, because we do not intend our method to be used for exact tests on a single p-frame, the low reliability compared to other methods (e. g., <ref type="bibr" target="#b3">Bröder &amp; Schiffer, 2003)</ref> is only of theoretic interest. Because p-frames correspond to the most general predictions that can be made under the corresponding hypothesis, they must have much larger complexity than other more specific models, leading to a higher number of false positives. In addition, p-frames can currently only use the choice data and no additional experimental data such as reactions times or confidences. Because more specific models can incorporate such data in additon to the choice data <ref type="bibr" target="#b14">(Glöckner, 2009)</ref>, model fitting approaches should be expected to show a much higher recovery rate, even under comparable conditions to our test.</p><p>The main approach that we recommend p-frames to be used is to test hypotheses if some information is used or not. Because the algebraic approach we present here allows construction of a larger number of p-frames and therefore to generate the most general predictions in line with the hypothesis, this approach differs strongly from strategy fitting. Whereas in strategy fitting methods each model is tested in isolation to find the best model in a set of models, we specificially test subsets of p-frames corresponding to a hypothesis against all other possible p-frames. We show that the method can be used with high reliability to test hypotheses about the information to be used. One benefit of using the proposed method is that other information is automatically controlled for during the analysis. For example, the usage of the gender information is still detected, even if the participants combine it in an arbitrary fashion with the cue information.</p><p>Finally, we show that our method is efficient. We explicitely chose a very high number of participants to show that even very large experiments can be analysed in a small amount of time. In addition, our current method computes the Bayes weights for each p-frame in isolation, such that it scales linearly (O(n)) with the number of p-frames used in the analysis. This means, that our method can support a much larger number of p-frames than the 8 p-frames we used here. This may be required for more complex hypotheses or designs.</p><p>Also, the calculation of the Bayes weights can be trivially parallized if more speed is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Example Analysis of Real-World Data</head><p>In Experiment 2 we make an example how an analysis of a dataset from real participants may be carried out. Thus, we will focus on the methodological aspects of the analysis and mostly ignore the psychological aspects. A full discussion of the implications of this experiment will be presented together with the results from other experiments in an upcomming publication (N. <ref type="bibr">Nett, Nett, Gaschler, &amp; Glöckner, in preparation)</ref>.</p><p>Materials and Methods. Participants and Design. The participants in this study were 109 students (78 female, 31 male) from the University of Trier recruited via a student trainee. The mean age of the participants at the time of the experiments was M = 23 (min = 19, max = 33, SD = 2.7). Participants were tested in single sessions in soundproof chambers. For each decision, participants were also paid 4 Euro Cents, if their decision followed the rational Bayesian solution <ref type="bibr" target="#b31">(Lee &amp; Cummins, 2004)</ref>. That is, they could receive between 11.52 € and nothing, for all correct vs. all wrong choices, respectively The total reward and the number of correct decision was not revelead until the end of the experiment.</p><p>Decision tasks were varied as a within-subject factor. During each trial one of four 5 cue matrices was shown together with two names and a job title. In addition, participants were divided into two conditions as a between-subject factor. In the first condition, the job title and the names were presented before the cue matrix (SOA = 1500ms) was shown, whereas in the second condition all information was shown simultaneously (SOA = 0ms). Task and Procedure. Througout the experiments we used decision matrices like the one shown in <ref type="figure" target="#fig_14">Figure 8</ref>. The participants had to repeatedly decide between two named candidates applying for a job. The decision criterion was the future job performance. Participants chose one option by mouse click. The options could have two female names, two male names, or a male name and a female name in either order. Above the matrix, a job title was presented, indicating for which job the candidate should be hired. The jobs were either strongly associated with female workers or to male workers <ref type="bibr" target="#b27">(Koch, D'Mello, &amp; Sackett, 2015)</ref>. There were four different jobs: two that were associated to be female jobs (elementary school teacher, physician's assistance) and two that were associated to be male jobs (computer scientists, electrician). Job titles were presented in a gender-neutral form.</p><p>We refer to the associations of the jobs and the gender of the candidates as the gender information. Experts recommended the options. We refer to these recommendations as the cue pattern. The names we used were taken from the preliminary results of a different study, which aimed at validating associations to German first names (T. <ref type="bibr" target="#b38">Nett, Dorrough, Jekel, &amp; Glöckner, 2019)</ref>. The experiment was programmed in Visual Basic 10. The complete experimental software has been uploaded to OSF at https://osf.io/cxmus/.</p><p>In each trial participants had to imagine that they were hiring one of two candidates. The information they received to choose between the two candidates was the recommendation from four different experts. They were asked to hire the person that would be better suited for the job. They were told, to only use the expert recommendations, and ignore any additional information. Also, participants were not given feedback if their decision was correct or incorrect during the experiment. After the instructions, participants had to perform one test trial. In this test trial a different cue pattern and job title (journalist) than in the experiment was used. After the test trial, the participants were notified that the main experiment would start and after clicking a button they were directed to the first trial.  <ref type="figure" target="#fig_10">Figure 9</ref> . Hasse diagram of the example experiment for hiring decisions with gender information. Perceptive frames with grey background are defined manually in the experiments whereas all others are computed automatically. Green borders mark all p-frames for which the cue information is used and orange borders are p-frames that use any kind of gender information.</p><p>Results. We defined four basic p-frames, a "Cue" p-frame, which corresponded to usage of only the cue information; a "Bias" p-frame, which corresponded to preferences of either male or female candidates without regard of the job; a "Match" p-frame, which corresponded to preference for females for one type of jobs (male of female associated) and males for the other type of job; and a "Complex" p-frame, which corresponded to different preferences of male or female candidates in each of the four jobs. Based on these four p-frames we automatically completed the lattice to get 10 p-frames, which were included in our analysis. Since gender information is used for both the "Bias" as well as for the "Match" and "Complex" p-frame, we selected all p-frames above these three vs. all other p-frames when testing if the gender information was used. Based on the 10 p-frames resulting from completing the lattice, we tested two types of behavior separately for each participant. First, we tested how much evidence can be found that each participants used the cue information (Cues used: 10 p-frames vs. Cues not used: 10 p-frames). For this we included all p-frames above the Cue p-frame in the completed lattice in one subset (green border in <ref type="figure" target="#fig_10">Figure 9</ref>) and compared the total Bayes weight for these frames to the Bayes weight of all remaining p-frames. Second, we also tested how much evidence can be found for the use of any gender information (Gender information used: 8 p-frame vs. Gender information not used: 2 p-frames). For this we included all p-frames above the Bias, Match, or Complex p-frames (orange border in <ref type="figure" target="#fig_10">Figure 9</ref>) and compared the total Bayes weight for these frames to the Bayes weight of all remaining p-frames. To test for different levels of guessing, we varied the guessing parameter in the range between α = 0.50 to α = 2.00 in increments of 0.01. The evidence resulting from this experiment is visualized in <ref type="figure" target="#fig_0">Figure 10</ref>. We found decisive evidence that the cues were used by all our participants (BF &gt; 10 1 3, across all values of the guessing parameter of a). To test for evidence that the gender information was used, we picked the highest BF across all values of a. We found very strong evidence (BF ≤ 10 −2 ) that the gender information was not used for 99 % of the participants and decisive evidence (BF ≤ 10 −3/2 ) that it was not used for 98 % of the participants. Only for 1 % (1 participant) of the participants the evidence was inconclusive (10 −1/2 ≤ BF ≤ 10 1/2 ). No positive evidence was found that the gender information was used for any of the participants. Since we found no evidence, that the gender information was used, we did not analyze the two SOA conditions any further.</p><p>To compare our result to other methods, we also performed a several logistic regressions of the choice data on the evidence provided by the cues and the gender information. For this we used a guessing model, which only used an intercept; a rational model, in which the evidence was determined based on the Bayesian probability of Option 1 being the correct option; an unspecific gender bias model, which included both the cue information as well as another predictor that was -1 if Option 1 was female and Option 2 was male, 1 if Option 1 was male and Option 2 was female, and 0 if both options had the same gender; a specific gender bais model, which included both the cue information as well as another predictor that was -1 if Option 1 did not match with the gender associated to the job but Option 2 did, 1 if Option 1 matched, but Option 2 did not match, and 0 if either both options matched or neither option matched; And a complete model, that combined all three predictiors. Models were fit using as Bayesian logistic regression (arm R-Package; <ref type="bibr" target="#b10">Gelman &amp; Su, 2018)</ref>. We initially also attempted to fit models using maximul likelihood, but the data showed perfect separation for some of the participants. We compared the models based on the BIC <ref type="bibr" target="#b46">(Schwarz, 1978)</ref>. Using this approach, we found that 97 % of the participants were best described by the rational model. The remaining 3 % were classified as using one of the strategies that included the gender bias. To check how much evidence this analysis provides, that the gender bias was used, we computed the BF from the BICs <ref type="bibr" target="#b48">(Wagenmakers, 2007)</ref> for the rational model vs. the best (in terms of BIC) competing model that included a gender Bias. This analysis showed substantial evidence (BF ≥ 10 1/2 ) that no gender bias was used for 93 % of the participants, and even strong evidence (BF ≥ 10 1 ) for 67 % of the participants. Only for 1 % (1 participant) of the participants we found substantial evidence that the gender bias was used. The single participant for which evidence of was found was the same participant that was inconclusive when we analyzed the data using p-frames.</p><p>Discussion. The analysis shows, that our method can be used to analyze real behavioral data. By defining p-frames corresponding to some simple behaviors, we can automatically generate p-frames which correspond to more complex behavior that may result from the combination of different information. Most importantly, we can define p-frames corresponding to the comined usage of cue and gender information, without having to specify how this information is used. With this method we can find clear evidence that the cue information is used in some way by all of our participants. This implies, that models which made the incorrect assumptions about how this information is used, either in combination with the gender information or without it, might have produced incorrect results. In fact, when we tested models that assumed that cue information is used in a rational fashion, the evidence was much weaker. The reason is, that the BF is always the result of comparing sets of models, and thus is highly dependent on the choice of the models. In addition, using p-frames we found very clear evidence that gender information is not used by almost all participants. Again, using another approach that made more assumptions on how the information is used produced less clear evidence. Still, the results from both analysis methods were very similar, showing that our method can be reliably used to identify which information is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We demonstrated a method that allows identification of the information that is used by participants in decision making tasks. Because this method does not rely on any assumptions on how the information is used, it is more general than approaches that use decision strategies to analyze the behavior <ref type="bibr" target="#b3">(Bröder &amp; Schiffer, 2003;</ref><ref type="bibr" target="#b22">Hilbig &amp; Moshagen, 2014;</ref><ref type="bibr" target="#b30">Lee, 2016)</ref>. Our approach relies on the identification of the most general predictions that can be made about the behavior, if the used information is restricted in some way. Because the general nature of these predictions, we do not rely on predictions of specific choice probabilities <ref type="bibr" target="#b3">(Bröder &amp; Schiffer, 2003</ref>; see also <ref type="bibr" target="#b33">Moshagen &amp; Hilbig, 2011)</ref>. Rather, we the predictions are made up of patterns that must be consistent during the experiment. These patterns may be encoded as equality constraints over the choice probabilities for the trials. Because this method is based on constraints which are placed on the choice probabilities, it provides a geometric interpretation of the predictions. Therefore, this method closely related to other methods which are based on geometric interpretations of the models <ref type="bibr" target="#b6">(Davis-Stober, 2009;</ref><ref type="bibr" target="#b21">Heck &amp; Davis-Stober, 2019;</ref><ref type="bibr">Regenwetter et al., 2014)</ref>. However, because the equality constraints can be shown to relate to subspaces of the R N , we were also able to develop methods that do not require sampling techiques to calculate the integrals necessary for statistical testing. Instead, the special form of the constraints allow these integrals to have analytical solutions that can be calculated using matrix algebra.</p><p>Our simulation study shows that this method can be used with high reliability to identify if some information is used or not. In addition we compared our method to other comparable methods on a real world dataset to show that the results are comparable. However, since other methods require more detailed assumptions on how the information is used, our method can provide more conclusive evidence if some information is used or not used. In addition, whereas other methods require manual definition of a large number of possible models how some information is used in combination with other information, we showed that the algebraic structure of the p-frames we defined here can be used to automatically generate all possible combinations of the provided information. In addition, the poset structure of the p-frames makes it possible to automatically determine sets of p-frames corresponding to the hypothesis that is tested. While we presented this method in the context of decision making experiments using decision matrices, it has applications that are much broader. Because this method only requires that each trial must have exactly two options, it may be employed to a large number of other experimental domains, such as social dilemas or binary classification tasks. For example, <ref type="bibr" target="#b9">Fiedler, Glöckner, Nicklisch, and Dickert (2013)</ref> used eye-tracking and reaction-time measures to determine differences in the information search pattern of different individuals in a social value orientation task. However, eye-tracking is only able to indicate that some information was not seen by an individual, not that it was actually used. Instead our method allows a direct analysis of the choices to see if the information was actually included.</p><p>Nevertheless, there are some limitations of our method. First, our method assumes that the behavior is somewhat consistent during the experiment. That is, the behavior must only be determined by the information provided in the trials, not by additional factors. Thus, we could still analyze a participant that uses the information to first choose among different strategies and then apply the chosen strategy (adaptive toolbox approach; <ref type="bibr" target="#b11">Gigerenzer, 2002)</ref>. In this case, the overall behavior is still consistent and determined by the information provided in the trials. However, in other case, for example if the participant simply gets bored during the experiment and then decides to switch to a simpler strategy after a while, this behavior could not be analyzed correctly by our method. The reason is, that the information that leads to the strategy switch in this case is not part of the information that is being analyzed. While this may seem like a big drawback of our method, this drawback is shared by most other methods currently used to analyze decision making experiments <ref type="bibr" target="#b3">(Bröder &amp; Schiffer, 2003;</ref><ref type="bibr" target="#b22">Hilbig &amp; Moshagen, 2014;</ref><ref type="bibr" target="#b30">Lee, 2016;</ref><ref type="bibr">Regenwetter et al., 2014)</ref>.</p><p>A second limitation is, that currently our method can only be applied to binary choice experiments. At the moment our method relies on the fact that probabilities in reversed trials must be opposite from the original trials, which allows the definition of the required operators. While we currently cannot provide a generalization of p-frames for more than two trials, we do not see any theoretical reason why such generalization should not be possible, for example by replacing the operators we defined here with operators based on permutations instead.</p><p>Thus, we must also have ω ⊥ i = 0. Now let i such that ω ⊥ i = 0. By the above reasoning, there must be an ∈ {1, . . . , L} such that b ,i = 0. For ω ⊥ to be orthogonal to b there must be at least one other j ∈ {1, . . . , N } such that b ,j = 0. However, then we either have τ j ≡ τ i or τ j ≡ Ω(τ i ). In the first case we must have ω i = ω j and in the second one we must have ω i = −ω j . Similarly, we must have b ,i = b ,j in the first case and b ,i = −b ,j in the second case, such that we can write b ,i ω i = b ,j ω j . Since ω i = ω i + ω ⊥ i we can rewrite this as</p><formula xml:id="formula_52">b ,i ω i + b ,i ω ⊥ i = b ,j ω j + b ,j ω ⊥ j .</formula><p>However, since ω ∈ F , we must have b ,i ω i = b ,j ω j . By subtracting this from the previous equation we get b ,i ω ⊥ i = b ,j ω ⊥ j =: c = 0. However, then for some k ∈ N &gt;1 we have b T ω ⊥ = k • c = 0 and ω ⊥ is not orthogonal to b .</p><p>Lemma 3. Let F be a p-frame with γ ∈ π such that Ω(γ) = γ. Let B = (b 1 , . . . , b L ) ⊆ Gen(F) be a basis for F . For any ω ∈ F let λ = Φ + B ω. Furthermore, for &gt; 0 let n denote the number of non-zero entries of the vector b and n 0 = |γ|. Then the log-likelihood of ω is</p><formula xml:id="formula_53">Lȳ(ω) = −n 0 ln 2 − 1 2 L =1 (n + b T •ȳ) ln(1 + e −λ ) + (n − b T •ȳ) ln(1 + e λ ).<label>(11)</label></formula><p>Proof. Because ω i = 0 if τ i ∈ γ, the probability for these trials must always be .5. Therefore, this part of the log-likelihood is independent of λ and can be given as −n 0 ln 2. Thus, we can rewrite (4) as</p><formula xml:id="formula_54">Lȳ(ω) = −n 0 ln 2 − i=1 τ i ∈γ ln(1 + e −ȳ i ω i ).</formula><p>By Lemma 2 for all other trials, there exists a λ j such that ω i = ±λ j . Because Φ has at most one non-zero entry in each row, for any i ∈ {1, . . . , N }, there is an ∈ {1, . . . , L} with b ,i = 0 and for any = we have b ,i = 0. Furthermore, b ,i = ±1 such that b 2 ,i = 1 and b 2 ,i = 0. Finally, we must have</p><formula xml:id="formula_55">ω i = b ,i λ , such that we can conclude ln(1 + e −ȳ i ω i ) = L =1 b 2 ,i ln(1 + e −ȳ i b ,i λ )</formula><p>. We can substitute this into the above to get</p><formula xml:id="formula_56">Lȳ(ω) = −n 0 ln 2 − N i=1 L =1 b 2 ,i ln(1 + e −ȳ i b ,i λ ).</formula><p>We can exchange the order of summation and restructure this as</p><formula xml:id="formula_57">Lȳ(ω) = − n 0 ln 2 − L =1 N i=1 b 2 ,i ln 1 + e −ȳ i b ,i λ = − n 0 ln 2 − 1 2 L =1 N i=1 2b 2 ,i ln 1 + e −ȳ i b ,i λ = − n 0 ln 2 − 1 2 L =1 N i=1 b 2 ,i + b ,iȳi ln 1 + e −ȳ i b ,i λ + N i=1 b 2 ,i − b ,iȳi ln 1 + e −ȳ i b ,i λ .</formula><p>We observe that b 2 ,i + b ,iȳi = 2 iff b ,iȳi = 1 and 0 otherwise and similarly b 2 ,i − b ,iȳi = 2 iff b ,iȳi = −1 and 0 otherwise. Thus in the first sum all expressions where b ,iȳi = 1 vanish, and we can rewrite this as</p><formula xml:id="formula_58">Lȳ(ω) = − 1 2 L =1 N i=1 b 2 ,i + b ,iȳi ln 1 + e −λ + N i=1 b 2 ,i − b ,iȳi ln 1 + e λ .</formula><p>We can simplify this using N i=1 b ,iȳi = b T ȳ and n = b T b to get the desired result.</p><p>Theorem 3. p-frames together with the relationship of full containment are a partially ordered set, that is the relation is 1. reflexiv: F F 2. antisymmetric:</p><formula xml:id="formula_59">F 1 F 2 and F 2 F 1 imply F 1 = F 2 3. transitiv: F 1 F 2 and F 2 F 3 imply F 1 F 3 .</formula><p>Proof. Reflexivity, transitivity, and antisymmetry for the partitions of the p-frames follow directly from the poset structure of the partitions. Therefore, it is sufficient to show that these conditions are satisfied for the operators.</p><p>• Reflexivity is trivially given by the definition.</p><p>• Antisymmetry:</p><p>Proof. Let F 1 and F 2 be any two with F 1 F 2 and F 2 F 1 . By the antisymmetry of we must have π 1 = π 2 . Also by definition Ω 2 (τ ) ⊆ Ω 1 (τ ) and vice versa, which implies Ω 1 (τ ) = Ω 2 (τ ) by antisymmetry of ⊆ for any τ ∈ T .</p><p>• Transitivity:</p><p>Proof. Let F 1 , F 2 , F 3 be any three p-frames such that F 1 F 2 and F 2 F 3 . By the transitivity of we can conculate that π 1 π 3 . By the same argument for any trial τ ∈ T we have Ω 3 (τ ) ⊆ Ω 2 (τ ) ⊆ Ω 1 (τ ) and hence Ω 3 (τ ) ⊆ Ω 1 (τ ) by transitivity of ⊆.</p><p>Lemma 4. Let T be any set of trials. Then Ψ(T ) is a simple category, that is, there is at most one arrow between any two objects in Ψ(T ).</p><p>Proof. Let f and g be two objects in Ψ(T ). If Υ f,g = ∅ then no arrow between f and g exists and we are done. Thus, assume there is at least one Y : Z → Z such that Y (−z) = −Y (z) and Y • f = g. Let Y : Z → Z be in Υ f,g and z ∈ img f . Then by definition, there must be a τ ∈ T such that f (τ ) = z and therefore Y (f (τ )) = g(τ ) = Y (f (τ )). This proves that Y • i f = Y • i f and therefore |Hom(f, g)| = 1.</p><p>Lemma 5. Let f be an object in Ψ(T ). Then there exists a unique p-frame F such that ψ (f ) = F.</p><p>Proof. Define the family of sets β z = { τ | f (τ ) = z } for z ∈ Z and the set π = { β z | β z = ∅ }. Then π ∈ Part T , because each trial must appear in exactely one β z . Define the function Ω : π → π ∪ {∅} with Ω(β z ) = β −z . Note that either β −z ∈ π or β −z = ∅ and therefore the codomain of Ω is correct. For any z = 0 we have β z = β −z , and therefore there is at most one β z with β z = Ω(β z ). Also for any β z with Ω(β z ) = ∅ we have β z = Ω(Ω(β z )) and therefore F := (π, Ω) is a p-frame and by construction F = ψ (f ). To show the uniqueness, assume any two p-frame F and F , such that f is a representation of both. For the partititions to differ, there must be at least pair of trials τ and τ such that either τ ≡ F τ but τ ≡ F τ or vice versa. Without loss of generality assume that τ ≡ F τ . However, since F = ψ (f ), we must then have f (τ ) = f (τ ) and this ensures that τ ≡ F τ and thus F and F must have the same partition. The same argument can be used to show that Ω = Ω and therefore F = F . Lemma 6. Let F 1 and F 2 be two p-frames with any two representations f and g in Ψ(T ) such that f = ψ (F 1 ) and g = ψ (F 2 ).</p><p>If F 1 F 2 then there is an arrow f ← g in Ψ(T ). Similarly, if there is an arrow f ← g in Ψ then F 1 F 2 .</p><p>Proof. We will proof both directions separatedly.</p><p>• If F 1 F 2 , then there is an arrow f ← g in Ψ(T ).</p><p>Proof. Let ψ (f ) = F 1 and ψ (g) = F 2 . Assume that no function Y : Z → Z with Y (−z) = −Y (z) and Y • g = f can be defined, that is for all Y there must be at least one τ ∈ T such that Y (g(τ )) = f (τ ). For any Y let T Y = { τ | τ ∈ T , Y (g(τ )) = f (τ ) }. Since there exists at least one τ with Y (g(τ )) = f (τ ) we conclude that T Y = ∅ for all Y . Pick any Y such that T Y is minimal, that is there is no Y with T Y ⊂ T Y a true subset. Because T Y is not empty, let τ ∈ T Y and z := g(τ ). Define Y : Z → Z with Y (z) = f (τ ) and Y (−z) = −f (τ ) or Y (x) = Y (x) for x ∈ {z, −z}. Then we have Y (g(τ )) = f (τ ) and therefore τ ∈ T Y . Since T Y is minimal, there must be some τ ∈ T Y \ T Y , and therefore Y (g(τ )) = f (τ ). Also, this implies that Y (g(τ )) = Y (g(τ )) such that g(τ ) = z or g(τ ) = −z. Then τ ≡ F 2 τ or τ ≡ F 2 Ω 2 (τ ). Since τ ∈ T Y we know that f (τ ) = f (τ ) and f (τ ) = −f (τ ) such that τ ≡ F 1 τ and τ ≡ F 1 Ω 1 (τ ). But then F 1 F 2 which is a contradiction.</p><p>• If there is an arrow f ← g in Ψ(T ), then F 1 F 2 .</p><p>Proof. Since there is an arrow in Ψ(T ) there must be a function Y as given by Definition 5. Let τ, τ ∈ T such that τ ≡ F 2 τ . Then g(τ ) = g(τ ) and also f (τ ) = Y (g(τ )) = Y (g(τ )) = f (τ ) such that τ ≡ F 1 τ . Since this is true for any τ and τ j , we have π 1 π 2 . Furthermore, let τ, τ ∈ T such that τ ≡ F 2 Ω 2 (τ ). Since ψ (g) = F 2 we know that g(τ ) = −g(τ ) such that f (τ ) = Y (g(τ )) = Y (−g(τ )) = −Y (g(τ )) = −f (τ ) and therefore τ ≡ F 1 Ω 1 (τ ). Since this is true for any τ ≡ F 2 Ω 2 (τ ) we can conclude that Ω 2 (τ ) ⊆ Ω 1 (τ ).</p><p>Theorem 4. The category of p-frames with Hom(F 1 , F 2 ) = {(F 1 , F 2 )} iff F 2 F 1 and the category Ψ(T ) are equivalent.</p><p>Proof. By Lemma 5 and Lemma 6 we can define a functor F from Ψ(T ) onto the category of p-frame. Here Lemma 5 ensures that we can map any object f from Ψ(T ) onto a p-frame F with F = ψ (f ), while Lemma 6 ensures that the arrows can be mapped as well, that is, if there is an arrow between two objects f and g in Ψ(T ), then there also is an arrow in the category of p-frames. Furthermore, Lemma 4 shows that Hom(f, g) has at most one element. Since the category of perceptive frames also is a simple category, the function F : Hom(f, g) → Hom(F (f ), F (g)) must be a bijective function. Thus, we only have to show that any p-frame F is isomorphic to ψ (f ) for some object f of Ψ(T ). Since the relation is antisymmetric, if F 1 F 2 and F 2 F 1 we have F 1 = F 2 . Thus, showing that for each F there exists an f such that F ∼ = ψ (f ) is equivalent to showing that for each F there exists an f such that F = ψ (f ). To show this let B = b 1 , . . . , b L ⊆ Gen(F) be a basis of F . Define f : T → Z as f (τ i ) = L =1 b ,i . For any τ i , τ j if f (τ i ) = f (τ j ) there must be an ∈ {1, . . . , L} with b ,i = b ,j and therefore τ i ≡ τ j . The reverse is true by the same argument. Similarly, if f (τ i ) = −f (τ j ) then there must be an ∈ {1, . . . , L} with b ,i = −b ,j and therefore τ i ≡ Ω(τ j ). Thus F = ψ (f ) and the two categories are equivalent.</p><p>Theorem 5. Let F 1 and F 2 be two p-frames with representations ψ (f ) = F 1 and ψ (g) = </p><p>Then f × c g is a representation of the join of the two p-frames, i. e. ψ (f × c g)</p><formula xml:id="formula_61">= F 1 F 2 . f × c g f g h Y f Yg Y f,g Y f Y g</formula><p>commutes for any h with Y f : h → f and Y g : h → g and all arrows are unique. By Lemma 4 Ψ(T ) is a simple category, and therefore if an arrow exits, it must be unique. We construct Y f and Y g as follows. Let x denote the value of x rounded towards 0 (i. e. negative numbers rounded up, positive numbers rounded down). Define Y f : Z → Z as Y f (z) = z 2c+1 . Then Y f ((f × c g)(τ )) = f (τ ) as well as Y f (−z) = −Y f (z), such that there is an arrow Y f : f × c g → f in Ψ(T ). Define Y g : Z → Z as Y g (z) = z − (2c + 1) z 2c+1 . Then Y g ((f × c g)(τ )) = g(τ ) as well as Y g (−z) = −Y g (z) which defines the arrow Y g : f × c g → g.</p><p>Finally, let h be any object in Ψ(T ) such that there is a pair of arrows Y f : h → f and Y g : h → g in Ψ(T ). Define Y f,g : Z → Z as Y f,g (z) = Y f (z)(2c + 1) + Y g (z). Then Y f,g (h (τ )) = Y f (h (τ ))(2c + 1) + Y g (h (τ )) = f (τ )(2c + 1) + g(τ ) = (f × c g)(τ ) and Y f,g (−z) = −Y f (z)(2c + 1) − Y g (z) = −Y f,g (z) such that Y f,g : h → f × c g is an arrow in Ψ(T ). Corollary 1. Let F 1 and F 2 be two p-frames and F 3 = F 1 F 2 . Then for any two trials τ and τ</p><formula xml:id="formula_62">• τ ≡ F 3 τ iff τ ≡ F 1 τ and τ ≡ F 2 τ . • τ ≡ F 3 τ iff τ ≡ F 1 τ or τ ≡ F 2 τ . • τ ≡ F 3 Ω 3 (τ ) iff τ ≡ F 1 Ω 1 (τ ) and τ ≡ F 2 Ω 2 (τ ).</formula><p>Proof. Let F 1 = ψ (f ) and F 2 = ψ (g). Then F 3 = ψ (f × c g). Assume τ ≡ F 1 τ and τ ≡ F 2 τ . Then f (τ ) = f (τ ) and g(τ ) = g(τ ) such that (f × c g)(τ ) = (f × c g)(τ ). Now assume that τ ≡ F 3 τ . Then (f × c g)(τ ) = (f × c g)(τ ). In addition, there must be some Y f such that there is an arrow in Ψ(T ) and therefore f (τ ) = Y f ((f × c g)(τ )) = Y f ((f × c g)(τ )) = f (τ ) and likewise for g which gives τ ≡ F 1 τ and τ ≡ F 2 τ . Statement (2) is the negation of (1). Statement (3) can be proven using f (τ ) = −f (τ ) instead.</p><p>Corollary 2. For any p-frame F we have F guess F F sat where π guess := T and Ω guess (β) = T as well as π sat := τ 1 |τ 2 | . . . |τ N and Ω sat (β) = ∅.</p><p>Proof. For any F we must have π guess π π sat . Since also, Ω(τ ) ⊆ T = Ω guess (τ ) for any τ ∈ T , we have F guess F. By similar argument for any τ ∈ T we have Ω sat (τ ) = ∅ ⊆ Ω(τ ), which implies F F sat .</p><p>Lemma 7. Let F be a p-frame. Then F is an atom in the poset of p-frames iff dim F = 1.</p><p>Proof. Let F be an atom in the poset of p-frames and let B = {b 1 , . . . , b L } ⊆ Gen(F) be a basis of F . By the proof for Theorem 4 we know that f : T → Z defined as f (τ i ) = L =1 b ,i is a representation of F. By definition of Gen(F) for each vector b ∈ B there must be an i such that b ,i = 1. Thus, we know that {1, . . . , L} ⊆ img f . Assume dim F &gt; 1, then L &gt; 1 and we define the function Y : Z → Z with Y (1) = Y (2) = 1 and Y (−1) = Y (−2) = −1 or Y (z) = z otherwise. The function Y defines an arrow in Ψ(T ) and ψ (Y • f ) = ⊥ as well as ψ (Y • f ) = f . This is a contradiction to the two categories being equivalent. For the reverse direction let F such that dim F = 1 and let B = {b} ⊆ Gen(F) be a basis of F . We define f : T → Z with F = ψ (f ) by f (τ i ) = b i . Thus, img f ⊆ {−1, 0, 1}. For any object g in F, if there is an arrow f → g then there must be a function Y such that Y • f = g. Thus, img g ⊆ {−a, 0, a} for some a ∈ Z. First assume a = 0, then ⊥ = ψ (g). Now assume that a = 0, then we define Y (z) = sgn(Y (1)) • sgn(z). Thus Y (Y (1)) = sgn(Y (1)) sgn(Y (1)) = 1 and Y (Y (−1)) = sgn(Y (1)) sgn(Y (−1)) = sgn(Y (1)) sgn(−Y (1)) = −1 such that Y defines an arrow g → f and f ∼ = g. Thus for any ⊥ F F we either have F = ⊥ or F = F such that F is an atom.</p><p>Lemma 8. Let T be a set of trials and N := |T | the number of trials. Then the number of atoms is O(3 N ), that is there exponentially many atoms.</p><p>Proof. By Lemma 7 if F is an atom, then dim F = 1. Thus it is sufficient to calculate how many geometries of dimension one there are. Let F be any p-frame with dim F = 1. Then by Lemma 1 |Gen(F)| ≤ 3. Most importantly, there is an b ∈ Gen(F) such that the first non-zero entry in b is 1 and B = {b} is a basis of F . Thus for any atom we can assume that there is a basis with a single basis vector of the following structure</p><formula xml:id="formula_63">b =              0 . . . 0 1 a 1 . . . a k                   N − 1 − k      k                          N</formula><p>with k ∈ 0, . . . , N − 1 and a i ∈ {−1, 0, 1}. For any k there are exactely 3 k many vectors that have this structure. 6 Now let b and b be two vectors of this structure with b = b and b i = b i = 1 as well as b j = b j = 0 for j &lt; i (i. e. , the first non-zero entry is in row i). Then b and b cannot be linear dependent, because b i = b i would imply b = b i if they were linear dependent. Therefore, each of the 3 k vectors defines a different geometry and 6 A similar construction can be done to construct geometries of p-frames with dim F &gt; 1. Let T (L, N ) be the number of p-frames for |T | = N with dim F = L. Then this can be recursively calculated as</p><formula xml:id="formula_64">T (N, L) = N −L k=0 (2L + 1) k T (L − 1, N − 1 − k)</formula><p>with T (N, 0) = 1 (without proof). This recursion formula produces the Triangle of B-analogs of Stirling numbers of the second kind (sequence A039755 in the On-Line Encyclopedia of Integer Sequences, https://oeis.org/A039755). No sequence that describes the sum of these numbers for constant N could be found, so we currently cannot provide a calculation of the total number of p-frames for N trials.</p><p>therefore also corresponds to a different atom. Now let b and b have this structure with b i = 1 and b j = 0 for j &lt; i as well as b j = 0 for j ≤ i. Then again these two vectors cannot be linear dependent, because b i = 1 = 0 = b i , such that two vectors of this structure with different values of k define different geometries. Therefore in total, we have N −1 k=0 3 k different geometries, each of which represents a different atom. This geometric series can be rewritten as (3 N − 1)/2 = O(3 N ).</p><p>Lemma 9. Let F 1 and F 2 be two p-frames for the set T of trials with N := |T |. For any two sets of vectors A, B ⊆ R N define the Minkowski sum of A and B as A B :</p><formula xml:id="formula_65">= { a + b | a ∈ A, b ∈ B }. Then F 1 F 2 ⊆ F 1 F 2 .</formula><p>Proof. Let v 3 := v 1 + v 2 be any vector in F 1 F 2 with v 1 ∈ F 1 and v 2 = F 2 . By Theorem 2 it is sufficient that v 3 is consistent with F 3 := F 1 F 2 . Let i, j ∈ {1, . . . , N } such that τ i ≡ F 3 τ j . Corollary 1 implies that τ i ≡ F 1 τ j and τ i ≡ F 2 τ j . Since v 1 and v 2 must be consistent with F 1 and F 2 , respectively, we conclude that v 3,i = v 1,i +v 2,i = v 1,j +v 2,j = v 3,j . The case τ i ≡ F 3 Ω 3 (τ j ) is similar, and togethter this proves that v 3 is consistent with F 3 .</p><p>Lemma 10. The number of dimensions increases strictly monotonical with the lattice of the p-frames. That is, let F 1 and F 2 be two p-frames with F 1 F 2 then dim F 1 &lt; dim F 2 .</p><p>Proof. Let A ⊆ R N be any set of vectors that contains the zero vector. Then for any set B ⊆ R N we have B ⊆ A B. Let F 1 and F 2 be two p-frames such that F 1 F 2 . Then F 1 F 2 = F 2 by definition of the join. Since the geometries are subspaces of the R N they must contain the zero vector and thus F 1 ⊆ F 1 F 2 ⊆ F 2 . Since F 1 = F 2 we conclude that F 1 ⊂ F 2 which gives the desired proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example trial from our experiment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Prior on the parameter θ with θ∼ Beta(a, a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>be the image of f and i f : img f → Z with i f (z) = z the inclusion function of img f in Z. Then the arrows of Ψ(T ) are defined by Hom(f, g)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 6 (</head><label>6</label><figDesc>Representation of a Perceptive Frame). An object f in Ψ(T ) is a representation of a p-frame F if ∀τ, τ ∈ T 1. f (τ ) = f (τ ) iff τ ≡ τAlgorithm 1 Construction of the set Hom(f, g) Require: f, g : T → Z are objects in Ψ(T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>× b if for any other object c such that f 1 : c → a and f 2 : c → b are arrows in the category the diagram there is exactely one arrow f 1, f 2 : c → c such thatf 1 = p 1 • f 1, f 2 and f 2 = p 1 • f 1, f 2 .For any poset (S, ≤) we can define a category with the objects the elements from S and Hom(a, b) = {(a, b)} if b ≤ a. Then c = a ∨ b is the join in the poset if c = a × b is the product in the corresponding category. This gives a direct implementation of the join for two p-frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 5 .</head><label>5</label><figDesc>Let F 1 and F 2 be two p-frames with representations ψ (f ) = F 1 and ψ (g) = F 2 . For any number c ≥ maxτ ∈T |g(τ )|let f × c g be defined as the function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Hasse diagram of the example experiment presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 9 .</head><label>9</label><figDesc>Let F 1 and F 2 be two p-frames for the set T of trials with N := |T |. For any two sets of vectors A, B ⊆ R N define the Minkowski sum of A and B as A B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 .</head><label>6</label><figDesc>Recovery rate for all p-frames Recovery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 .</head><label>8</label><figDesc>Example decision matrix as used in the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 .</head><label>10</label><figDesc>BFs for Cue information usage, Side information usage, and Gender information usage at different levels of guessing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>F 2 .</head><label>2</label><figDesc>For any number c ≥ maxτ ∈T |g(τ )| let f × c g be defined as the function (f × c g)(τ ) = f (τ ) • (2 • c + 1) + g(τ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>For all p-frames except "Side" numbers of the mirrored trials in each representation are the negative of the given numbers. For "Side" all trials and mirrored trials are coded as 1 in the representation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>Cue 1</cell><cell>+</cell><cell>-</cell><cell>+</cell><cell>-</cell><cell>+</cell><cell>-</cell></row><row><cell>Cue 2</cell><cell>+</cell><cell>+</cell><cell>-</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>Cue 3</cell><cell>+</cell><cell>-</cell><cell>-</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>Cue 4</cell><cell>-</cell><cell>+</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>+</cell></row><row><cell>P-frames:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Guess</cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>Side</cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>FirstCue</cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>Count</cell><cell>1</cell><cell></cell><cell>-1</cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>Ordered</cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell></row><row><cell cols="4">Side Note. ⊥ First First Side First Side Count Side Count Side</cell><cell></cell><cell>Count Ordered Ordered Ordered</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The carefull reader may observe that for any given n there exists infinitely many sets A ⊂ N0 with |A| = n, whereas the number n only exists once. However, for any two sets A and B we can define a bijection f : A → B. Thus, while A and B constitute different subsets of N0, they are isomorphic and A must relate to all other subsets of N0 in exactely the same way as B. Thus, for all intent and purposes A and B can be treated as the same object in the category. We will use a similar method when introducing representations of perceptive frames</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proofs Lemma 1. Let F be any p-frame and let v j , v j ∈ Gen <ref type="bibr">(F)</ref>.</p><p>Proof. If j = j , then v j = v j is trivially true. Similarly, if Ω(β j ) = β j , then all terms in (5) cancel and v j,i = 0, such that v T j • v j = 0 is trivially true. Thus, assume that Ω(β j ) = β j and likewise for β j . If Ω(β j ) = β j then all terms in (5) are reversed and we have v j = −v j . Now assume that Ω(β j ) = β j = β j . In that case, any e i that appears in (5) for v j cannot appear in the same equation for v j . Because the e i are orthogonal, disjunct sums of these vectors must also be orthogonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Let F be any p-frame and let</head><p>Then B is an orthogonal basis. Also, for each i there is at most one b such that b ,i = 0.</p><p>Proof. The previous lemma shows that two vectors in Gen(F) either contain the zero vector, are linear dependent, or orthogonal. This proves that after we remove all linear dependent vectors and the zero vector from Gen(F) the remaining vectors must constitute an orthogonal basis. The rest of the theorem follows trivially from the fact that π is a partition, such that each τ i can only appear in one β j .</p><p>Lemma 2. Let F be a p-frame and B ⊆ Gen(F) be a basis of F .</p><p>Proof. This lemma follows directly from the fact that Φ B only has entries in {−1, 0, 1}.</p><p>Theorem 2. Let F be a p-frame and ω ∈ R N . Then ω is consistent with F iff ω ∈ F .</p><p>Proof. We will proof both directions separately.</p><p>• ω ∈ F implies ω is consistent with F.</p><p>Then b ,i = 0 for all i ∈ {1, . . . , N } and therefore ω i = 0. Also ω j = ω i = 0 for all trials τ j ≡ τ i and ω j = −ω i = 0 for all trials τ j ≡ Ω(τ i ). Now take τ i such that τ i ≡ Ω(τ i ). Then there must be some b such that b ,i = ±1. Let τ j ≡ τ i , then this implies b ,i = b ,j and therefore ω i = ω j . Similarly, let τ j ≡ Ω(τ i ), then b ,i = −b ,j and therefore ω i = −ω j .</p><p>• ω is consistent with F implies ω ∈ F .</p><p>Proof. Let ω ∈ R N be consistent with F and let B ⊆ Gen(F) be a basis of F . For ω not to be in F we must be able to write it as the sum of two vectors ω = ω + ω ⊥ where ω ∈ F and ω ⊥ is a non-zero vector that is orthogonal to all basis vectors b ∈ B. We will show a contradiction by proving that if ω is consistent, then either all entries of ω ⊥ must be zero or it is not orthogonal to at least one of the b . First, let i such that ∀ ∈ {1, . . . , L} : b ,i = 0. Because ω ∈ F we can conclude that ω i = 0. However, to have b ,i = 0 we must have τ i ≡ Ω(τ i ) and therefore ω i = 0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selection for action: Some behavioral and neurophysiological considerations of attention and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Allport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perspectives on perception and action</title>
		<editor>Hillsdale, N.J: Erlbaum</editor>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="395" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Category Theory for Computing Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wells</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intuition in Judgment and Decision Making: Extensive Thinking Without Effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Betsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<idno>doi:10 . 1080 / 1047840X.2010.517737</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Inquiry</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="294" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian strategy assessment in multi-attribute decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schiffer</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.442</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="213" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dangerous enough: Moderating racial bias with contextual threat cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wittenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyle</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2010.08.017</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="189" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to Lattices and Order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Priestley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of multinomial models under inequality constraints: Applications to measurement theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Davis-Stober</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2008.08.003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When knowledge activated from memory intrudes on probabilistic inferences from description -the case of stereotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dorrough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Betsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wille</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actpsy.2017.08.006</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The logic of partitions: Introduction to the dual of the logic of subsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellerman</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1755020310000018</idno>
	</analytic>
	<monogr>
		<title level="j">The Review of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social Value Orientation and information search in social dilemmas: An eye-tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicklisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickert</surname></persName>
		</author>
		<idno>doi:10. 1016/j.obhdp.2012.07.002</idno>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes. Social Dilemmas</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="272" to="284" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Arm: Data analysis using regression and multilevel/hierarchical models. R package version 1.10-1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Su</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=arm" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Adaptive Toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bounded rationality: The adaptive toolbox</title>
		<editor>G. Gigerenzer &amp; R. Selten</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Betting on one good reason: The take the best heuristic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<editor>G. Gigerenzer, P. M. Todd, &amp; ABC Research Group</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="75" to="95" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Simple heuristics that make us smart</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simple heuristics that make us smart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<editor>&amp; ABC Research Group</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Investigating intuitive and deliberate processes statistically: The multiplemeasure maximum likelihood strategy classification method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="199" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modelling option and strategy choices with connectionist networks: Towards an integrative model of automatic and deliberate decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Betsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">215</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The empirical content of theories in judgment and decision making: Shortcomings and remedies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Betsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="711" to="721" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian Multiple Comparisons Using Dirichlet Process Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<idno>doi:10. 2307/2669856</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">443</biblScope>
			<biblScope unit="page" from="1130" to="1139" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grätzer</surname></persName>
		</author>
		<title level="m">General Lattice Theory: Second edition</title>
		<meeting><address><addrLine>Basel</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhäuser</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking attribute selection techniques for discrete class data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2003.1245283</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Über die Klassenzahl abelscher Zahlkörper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multinomial models with linear inequality constraints: Overview and improvements of computational methods for Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Davis-Stober</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2019.03.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="70" to="87" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized outcome-based strategy classification: Comparing deterministic and probabilistic choice models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Hilbig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moshagen</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-014-0643-0</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1431" to="1443" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SQLite (Version 3.23.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mistachkin</surname></persName>
		</author>
		<ptr target="https://www.sqlite.org/src/info/4bb22940" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounding attention in action control: The intentional control of selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hommel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Effortless attention: A new perspective in the cognitive science of attention and action</title>
		<editor>B. Bruya</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="121" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maps of Bounded Rationality: Psychology for Behavioral Economics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<ptr target="Retrievedfromwww.jstor.org/stable/3132137" />
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1449" to="1475" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayes Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1995.10476572</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A meta-analysis of gender stereotypes and bias in experimental simulations of employment decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Sackett</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0036734</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Doing Bayesian Data Analysis: A Tutorial Introduction with R and BUGS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kruschke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Academic Press</publisher>
			<pubPlace>Burlington, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conceptual Mathematics: A First Introduction to Categories (2nd)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Lawvere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Schanuel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian outcome-based strategy classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-014-0557-9</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evidence accumulation in decision making: Unifying the &quot;take the best&quot; and the &quot;rational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D R</forename><surname>Cummins</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03196581</idno>
	</analytic>
	<monogr>
		<title level="j">models. Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Manifesto on Psychology as Idiographic Science: Bringing the Person Back Into Scientific Psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C M</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15366359mea0204_1</idno>
	</analytic>
	<monogr>
		<title level="m">This Time Forever. Measurement: Interdisciplinary Research and Perspectives</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Methodological notes on model comparisons and strategy classification: A falsificationist proposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Hilbig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment &amp; Decision Making</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Science faculty&apos;s subtle gender biases favor male students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Moss-Racusin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Dovidio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Brescoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Handelsman</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1211286109.pmid:22988126</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="16474" to="16479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">When irrelevance matters: Stimulus-response binding in decision making under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frings</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000109</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1831" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distractor-based stimulus-response bindings retrieve decisions independent of motor programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frings</surname></persName>
		</author>
		<idno>doi:10. 1016/j.actpsy.2016.09.006</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<title level="m">Missing Gender Bias in Hiring Decision Matrices</title>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Perceived Biological and Social Characteristics of a Representative Set of German First Names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dorrough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glöckner</surname></persName>
		</author>
		<idno type="DOI">10.1027/1864-9335/a000383</idno>
	</analytic>
	<monogr>
		<title level="j">Social Psychology</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Take a quick click at that! Mouselab and eye-tracking as tools to measure intuition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulte-Mecklenbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations for tracing intuition</title>
		<editor>A. Glöckner &amp; C. Witteman</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Python for Scientific Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2007.58</idno>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Guide to NumPy (2nd)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CreateSpace Independent Publishing Platform</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Duchesnay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Woike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gaissmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">344</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regenwetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Davis-Stober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zwilling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">QTest: Quantitative testing of theories of binary choice. Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Messner</surname></persName>
		</author>
		<idno type="DOI">10.1037/dec0000007</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Estimating the dimension of a model. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Models of man; social and rational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A practical solution to the pervasive problems of p values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03194105</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="804" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">National hiring experiments reveal 2:1 faculty preference for women on STEM tenure track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ceci</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1418878112</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="5360" to="5365" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Because the category of p-frames are equivalent, if f × c g is a product of f and g in Ψ(T ), then ψ (f × c g) = F 1 F 2 . Thus, we must show that the diagram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proof</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
