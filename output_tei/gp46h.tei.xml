<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can we infer inter-individual differences in risk-taking from behavioural tasks?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Palminteri</surname></persName>
							<email>stefano.palminteri@ens.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Neurosciences Cognitives</orgName>
								<orgName type="institution">Institut National de la Santé et de la Recherche Médicale</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;Etudes Cognitives</orgName>
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut d&apos;Etudes de la Cognition</orgName>
								<orgName type="institution">Université de Paris Sciences et Lettres</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coralie</forename><surname>Chevallier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Neurosciences Cognitives</orgName>
								<orgName type="institution">Institut National de la Santé et de la Recherche Médicale</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;Etudes Cognitives</orgName>
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut d&apos;Etudes de la Cognition</orgName>
								<orgName type="institution">Université de Paris Sciences et Lettres</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can we infer inter-individual differences in risk-taking from behavioural tasks?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Risk-taking</term>
					<term>inter-individual variability</term>
					<term>behavioural phenotype</term>
					<term>behavioural economics</term>
					<term>correlational psychology</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Investigating the bases of inter-individual differences in risk-taking is necessary to refine our cognitive and neural models of decision-making and to ultimately counter risky behaviours in real-life policy settings. However, recent evidence suggests that behavioural tasks fare poorly compared to standard questionnaires to measure individual differences in risk-taking. Crucially, the results were not improved using model-based measures of risk taking. Here we propose two possible-not mutually exclusive-explanations for these results. Finally, we suggest future avenues of research to improve the assessment of inter-individual differences in risk-taking that combine repeated online testing and the development of mechanistic computational models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>In a recent series of studies Frey, Hertwig, Pedroni, Rieskamp and colleagues investigated the relationship between different measures of risk-sensitivity in a laboratory-based experiment involving over a thousand participants (N~1500) (1,2). By comparing standard behavioural tasks, personality questionnaires, and reports of actual frequency of risky behaviours, the authors were able to demonstrate that cognitive tasks are consistently less reliable than questionnaires. First, performance in risk-taking tasks are less correlated to actual frequency of risky behaviours than personality scores, which suggests that external validity is poor. Second, behavioural measures are less correlated among themselves than personality scores and frequency measures, which suggests that they tap constructs that are less consistent (poor internal validity). The internal consistency of behavioural measures is not improved when considering the model parameters derived from the cumulative prospect theory model instead of raw behavioural measures. Finally, test-retest reliability is lower for behavioural tasks than for personality scores. Strikingly, preliminary evidence suggests that these issues generalize to behavioural tasks beyond decision-making under risk, such as reinforcement learning (3).</p><p>The unreliability of behavioural measures is particularly worrying for research frameworks, such as computational psychiatry, that have recently placed great emphasis on the use of cognitive and behavioural phenotyping tools. The idea behind these frameworks is that behavioural measures can be used to phenotype patients at the individual level and ultimately work as tools to perfect diagnosis, personalize care, and assess the efficacy of new treatments or drugs in clinical trials (4). In this context, it is therefore vital that behavioural tasks generate results that are stable and predictive of real life outcomes.</p><p>In addition to questioning approaches based on behavioural phenotyping tools, these findings also raise a profound epistemological challenge. Given that real life frequency of risky behaviours is the mere cumulative result of past choices, why then, do personality measuresthat are based on questionnaires -explain real life behaviours better than behavioural measures -that are based on choices? And why would the same subjects produce different choices when presented twice with the very same task?</p><p>We propose two possible answers for these puzzling results and fundamental questions (low external validity and consistency of behavioural measures):</p><p>• The first possibility is that these findings reflect a problem with the instrument • The other possibility is that these findings reflect a problem with the measure.</p><p>The 'problem with the instrument' argument has been explicitly put forward by the authors of the studies (1-2). According to this hypothesis, the low external and internal validity of the behavioural results derive from intrinsic limitations of the tasks. Accordingly, it has been argued that low between-task consistency between behavioural measures derives from the fact that each task involves both central (risk sensitivity) and peripheral processes (responses, stimuli), whose variability may affect the results. Low test-retest reliability may indeed come from the fact that behavioural and cognitive tasks are traditionally designed to reduce between-subjects variance and to maximize between-conditions variance, such that the very features that make a behavioural task 'successful' (high reproducibility of the 'average' results) make it unsuited to assess inter-individual differences (5).</p><p>The 'problem with the instrument' argument also goes in the opposite direction. First, propensity measures are designed to maximize inter-individual differences. Second, a good test-retest reliability is a condicio sine qua non for the publication of personality questionnaires, hence their good temporal consistency. Finally (this issue is more specific to the mentioned studies, rather than general), the frequency measures were assessed using self-report questionnaires, which is the same response mode as the personality measures. Furthermore, risk propensity and risk frequency assessments share similar items and it should come as no surprise that subjects provide similar responses to similar questions, e.g. in order to present a coherent image of themselves (a good 'narrative'). Taken together, these features may inflate the consistency and validity of the personality measures.</p><p>Finally, 'the problem of the instrument' applies also to the mathematical model used to quantify risk propensity parameters. The authors focused on the cumulative prospect theory (CPT) by Kahneman and Tversky (6), which is a widely used descriptive model originally designed to explain one-shot decisions. CPT assumes different parameters (decreasing marginally utility, loss aversion and subjective weighting of probabilities) to describe individual utility functions. Two features of the CPT may have undermined the internal consistency of the model-based measures of risk sensitivity. First, the functional form is supposed to be independent from the behavioural task considered, a strong assumption given the fact that difference tasks engage different peripheral processes that may, in turn, affect the corresponding computations. Second, and more importantly, in the CPT -a descriptive model -the parameters are supposed to be static and not affected by the individual history of choices, relevant contextual factors and feedback.</p><p>The 'problem with the measure' argument implies that behavioural tasks provide a genuine estimate of the subject's momentary risk attitude at the time of testing, but that risk attitude itself changes over time. This is possible if we assume that momentary risk attitude is influenced by several factors. To illustrate this idea, we now consider a simplified case involving two possible phenotypes, a risk-seeking phenotype (red) and a risk-averse phenotype (blue), and we propose a multi-layer model in which the momentary risk attitude corresponds to the weighted sum of different sources of influence that change with different time constants <ref type="figure" target="#fig_0">(Figure 1)</ref>. In this toy example, the first layer corresponds to the subject's 'trait', which is determined by her genotype and which remains stable over the lifespan. The last layer corresponds to random (or unpredictable) factors, such as unexpected external stimuli and contextual factors. In between these two extremes, we hypothesize that additional sources of influence are at play, such as very slow age-related changes and very fast circadian rhythms. According to this model, a subject tested twice with the same behavioural task at different time points will not necessarily display the same phenotype. Within this framework, the fact that propensity measures produce more stable results can be explained by the fact that filling the questionnaires relies on cognitive processes that do not involve risk attitude per se, such as robust averaging of previous experiences stored in episodic memory or introspection.</p><p>Crucially, there is evidence demonstrating that these various layers are indeed relevant to understand decision-making under risk: genetic factors influence risk-related behaviours (7), behavioural measures of risk sensitivity evolve across life-span (8), and are affected by hormonal and circadian factors (9-10), as well as momentary arousal (11). Importantly, the same factors have been shown to influence other decision-making process such as cooperation in economics dilemmas: a field where behavioural tasks also predict real life behaviours poorly (12-13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and perspectives</head><p>Recent evidence based on large-scale behavioural testing shows that behavioural measures (in cognitive tasks) are outperformed by propensity measures (from personality questionnaires) in terms of external validity (i.e., correlation with frequency measure) and internal validity (between-measure consistency and test-retest reliability). We delineate two possible -not mutually exclusive -interpretations of these results. The pessimistic 'problem with the instrument' argument states that behavioural tasks are not suited to investigate interindividual differences. The optimistic 'problem with the measure' argument states that variability in behavioural tasks reflects true changes in momentary risk attitude.</p><p>At the moment, personality questionnaires appear to be the best psychological tools to predict the frequency of real life risky behaviour and for most applications, this probably makes little difference. Should we then, abandon the quest for behavioural measures of individual variability? Probably not. Questionnaires are hugely informative when it comes to providing an accurate description of the variability with which personality traits manifest but they cannot be used to trace back the cognitive and neural mechanisms that together produce such variability. The paucity of robust behavioural tools to characterize inter-individual differences therefore constitutes an important obstacle in building proper models of cognitive variability.</p><p>Developing behavioural biomarkers however, requires a proper re-think in the way cognitive scientists design tasks so that they maximize between-subjects variance. One promising possibility is to shift from fixed and passive designs to active and adaptive ones. Adjusting task parameters online could indeed correct for momentary changes in baseline performance that may affect the assessment of risk preferences.</p><p>These results also highlight the importance of repeated testing, which has now become considerably easier with the development of smart-phone based behavioural experiments. Repeated testing will allow validate the multi-layer hypothesis, attribute precise coefficients to the different layers and, by averaging over experiments, infer the trait-level phenotype. It is also possible that developing and refining mechanistic and dynamic models of risk preferences, that integrate learning processes and contextual factors, will allow better quantify risk preferences at the individual level. A promising way to design these models could be the development of choice prediction competitions as it is already common in machine learning literature (14). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers of influence</head><p>Change speed Legend simplified case in which only two phenotypes are possible: red (risk seeking) and blue (risk aversion). The different layers change at different time constants (as exemplified by the grey triangle on the left). A given subject is tested in two experimental sessions (ES1, and ES2) with two behavioural tasks supposed to measure the same behavioural phenotype (T1 and T2). The multi-layer model may explain why behavioural measures are not consistent between-tasks and between-sessions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The figure schematizes how low consistency of behavioural measures of risk may arise from the multi-layer model. At the top, we represent the different factors that influence the probability to express a given behavioural phenotype at a given time point. We consider a</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>SP is supported by an ATIP-Avenir grant (R16069JS) Collaborative Research in Computational Neuroscience ANR-NSF grant (ANR-16-NEUC-0004), the Programme Emergence(s) de la Ville de Paris, and the Fondation Fyssen. The Institut d'Etudes de la Cognition is supported financially by the LabEx IEC (ANR-10-LABX-0087 IEC) and the IDEX PSL* (ANR-10-IDEX-0001-02 PSL*).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>(1) Frey, Pedroni, Mata, Rieskamp &amp; Hertwig. Risk preference shares the psychometric structure of major psychological traits. Science Advances (2017).</p><p>(2) Pedroni, Frey, Bruhin, Dutilh, Hertwig &amp; Rieskamp. The risk elicitation puzzle. Nature Human Behaviour (2017). </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
