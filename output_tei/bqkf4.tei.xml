<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POINT ESTIMATE OBSERVERS: A NEW CLASS OF MODELS FOR PERCEPTUAL DECISION MAKING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">October 24, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Preprint</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><forename type="middle">H</forename><surname>Schütt</surname></persName>
							<email>heiko.schuett@nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Neural Science and Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aspen</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center for Neural Science and Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Calder-Travis</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Ji</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center for Neural Science and Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<postCode>10003</postCode>
									<settlement>Washington Place, New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Berkeley; Joshua Calder-Travis: Institute of Neurophysiology and Pathophysiology</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Universitätsklinikum Hamburg-Eppendorf</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POINT ESTIMATE OBSERVERS: A NEW CLASS OF MODELS FOR PERCEPTUAL DECISION MAKING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">October 24, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/rev0000402</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Perceptual decision making</term>
					<term>Point estimate observer</term>
					<term>Bayesian observer</term>
					<term>Observer model</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Bayesian optimal inference is often heralded as a principled, general framework for human perception. However, optimal inference requires integration over all possible world states, which quickly becomes intractable in complex real-world settings. Additionally, deviations from optimal inference have been observed in human decisions. A number of approximation methods have previously been suggested, such as sampling methods. In this study, we additionally propose point estimate observers, which evaluate only a single best estimate of the world state per response category. We compare the predicted behavior of these model observers to human decisions in five perceptual categorisation tasks. Compared to the Bayesian observer, the point estimate observer loses decisively in one task, ties in two and wins in two tasks. Two sampling observers also improve upon the Bayesian observer, but in a different set of tasks. Thus, none of the existing general observer models appears to fit human perceptual decisions in all situations, but the point estimate observer is competitive with other observer models and may provide another stepping stone for future model development.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A central question in cognitive science is how humans make decisions based on uncertain information about the world. This question has been studied extensively in the realm of perceptual inference. This area is particularly suited to precise quantitative modelling of decision making, as perceptual tasks can be carefully controlled and large amounts of data can be collected efficiently. Nonetheless, few models provide a unifying explanation for decisions across many different perceptual tasks. This most successful framework for explaining human perceptual decision making is based on the Bayesian optimal solution to the task faced by participants <ref type="bibr" target="#b3">(Banks, Geisler, &amp; Bennett, 1987;</ref><ref type="bibr" target="#b13">Burge &amp; Geisler, 2011;</ref><ref type="bibr" target="#b24">Ernst &amp; Banks, 2002;</ref><ref type="bibr" target="#b28">Geisler, 1989</ref><ref type="bibr" target="#b29">Geisler, , 2011</ref>. In this framework, one assumes perfect use of all available information as a starting point and then adds restrictions to this optimal observer to adjust the model to human behaviour. Similar models have also been successfully applied in other contexts, such as cognitive decision making <ref type="bibr" target="#b35">(Griffiths &amp; Tenenbaum, 2006;</ref><ref type="bibr" target="#b85">Tenenbaum &amp; Griffiths, 2001;</ref><ref type="bibr" target="#b90">Vul, Goodman, Griffiths, &amp; Tenenbaum, 2014)</ref> and motor control <ref type="bibr" target="#b42">(Körding &amp; Wolpert, 2004;</ref><ref type="bibr" target="#b62">Najemnik &amp; Geisler, 2005;</ref><ref type="bibr" target="#b95">Wolpert, Ghahramani, &amp; Jordan, 1995)</ref>. There is strong evidence that humans take into account prior information <ref type="bibr" target="#b1">(Adams, Graf, &amp; Ernst, 2004;</ref><ref type="bibr" target="#b83">Tassinari, Hudson, &amp; Landy, 2006)</ref> and the uncertainty associated with sensory variables <ref type="bibr" target="#b22">Denison, Adler, Carrasco, &amp; Ma, 2018;</ref><ref type="bibr" target="#b24">Ernst &amp; Banks, 2002)</ref>, which are two predictions of Bayesian models. However, the success of Bayesian optimal observer models does not necessarily imply that humans perform full Bayesian probabilistic computation <ref type="bibr" target="#b49">(Ma, 2012;</ref><ref type="bibr" target="#b53">Maloney &amp; Mamassian, 2009)</ref>, as near-optimal performance can be achieved by other means <ref type="bibr" target="#b38">(Jones &amp; Love, 2011;</ref><ref type="bibr" target="#b51">Ma &amp; Jazayeri, 2014)</ref>.</p><p>A key operation of the Bayesian approach is marginalisation. Marginalisation refers to a mathematical procedure which allows one to compute a probability distribution over a specific variable of interest (referred to as a marginal distribution), from a probability distribution over a larger number of variables (joint distribution). This is achieved by integrating "out" all variables that are not of interest. Marginalisation scales badly with the dimensionality of the distributions involved. This is because the required integration can often only be performed by summing the probabilities of all possible combinations of the irrelevant variables 1 , an example of the phenomenon known as the curse of dimensionality <ref type="bibr" target="#b7">(Bishop, 2006;</ref><ref type="bibr" target="#b37">Hinrichs, Novak, Ullrich, &amp; Woźniakowski, 2014)</ref>. While performing marginalisation may be feasible in simple experimental settings (where only a few variables need to be integrated out), even simple perceptual inferences become intractable in a world with many objects, whose many features interact in complicated ways <ref type="bibr" target="#b68">(Pouget, Beck, Ma, &amp; Latham, 2013)</ref>. For example, Bayesian inference for the colour of a single surface under a single source of illumination is not trivial, but manageable <ref type="bibr" target="#b11">(Brainard &amp; Freeman, 1997)</ref>. Bayesian inference for the surface colour of many objects, that reflect light onto each other, implies handling the joint distribution over the surface reflectances and positions of all objects. Thus, it seems unlikely that the marginalisation assumed by the Bayesian observer is a good mechanistic explanation of human decision making. Indeed, for the example of inferences about colour, models that work in naturalistic environments are not Bayesian <ref type="bibr" target="#b41">(Kraft &amp; Brainard, 1999</ref>).</p><p>Here we present point estimate observers, a class of general models for decision making that avoid marginalisation. A point estimate observer bases their response on values obtained by maximising over irrelevant variables, rather than values obtained by integrating them out. To be more precise, the Bayesian observer finds the probability a possible response is correct by summing (through integration) the probabilities of all possible world states associated with that response (i.e. all possible values of the irrelevant variables). The point estimate observer evaluates a response by maximisation to find the most probable world state associated with that response (i.e. most probable combination of values for the irrelevant variables). 2 By using maximisation, instead of marginalisation over the space of all possible world states, point estimate based inference is computationally cheaper than full Bayesian inference. Despite this difference in computation, point estimate observers can reach near optimal performance.</p><p>One can arrive at point estimate observers through at least three routes. First, point estimate observers represent an approximation to the Bayesian observer and similar approximations are sometimes used in statistics. Second, point estimate observers may be understood as performing frequentist statistical inference. In frequentist model comparisons, models are fit and evaluated based only on the best-fitting parameters, just as the point estimate observer does for the response categories. In contrast to typical frequentist analyses, though, the point estimate observer takes the prior into account for fitting and evaluating the model. Finally, we can understand the point estimate observer as the best approximation of the posterior with a point mass. In all cases, the point estimate observer is a particularly simple or reduced incarnation of the framework, corroborating the idea that the point estimate observer implements a theoretically simple and computationally cheap solution.</p><p>By arguing for the point estimate observer on the basis of lower computational complexity, we assume that optimization is indeed easier than marginalization or integration. For general computer algorithms solving these problems this is almost universally true <ref type="bibr" target="#b70">(Quarteroni, Sacco, &amp; Saleri, 2000</ref>, compare chapter 7 and 9). For convex functions for example, optimization algorithms like gradient descent achieve quadratic convergence, i.e. their error scales with k −2 with the number of steps k, while the amount of computation scales linearly with the number of dimensions and can be parallelised across these <ref type="bibr" target="#b10">(Boyd &amp; Vandenberghe, 2004)</ref>. In contrast, the error of sampling based algorithms for integration scales only with k −1/2 and the number of dimensions enters with a higher exponent than 1 ( 5 4 for Hamiltonian sampling for example, see <ref type="bibr" target="#b63">Neal, 2011)</ref>. Therefore, sampling and numerical integration are certainly computationally more expensive than optimization and we are not aware of any situation in which finding the maximum is more difficult than integration. Nonetheless, it requires a leap of faith to assume that this is also true for the brain solving real world problems. After all, there are also problems for which marginalization or optimization have closed form solutions that require virtually no computation, and specialized methods for specific probabilistic graphical models can perform much better than the general purpose algorithms <ref type="bibr" target="#b40">(Koller &amp; Friedman, 2009</ref>). As we do not know the exact form of the problem the brain solves and much less the algorithm and implementation it uses, it remains possible that the brain employs a clever combination of problem formulation and solving-algorithm that avoids the computational inefficiencies of the general purpose algorithms.</p><p>Other approximate solutions to the marginalisation problem have been proposed as models of human decision making before. Perhaps the most common proposal is sampling from the posterior <ref type="bibr" target="#b6">(Berkes, Orban, Lengyel, &amp; Fiser, 2011;</ref><ref type="bibr" target="#b20">Deneve, 2008;</ref><ref type="bibr" target="#b36">Haefner, Berkes, &amp; Fiser, 2016)</ref>, an idea that is based on a common method for computing posteriors in statistical analyses (Markov chain Monte Carlo; <ref type="bibr" target="#b30">Gelman et al. 2013)</ref>. While this proposal reduces the impact of the argument that marginalisation is hard, the approximation converges to the full Bayesian optimal solution, such that the behavioural predictions with many samples are identical to the Bayesian observer. To make predictions that differ from the Bayesian observer, sampling accounts assume that only few samples are taken <ref type="bibr" target="#b45">(Lieder &amp; Griffiths, 2019;</ref><ref type="bibr" target="#b90">Vul et al., 2014)</ref>.</p><p>Another way to avoid complex marginalisation problems is to approximate the true posterior with a factorised distribution. By factorised distribution we mean a probability distribution that is the product of independent distributions for each variable <ref type="bibr" target="#b7">(Bishop, 2006)</ref>. Hence, any patterns in the joint distribution of variables-for example, correlations-are lost when a factorised distribution is used as an approximation. This type of representation is attractive, because it allows a representation of uncertainty about each stimulus dimension and can remove the need to marginalise: a factorised distribution already contains a distribution over the relevant variable, independent from the other variables. Several techniques have been proposed to find factorised approximations. Two popular techniques, which are related to each other, are variational inference and expectation propagation <ref type="bibr" target="#b54">(Minka, 2005)</ref>. Variational inference has been proposed as a general inference scheme humans might employ in the context of the "free energy principle" <ref type="bibr" target="#b26">(Friston, 2010)</ref>. In perception research, variational inference is mostly discussed as a normative explanation for interactions within and between brain areas, that show similarities to the messages passed in message passing implementations of variational inference <ref type="bibr" target="#b26">(Friston, 2008</ref><ref type="bibr" target="#b26">(Friston, , 2010</ref><ref type="bibr" target="#b27">Friston &amp; Kiebel, 2009)</ref>. In cognition research, the focus is more on the effects variational approximations have on decision behaviour, due to splitting the representation into dimensions that are represented separately <ref type="bibr" target="#b73">(Sanborn, 2017;</ref><ref type="bibr" target="#b77">Sanborn &amp; Silva, 2013)</ref>. Expectation propagation is discussed less, but has the advantage that the inferred distributions for the stimulus dimensions converge to the marginal distributions of the correct posterior, such that a better approximation of full Bayesian inference is achieved.</p><p>To qualify as a theory or explanation, a model needs to apply to a large collection of decision making tasks. At least, such a theory should capture all perceptual categorisation tasks, i.e. any categorical decision made on the basis of incoming perceptual information. This encompasses a wide range of decisions from simple detection (e.g. is the object I am searching for at the locations I am looking at?) through typical categorization examples (e.g. what kind of animal is that? Is this fruit edible?) to inferring a general context (e.g. what situation am I in?).</p><p>We will first describe and analyse the different observer models on a theoretical level. In this theoretical analysis, we will notice that the variational observer cannot handle situations where some combinations of variable values are consistent with one category, but are impossible under another category. By "category" we refer to sets of world states between which the observer is deliberating. This restriction excludes the application of the variational observer to a broad range of situations. Furthermore, we find that the expectation propagation observer makes the same predictions as full Bayesian inference in our tasks. We find two different sampling observers that each apply to all our tasks. The importance sampling observer samples from the prior under each category and uses the samples to estimate the evidence in favor of the category. The joint sampling observer samples from the joint posterior over the category and the proximate stimulus.</p><p>Next, we test the point estimate observer against its competitors. To make this exercise tractable, we only consider competitor models that apply to a broad range of tasks, and make different predictions to the Bayesian observer without <ref type="figure">Figure 1</ref>: A: Graphical model for the general task structure. There are categories of stimuli C, to be discriminated by the participants. Each category defines a distribution over true stimulus values s. The participant makes a noisy observation x of s. Based on this observation, the participant determines which category the stimulus belongs to. B: Flow diagram for the Bayesian observer model and the point estimate observer model. Both model observers operate on the joint posterior over category and stimulus. The Bayesian observer integrates evidence over all stimulus values to compute the marginal posterior over categories. By contrast, the point estimate observer finds the most probable stimulus for each category and bases their decision on the posterior density at these stimulus values. Proto-posterior over stimulus and category here refers to the unnormalised distributionp(s, C|x) = p(x|s)p(s|C)p(C), which is used by our observer models instead of the posterior, because the normalisation is a potentially costly further processing step, which does not change the final decision. further constraints. Concretely, this rules out the variational inference observer model and the expectation propagation observer model respectively. We are thus left with the point estimate observer the two sampling observers and the Bayesian observer for empirical comparisons; for completeness, we keep the variational observer model for the one task to which it applies. To empirically test these observer models, we compare their predictions to human behavioural data. This comparison would be futile if all human behaviour was well explained by the Bayesian observer model already. However, optimal-observer models have been found to make predictions that deviate from human behaviour <ref type="bibr" target="#b71">(Rahnev &amp; Denison, 2018)</ref> and some of these deviations are significantly better explained by other (task specific) models than by the Bayesian model <ref type="bibr" target="#b81">Stengård &amp; van den Berg, 2019)</ref>. Thus, we know there are some deviations to be explained. Additionally, there are many more datasets that were never formally tested for such deviations, partially due to the lack of a serious competing model <ref type="bibr" target="#b9">(Bowers &amp; Davis, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical analysis</head><p>A simple testing ground for point estimate observers is provided by binary categorization tasks. These tasks have the following structure (see <ref type="figure">Figure 1 A</ref>). There are two categories, and in each trial one of these is the true category (C). Each category is defined by a probability distribution p(s|C) over a stimulus variable denoted by s. The observer makes a noisy observation of s, denoted by x. Based on this observation, the observer decides which category was presented.</p><p>Bayesian Observer The Bayesian observer bases their decision on the log-posterior ratio between the two categories:</p><formula xml:id="formula_0">d B = log p(C = 1|x) p(C = 0|x) = log p(C = 1)p(x|C = 1) p(C = 0)p(x|C = 0) .<label>(1)</label></formula><p>This is the optimal decision variable, i.e. choosing category 1 if d B is greater than 0 and category 0 otherwise, maximises the probability that the decision is correct. In Eq. (1), the category-conditioned distributions of the observations, p(x|C = 0) and p(x|C = 1), are not directly known. Therefore, we need to rearrange this equation to replace these distributions with distributions that we do know. Per the rules of probability calculus, Eq. (1) becomes  <ref type="figure">Figure 2</ref>: Illustration of the model observers using two simple tasks (rows). A: Prior distribution for the stimulus and the two categories, P (s, C), and likelihood for an example observation x, P (x|s). The coloured lines each represent the distribution for s for one category. Together, they represent the joint prior over stimulus level and category. The area under the curves corresponds to the prior probability of the categories, which is equal here. In Task 1, the two categories are Gaussians with different means at ±µ. In Task 2, the two categories are the positive and negative halves of a single Gaussian. The black dashed lines represent the likelihood derived from an observation x, which all further illustrations are conditioned on. B: Joint posterior over the stimulus level and the category, P (s, C|x), which can be computed by multiplying the prior and likelihood from the first column. Both curves together represent the joint posterior over category and stimulus level. The Bayesian optimal observer compares the posterior probability of the categories, which corresponds to the area under the curve, as for the prior (shaded). The point estimate observer instead compares the maximums of the posterior for the two categories (dashed lines). C: Variational mean field approximation of the posterior q(s, C) = q(s)q(C), i.e. an approximation which assumes a category independent distribution over the stimulus. In this factorised approximation, comparing the maximums, comparing the areas and using the computed marginal over categories all result in the same ratio between categories which is the basis for the decision of these observer models. In Task 2, the variational approximation fails entirely because variational inference never assigns any probability density in its factorised approximation to combinations of s and C for which the true posterior is 0. Expectation propagation (not shown) finds a factorised approximation in this task, but produces the exact same ratio of probabilities for the two categories as the Bayesian optimal observer. using the information that x is independent of C given s. Computing this decision variable requires the computation of integrals over possible stimuli, s. This may be computationally hard and scales poorly with the complexity of the stimulus (or more generally, of the world state).</p><formula xml:id="formula_1">d B = log p(C = 1) p(x|s)p(s|C = 1)ds p(C = 0) p(x|s)p(s|C = 0)ds ,<label>(2)</label></formula><p>Point estimate Observer The point estimate observer replaces the integral over the world state s with a maximisation:</p><formula xml:id="formula_2">d P = log p(C = 1) max s [p(x|s)p(s|C = 1)] p(C = 0) max s [p(x|s)p(s|C = 0)]<label>(3)</label></formula><p>The primary computation necessary for the point estimate observer is optimisation, while for the Bayesian observer it is integration or marginalisation. Optimisation avoids the computational cost of marginalisation, because it can be solved by gradually adapting the estimated world state to the observations and the prior knowledge. This is computationally far cheaper than evaluating a wide range of possible world states, especially in high dimensions.</p><p>The point estimate observer uses a separate estimate of the world state for each category (note the two separate maximisations over s in Eq. 3). At first glance, an observer model that estimates the world state only once may seem attractive, but this observer model is ill defined for tasks that contain categories that restrict the range of possible stimuli to different sets. First, if the optimisation ignored the restrictions given by the categories, it would sometimes yield world state estimates which are impossible according to all categories which means that its behaviour is undefined. Second, if the optimisation allowed all stimulus values that are possible under any category, it could still yield estimates of the world state that are impossible under some of the categories. In this case, the observer model would suddenly be infinitely confident in its decision and would loose its probabilistic interpretation. This second case contains taking the average prior over all categories p(s) = i p(s|C = i) as a special case. Finally, if the optimisation allowed only stimuli that are possible under all categories, the response probability for a category would depend mostly on its overlap with the shared space with other categories. This is problematic because a) adding another category could then change the response probabilities for the existing categories completely and b) the categories can be entirely disjoint, such that there are no stimuli which are possible under all categories. In fact, because there is no adequate way to take these restrictions into account within a single optimisation, an observer model with only one optimisation would not be able to do four of our five tasks.</p><p>By looking only at the maximum of p(x|s)p(s|C = i)p(C = i) in Eq. 3, the point estimate observer ignores how broad this distribution is. When one category allows a broader range of stimuli than the other, a priori (i.e. p(s|C = i) is wider), this leads to a systematic bias in the decisions of the observer. For any single condition, a bias can be accommodated through the decision criterion. However, the direction and magnitude of this bias depends on the task and on the amount of noise in the observation x, such that different conditions in an experiment are biased to different extents. We created two variants of the point estimate observer: one with a fixed criterion, such that the model inherits the bias pattern described, and one with an optimally adjusted criterion that optimises task performance, such that the model is equally biased or unbiased in all conditions. In statistics, such corrections are often discussed in the context of model selection. We can view observers as doing a kind of "model selection" when performing the categorisation tasks studied here. In statistics, a full Bayesian solution (based on the model evidence) automatically accounts for the bias that would otherwise be introduced by model complexity, while an analysis that relies on maximum likelihood requires corrections such as the AIC and BIC, the degrees of freedom in a likelihood ratio tests, or more data driven methods like cross-validation.</p><p>A related type of model called the self-consistent Bayesian observer has been proposed before <ref type="bibr" target="#b48">(Luu &amp; Stocker, 2018;</ref><ref type="bibr" target="#b82">Stocker &amp; Simoncelli, 2008)</ref>. In this model the observer commits to a categorisation of the stimulus, unlike the point estimate observer who commits to (two) values of the stimulus itself (through the two maximisations). This type of model was proposed to explain post-decision biases present when observers were first asked to categorise the stimulus, and then to report the stimulus. However, such biases are also observed and explainable when observers are not asked to categorise the stimuli (e.g. <ref type="bibr" target="#b97">Zamboni, Ledgeway, McGraw, &amp; Schluppeck, 2016)</ref>. Such a scheme is suboptimal <ref type="bibr" target="#b25">(Fleming, Maloney, &amp; Daw, 2013)</ref>, although it can be beneficial in the presence of later distortions of the stimulus representation <ref type="bibr" target="#b69">(Qiu, Luu, and Stocker 2020;</ref><ref type="bibr">and closely related Li, Herce Castañón, Solomon, Vandormael, and Summerfield 2017)</ref>. Also, in contrast to our point estimate observers, the original categorisation decision is usually based on full Bayesian inference. Thus, the self-consistent Bayesian observer makes the same predictions for the categorisation decision and is equally computationally expensive as the full Bayesian observer. A similar proposal in the cognitive literature can be found in the context of reasoning with uncertain categories <ref type="bibr" target="#b15">(Chen, Ross, &amp; Murphy, 2014;</ref><ref type="bibr" target="#b58">Murphy, Chen, &amp; Ross, 2012;</ref><ref type="bibr" target="#b72">Ross &amp; Murphy, 1996)</ref>, where humans sometimes appear to only rely on the most likely category for their decisions. In other cases however, humans appear to take all possible categories into account <ref type="bibr" target="#b16">(Chen, Ross, &amp; Murphy, 2016;</ref><ref type="bibr" target="#b59">Murphy &amp; Ross, 2010)</ref>.</p><p>Sampling A broad class of models for how humans may implement Bayesian decision making is based on sampling (e.g. <ref type="bibr" target="#b46">Lieder, Griffiths, M. Huys, &amp; Goodman, 2018;</ref><ref type="bibr" target="#b90">Vul et al., 2014)</ref>. This kind of observer approximates the integrals in equation <ref type="formula" target="#formula_1">2</ref>with a (potentially weighted) sum over samples from the posterior (or other appropriate distribution). Importantly, samples can be generated for the full joint posterior and marginalization can then be performed simply by ignoring all variables which are not of interest, so that no integrals need to be computed <ref type="bibr" target="#b74">(Sanborn &amp; Chater, 2016)</ref>.</p><p>There are many different sampling algorithms, but typically, models of human decision making employ a form of Markov Chain Monte Carlo (MCMC) sampling. The distinctive feature of these algorithms is that samples are taken sequentially and the next sample depends only on the current one. These sequential dependencies can explain anchoring effects if a small number of samples is taken for the approximation <ref type="bibr" target="#b46">(Lieder et al., 2018)</ref>. The origin of decision noise and some deviations from optimal decision making can also be explained by a small number of samples even if the samples are independent <ref type="bibr" target="#b90">(Vul et al., 2014)</ref>. In particular, the deviations caused by sampling can explain distortions in the handling of probabilities and reconcile the approximate Bayesian observer hypothesis with the observations that humans often make errors when handling probabilities <ref type="bibr" target="#b100">(Zhu, Sanborn, &amp; Chater, 2020</ref>).</p><p>Most sampling accounts of human perceptual inference are concerned with estimating the proximate stimulus (s in our formulation, e.g. Moreno-Bote, <ref type="bibr" target="#b56">Knill, &amp; Pouget, 2011;</ref><ref type="bibr" target="#b65">OrbÃ¡n, Berkes, Fiser, &amp; Lengyel, 2016;</ref><ref type="bibr" target="#b90">Vul et al., 2014)</ref>. To do so, these approaches draw samples from the posterior p(s|x). Such samples are not immediately usable for making decisions about the category C. To generate sampling observer models that apply to the decision about the category C we had to adapt the idea slightly. We implement two sampling observer models: One importance sampling observer that samples s values from the prior of each category, and a MCMC algorithm that samples from the joint posterior p(C, s|x) and bases its decision on the sample frequencies of the two categories.</p><p>Importance sampling The first sampling based observer we test is based on an importance sampling estimate of the integrals required for the Bayesian observer. Its implementation is straight forward. For all our experiments we can directly sample stimulus values s i from the prior distribution under each category p(s|C) and compute an approximation to the probabilities in the ratio used by the Bayesian observer:</p><formula xml:id="formula_3">p(x|C) = p(x|s)p(s|C)ds ≈ 1 N s Ns i=1 p(x|s i )<label>(4)</label></formula><p>We then use the sampling estimates of these integrals for the two categories in the same way that we used the analytic solutions of these integrals for the full Bayesian observer.</p><p>Joint posterior sampling The other sampling observer we implemented is based on sampling from the joint posterior over category and stimulus p(C, s|x). Sampling from this posterior is not trivial, because the stimulus s may have different dimensionality for the different categories, and the stimulus distributions according to the two categories may not overlap. Thus, designing a proposal distribution for MCMC algorithms that can switch category is not trivial for general situations, especially if we additionally aim to produce proposals that are in some sense close to the current sample to achieve high acceptance rates. To avoid this problem, we simply use the prior p(C, s) as a proposal distribution independent of the current sample. This choice allows the same model to apply to all tasks and has no additional parameters to be tuned to the distribution to be sampled.</p><p>Using a Metropolis-Hastings rejection sampler, this observer takes a fixed number of samples to make their decision. We then convert the number of samples for the two categories into a decision variable in analogy to the other observers as follows:</p><formula xml:id="formula_4">d s = log 1 + i 1 Ci=1 1 + i 1 Ci=0<label>(5)</label></formula><p>Adding one to each category guarantees a valid decision variable for any sampling outcome. We then pass this decision variable through the same mapping to behavior as for all other observer models.</p><p>Variational Inference Observer Variational inference is a general method that approximates complex posteriors with simpler distributions <ref type="bibr" target="#b7">(Bishop, 2006;</ref><ref type="bibr" target="#b8">Blei, Kucukelbir, &amp; McAuliffe, 2017;</ref><ref type="bibr" target="#b54">Minka, 2005)</ref>. To do this, one chooses a family of simple distributions and optimises within this family to find the distribution which is closest to the full posterior. This approach has been proposed prominently as part of the free energy principle framework <ref type="bibr" target="#b26">(Friston, 2010)</ref>, which aims not only to explain inference and decision making, but also how observers learn the world model, i.e. how the observers understanding of the world is adjusted to match their oservations better. While there have been some attempts to experimentally test this type of observer model with neural data (e.g. <ref type="bibr" target="#b33">Grabska-Barwińska et al., 2017)</ref>, most discussion of the approach has remained on a theoretical level <ref type="bibr" target="#b31">(Gershman, 2019)</ref>.</p><p>Here, we use the family of factorising distributions as the family to optimise within. This means that we require our approximate posterior to be expressible as the product of separate distributions over s and C (q(s) and q(C)). In other words, we require that the approximate posterior "factorises over" the categorical variable C and the nuisance variable s. The best factorising distribution is sometimes referred to as the mean-field approximation. This model observer then bases their decision on the approximate posterior. To find the approximate posterior, we search for the q(C, s) = q(C)q(s) that minimises the Kullback-Leibler (KL) divergence between q and the true posterior p, KL(q||p) (KL being a quantity that increases as two distributions become more dissimilar). We then define:</p><formula xml:id="formula_5">d V = q(C = 1) q(C = 0)<label>(6)</label></formula><p>In many tasks, and most of the tasks we discuss here, variational inference fails. Any distribution q that assigns a nonzero probability to a case which has zero probability under p, is an infinitely bad approximation of p as measured by the KL divergence. In most tasks, the support of s differs between categories, i.e. under p there are some values of s, which happen in category C = 1, but never in category C = 0 and/or vice versa. As we try to find a factorised approximation, a combination of C and s will only have zero probability according to the approximation if one of the corresponding factors q(C) and q(s) has zero probability (recall the approximation is q(C, s) = q(C)q(s)). Thus, for any s, which lie outside of the support of one category C, either q(s) must be zero or q(C) must be zero. As a consequence, all q with finite KL divergence commit fully to one of the categories and/or restrict s to the overlap of all supports (i.e. to values of s which are possible under both categories). If our approximation, q, commits to one category completely, there is no distribution over C and the model observer is perfectly sure of their response, which is nonsensical. If s is restricted to the shared support, the inference will be strongly biased towards the narrower category, such that the model observer would always report that category, which is also nonsensical.</p><p>For example, take a simple classification task in which the participants distinguish between positive and negative values of s, like judging whether a stimulus is tilted left or right (illustrated in Task 2 of <ref type="figure">Figure 2</ref>). Then any value of s can only happen under one of the categories, i.e. there is no overlap in the supports of the two probability distributions. Thus, all acceptable factorised solutions assign zero probability to one category, and commit to the other category fully.</p><p>As another example, take the collinearity judgement task of <ref type="bibr" target="#b99">Zhou, Acerbi, and Ma (2019)</ref>; discussed in detail below. There, the two entries of s are equal in category C = 1 and are independently drawn in category C = 0. Thus, all distributions with q(C = 1) &gt; 0 and q(s 1 ̸ = s 2 ) &gt; 0 are considered to be infinitely bad approximations. If we restrict q(s) to be nonzero only for s 1 = s 2 then C = 1 is infinitely more likely than C = 0. Thus, the model observer would always conclude with full confidence that C = 1.</p><p>Thus, the variational inference scheme fundamentally fails as a general scheme for probabilistic inference for many typical cognitive tasks.</p><p>Expectation Propagation Observer The expectation propagation observer is similar to the variational inference observer. It also tries to find a factorised distribution q that approximates the true posterior p. However, expectation propagation uses KL(p||q) instead of KL(q||p) as a measure for how well the true posterior is approximated. This might seem like a small difference, but it fundamentally changes what kind of approximation we search for. In particular, this change reverses the restriction on zeros, such that any combination which has non-zero probability under p also has to have non-zero probability under q <ref type="bibr" target="#b54">(Minka, 2005)</ref>.</p><p>Upon convergence to the global minimum, the approximation will reproduce the marginals of the true posterior distribution such that q(C) = p(C|x). In all tasks presented here, participants were only asked about C. Because participants only responded to one stimulus, the expectation propagation observer would produce the same responses as the Bayesian observer (unless further approximations are enforced on the distribution, q). Thus, we do not explore expectation propagation further here.</p><p>We chose to call this model the "expectation propagation observer", as it has similarities to the approximate inference technique "expectation propagation". Technically, expectation propagation is the name for a specific message passing algorithm to compute the best factorised approximation according to the KL(p||q) criterion. This algorithm requires the projection of the problem onto an exponential family distribution, which we do not perform here. As we are not aware of a succinct name for the best factorised approximation according to the KL(p||q) criterion, and we do not commit to any algorithm for the computation of this approximation that we could name our observer model after, we settled on expectation propagation observer as a name, despite its inaccuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal task description</head><p>For all tasks we have the following general layout: Trials come from two different categories which we index with a binary random variable C. Different categories result in different distributions over one or more true stimulus values s. For some tasks there is an additional categorical variable L, which indexes the target location. We write L = i to denote that the target location is i. (In this case the point estimate observer maximises over both stimulus values s and target location L.)</p><p>We assume that all observer models base their inference on observations x, which are generated by adding normal or von Mises distributed sensory noise to s. The strength of the noise is measured by its standard deviation σ n , or concentration parameter κ. This value may be different for different models and is estimated separately for each stimulus signal strength, which was varied using different approaches in the different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic response model</head><p>This results in the following general probabilistic model which applies to all tasks and (a simplified version) is illustrated in <ref type="figure">Figure 1</ref>:</p><formula xml:id="formula_6">P (x, s, C, L) = P (x|s)P (s|C, L)P (C)P (L)<label>(7)</label></formula><p>(Note that for the visual search task we did not treat C and L as independent, although we did consider this choice. See Appendix B.5 for details.) In each experiment, the task for the observer was to infer the value of C based on x.</p><p>We add decision noise <ref type="bibr" target="#b47">(Luce, 1959;</ref><ref type="bibr" target="#b57">Mueller &amp; Weidemann, 2008)</ref>, a bias <ref type="bibr" target="#b34">(Green &amp; Swets, 1966</ref>) and a lapse rate <ref type="bibr" target="#b94">(Wichmann &amp; Hill, 2001</ref>) to all our models. These additions make the observer suboptimal and are thus not justified by considerations of optimality. These additions do however improve the match between data and observer models as has been shown in general and, in many cases, for the tasks and data we model here in the original publications on the datasets <ref type="bibr" target="#b14">Calder-Travis &amp; Ma, 2020;</ref><ref type="bibr" target="#b79">Shen &amp; Ma, 2016;</ref><ref type="bibr" target="#b96">Yoo, Acerbi, &amp; Ma, 2021;</ref><ref type="bibr" target="#b99">Zhou et al., 2019)</ref>. We ensured our conclusions depended minimally on these additional components by adding them to all inference schemes considered: If, for example, bias really does exist, then because this is a component of all models, no model in particular will be favoured. Hence, we would not incorrectly favour one perceptual inference scheme over another.</p><p>To implement these mechanisms we start with the decision variable d which each observer generates. This decision variable is transformed into response probabilities by adding a bias term β 0 , soft-max noise, and a lapse rate λ. This results in the following formula for the transformation:</p><formula xml:id="formula_7">P (respond C = 1) = λ 2 + (1 − λ) exp(βd + β 0 ) 1 + exp(βd + β 0 )<label>(8)</label></formula><p>Alternatively and equivalently, this type of noise can be implemented by exponentiating the probabilities of the categories and renormalising to yield the probability for each response <ref type="bibr" target="#b76">(Sanborn, Griffiths, &amp; Shiffrin, 2010;</ref><ref type="bibr" target="#b91">Vulkan, 2000)</ref>.</p><p>The observer models we consider do not have any additional parameters. That is, their free parameters are the sensory noise standard deviations σ n for the different conditions, β, β 0 and λ.</p><p>A truly "optimal" observer would not lapse (λ = 0), would not have a bias (β 0 = 0) and would not have decision noise (β → ∞). We use the term "Bayesian Observer" to refer to observer models based on this decision variable, allowing for an additive bias, decision noise, and lapses. Note that in building this "Bayesian observer", we added substantive and important additional mechanisms beyond simply assuming optimal inference <ref type="bibr" target="#b71">(Rahnev &amp; Denison, 2018)</ref>.</p><p>For the point estimate observer we created a fixed criterion variant and an optimal criterion variant: The fixed criterion variant takes d P as defined above and thus inherits any noise-level dependent bias. To generate the optimal criterion variant, we add a noise-level dependent term to d P such that β 0 = 0 leads to maximal performance regardless of the noise-level (for an otherwise optimal observer with β → ∞, λ = 0). In most cases, we could not find a closed form solution for the noise-level dependent bias. To estimate it we simulated 100, 000 trials for each category and calculated d for each sample (or 50,000 trials when fitting the visual search dataset due to computational cost). We then found the optimal criterion with a bisection search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fitting and Comparison</head><p>The parameters for each model and task are reported in <ref type="table">Table 1</ref>. For each task, all our models have equivalent parameters and the two sampling observers have one additional parameter for the number of samples.</p><p>For some of the models we use in this paper, we cannot directly compute likelihoods as this would require us to integrate out the measurement x, and this integral can be intractable. Thus, we chose to estimate all log-likelihoods using Inverse Binomial Sampling <ref type="bibr">(van Opheusden, Acerbi, &amp; Ma, 2020)</ref>. We ran inverse binomial sampling repetitions until the estimated variance of the log-likelihood became smaller than 4.</p><p>We fit the parameters of each model to each participant separately, by maximising the estimated log-likelihood using Bayesian adaptive direct search <ref type="bibr" target="#b0">(Acerbi &amp; Ma, 2017)</ref>. As bounds for the parameter optimisation we chose values reported in table 1. We initially used 20 different starting positions, uniformly choosing positions within the plausible bounds, to reduce the probability of finding only a local maximum. For each optimisation we aimed for at least 5 other starting positions leading to an optimisation result within 2 log-likelihood points of the maximum log-likelihood found, so that we are confident that model differences were not due to failed optimisations. We continued to add more starting points, if we did not satisfy the log-likelihood criterion. There was an exception to this policy: For two participant-model combinations in the visual search task, even after running the fitting from 180 starting points, there were not 5 runs which ended within 2 log-likelihoods of the maximum log-likelihood found.</p><p>To estimate the log-likelihoods at the candidate optima more accurately, we reevaluated the log-likelihood using 100 repetitions of our inverse sampling approach, which each generated estimates of variance 4 or less. Hence, after 100 repetitions the estimated variance of the log-likelihood at the optimum is 0.04, i.e. the standard deviation is 0.2. We then took the parameter combination corresponding to the minimum negative log-likelihood over our runs as the maximum likelihood estimated parameters. We report differences in raw log-likelihood values for model comparison, which are equivalent to both AIC and BIC values, as the number of parameters does not differ between models.</p><p>For plotting model fits against behaviour we simulated new datasets of the same size as the original datasets. On a participant-by-participant basis we took the best fitting parameters, and used these to generate simulated responses to the stimuli that were used in the real experiment.</p><p>To keep the computational requirements for the sampling observers manageable, we restricted the number of samples taken to be less than 1000. Restricting the number of samples is also of theoretical interest; with a sufficiently large number of samples, sampling observers should converge to the Bayesian observer. Because we are interested in how these models may diverge, specifically, how sampling observers may fit human data better than a Bayesian observer, we are interested in lower sampling regimes. When the fits of these observer-models converge to this bound while still yielding worse goodness-of-fit than the Bayesian observer, we interpret this as evidence that the sampling observers would eventually converge to the Bayesian observer if we had more computational resources available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Data sets and analysis code are available at https://osf.io/x8q6j/ and https://github.com/NYUMaLab/AISP or dx.doi.org/10.17605/OSF.IO/X8Q6J. This study was not preregistered and all data were collected for previous publications. Ethics approval from the institutional review boards were obtained for the original publications and is reported in these articles <ref type="bibr" target="#b14">Calder-Travis &amp; Ma, 2020;</ref><ref type="bibr" target="#b79">Shen &amp; Ma, 2016;</ref><ref type="bibr" target="#b96">Yoo et al., 2021;</ref><ref type="bibr" target="#b99">Zhou et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model evaluations</head><p>As we aim to find observer models that generalise to a wide range of tasks, we chose five typical perceptual decision making tasks to test the point estimate observer on. For all five tasks, data had been collected and published before. For the first task, we chose the categorisation task by <ref type="bibr" target="#b2">Adler and Ma (2018)</ref>, which showed evidence for non-Bayesian decision making and confidence reports. Second, we chose a study on colinearity judgements by <ref type="bibr" target="#b99">Zhou et al. (2019)</ref>, which again showed a slight preference for non-Bayesian decision making and emphasised the necessity to include decision noise and lapse rates. Third, we chose a visual search task from Calder-Travis and Ma (2020), to collect more information on how information is pooled from multiple items in a single stimulus. Previous Bayesian and non-Bayesian models had fit this dataset approximately equally well. Fourth, we chose an outlier classification task by <ref type="bibr" target="#b79">Shen and Ma (2016)</ref>, which provided strong evidence in favour of Bayesian inference and especially for near optimal integration over target locations. Finally, we chose a working memory change detection task by <ref type="bibr" target="#b96">Yoo et al. (2021)</ref>, which showed evidence in favour of Bayesian decision making. Both Bayesian and non-Bayesian models had performed well in this dataset. These studies cover a range of tasks that are often employed, computations required over different Joint sampling observer -20 -10 0 10 ±20 -10 0 10 ±20 -10 0 10 ±20 -10 0 10 ±20 -10 0 10 20 precision Importance sampling joint sampling <ref type="figure">Figure 3</ref>: Model fits for the simple categorisation task . A: Illustration of the stimulus distributions under the two categories: Participants were asked to judge whether a stimulus came from category 1 or category 2 based on the orientation of an ellipse or of a drifting Gabor patch. B: Log-likelihood comparison against the Bayesian observer. Gray dots represent individual participants, the black represents their average. The sampling observers have the number of samples as an additional parameter for each participant. Typical corrections like AIC or BIC are very small compared to the differences between models observed for this task. This is the only task that we consider where the variational inference observer is applicable. For this task we do not show the point estimate observer with an optimal criterion, because it is equivalent to the Bayesian observer. C: Illustration of the decision boundaries of the five different observers. For each observer model the probability for responding category 2 is plotted against the standard deviation of the noise, and against the measurement, x, which is assumed to be distributed normally around the true stimulus level, s. D: Model predictions of the five observer models (shaded regions) plotted with the data (points in centre of error-bars). The shaded regions and error-bars represent SEMs over participants. The level of perceptual noise was varied by changing the aspect ratio of the ellipses or the contrast of the Gabor patches respectively, in 6 steps as illustrated on the right. sub-hypotheses, and degrees of support for Bayesian optimal decision making. We provide details of the individual experiments in Appendix A, and derivations of the decision rules in Appendix B.</p><p>Simple categorisation In the simple categorisation task , participants were asked to report whether a Gabor or ellipse stimulus came from a narrow central category or from a wider category with the same mean ( <ref type="figure">Fig. 3 A)</ref>. The precision of the observation was varied by changing the aspect ratio of the ellipse or by changing the contrast of the Gabor patch. For both types of stimuli, there were 6 precision levels. Each participant saw either Gabors or ellipses only.</p><p>Qualitatively, participants showed the patterns expected for rational behaviour <ref type="figure">(Fig. 3 C)</ref>. At all reliability levels, participants increasingly preferred the wider category 2 when presented with more tilted stimuli. Also, with decreasing precision this dependence became flatter and, at small precisions, the preference for the narrow category 1 at small tilts diminished. These patterns are all qualitatively consistent with optimal decision behaviour. However, the relationship between the broadening of the curve and the scaling of the peak is quantitatively different from the optimal Bayesian observer prediction.</p><p>For this task, we evaluate the full Bayesian observer, the point estimate observer with a fixed criterion, the two sampling observers, and a variational inference observer. We do not evaluate the point estimate observer with the optimal criterion because, in this task, this observer is equivalent to the Bayesian observer (see Appendix B.1).</p><p>We find that all models can capture the qualitative trends in the data. In the formal model-evaluation the point estimate observer performs better (on average 25.1 log-likelihood points) than the Bayesian observer. However, the relative evaluations differ dramatically between participants, so that the conclusion is not consistent across participants. The variational inference observer has an average performance between the Bayesian and point estimate observers (on average 19.5 log-likelihood points better than the Bayesian observer and 5.6 points worse than the Point estimate observer). The difference to the Bayesian observer for the variational observer is considerably less variable than for the point estimate observer, but there are still participants who are slightly better fit by the Bayesian observer than the variational one. Interestingly, the two sampling observer models also perform better than the full Bayesian observer (on average 28.3 log-likelihood points better for the importance sampling observer, and 19.7 for the joint sampling observer). The sampling models have one parameter more for each subject than the other models. If we applied a correction for that like AIC (1 likelihood point) or BIC (3.8 likelihood points), importance sampling-the better of the two-would still marginally win against the point estimate observer model, but the two are very close.</p><p>For this task <ref type="bibr" target="#b2">Adler and Ma (2018)</ref> found poor performance of the Bayesian observer when this model was compared to ad hoc decision models specific to this task. It appears that the point estimate and variational inference observers can explain a part of these deviations from Bayesian decision making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collinearity judgements</head><p>In this task, participants were asked to report whether two peripherally presented line segments disappearing behind an occluder were collinear, such that they could be part of the same straight line. The line positions were collinear in half the trials. In those trials they were generated as a single draw from a normal distribution. In the other half of trials, they were drawn independently from the same normal distribution. The experiment was originally published by <ref type="bibr" target="#b99">Zhou et al. (2019)</ref>.</p><p>Human behaviour qualitatively follows the expectations for rational behaviour again ( <ref type="figure">Fig. 4 D)</ref>: Larger offsets lead to fewer collinearity reports and this effect decreases with smaller precision. We evaluate the Bayesian observer, a point estimate observer with a fixed criterion and a point estimate observer with an optimal criterion. The variational observer has no sensible interpretation for this task (see Appendix B.2 and discussion above).</p><p>For this task, all models can capture the data well both qualitatively and quantitatively. In the formal model comparison the point estimate observer with optimal criterion has a slight advantage of 5.81 log-likelihood points, and the point estimate observer with a constant criterion had a slight disadvantage of 2.96 log-likelihood points, but these differences are very small. The two sampling observers perform worse than the Bayesian observer model (by 2.8 and 20.3 likelihood points for the importance sampling and joint sampling observer respectively), even without penalties for their extra parameter. As these observers converge to the Bayesian observer model with increasing number of samples, this indicates that the limit of 1000 samples we imposed, limited performance of these observers. Indeed many observers are fitted with a number of samples above 900 (4 of 8 for the importance sampling observer, 7 of 8 for the joint sampling observer). We take this as evidence that sampling does not explain any additional patterns in the participants' behaviour, but just gives an imperfect approximation to the Bayesian observer model. These results are consistent with that of the original publication, where the authors found a slight advantage for a non-Bayesian ad hoc observer.</p><p>Visual search We next evaluated the models using data from a visual search task. Besides visual search being a heavily used cognitive function <ref type="bibr" target="#b23">(Eckstein, 2011)</ref>, it is interesting to study this task because participants must pool information from many items in a display. In this task participants were asked to report the presence or absence of a target Gabor patch oriented at 45 • clockwise from vertical, among distractor Gabors <ref type="figure" target="#fig_2">(Figure 5 A)</ref>. A display containing between 2 and 6 Gabor patches was presented for 100 ms, and then participants had as long as they wanted to respond whether the target was present using a key press. The orientation of each distractor was randomly drawn from either a uniform distribution, or a von Mises distribution centred on the target orientation, depending on the current block. Further details can be found in Appendix A.5, and full details in the study for which the data was originally collected (Calder-Travis &amp; Ma, 2020).</p><p>Behaviour in the task, as a function of three statistics summarising the distractors presented on each trial, is plotted in <ref type="figure" target="#fig_2">Figure 5</ref> C. Behaviour is plotted using error bars (note behaviour plots are duplicated five times). On trials in which the mean of the distractors was further from the target orientation (T-D mean), and trials in which the difference between the target and the most similar distractor was larger (min T-D difference), participants were less likely to report that the target was present. The effect of distractor variance (D variance) was less clear.</p><p>For this task we evaluated the performance of the Bayesian model, the point estimate observer, the optimal point estimate observer, and the two sampling observers. Again, variational inference did not provide a feasible approach  <ref type="figure">Figure 4</ref>: Model fits for the collinearity task <ref type="bibr" target="#b99">(Zhou et al., 2019)</ref>. A: Illustration of the task: Participants were asked to judge whether two peripheral line segments were collinear or not. B: Proportion same responses plotted against the left and right measurements x. C: Log-likelihood comparison against the Bayesian observer. For this task we evaluate the two point estimate observer models, the two sampling observers models and the Bayesian observer model. The variational observer does not work for this scenario, caused by the unequal support for the two categories. D: Predicted responses (shaded regions) and measured data (error bars) plotted against the Offset between the two lines.</p><p>(Appendix B.5). All models accounted well for the relationship between both hit and false alarm rate, and the three distractor statistics considered <ref type="figure" target="#fig_2">(Fig. 5 C)</ref>. When evaluating the models using the mean maximum log-likelihood, the Bayesian model outperformed the point estimate and sampling models ( <ref type="figure" target="#fig_2">Figure 5</ref> B; all models share the same number of parameters apart from the sampling observers which have an additional parameter). It is important to note that, while the mean maximum log-likelihood was slightly greater for the Bayesian model than the optimal point estimate model, the optimal point estimate model described many participants better than the Bayesian observer model. Even with the extra flexibility of an additional parameter, the sampling models did not in general outperform the Bayesian observer model. The fitted values for the free parameter governing how many samples the sampling observers used were very high. They often reached the upper limit set during the fitting, suggesting again that the deviation from Bayes optimal performance, introduced by limiting the number of samples, is only detrimental to model fit.</p><p>Outlier classification In the outlier classification task, participants were shown four Gabor stimuli, three of which had the same orientation. They were then asked to report whether the one with a different orientation was tilted left or right. Orientations for the target and the distractors were drawn from the same normal distribution. This task was originally published by <ref type="bibr" target="#b79">Shen and Ma (2016)</ref>.</p><p>In this task, the optimal decision pattern is relatively complex, because the decision of which item is the target needs to include the orientations of all items in the display. Importantly, this leads to an S-shaped influence of the distractors' orientation on the judgements. When the distractors are only slightly tilted they can be confused with a target near the decision boundary, and thus bias the decision towards their orientation. However, strongly tilted distractors do not influence the decision, because they can only be confused with the target if the target clearly has the same tilt direction. Thus, the bias caused by the distractors diminishes at strong tilts. This pattern of decisions is also produced quite closely by participants in this task (Figure 6 C).</p><p>For this task, we only evaluate the Bayesian observer, the two sampling observers, and the point estimate observer. In this case, the whole experiment was performed at a constant noise level, removing the necessity to implement the Optimal-Criterion Point Estimate Observer. Again, the variational observer does not work for this task because the different categories and target locations imply incompatible distributions for the stimulus s. For this task we evaluate the two point estimate observer models, the two sampling models, and the Bayesian observer. The variational observer does not work for this scenario. C: Behavioural data, and model fits. Data is plotted using error bars (±1 SEM across participants), and model fits with error shading (width ±1 SEM across participants). All models captured the patterns in the data well.  <ref type="figure">Figure 6</ref>: Model fits for the outlier classification task <ref type="bibr" target="#b79">(Shen &amp; Ma, 2016)</ref>. A: Illustration of the task: Participants were asked to report whether the Gabor with the different orientation was tilted left or right. B: Log-likelihood difference from the Bayesian observer, dots represent the individual participants. The bar represents the average. For this task, we compare only one version of the point estimate observer to the Bayesian one. The two variants of the point estimate observer are equal here, because the noise was not varied in this experiment. The variational observer once again fails, because the support of the categories does not overlap. C: As B, but for the sampling observers, separated to make the much smaller differences to the Bayesian observer visible. D: Proportion of 'right' reports plotted against the orientation of the target and the orientation of the distractors, binned into 9 quantiles of the normal distribution.</p><p>The fits for the models are displayed in <ref type="figure">Figure 6</ref>. Comparing the point estimate observer to the Bayesian observer we find strong evidence in favour of the full Bayesian observer. The data plots show a clear discrepancy between the data and the prediction of the point estimate model. This is also visible in the formal model comparison: All participants are better fit by the Bayesian observer than by the point estimate observer, and the fit is on average 174 log-likelihood points better. The differences between the sampling observers and the Bayesian observer are so much smaller, that we need to plot them on a different scale to make them visible <ref type="figure">(Fig. 6C)</ref>. The importance sampling observer beats the Bayesian observer by a noticeable margin (on average 9.6 log-likelihood points). The joint sampling observer performs about equal to the Bayesian observer (on average 1.4 log-likelihood points worse).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change detection</head><p>In this task, participants viewed four oriented ellipses, remembered their orientation over a working memory delay, then viewed four ellipses again. Participants indicated whether they believed the orientation of one of the ellipses changed. On every trial, there was a 0.5 probability that one of the ellipses changed orientation. If there was a change, the change was drawn from a Uniform distribution and was equally probable to occur in any of the ellipses. Ellipses provided either high-or low-reliability orientation information, the high-reliability ellipses being longer and narrower. The probability of each ellipse being high reliability was 0.5, independent of the reliability of the other ellipses. This experiment was originally published as the "ellipse condition" by <ref type="bibr" target="#b96">Yoo et al. (2021)</ref>.</p><p>To maximize performance in this task, the participants should take into account the item-to-item uncertainty when making the change detection decision. For this task, we evaluate the Bayesian observer, the point estimate observer with a fixed criterion, the point estimate observer with an optimal criterion, and the two sampling observers. Again, the variational observer has no sensible interpretation for this task (see Appendix B.4).</p><p>All models provided similar fits, qualitatively (Figure 7 C) and quantitatively <ref type="figure">(Figure 7 B</ref>). All models perform similarly well across participants; the Bayesian model has an average log likelihood 2 higher than point estimate model, 1.3 higher than the Important sampling model, and 3.5 higher than the Joint posterior sampling model. The Bayesian and Optimal point estimate models perform almost identically (0.3 LL difference). We do not consider any of the differences across models meaningful.</p><p>Performance comparison In discussing approximate observer models, one question of immediate interest is how well such approximate observers perform the tasks, i.e. what level of task performance these observers achieve, as opposed to how well do these observer models match human behaviour. In this section, we investigate how sensory  <ref type="figure">Figure 7</ref>: Model fits for the change detection task <ref type="bibr" target="#b96">(Yoo et al., 2021)</ref>. A: Illustration of the task: Participants indicated whether they believed an ellipse changed in orientation over the working memory delay. B: Log-likelihood differences of the two point estimate observer models and the two sampling observers models, relative to the Bayesian observer model (all models share the same number of parameters). Dots represent the individual participants. The bar represents the average. The variational observer fails for this experiment. C: Proportion report "change" as a function of number of high-reliability ellipses, conditioned on whether there was no actual change (false alarm, FA, blue), a change in a low-reliability ellipse (H low , red), a change in a high-reliability ellipse (H high , purple), or a change in any ellipse (hit, H, yellow).</p><p>noise affects the performance of each of these model observers, simulating responses to a large number of trials <ref type="figure">(Fig. 8,  100000</ref> at each level).</p><p>For the plots sensory noise was varied across a range of values. Lapse rate (λ) was set to 0, bias (β 0 ) was set to 0, number of samples in the sampling observer models (N s ) was set to 10. Other parameters were set to their mean value across-participants from the Bayesian observer model fits. For the visual search task we plot performance in one specific condition (3 Gabors in the display, uniformly distributed distractors).</p><p>In general, the optimal point estimate observer and the variational observer primarily lead to performance drops at the high noise levels that yield generally low performance <ref type="figure">(Fig. 8 B &amp; E)</ref>, if there were any performance drops for these models at all (see <ref type="figure">Fig. 8</ref> C, D &amp; F for the collinearity, visual search and detection tasks with essentially no losses). In contrast, the sampling observers lead to an overall scaling of the response curves, which incurs the highest losses at low noise levels with otherwise high performance throughout <ref type="figure">(Fig. 8 B-F)</ref>. Note, that we chose a very low number of samples here to make the differences between models clear. At higher sample numbers-as we fit to our human observers-the performance of the sampling observers converges to the Bayesian observer as expected. Finally, the non-optimized criterion point estimate observer performs worse primarily at low noise values with high performance <ref type="figure">(Fig. 8 C, D &amp; F)</ref>. The comparison to the optimal point estimate observer shows the substantial performance drop observers can incur by being biased. Taken together, these results show that the different approximate observer models lead to different relationships between noise level and strength of deviation from optimal performance.</p><p>Interestingly, we observe that for the simple categorization task, the optimal criterion point estimate observer performs even better than the nominally optimal Bayesian one <ref type="figure">(Fig. 8 A)</ref>. This happens due to an interaction with the late noise we inject by mapping the decision variable to response probabilities, instead of applying a hard threshold. If we use a much higher slope to approximate a hard threshold, the Bayesian observer performs best as expected <ref type="figure">(Fig. 8 B)</ref>. This serves as a reminder, that subsequent decision noise can change what the optimal solution for the decision variable is, as has been observed earlier <ref type="bibr" target="#b44">(Li et al., 2017;</ref><ref type="bibr" target="#b87">Tsetsos et al., 2016)</ref>. Figure 8: Performance comparisons between observers, plotted against level of sensory noise. Unless noted otherwise, we used the mean fitted slope (β) for the Bayesian observer, no lapse rate (λ = 0), and no bias (β 0 = 0) and created 100000 new random trials for evaluation at each noise level. Ticks mark the mean noise parameters across subjects for the conditions measured in the experiments. For the sampling observers we took 10 samples for each trial. A: Simple categorization task, plotting against the noise standard deviation. B: Simple categorization task, as in A, but with a much higher slope (100). C: Collinearity task, plotting against the noise standard deviation. D: One condition in the visual search task (3 Gabors in the display, uniformly distributed distractors), plotting against the precision parameter of the von-Mises noise distribution. E: Outlier classification task, plotting against the noise standard deviation. F: Change detection task, plotting against the average precision of the variable precision model, setting the bias to the arbitrary value of 10 for the biased point estimate observer model as 0 is outside the distribution of decision values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>There is a lack of task-general alternatives to the Bayesian observer model that avoid the computationally intensive marginalisation operation. In response to this deficit, we introduced the point estimate observer model, and evaluated its ability to account for a wide range of behavioural data. These point estimate observers evaluate the plausibility of a category based only on the world state that is most likely, if that category were indeed correct. This model directly applies to all perceptual categorisation tasks, and slightly different read outs may apply to most other decision making tasks. The point estimate observer takes a statistical model of the world into account, but it is not equivalent to the full Bayesian solution.</p><p>Comparing the point estimate observer to the Bayesian observer in five tasks, we found that the point estimate observer model performs somewhat better in two tasks (simple categorisation and collinearity judgements), ties in two tasks (working memory change detection task and visual search) and loses clearly in one task (outlier classification). Overall, the point estimate observer model performs competitively to the Bayesian observer model and cannot be outright rejected.</p><p>As the evidence in favour of the Bayesian observer model is mostly provided by a single task, the outlier classification task of <ref type="bibr" target="#b79">Shen and Ma (2016)</ref>, we wondered how this task might be different from the others. First, this task has the most complex statistical dependencies between stimuli, which might trigger more complex cognitive processing, although this was neither expected nor encouraged. Second, a prominent feature of this task is the detection of an outlier from a set of equal stimuli. There is some evidence that percepts corresponding to groups of stimuli are summarised, while individual stimuli with prominent differences to the group "pop out" and are preferentially processed, starting relatively early in visual processing <ref type="bibr" target="#b61">(Müller, Heller, &amp; Ziegler, 1995;</ref><ref type="bibr" target="#b86">Treisman &amp; Gormican, 1988;</ref><ref type="bibr" target="#b93">Whitney &amp; Yamanashi Leib, 2018)</ref>. Thus, it could be that this task is solved by specialised processes. Third, different tasks were done by different participants and it is possible that the participants in this experiment were particularly Bayesian or diligent. <ref type="bibr">3</ref> Finally, our analysis of simulated behavioural performance for the various observer models <ref type="figure">(Fig. 8)</ref> suggested that in the outlier classification task, observers may pay an especially heavy price for performing point estimate inference, compared to performing Bayesian inference. If observers could choose when to engage the Bayesian observer instead, this task would thus encourage them most to do so. These ideas are all speculative, and our data does not allow us to state any with certainty.</p><p>We also compare to two sampling observers, which generally perform well. In three of our tasks (collinearity judgements, change detection, and visual search), their fit is worse than or equal to the full Bayesian observer and the fitted sample sizes are high, which we interpret as evidence that the sampling assumption does not help to improve fits in these tasks beyond the full Bayesian observer. For the outlier classification task, the importance sampling observer beats both the Bayesian observer and the point estimate observer. For the simple classification task, the importance sampling observer fit better than the Bayesian observer and almost exactly equally well or even slightly better than the point estimate observer.</p><p>Interestingly, the importance sampling observer performed better in all tasks than the joint sampling observer. This highlights that "sampling observer" might not be a sufficiently precise description and future research should aim to understand which sampling observers are particularly good models of human decision making. A key difference here is that the importance sampling observer, which performs better throughout, takes samples independently. Thus, it does not display sequential dependencies, which earlier research used as evidence in favor of sampling based, resource constrained models <ref type="bibr" target="#b46">(Lieder et al., 2018)</ref>.</p><p>Compared to the point estimate observer, the importance sampling observer loses once while tying with the Bayesian observer (collinearity judgements), wins decisively once (outlier classification), and otherwise essentially ties with the point estimate observer. Thus, the importance sampling observer and the point estimate observer do not dominate each other either way. They are not equal though. They perform better than the Bayesian observer in different tasks (collinearity exclusively for the point estimate observer, outlier classification exclusively for the importance sampling observer) and in the simple categorization task where they perform similarly they still make different predictions.</p><p>Finally, we also tried to apply a variational inference observer which fits a factorized approximation to the posterior. This observer model was not applicable to most of our tasks, because the supports of the stimulus distributions under the two categories were often distinct 4 . As discussed, in this situation, all sensible distributions that factorise over category and inferred stimulus are considered infinitely bad approximations from the perspective of variational inference. This highlights a shortcoming of variational inference that is widely recognised in machine learning <ref type="bibr" target="#b7">(Bishop, 2006;</ref><ref type="bibr" target="#b54">Minka, 2005)</ref>, but has been largely overlooked when considering variational inference as a principle for human perception <ref type="bibr" target="#b26">(Friston, 2010)</ref>. This shortcoming applies broadly across different tasks, as most experiments use mutually exclusive categories of stimuli in some aspect of experimental design. In the real world, many combinations of variables are impossible as well. A potential solution would be to assume that the observer's internal world model differs from the true world model, even before the mean field approximation. The free energy principle (which is based on variational inference) makes predictions for how a parametrized internal world model should be adjusted towards fitting observations better. However, there is no general specification of what world models human observers use, and the predictions of the theory substantially depend on this choice. Thus, apart from assuming observers use the true world model, there is no uniquely defined theory that we could test. This highlights the importance of specifying the internal world model and its deviations from the true statistical structure <ref type="bibr" target="#b71">(Rahnev &amp; Denison, 2018)</ref>. Making the only obvious choice, of requiring observers to use the true world model, is far less trivial than it sounds and dramatically constrains the range of tasks to which this approach can be applied. In comparison, other approximate inference schemes do not share this restriction.</p><p>We found that the expectation propagation observer (as defined here) has a different problem when used as a model of human perception. Namely, it converges to the same marginal distributions as the Bayesian observer. As a result, this model cannot account for observed differences between human behaviour and Bayesian optimal behaviour <ref type="bibr" target="#b71">Rahnev &amp; Denison, 2018)</ref> as long as tasks only ask about a single feature. This is because behavior in these cases depends on the marginal distribution over that single feature. As most tasks in general, and all tasks we discuss here ask only about a single feature, we excluded expectation propagation observers from formal model comparison on these theoretical grounds. Extensions of this model which commit to other approximate posteriors, or experiments which can distinguish this type of model from others, may well provide evidence for this model though.</p><p>In many of our analyses, some of the observer models are indistinguishable from each other. This is true despite all original publications containing model recovery analyses showing that similar models to the ones we investigate here could be successfully distinguished with essentially identical model comparison techniques. In situations where models are indistinguishable, either, the two models make the same predictions matching human behaviour, or the two models make different predictions, but are equally wrong in predicting human behaviour. In the first case, the models are valid, but we need other experiments to distinguish them. In the second case, we need to develop better models. Both cases happen in our analyses. The visual search task (Calder-Travis &amp; Ma, 2020) and the working memory task <ref type="bibr" target="#b96">(Yoo et al., 2021)</ref> seem to fall under the first case of essentially indistinguishable model predictions. In the categorization task by <ref type="bibr" target="#b2">Adler and Ma (2018)</ref>, the point estimate observer model and the importance sampling observer perform very similarly well, but make distinguishable predictions. We thus have to conclude that our models are not yet perfect for some data we have, but at the same time many datasets are not helpful for contrasting different approximations to the Bayesian observer.</p><p>Having established the point estimate observer model as a viable alternative to the Bayesian observer, it provides a new starting point for model development. Here, we intentionally present the point estimate observer in a simple form. Future versions may extend the point estimate observer. For example, the point estimate observer could be extended to more complex situations like processing over space or over time where the general principle of finding the most likely state of the world instead of the full distribution of possible world states should still apply. Also, one could consider variants, which allow for multiple optimisations, for example to explore sub-hypotheses, such as possible target locations in our search tasks. Finally, one could aim at unifying the point estimate observer model with other observer models. One such unification could be based on exponentiating and renormalizing the posterior with some exponent α. For α = 1 this yields the original Bayesian observer, for large α the posterior collapses around the maximum of the posterior and thus towards the point estimate observer. This rescaling does not simplify the marginalization, but is compatible with any of the approximate solutions available for full Bayesian inference (e.g. sampling or variational inference). Alternatively, one could try observer models which marginalize over some variables and use a point estimate for others <ref type="bibr" target="#b43">(Lee &amp; Ma, 2021)</ref> or other combinations of intermediate complexity. These mixtures open the possibility that humans adaptively apply different inference algorithms depending on task demands <ref type="bibr" target="#b84">(Tavoni, Doi, Pizzica, Balasubramanian, &amp; Gold, 2022)</ref>. Comparisons between these extensions and to the Bayes optimal solutions may be more informative than the global question whether our observer model is a complete model of human behaviour, just as these more detailed questions are more informative when testing human behaviour against optimal behaviour <ref type="bibr" target="#b71">(Rahnev &amp; Denison, 2018)</ref>.</p><p>Response biases are a frequently observed type of deviation from optimality <ref type="bibr" target="#b71">(Rahnev &amp; Denison, 2018)</ref>. For example, participants tend to favour one of the responses, favour repeating or switching responses, or have similar preferences, which do not improve task performance <ref type="bibr" target="#b12">(Braun, Urai, &amp; Donner, 2018;</ref><ref type="bibr">de Gee et al., 2017;</ref><ref type="bibr" target="#b88">Urai, de Gee, Tsetsos, &amp; Donner, 2019)</ref>. Such biases are usually modelled as shifts and miscallibrations of the decision criterion, or equivalently, as wrong prior expectations. This approach works for all observer models that produce a continuous decision variable, that is thresholded to make a decision. We have used this approach for all observer models here to model biases towards one of the response categories (see Methods). For the Bayesian observer model this modelling approach is a bit odd, as it implies that the participants actively distort an inherently unbiased estimate, for which the optimal criterion is always the same. In contrast, the point estimate observer has to adjust the criterion frequently anyway, as the optimal criterion varies between situations and the computations it performs do not yield the optimal criterion. The inclusion of a bias to correct for misscalibrated criterion to fit human behaviour is therefore justified for the point estimate observer. Our results provide some evidence that observers adjust their criteria towards optimal criteria, because the point observer model with optimally set criteria consistently models human decisions better than the one with a fixed criterion on the decision variable.</p><p>One interesting alternative explanation for how humans may cope with the marginalisation problem, is that humans may reuse computations carried out for past inferences, to produce an approximate posterior (amortised inference; Dasgupta, Schulz, Goodman, and Gershman 2018). Such approximations are expected to be more accurate for frequently experienced situations <ref type="bibr" target="#b18">(Dasgupta, Schulz, Tenenbaum, &amp; Gershman, 2020)</ref>. Alternatively, observers may stop their internal computations early, resulting in an imperfect approximation <ref type="bibr" target="#b75">(Sanborn, Griffiths, &amp; Navarro, 2010;</ref><ref type="bibr" target="#b80">Shi, Griffiths, Feldman, &amp; Sanborn, 2010;</ref><ref type="bibr" target="#b90">Vul et al., 2014)</ref>. Such approximations can be justified as resource-rational <ref type="bibr" target="#b32">(Gershman, Horvitz, &amp; Tenenbaum, 2015;</ref><ref type="bibr" target="#b45">Lieder &amp; Griffiths, 2019)</ref>, i.e. as the best solution achievable with a given computational resource budget. These ideas have mostly been applied to sampling-based approximations, but they could also be applied easily to the point estimate observer. If the optimisation to find the most probable stimulus is an iterative procedure, it could be stopped early to save computational resources. Additionally, choosing the starting values through an amortised procedure would improve convergence speed substantially. This would result in estimates being biased towards the initial or amortised estimates, as is the case for the sampling models, a feature that may account for anchoring effects <ref type="bibr" target="#b17">(Dasgupta et al., 2018</ref><ref type="bibr" target="#b18">(Dasgupta et al., , 2020</ref><ref type="bibr" target="#b46">Lieder et al., 2018)</ref>. In contrast to the idea that observers may stop computation early, our fits of the sampling observer models yield high estimates for the number of samples. In many cases, the full Bayesian observer is a better fit than the sampling observers with 1000 samples taken per category. While our results generally favour the sampling observers, different tasks lead to extremely different sample sizes or levels of sampling efficiency, which will eventually require explanation.</p><p>Future studies could also investigate how the point estimate observer might be implemented in the brain. For the Bayesian observer and the variational observer, some neuronal implementation solutions have been proposed and continue to be developed (e.g. <ref type="bibr" target="#b4">Beck, Latham, &amp; Pouget, 2011;</ref><ref type="bibr" target="#b5">Beck et al., 2008;</ref><ref type="bibr" target="#b36">Haefner et al., 2016;</ref><ref type="bibr" target="#b50">Ma, Beck, Latham, &amp; Pouget, 2006;</ref><ref type="bibr" target="#b67">Parr, Markovic, Kiebel, &amp; Friston, 2019;</ref><ref type="bibr" target="#b98">Zemel, Dayan, &amp; Pouget, 1998)</ref>. Given that the variational observer model already requires optimisation, it seems probable that a neural network could be designed that implements the necessary optimisation for the point estimate observer (similar to <ref type="bibr" target="#b21">Deneve, Latham, &amp; Pouget, 1999)</ref>.</p><p>It would also be useful to test the point estimate observer on more empirical data. As the point estimate observer requires no marginalisation, comparing the point estimate observer to the full Bayesian observer allows us to judge whether humans do or do not use marginalisation. To do so effectively, one should focus on experiments where the point estimate observer and the Bayesian observer produce different predictions.</p><p>The analogy of our point estimate observer model to frequentist statistics may provide some intuition for when the point estimate observer can make different predictions to the Bayesian observer model. In a frequentist context, the inference that our point estimate observer performs corresponds to the likelihood ratio test 5 , which is often the most powerful test <ref type="bibr" target="#b64">(Neyman, Pearson, &amp; Pearson, 1933</ref>). If the model is correct and the sample is sufficiently large, the likelihood converges to a function with a narrow peak around the true state of the world under quite general conditions <ref type="bibr" target="#b92">(Wald, 1949)</ref>. In this case, Bayesian and frequentist analysis largely agree <ref type="bibr">(Gelman et al., 2013, chapter 4.2)</ref>. Conversely, the point estimate observer and the Bayesian observer can only disagree if either the assumed model is wrong, or the sensory information is sufficiently weak that the estimates are far from convergence. Situations with substantial deviations between the true model and the model assumed by the subjects may exist, but are problematic for experiments, because it is almost impossible to experimentally fix or determine the model assumed by the subjects in this case. These observations suggest that the most promising situations to distinguish Bayesian and point estimate observers are situations in which the subject is particularly uncertain about some intermediate sensory variables, such as our s. In particular, experiments in which the width of the posterior can be manipulated independent of the overall probabilities of the categories should be informative. The recommendation to look at the high-uncertainty regime is further supported by our observation that the biggest performance differences between the Bayesian optimal observer and the point estimate observer occur at high noise levels.</p><p>If one wanted to know whether humans use factorized representations as the variational observer or the expectation propagation observer do, it might be particularly informative to run experiments in which participants are asked to simultaneously classify the stimulus on two different dimensions, C 1 and C 2 . Building in interesting interactions between the two sets of categories might make the experiment even more powerful. When we ask about multiple features simultaneously we may even detect a difference between expectation propagation and full Bayesian inference. However, we do not know of an experiment that allows this distinction. Indeed, the design of experiments that reliably distinguish the different observer models is further hampered by sensory noise, decision making noise and cognitive response distortions, which may all distort the outcomes of such experiments.</p><p>In sum, the point estimate observer demonstrates the feasibility of a general perceptual decision model that does not rely on marginalisation. Although it does not explain all of human decision-making, it outperforms the Bayesian observer and our sampling observer models in some tasks, and is thus a competitive model in its own right. Also, our results highlight the importance of comparing Bayesian observer models to other observer models like the point estimate observer, since a dataset which is well described by many observer models might provide little evidence in favour of either of them. Thus, even if the point estimate observer is not ultimately the correct model, it certainly extends our toolbox for modeling human behaviour. Specifically, it may serve as a comparison model, helping us to determine which tasks provide evidence for humans using marginalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Simple Categorisation Task</head><p>A full description of the experiment can be found in <ref type="bibr" target="#b2">Adler and Ma (2018)</ref>. We only use choice data from task B here. The experiment additionally collected confidence data, which we will ignore here for consistency with the other tasks and with the modelling framework. Furthermore we did not analyse task A, in which the categories had the same standard deviation, but differed in mean. This task A was generally found not to distinguish between models in the primary publication. Data are available at https://github.com/wtadler/confidence/tree/master/human_data. In summary, 11 participants performed the task over 5 sessions each, which were all shared with task A. Experiments were performed with a linearised 2013 Apple IPad display controlled using Psychtoolbox in MATLAB <ref type="bibr" target="#b39">(Kleiner, Brainard, &amp; Pelli, 2007)</ref>.</p><p>Stimuli The background was mid-level gray (199 cd/m2). The stimulus was either a drifting Gabor (Participants 3, 6, 8, 9, 10, and 11) or an ellipse <ref type="figure" target="#fig_2">(Participants 1, 2, 4, 5, and 7)</ref>. The Gabor had a spatial frequency of 0.5 cycles per dva, a speed of 6 cycles per second, a Gaussian envelope with a standard deviation of 1.2 dva, and a randomised starting phase. Each ellipse had a total area of 2.4 dva 2 , and was black (0.01 cd/m2). Contrast and aspect ratio of the stimuli were chosen uniformly randomly per trial to vary difficulty of the task. The 6 contrast levels for the Gabors were 0.4%, 0.8%, 1.7%, 3.3%, 6.7%, or 13.5% and ellipses had 0.15, 0.28, 0.41, 0.54, 0.67, or 0.8 eccentricity. In task B, which we model here, stimulus orientations were drawn from normal distributions with mean 0, and standard deviations σ 0 = 3 for category 1 and σ 1 = 12 • for category 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Collinearity Judgements</head><p>A full description of the experiment can be found in <ref type="bibr" target="#b99">Zhou et al. (2019)</ref> and data is available at https://github.com/ yanlizhou/collinearity. In summary, 8 participants were asked to judge whether two line segments presented peripherally at the two sides of an occluder were collinear or not. The experiment took 4 one hour sessions and was performed on the same Apple IPad display as the first task. We only use the collinearity judgements here, i.e. we ignore the additionally collected height judgements and confidence data.</p><p>Stimuli A dark gray occluder (23 cd/m2) with a width of 5.6 degrees of visual angle (dva) was displayed against a light gray background (50 cd/m2). A white (159 cd/m2) fixation dot 0.24 dva in diameter was shown in the lower central part of the occluder. The stimuli consisted of two horizontal white line segments on both sides of the occluder. The line segments were 5.6 dva long and 0.16 dva wide. The mean of the normal distributions the line positions were drawn from was set to 0, 4.8, 9.6, or 16.8 dva above the fixation location. The standard deviation of the line positions was 0.48 dva. The occluder and the fixation dot were displayed for 850 ms, followed by the stimulus for 100 ms. In each session of 200 trials contained 100 collinear and 100 non-collinear trials. The participant pressed one of 8 keys, corresponding to 8 choice-confidence combinations, ranging from high-confident collinear to high-confident non-collinear. Response time was not constrained. No performance feedback was given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Shen &amp; Ma</head><p>A full description of this experiment can be found in <ref type="bibr" target="#b79">Shen and Ma (2016)</ref> and data is available at https://github .com/shenshan/optimal_simple In summary, 9 participants were asked whether the 1 of 4 stimuli with a different orientation than the others was tilted left or right. Stimuli were displayed on a 21 inch LCD monitor with a refresh rate of 60 Hz and a resolution of 1,280 x 1024 pixels on a grey background with a luminance of 29.3cd/m 2 .</p><p>Stimuli The four stimuli were placed 5 dva diagonaly away from fixation. Each stimulus was a Gabor with a peak luminance of 35.2 cd/m 2 , a spatial frequency of 3.13 cycles per degree, a standard deviation of 0.254 degrees of visual angle, and a phase of 0. Target and distractor orientations were independently drawn from a Gaussian distribution with a standard deviation of 9.06 • around vertical. Stimuli were shown for 50 ms after a 500ms display of only the fixation cross. Response time was not limited. After the response, correctness feedback was given by coloring the fixation dot red or green for 500 ms. The experiment consisted of three sessions with 1000 trials each, of which the first was excluded from the analysis as training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Change Detection</head><p>A full description of this experiment can be found in <ref type="bibr" target="#b96">Yoo et al. (2021)</ref> and data is the Ellipse condition available at https://github.com/aspenyoo/uncertaintyWM. In summary, 13 participants indicated whether they believed the orientation of any of four oriented ellipses changed after a working memory delay. Stimuli were displayed on a 23 inch LED monitor with a refresh rate of 60 Hz and a resolution of 1920 x 1080 pixels.</p><p>Stimuli Stimuli were four, light-grey, oriented ellipses on a medium-grey background. Each ellipse could be long or short, to provide respectively higher or lower reliability information regarding the orientation of the ellipses. All ellipses had an area of 1.19 degrees of visual angle (dva). The high-reliability ellipse had an ellipse eccentricity of 0.9, such that the major axis and minor axes were 1.02 and 0.37 dva, respectively. The low-reliability ellipse eccentricity was determined separately for each participant to equate performance.</p><p>On every trial, a stimulus display consisted of four ellipses. The probability of each ellipse being high reliability was 0.5, independent of the reliability of the other ellipses. The location of the first ellipse was drawn from a uniform distribution between polar angles 0 • and 90 • . Each ellipse after that was placed such that all ellipses were 90 • apart on an imaginary annulus that was 7 dva away from fixation. Afterward, the x-and y-location of the ellipses were independently jittered -0.3 to 0.3 dva.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Visual Search</head><p>A full description of the study can be found in the original paper <ref type="bibr" target="#b14">(Calder-Travis &amp; Ma, 2020)</ref>. Data are available at https://doi.org/10.17605/OSF.IO/NERZK. In summary, 14 participants took part in the experiment. Only data from the 13 participants who completed all 4 one-hour sessions of the study is analysed. Participants viewed an LCD monitor with 60Hz refresh rate and 1920 × 1080 resolution at a distance of approximately 60cm.  <ref type="table">Table 1</ref>: Parameters for the tasks with limits for BADS based optimisation. ϵ stands for the smallest number different from 0 in machine precision. In the visual search task there is one log κ parameter for each possible value of the number of items in the display.</p><p>Stimuli Stimuli were comprised of between 2 and 6 Gabor patches located on the circumference of an imaginary circle, and were all 4.99 dva from the imaginary line running from the participant to the centre of the screen. The 6 possible Gabor locations were fixed throughout the experiment. The standard deviation of the Gaussian window for each Gabor was 0.25 dva. On trials in which the target was present, one of the Gabors was oriented at 45 • clockwise from vertical. The target could be at any of the 6 possible Gabor locations with equal probability. The orientation of all the other Gabors (i.e. the distractors) was drawn from one of two distributions that depended on the current block. In "uniform" distractor blocks distractors took any orientation with equal probability. In "concentrated" distractor blocks distractors were more likely to have an orientation similar to that of the target orientation. Every time the block type switched participants were informed, and were provided with examples of the distractors in the upcoming block.</p><p>Trials began with the presentation of a fixation cross for 500ms, followed by the presentation of the stimulus for 100ms. Participants had unlimited time to report "target present" or "target absent" using a key press. Participants received trial-by-trial feedback on the accuracy of their responses.</p><p>Plotting For plotting the data and model fits, we quantile binned the distractor statistics separately for each participant and data series. We computed the mean value in each bin, and the mean of this mean value across participants then determined the x-location of the bin. The y-values were determined by the mean (and SEM) across participants, of the variable plotted on the y-axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Observer Model Details</head><p>Here we derive formulas for the responses of the different observer models to the individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Simple Categorisation Task</head><p>There is a single oriented stimulus. When C = 0, the stimulus is drawn from a Gaussian with mean 0 and standard deviation σ 0 . When C = 1, the stimulus is drawn from a Gaussian with mean 0 and standard deviation σ 1 &gt; σ 0 . We assume Gaussian measurement noise. Stimulus reliability was varied in 6 levels, which means that the variance of the measurement noise σ 2 n varies. For each individual trial the distribution P (x|C) thus follows the following distribution:</p><formula xml:id="formula_8">P (x|C = 0) = N (x; s, σ 2 n )N (s; 0, σ 2 0 )ds = N (x; 0, σ 2 0 + σ 2 n ) P (x|C = 1) = N (x; s, σ 2 n )N (s; 0, σ 2 1 )ds = N (x; 0, σ 2 1 + σ 2 n )</formula><p>Bayesian Model Inserting these distributions into the formulas for the Bayesian model yields:</p><formula xml:id="formula_9">d B = log N (x; 0, σ 2 0 + σ 2 n ) N (x; 0, σ 2 1 + σ 2 n ) = log 2π(σ 2 1 + σ 2 n ) 2π(σ 2 0 + σ 2 n ) − x 2 2(σ 2 0 + σ 2 n ) + x 2 2(σ 2 1 + σ 2 n ) = 1 2 log σ 2 1 + σ 2 n σ 2 0 + σ 2 n − x 2 2 σ 2 1 − σ 2 0 (σ 2 0 + σ 2 n )(σ 2 1 + σ 2 n )</formula><p>Point estimate Model For the point estimate observer we derive the following decision variable:</p><formula xml:id="formula_10">d P = log max s p(x|s, C = 0)p(s|C = 0) max s p(x|s, C = 1)p(s|C = 1) = log max s N (s; x, σ 2 n )N (s; 0, σ 2 0 ) max s N (s; x, σ 2 n )N (s; 0, σ 2 1 ) = log max s N s; σ 2 0 x σ 2 0 +σ 2 n , 1 1 σ 2 0 + 1 σ 2 n N (x; 0, σ 2 0 + σ 2 n ) max s N s; σ 2 1 x σ 2 1 +σ 2 n , 1 1 σ 2 1 + 1 σ 2 n N (x; 0, σ 2 1 + σ 2 n ) = log N µ 1 ; µ 1 , σ 2 0 σ 2 n σ 2 n +σ 2 0 N (x; 0, σ 2 0 + σ 2 n ) N µ 2 ; µ 2 , σ 2 1 σ 2 n σ 2 n +σ 2 1 N (x; 0, σ 2 1 + σ 2 n ) = 1 2 log σ 2 1 σ 2 n σ 2 n +σ 2 1 σ 2 0 σ 2 n σ 2 n +σ 2 0 − x 2 2(σ 2 0 + σ 2 n ) + x 2 2(σ 2 1 + σ 2 n ) + 1 2 log σ 2 1 + σ 2 n σ 2 0 + σ 2 n = 1 2 log σ 2 1 σ 2 0 − x 2 2 σ 2 1 − σ 2 0 (σ 2 0 + σ 2 n )(σ 2 1 + σ 2 n )</formula><p>This has a different constant term than the Bayesian observer, but shows the same dependence on x. Thus, the main difference created is a different distribution of bias for the different noise strengths σ n .</p><p>Optimal Decision Rule Point estimate As the dependence of d on x is the same for the Bayesian observer and the point estimate observer, we can equate their behavioural predictions by shifting the criterion of one of them by an amount equal to:</p><formula xml:id="formula_11">1 2 log σ 2 1 + σ 2 n σ 2 0 + σ 2 n − 1 2 log σ 2 1 σ 2 0</formula><p>As this makes the model equivalent to the Bayes-optimal observer model, this is the optimal setting of the criterion and the optimal decision rule point estimate observer behaves the same way as the Bayesian observer.</p><p>Variational Inference This task is the only one where we can find a sensible factorised approximation to the true posterior using variational inference. To do so, we search for an approximate distribution q(C, s) = q(C)q(s) ≈ p(C, s|x) which minimises the KL divergence KL(p||q) = C p(C, s|x) log p(C,s|x) q(C)q(s) ds. To solve this, we can use a general inference scheme for variational inference to iterate between updating q(C) and q(s). If all other factors are fixed, the best solution for the logarithm of factor q(y) is the expectation of the logarithm of the true posterior distribution, based on the product of all other factors q \y <ref type="bibr" target="#b7">(Bishop, 2006)</ref>:</p><formula xml:id="formula_12">log(q(y)) = ⟨log p⟩ q \y + Const</formula><p>As an update for our factor q(s), this results in:</p><formula xml:id="formula_13">log (q(s)) = ⟨log p(x, C, s)⟩ q(C) + Const 0 = q(C = 0) log[N (x; s, σ 2 n )N (s|0, σ 2 0 )] + q(C = 1) log[N (x; s, σ 2 n )N (s; 0, σ 2 1 )] + Const 0 = −q(C = 0) s 2 2σ 2 0 − q(C = 1) s 2 2σ 2 1 − (s − x) 2 2σ 2 n + Const 0 = − q(C = 0)σ 2 1 σ 2 n + q(C = 1)σ 2 0 σ 2 n + σ 2 0 σ 2 1 s 2 − 2σ 2 0 σ 2 1 xs + σ 2 0 σ 2 1 x 2 2σ 2 0 σ 2 1 σ 2 n + Const 0 Now define σ 2 new = σ 2 0 σ 2 1 q(C=0)σ 2 1 σ 2 n +q(C=1)σ 2 0 σ 2 n +σ 2 0 σ 2 1 log (q(s)) = − s 2 − 2σ 2 new xs + σ 2 new x 2 2σ 2 new σ 2 n + Const 0 = − (s − σ 2 new x) 2 2σ 2 new σ 2 n + Const 1 (x)</formula><p>Thus q(s) is normally distributed:</p><formula xml:id="formula_14">q(s) = N s; σ 2 new x, σ 2 new σ 2 n</formula><p>For q(C), we can use the fact that q(s) is a normal distribution to derive the following update equation based on the mean µ s and standard deviation σ s of the current q(s):</p><formula xml:id="formula_15">log (q(C = 1)) = ⟨log p(x, C = 1, s)⟩ q(s) + Const 0 = N (s; µ s , σ 2 s ) log N (s; x, σ 2 n )N (s; 0, σ 2 0 ) ds + Const 0 = N (s; µ s , σ 2 s ) log N (s; x, σ 2 n ) ds + N (s; µ s , σ 2 s ) log N (s; 0, σ 2 0 ) ds + Const 0</formula><p>The first integral does not depend on C, i.e. it will cancel once we get q(C). Thus:</p><formula xml:id="formula_16">log (q(C = 1)) = N (s; µ s , σ 2 s ) log N (s; 0, σ 2 0 ) ds + Const 1</formula><p>Using a formula for the cross-entropy of Gaussians:</p><formula xml:id="formula_17">log (q(C = 1)) = − 1 2 log(2πσ 2 0 ) − σ 2 s + µ 2 s 2σ 2 0 + Const 1</formula><p>The derivation for q(C = 2) is analogous. Thus, we get the following formula for q(C = 1):</p><formula xml:id="formula_18">q(C = 1) = exp − 1 2 log(2πσ 2 0 ) −</formula><p>To evaluate this formula, we compute 1 σ0 exp −</p><formula xml:id="formula_19">σ 2 s +µ 2 s 2σ 2 0 and 1 σ1 exp − σ 2 s +µ 2 s 2σ 2 1</formula><p>and normalise to get q(C = 1) and q(C = 2). We iterate this scheme for up to 50 iterations or until the change in q(C) becomes smaller than 10 −5 . After convergence we compute the decision variable from q(C) as:</p><formula xml:id="formula_20">d V = log q(C = 1) q(C = 2)</formula><p>Importance sampling As described in the main text this observer approximates the integrals in the equations for the Bayesian observer by sampling from the prior. For this particular situation the formula for the decision variable based on N s samples s 1,i and s 0,i from the priors under the two categories p(s|C = 1) and p(s|C = 0) respectively becomes:</p><formula xml:id="formula_21">d s = log 1 Ns Ns i=1 p(x|s 1,i ) 1 Ns Ns i=1 p(x|s 2,i ) = log Ns i=1 exp −(x − s 1,i ) 2 2σ n − log Ns i=1 exp −(x − s 2,i ) 2 2σ n</formula><p>Here, the normalization constant of the Gaussian cancels in the ratio, as the noise distribution has the same variance in both categories.</p><p>Joint posterior sampling This observer type performs Metropolis-Hastings sampling based on proposal samples from the prior over C and s. To add a new sample C i+1 , s i+1 we thus draw a proposal sampleC,s and accept it with the following probability, otherwise setting</p><formula xml:id="formula_22">C i+1 = C i , s i+1 = s i : p(accept) = min 1, p(s,C|x)p(s i , C i ) p(s i , C i |x)p(s,C) = min 1, p(x|s)p(s,C)p(s i , C i ) p(x|s i )p(s i , C i )p(s,C) = min 1, p(x|s) p(x|s i )</formula><p>This derivation holds for all our experiments. For this particular experiment the likelihood ratio p(x|s) p(x|si) becomes:</p><formula xml:id="formula_23">p(x|s) p(x|s i ) = exp − (x −s) 2 2σ n + (x − s i ) 2 2σ n B.2 Collinearity Judgements</formula><p>The task was to detect whether two lines are co-linear or not. Stimuli were presented at different peripheral locations, which in our models changes the standard deviation of the observations. For convenience, we here set C = 1 to mean separate s, as this is the more flexible category, and express s relative to the nominal eccentricity set to the mean of the s distribution(s).</p><formula xml:id="formula_24">P (s|C = 0) = 1 s=s1=s2 N (s; 0, σ 2 0 ) P (s|C = 1) = N (s 1 ; 0, σ 2 0 )N (s 1 ; 0, σ 2 0 ) P (x i |s i ) = N (x i ; s i , σ 2 n )</formula><p>Bayesian Model The Bayesian model was already derived in the original paper. The posterior probability for the two categories are:</p><formula xml:id="formula_25">P (x)P (C = 1|x) = P (x 1 |s 1 )P (x 2 |s 2 )P (s 1 , s 2 |C = 1)ds 1 ds 2 = N (s 1 ; x 1 , σ 2 n )N (s 1 ; 0, σ 2 0 )ds 1 N (s 2 ; x 2 , σ 2 n )N (s 2 ; 0, σ 2 0 )ds 2 = 1 (σ n σ 0 2π) 2 exp − 1 2 (s 1 − x 1 ) 2 σ 2 n + s 2 1 σ 2 0 ds 1 exp − 1 2 (s 2 − x 2 ) 2 σ 2 n + s 2 2 σ 2 0 ds 2 = 1 2π(σ 2 n + σ 2 0 ) exp − 1 2(σ 2 0 + σ 2 n ) (x 2 1 + x 2 2 ) P (x)P (C = 0|x) = P (x 1 |s)P (x 2 |s)P (s|C = 0)ds = N (x 1 ; s, σ 2 n )N (x 2 ; s, σ 2 n )N (s; 0, σ 2 0 )ds = 1 (2π) 3/2 σ 2 n σ 0 exp − 1 2 (x 1 − s) 2 σ 2 n + (x 2 − s) 2 σ 2 n + s 2 σ 2 0 ds = 1 (2π) 3/2 σ 2 n σ 0 exp   − 1 2 s 2 − 2 σ 2 0 2σ 2 0 +σ 2 n (x 1 + x 2 )s + σ 2 0 2σ 2 0 +σ 2 n (x 2 1 + x 2 2 ) σ 2 0 σ 2 n 2σ 2 0 +σ 2 n   ds = 1 (2π)σ n 2σ 2 0 + σ 2 n exp 1 2σ 2 n σ 2 0 2σ 2 0 + σ 2 n (x 1 + x 2 ) 2 − 1 2σ 2 n (x 2 1 + x 2 2 )</formula><p>From the log-ratio of these two probabilities we get the decision variable d B :</p><formula xml:id="formula_26">d B = log P (C = 1|x) P (C = 0|x) = log σ n 2σ 2 0 + σ 2 n σ 2 n + σ 2 0 − 1 2(σ 2 0 + σ 2 n ) (x 2 1 + x 2 2 ) − 1 2σ 2 n σ 2 0 2σ 2 0 + σ 2 n (x 1 + x 2 ) 2 + 1 2σ 2 n (x 2 1 + x 2 2 )</formula><p>Point estimate model For the point estimate observer we first have to find the MAP estimates forŝ. From standard formulas for the maximum of the product of two normal distributions we get the following estimates for s under C = 1:</p><formula xml:id="formula_27">s 1 = σ 2 0 σ 2 0 + σ 2 n x 1ŝ2 = σ 2 0 σ 2 0 + σ 2 n x 2</formula><p>Under C = 0, s 1 and s 2 have to be equal. Again using standard formulas for the mean of the product of Gaussians, we can calculate an estimate for s as:ŝ</p><formula xml:id="formula_28">=ŝ 1 =ŝ 2 = σ 2 0 2σ 2 0 + σ 2 n (x 1 + x 2 ).</formula><p>Based on these expressions, we can now compute d P :</p><formula xml:id="formula_29">d P = log N (ŝ 1 ; 0, σ 0 )N (ŝ 2 ; 0, σ 0 )N (ŝ 1 ; x 1 , σ n )N (ŝ 2 ; x 2 , σ n ) N (ŝ; 0, σ 0 )N (ŝ; x 1 , σ n )N (ŝ; x 2 , σ n )</formula><p>As usual, we compute the optimal criterion based on optimization over a sample.</p><p>Variational observer In this experiment, the distribution over s under category 0 is a one dimensional subset of the two dimensional distribution under category 1. One consequence of that is that the two cannot be expressed as densities relative to the same base measure. Informally speaking, the distribution under category 0 would have to be infinitely high to compensate for the infinitely small area it covers in the original space. Thus, the KL divergences the variational observer is meant to optimize are not defined and there are no sensible factorised approximations to the posterior.</p><p>Importance sampling As described in the main text this observer approximates the integrals in the equations for the Bayesian observer by sampling from the prior. For this particular situation the formula for the decision variable based on N s samples s c0,i = (s c0,i,1 , s c0,i,2 ) and s c1,i = (s c1,i,1 , s c1,i,2 ) from the priors under the two categories p(s|C = 0) and p(s|C = 1) respectively becomes:</p><formula xml:id="formula_30">d s = log Ns i=1 exp −(x 1 − s c1,i,1 ) 2 − (x 2 − s c1,i,2 ) 2 2σ n − log Ns i=1 exp −(x 1 − s c0,i,1 ) 2 − (x 2 − s c0,i,2 ) 2 2σ n</formula><p>Again, the normalization constant of the Gaussian cancels in the ratio, as the noise distribution has the same variance in both categories.</p><p>Joint posterior sampling The acceptance probability for the Metropolis-Hastings sampler depends only on the likelihood ratio as we showed for the first task above. For this experiment the likelihood ratio for accepting a new samples = (s 1 ,s 2 ) compared to a current sample s = (s 1 , s 2 ) becomes:</p><formula xml:id="formula_31">p(x|s) p(x|s) = exp 1 2σ n −(x 1 −s 1 ) 2 − (x 2 −s 2 ) 2 + (x 1 − s 1 ) 2 + (x 2 − s 2 ) 2</formula><p>Here C does not directly enter the acceptance probability, but changes the distribution we samples from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Shen &amp; Ma</head><p>Participants were presented with four Gabor targets s 1 , s 2 , s 3 , s 4 . Three of these shared the same orientation. The participants were asked to report whether the fourth target at location L was tilted left or right.</p><p>The generative model for this experiment is:</p><formula xml:id="formula_32">P (x, s, C, L) = P (x|s)P (s|C, L)P (C)P (L) P (x i |s i ) = N (x; s, σ 2 n ) P (s \L ) = N (s \L , 0, σ 2 0 ) P (s L |C = 0) = 1 s L &lt;0 N (s; 0, σ 2 0 ) P (s L |C = 1) = 1 s L &gt;0 N (s; 0, σ 2 0 ) P (L) = 1 4 P (C) = 1 2</formula><p>with a single noise variance σ 2 n and variance of the prior σ 2 0 . The notation s \L indicates the common orientation of the distractors, and s L denotes the orientation of the target at location L.</p><p>Bayesian Model The Bayesian model was already described in the original publication. The derivation is:</p><formula xml:id="formula_33">p(x)p(C = 1|x) = p(C = 1) p(s L |C)p(s \L p(L)p(x|L, s D , s T )ds T ds D dL = 1 2N N L=1 p(s L |C)p(s \L )p(x|L, s \L , s L )ds L ds \L = 1 N N L=1 1 s T &gt;0 N (s T ; 0, σ 2 )N (x L ; s T , σ 2 N )ds T N (s D ; 0, σ 2 ) i̸ =L N (x i ; s D , σ 2 N )ds D = 1 N N L=1 1 s T &gt;0 1 Z T N s T ; σ 2 x L σ 2 + σ 2 N ; σ 2 σ 2 N σ 2 + σ 2 N ds T 1 Z D N s D ; σ 2 i̸ =L x i 3σ 2 + σ 2 N ; σ 2 σ 2 N 3σ 2 + σ 2 N ds D = 1 N N L=1 1 Z D 1 Z T 1 s T &gt;0 N s T ; σ 2 x L σ 2 + σ 2 N ; σ 2 σ 2 N σ 2 + σ 2 N ds T = 1 N N L=1 1 Z D 1 Z T Φ σx L (σ 2 + σ 2 N )σ 2 N</formula><p>which is consistent with the paper with</p><formula xml:id="formula_34">Z T = N (s T ; σ 2 x L σ 2 +σ 2 N ; σ 2 σ 2 N σ 2 +σ 2 N ) N (s T ; 0, σ 2 )N (s T ; x L , σ 2 N ) = 2πσ 2 σ 2 N σ 2 σ 2 N σ 2 +σ 2 N exp   − 1 2 (− σ 2 x L σ 2 +σ 2 N ) 2 σ 2 σ 2 N σ 2 +σ 2 N   exp 1 2 (−x L ) 2 σ 2 N = 2π(σ 2 + σ 2 N ) exp 1 2 (x L ) 2 (σ 2 + σ 2</formula><p>N ) and analogous for Z D :</p><formula xml:id="formula_35">Z D = N s D ; σ 2 i̸ =L x L 3σ 2 +σ 2 N ; σ 2 σ 2 N 3σ 2 +σ 2 N N (s D ; 0, σ 2 ) i̸ =L N (s D ; x i , σ 2 N ) = (2πσ 2 N ) 3 σ 2 σ 2 σ 2 N 3σ 2 +σ 2 N exp   − 1 2 (− σ 2 i̸ =L xi 3σ 2 +σ 2 N ) 2 σ 2 σ 2 N 3σ 2 +σ 2 N    exp 1 2 i̸ =L (−x i ) 2 σ 2 N = (2π) 3 σ 2 N σ 2 N (3σ 2 + σ 2 N ) exp ( 1 3 i̸ =L x i ) 2 2(σ 2 + σ 2 N 3 ) + i̸ =L (x i −x \L ) 2 2σ 2 N</formula><p>Dropping all parts which do not depend on x we obtain:</p><formula xml:id="formula_36">p(C = 1|x) = 1 p(x) N L=1 exp − 1 2 σ 2 ( i̸ =L x i ) 2 (3σ 2 + σ 2 N )σ 2 N + 1 2 i̸ =L (x i ) 2 σ 2 N − 1 2 (x L ) 2 (σ 2 + σ 2 N ) Φ σx L (σ 2 + σ 2 N )σ 2 N = 1 p(x) N L=1 exp − ( 1 3 i̸ =L x i ) 2 2(σ 2 + σ 2 N 3 ) − i̸ =L (x i −x \L ) 2 2σ 2 N − (x L ) 2 2(σ 2 + σ 2 N ) Φ σx L (σ 2 + σ 2 N )σ 2 N ,</formula><p>which is equivalent to the original paper. In the original paper, the authors the authors modelled a Bayesian observer who reports the category for which P (C|x) was larger. Here, we use these values to calculate a posterior ratio and map it through the same decision noise and lapse rate machinery as all other observer models.</p><p>Point estimate model In this case, we cannot easily estimate s independent of L, because L determines which orientations have to be the same. We can however estimate the stimulus values for each category and base our decision on the likelihood of this interpretation: We thus evaluate each combination of C and L as a category:</p><formula xml:id="formula_37">P (s T , s D |L, C, x) ∝ p(s T |C)p(s D )p(x|L, s D , s T ) = p(s T |C)p(s D ) i p(x i |L, s D , s T )</formula><p>As all distributions here are normal we can calculate the point estimates of s analytically:</p><formula xml:id="formula_38">s T = 0 if C ̸ = sign x L σ 2 σ 2 +σ 2 N x L if C = sign x L (9) s D = 3σ 2 σ 2 N + 3σ 2   1 3 i̸ =L x i  <label>(10)</label></formula><p>With this point estimate we can calculate the maximum posterior density for each L, C combination as:</p><formula xml:id="formula_39">p(L, C) = p(ŝ D )p(ŝ T )p(x L |ŝ T ) i̸ =L p(x i |ŝ D ) = N (ŝ D |0, σ)N (ŝ T |0, σ)N (x L |ŝ T , σ N ) i̸ =L N (x i |ŝ D , σ N ) = 1 √ 2π 6 σ 2 σ 4 N exp   −ŝ 2 D 2σ 2 −ŝ 2 T 2σ 2 − (x L −ŝ T ) 2 2σ 2 N − i̸ =L (x i −ŝ D ) 2 2σ 2 N  </formula><p>Taking the logarithm and dropping parts that do not depend on x:</p><formula xml:id="formula_40">logp(L, C) = − 1 2  ŝ 2 D σ 2 +ŝ 2 T σ 2 + (x L −ŝ T ) 2 σ 2 N + i̸ =L (x i −ŝ D ) 2 σ 2 N   Inserting the formulas forŝ D andŝ T for C = sign x L : logp(L, C) = − 1 2    9σ 2 (σ 2 N + 3σ 2 ) 2   1 3 i̸ =L x i   2 + σ 2 (σ 2 N + σ 2 ) 2 x 2 L + σ 2 N (σ 2 N + σ 2 ) 2 x 2 L    − 1 2 i̸ =L x i − 3σ 2 σ 2 N +3σ 2 1 3 j̸ =L x j 2 σ 2 N</formula><p>As a result we get 8 numbers for the 4 locations times 2 directions. To combine these the most compatible version for the inference scheme is to take the maximum for each C effectively treating L as another nuisance parameter to maximise over.</p><p>Using the maximum over locations we get the following equation for d P :</p><formula xml:id="formula_41">d P = max s,L log [p(x|s, L, C = 1)p(s|L, C = 1)] − max s,L log [p(x|s, C = 0, L)p(s|L, C = 0)] = max L logp(L, C = 1) − max L logp(L, C = 0)</formula><p>Optimal-Criterion Point Estimate Observer This case is symmetric in the two categories and has equal complexity of the compared alternatives. Thus, in this case the optimal bias is 0! Importance sampling For importance sampling we again sample from the priors to estimate the integrals required for solving the Bayesian observer. In this case this implies first sampling a location for the outlier and then sampling the 4 stimulus orientations based on the location and the category. The formula for the decision variable based on N s samples s c0,i = (s c0,i,1 , s c0,i,2 , s c0,i,3 , s c0,i,4 ) and s c1,i = (s c1,i,1 , s c1,i,2 ), s c1,i,3 ), s c1,i,4 ) from the priors under the two categories p(s|C = 0) and p(s|C = 1) respectively becomes:</p><formula xml:id="formula_42">d s = log Ns i=1 exp − 4 j=1 (x j − s c1,i,j ) 2 2σ n − log Ns i=1 exp − 4 j=1 (x j − s c0,i,j ) 2 2σ n</formula><p>Again, the normalization constant of the Gaussian cancels in the ratio, as the noise distribution has the same variance in both categories.</p><p>Joint posterior sampling The acceptance probability for the Metropolis-Hastings sampler depends only on the likelihood ratio as we showed for the first task above. For this experiment the likelihood ratio for accepting a new samples = (s 1 ,s 2 ,s 3 ,s 4 ) compared to a current sample s = (s 1 , s 2 , s 3 , s 4 ) becomes:</p><formula xml:id="formula_43">p(x|s) p(x|s) = exp   1 2σ n   − 4 j=1 (x j −s j ) 2 + 4 j=1 (x j − s j ) 2    </formula><p>Both C and L do not directly enter the acceptance probability, but change the distributions we sample from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Change Detection</head><p>The task was to detect whether or not one of four ellipses changed in orientation. Using VM() to denote the von Mises distribution, the generative model for this experiment is:</p><formula xml:id="formula_44">p(C) = 1 2 p(∆) = 1 2π p(L) = 1 N p(∆ ∆ ∆|∆, C, L) = δ(∆ ∆ ∆ − C1 1 1 L (∆)) p(ξ ξ ξ) = 1 2π N p(ϕ ϕ ϕ|ξ ξ ξ, ∆ ∆ ∆) = δ(ϕ ϕ ϕ − ξ ξ ξ − ∆ ∆ ∆) p(x x x|ξ ξ ξ) = N i=1 VM(x i ; ξ i , κ x,i ) p(y y y|ϕ ϕ ϕ) = N i=1 VM(y i ; ϕ i , κ y,i ),</formula><p>where ∆ ∈ [0, 2π] is the size of the change, L is the location of the change, ∆ ∆ ∆ is the vector of changes, ξ ξ ξ is the vector of true orientations in the first display, each in [0, 2π], ϕ ϕ ϕ is the vector of true orientations in the second display and x x x, y y y are the observations of the orientations in the first and second display respectively.</p><p>Bayesian Model The Bayesian model was also derived in the original paper. In order to compute this value, we must marginalize over the unknown variables ξ, ϕ, ∆, and ∆.</p><formula xml:id="formula_45">p(x, y|C)p(C) = p(x x x|ξ ξ ξ)p (ξ) p(y y y|ϕ ϕ ϕ)p(ϕ|∆)p(∆|C, ∆)p(∆)p(C)dξ ξ ξdϕ ϕ ϕd∆ ∆ ∆d∆ = p(C) 1 2π N +1 p(x x x|ξ ξ ξ)p(y y y|ϕ ϕ ϕ)δ(ϕ − (ξ + C∆)) 1 N N i=1 δ(∆ i − C1 i ) dξ ξ ξdϕ ϕ ϕd∆ ∆ ∆d∆ = p(C) 1 2π N +1 1 N N i=1 p(x x x|ξ ξ ξ)p(y y y|ξ ξ ξ + C∆1 i )dξ ξ ξd∆</formula><p>Then, plugging this equation into the likelihood ratio:</p><formula xml:id="formula_46">p(x, y|C = 1)p(C = 1) p(x, y|C = 0)p(C = 0) = p(C = 1) p(C = 0) N i=1 p(x x x|ξ ξ ξ)p(y y y|ξ ξ ξ + C∆1 i )dξ ξ ξd∆ N i=1 p(x x x|ξ ξ ξ)p(y y y|ξ ξ ξ)dξ ξ ξd∆ = p(C = 1) p(C = 0) N i=1 p(x x x|ξ ξ ξ)p(y y y|ξ ξ ξ + C∆1 i )dξ ξ ξd∆ 2πN p(x x x|ξ ξ ξ)p(y y y|ξ ξ ξ)dξ ξ ξ</formula><p>Because the N items are conditionally independent, we can break the expression up into a product of each item and further simplify.</p><formula xml:id="formula_47">d B = p(C = 1) p(C = 0) N i=1 j̸ =i p(x j |ξ j )p(y j |ξ j )dξ j p(x i |ξ i )p(y i |ξ i + ∆)dξ i d∆ 2πN N i=1 p(x i |ξ i )p(y i |ξ i )dξ i = p(C = 1) p(C = 0) N i=1 j̸ =i p(x j |ξ j )p(y j |ξ j )dξ j p(x i |ξ i )p(y i |ξ i + ∆)dξ i d∆ 2πN j̸ =i p(x j |ξ j )p(y j |ξ j )dξ j p(x i |ξ i )p(y i |ξ i )dξ i = p(C = 1) p(C = 0) N i=1 p(x i |ξ i )p(y i |ξ i + ∆)dξ i d∆ 2πN p(x i |ξ i )p(y i |ξ i )dξ i = p(C = 1) p(C = 0) N i=1 VM(x i ; ξ i , κ x,i )VM(y i ; ξ i + ∆, κ y,i )dξ i d∆ 2πN VM(x i ; ξ i , κ i )VM(y i ; ξ i , κ i )dξ i = p(C = 1) p(C = 0) N i=1 1 2πN VM(x i ; ξ i , κ x,i )VM(y i ; ξ i , κ y,i )dξ i = p(C = 1) p(C = 0) N i=1 1 2πN I0(κ) 2πI0(κx,i)I0(κy,i) VM (ξ i ; µ, κ) dξ i ,</formula><p>where</p><formula xml:id="formula_48">µ = x i + arctan(sin(y i − x i ), (κ x,i /κ yi ) + cos(y i − x i )) and κ = κ 2 x,i + κ 2 y,i + 2κ x,i κ y,i cos(x i − y i ). = p(C = 1) p(C = 0) N i=1 I 0 (κ x,i )I 0 (κ y,i ) N I 0 (κ) VM (ξ i ; µ, κ) dξ i = p(C = 1) p(C = 0) 1 N N i=1 I 0 (κ x,i )I 0 (κ y,i ) I 0 (κ) ,</formula><p>This produces the final expression of the decision variable,</p><formula xml:id="formula_49">d B = p(C = 1) p(C = 0) 1 N N i=1 d i .</formula><p>where</p><formula xml:id="formula_50">d i = I 0 (κ x,i )I 0 (κ y,i ) I 0 κ 2 x,i + κ 2 y,i + 2κ x,i κ y,i cos(x i − y i ) .</formula><p>Point Estimate Model</p><formula xml:id="formula_51">d P = log max ξ,L,∆ [p(x, y|ξ, L, ∆, C = 1)p(ξ)p(L)p(∆)] max ξ [p(x, y|ξ, C = 0)p(ξ)p(L)p(∆)]</formula><p>Note that we do not need to additionally maximise over variables ∆ and ϕ, because they are deterministically related to the variables we are maximising, namely ξ, ∆, and L. Expanding the denominator first,</p><formula xml:id="formula_52">denominator = max ξ [p(x, y|ξ, C = 0)p(ξ)p(L)p(∆)] = max ξ [p(y|ϕ = ξ)p(x|ξ)p(ξ)p(L)p(∆)] = max ξ1,...,ξ N N i=1 VM(y i ; ξ i , κ y,i ) N i=1 VM(x i ; ξ i , κ x,i ) 1 2π N 1 2π 1 N = 1 N 1 2π N +1 N i=1 max ξi [VM(y i ; ξ i , κ y,i )VM(x i ; ξ i , κ x,i )] = 1 N 1 2π N +1 N i=1 max ξi I 0 (κ ξ,i ) 2πI 0 (κ x,i )I 0 (κ y,i ) VM(ξ i ; µ ξ,i , κ ξ,i ) ,</formula><p>where</p><formula xml:id="formula_53">µ ξ,i = x i + arctan(sin(y i − x i ), (κ x,i /κ y,i ) + cos(y i − x i )) and κ ξ,i = κ 2 x,i + κ 2 y,i + 2κ x,i κ y,i cos(y i − x i ) (Murray &amp; Morgenstern, 2010).</formula><p>We chooseξ i = µ ξ,i , to maximise:</p><formula xml:id="formula_54">= 1 N 1 2π N +1 N i=1 I 0 (κ ξ,i ) 2πI 0 (κ x,i )I 0 (κ y,i ) e κ ξ,i 2πI 0 (κ ξ,i ) = 1 N 1 2π 3N +1 N i=1 e κ ξ,i I 0 (κ x,i )I 0 (κ y,i )</formula><p>Expanding the numerator next,</p><formula xml:id="formula_55">numerator = max ξ,L,∆ [p(x, y|ξ, L, ∆, C = 1)p(ξ)p(L)p(∆)] = max ξ,L,∆ [p(y|ξ, ∆, L, C = 1)p(x|ξ)p(ξ)p(∆)p(L)] = max ξ1,...,ξ N ,L,∆ N i=1 VM(y i ; ξ i + I L ∆, κ y,i ) N i=1 VM(x i ; ξ i , κ x,i ) 1 2π N 1 2π 1 N where I L is 1 if i = L and 0 otherwise = 1 N 1 2π N +1 N i=1 max ξi,L,∆ VM(y i ; ξ i + I L ∆, κ y,i )VM(x i ; ξ i , κ x,i )</formula><p>For the L th location, this expression is maximized if we chooseξ L = x L , and</p><formula xml:id="formula_56">∆ = y L − x L .</formula><p>At the remaining N − 1 locations the stimulus did not change, and thus the same derivation applies as for items in the denominator. Further simplifying,</p><formula xml:id="formula_57">= max L   1 N 1 2π N +1 VM(y L ; y L , κ y,L )VM(x L ; x L , κ x,L ) i̸ =L e κ ξ,i 4π 2 I 0 (κ x,i )I 0 (κ y,i )   = 1 N 1 2π 3N +1 max L   e κ x,L e κ y,L I 0 (κ x,L )I 0 (κ y,L ) i̸ =L e κ ξ,i I 0 (κ x,i )I 0 (κ y,i )  </formula><p>Combining and simplifying the numerator and denominator:</p><formula xml:id="formula_58">exp(d P ) = max L   1 N 1 2π 3N +1 e κ x,L e κ y,L I 0 (κ x,L )I 0 (κ y,L ) i̸ =L e κ ξ,i I 0 (κ x,i )I 0 (κ y,i )   1 N 1 2π 3N +1 N i=1 e κ ξ,i I0(κx,i)I0(κy,i) = max L e κ x,L e κ y,L I0(κ x,L )I0(κ y,L )</formula><p>i̸ =L</p><formula xml:id="formula_59">e κ ξ,i I0(κx,i)I0(κy,i) N i=1 e κ ξ,i I0(κx,i)I0(κy,i) = max L e κ x,L e κ y,L I0(κ x,L )I0(κ y,L ) i̸ =L e κ ξ,i I0(κx,i)I0(κy,i) e κ ξ,L I0(κ x,L )I0(κ y,L ) i̸ =L e κ ξ,i I0(κx,i)I0(κy,i) = max L e κ x,L e κ y,L e κ ξ,L</formula><p>Keeping p(L)p(∆) only for C = 1 would yield an additional factor 1 2πN . We assume the model observer calculates this expression for each possible value of L, and chooses the one which provides the largest value. Writing out d P explicitly yields:</p><formula xml:id="formula_60">d P = max L [κ x,L + κ y,L − κ ξ,L ] = max L κ x,L + κ y,L − κ 2 x,L + κ 2 y,L + 2κ x,L κ y,L cos(y L − x L )</formula><p>which has a minimum of 0 at y L = x L and a maximum ≤</p><formula xml:id="formula_61">κ x,L + κ y,L for x L − y L = π.</formula><p>Optimal-Criterion Point Estimate Observer To compute the Optimal-Criterion Point Estimate Observer, we need to find the offsets, c(κ x , κ y ), to maximise task performance of the decision boundary</p><formula xml:id="formula_62">d H = d P + c(κ x , κ y )</formula><p>. This is difficult to do analytically, so we compute c(κ x , κ y ) numerically.</p><p>Variational observer The variational observer is not defined in this task. A variational observer would not be defined in a one-item change detection task for the same reasons laid out in the collinearity judgement task (B.2); it is even less clear how this observer would be defined in a four-item change detection task.</p><p>Importance sampling As described in the main text and earlier derivations, this observer approximates the integrals in the equations for the Bayesian observer by sampling from the prior.</p><formula xml:id="formula_63">p(x, y|C) = p(x, y|ξ, ϕ)p(ξ, ϕ|C)dξdϕ = p(x|ξ)p(y|ϕ)p(ξ, ϕ|C)dξdϕ = p(x|ξ)p(y|ξ + 1 C ∆)p(ξ)p(∆)dξd∆ = 1 p(x) p(ξ|x)p(y|ξ + 1 C ∆)p(∆)dξd∆ ≈ 1 N s p(x) Ns i=1 p(y|ξ i + 1 C ∆ i ),</formula><p>where we used Bayes rule p(ξ|x) = p(x|ξ)p <ref type="bibr">(ξ) p(x)</ref> for the third step. Now, we take the ratio of the two hypotheses:</p><formula xml:id="formula_64">d s = p(x, y|C = 1) p(x, y|C = 0) = Ns i=1 p(y|ξ i + ∆ i ) Ns i=1 p(y|ξ i )</formula><p>Joint posterior sampling This observer uses the same Metropolis-Hastings sampling method explained in text in earlier derivations, accepting proposal samples from the prior over C, ξ, and ϕ with the following probability:</p><formula xml:id="formula_65">min 1, p(x|ξ)p(y|φ) p(x|ξ)p(y|ϕ)</formula><p>We calculate p(x|ξ)p(y|ϕ):</p><formula xml:id="formula_66">p(x|ξ)p(y|ϕ) = N j=1 VM(x j ; ξ j , κ j )VM(y j ; ϕ j , κ y,j ) = N j=1 VM(x j ; ξ j , κ j )VM(y j ; ξ j + 1 j ∆, κ y,j ) ∝ N j=1 VM(ξ j ; x j , κ j )VM(y j ; ξ j + 1 j ∆, κ y,j ),</formula><p>where 1 j = 1 in location of change and 0 otherwise. We can directly sample from these distributions, to calculate the acceptance probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Visual Search</head><p>In this task, participants are presented with N items, each possessing an orientation s i . The task is to determine whether one of the items has been drawn from the "target" distribution (this case is denoted C = 1), with the remainder of items drawn from a "distractor" distribution, or whether the items are all drawn from the distractor distribution (C = 0). We use L = i to denote that item location i is designated as the "target" location. If item location i is the designated target location and C = 1, then the item at location i is drawn from the target distribution. If the item location i is not the designated target location, or if C = 0, then the item at location i is drawn from the distractor distribution.</p><p>A subtlety here is that we could frame the relationship between C and L in two different ways. One option is that we could conceptualise L as being independent of C. In this case</p><formula xml:id="formula_67">p(L = i|C) = 1 N</formula><p>where N is the number of items in a trial. When C = 0, a location is still designated as the target location, but this designation has no effect. However, we could also conceptualise L as dependent on C so that, </p><p>where we have used i = −1 to indicate that none of the possible item locations have been selected as the target location. While this distinction may appear inconsequential from a normative perspective, it affects the point estimate observer, as we will see below.</p><p>For the item at location i, the model observer receives a noisy measurement x i of the true orientation s i . By assumption or design,</p><formula xml:id="formula_69">p(x i |s i ) = VM(x i ; s i , κ) (12) p(s i |L ̸ = i, C = 1) = VM(s i ; 0, κ s ) (13) p(s i |C = 0) = VM(s i ; 0, κ s ) (14) p(s i |L = i, C = 1) = δ(s i ) (15) p(C) = 1 2 .<label>(16)</label></formula><p>δ() is the Dirac delta function. VM() is the von Mises distribution, and its two parameters (following the semi-colon) are the mean and concentration parameter. s i codes the orientation in a very specific way: It represents twice the difference between the orientation of an item, and the target orientation, in radians. This ensures all Gabor orientations are between −90 • and 90 • , avoiding any issues with the fact that a Gabor rotated 180 • is identical to the original Gabor. We assume the measurement x i is conditionally independent of the other measurements, given s i . This setup almost exactly matches the one used in the original paper <ref type="bibr" target="#b14">(Calder-Travis &amp; Ma, 2020)</ref>. We will use x and s without subscripts to denote vectors containing every x i and every s i , respectively.</p><p>An important detail (both here and in the original paper) is that we do not assume that κ, which is related to the precision of observers' noisy stimulus measurements, is independent of the number of items in the display. Instead, we allow the possibility that more items in a display may lead to noisier measurements of those items. κ therefore corresponds to one of four values</p><formula xml:id="formula_70">κ N =2 , κ N =3 , κ N =4 , or κ N =6</formula><p>, depending on the number of items in a trial, N .</p><p>Bayesian model The derivations in this case are similar to those in the original paper <ref type="bibr" target="#b14">(Calder-Travis &amp; Ma, 2020)</ref>. Note that L is defined differently here, for consistency with the other studies presented in the present paper (see description above). The decision variable used by the Bayesian observer is given by (2),</p><formula xml:id="formula_71">d B = log P (C = 1|x) P (C = 0|x)<label>(17)</label></formula><p>= log P (x|C = 1) P (x|C = 0) + log P (C = 1) P (C = 0)</p><p>= log P (x|C = 1) P (x|C = 0) .</p><p>We see d B is just the log-likelihood ratio. In the case we consider, the log-likelihood ratio can be written in terms of "local log-likelihood ratios" <ref type="bibr" target="#b52">(Ma, Navalpakkam, Beck, van den Berg, &amp; Pouget, 2011;</ref><ref type="bibr" target="#b66">Palmer, Verghese, &amp; Pavel, 2000)</ref>,</p><formula xml:id="formula_74">d B = log 1 N N i=1 e di .<label>(20)</label></formula><p>Each local log-likelihood ratio is given by,</p><formula xml:id="formula_75">d i = log p(x i |L = i, C = 1) p(x i |C = 0) = log p(x i |s i )p(s i |L = i, C = 1)ds p(x i |s i )p(s i |C = 0)ds = log VM(x i ; 0, κ) VM(x i ; s i , κ)VM(s i ; 0, κ s )ds ,</formula><p>where we have used the properties of the generative model set out above.</p><p>Using an expression given by <ref type="bibr" target="#b60">Murray and Morgenstern (2010)</ref> for the product of von Mises distributions, in Calder-Travis and Ma (2020) we noted,</p><formula xml:id="formula_76">d i = κ cos(x i ) + log I 0 (κ s ) I 0 κ 2 + κ s 2 + 2κκ s cos (x i ) ,</formula><p>where I 0 are modified Bessel functions of the first kind and order zero. We can then find the Bayesian observer's decision variable by using these local log-likelihoods in <ref type="bibr">(20)</ref>.</p><p>Point estimate observer model The point estimate observer uses the decision variable given by (3). In our case, in addition to s we also have the location variable L that the Bayesian observer marginalises out, and which the point estimate observer will maximise over. This gives us, for the decision variable, </p><formula xml:id="formula_77">d</formula><p>We will see that the distinction noted above, between treating L as independent of C, versus treating L as dependent on C, has an effect here. First consider the case in which we conceptualise L as independent of C. In this conceptualisation of the problem, p(L = i|C) = 1/N for both C = 1 and C = 0 and for all L = i. Hence, the decision variable in this case is, </p><formula xml:id="formula_79">d P =</formula><p>We refer to this model as the "point estimate Ind. L model".</p><p>Consider now the second possibility in which we treat L as dependent on C, and equal to a constant value when C = 0. In this case, (21) becomes, </p><formula xml:id="formula_81">d P =</formula><p>We refer to this model as the "point estimate Dep. L model". This is the model that we report as the "point estimate" model in the main text (in the sections for this visual search task).</p><p>We see that the point estimate observer uses a very similar decision variable, whichever formalisation of the generative model they assume. Comparing (22) and (23) we see that the two decision variables differ only in that one features 1/N in one of the maximisations. Regardless of the point estimate variant used, we need to find expressions for the following two distributions:</p><p>p(x, s|L, C = 1) = p(x|s)p(s|L, C = 1) p(x, s|C = 0) = p(x|s)p(s|C = 0) .</p><p>First consider p(x, s|C = 0). Using the assumption that the noise corrupting the measurement of s i is independent of the noise corrupting s j , and that the item orientations are independent of each other, conditional on none of them being the target, we can write,</p><formula xml:id="formula_83">p(x, s|C = 0) = i p(x i |s i )p(s i |C = 0) (24) = i VM(x i ; s i , κ)VM(s i ; 0, κ s ) (25) = i VM(s i ; x i , κ)VM(s i ; 0, κ s )<label>(26)</label></formula><formula xml:id="formula_84">= i I 0 (κ d (x i )) 2πI 0 (κ)I 0 (κ s ) VM(s i ; µ d (x i ), κ d (x i ))<label>(27)</label></formula><formula xml:id="formula_85">= i ρ(x i )VM(s i ; µ d (x i ), κ d (x i )) .<label>(28)</label></formula><p>We have again used the result in <ref type="bibr" target="#b60">Murray and Morgenstern (2010)</ref> for the multiplication of two von Mises distributions such that,</p><formula xml:id="formula_86">µ d (x i ) = x i + arctan(− sin(x i ), κ κ s + cos(x i )) κ d (x i ) = κ 2 + κ 2 s + 2κκ s cos(x i ) ,</formula><p>and we have used the following abbreviation,</p><formula xml:id="formula_87">ρ(x i ) = I 0 (κ d (x i )) 2πI 0 (κ)I 0 (κ s )</formula><p>.</p><p>Note the "(x i )" in front of µ d , κ d and ρ is used to indicate that these are three functions of x i . Looking at the result in (28), we see that the s i are decoupled in the sense that they all appear in separate factors that are multiplied together. Therefore, we can find the value of s that maximises B by maximising each of these factors individually. Each factor in the product is maximal when, </p><formula xml:id="formula_88">s i = µ d (x i ) ,<label>and</label></formula><formula xml:id="formula_89">= VM(x i ; s i , κ)δ(s i ) j̸ =i VM(x j ; s j , κ)VM(s j ; 0, κ s ) = VM(x i ; s i , κ)δ(s i ) j̸ =i ρ(x j )VM(s j ; µ d (x j ), κ d (x j )) .</formula><p>We see that the different s k again appear in separate factors that are multiplied together. Hence, we can again maximise this expression over s by maximising over each s k individually. This gives, s i = 0 where L = i, and</p><formula xml:id="formula_90">s j = µ d (x j )</formula><p>where L ̸ = j. Therefore, This is an expression for the maximal value of p(x, s|L = i, C = 1), assuming a specific target location (L = i).</p><p>Returning to (22) and (23), we also have a maximisation over L. Using our expressions for p(x, s|L = i, C = 1) and p(x, s|C = 0), we have in (23), i.e. for the point estimate Dep. L model,</p><formula xml:id="formula_91">d P = log 1 N max i VM(x i ; 0, κ) j̸ =i ρ(x j )VM(0; 0, κ d (x j )) j ρ(x j )VM(0; 0, κ d (x j )) (29) = log 1 N max i VM(x i ; 0, κ) ρ(x i )VM(0; 0, κ d (x i )) j ρ(x j )VM(0; 0, κ d (x j )) j ρ(x j )VM(0; 0, κ d (x j )) (30) = max i log 2πI 0 (κ s )e κ cos(xi)−κ d (xi) − log N .<label>(31)</label></formula><p>The result for the decision variable described by (22), and used in the point estimate Ind. L model, is almost the same and only differs in not having a factor of 1/N in the argument of the logarithm, which only results in the removal of the log N shift.</p><formula xml:id="formula_92">d P = max i log 2πI 0 (κ s )e κ cos(xi)−κ d (xi) .<label>(32)</label></formula><p>As mentioned, for the result reported in the main text we exclusively used the Dep. L model in which the model observer treats the location variable as dependent on the category. This model is arguably more theoretically sound: In the alternative Ind. L model, where the model observer assumes that the location variable and category variable are independent, the model observer maximises over target location when evaluating the merit of the hypothesis that no target is present. Nevertheless, we fitted both models and report the model comparison below.</p><p>Optimal-Criterion Point Estimate Observer On each trial there will be some value of κ (which varies with the number of items in the display), and some value of κ s (which varies depending on the distribution from which distractors are drawn; see Appendix A.5). For each value of κ and κ s we evaluated, via simulation, the optimal offset to apply to the standard point estimate observer's decision variable, as described in the section "Methods".</p><p>Variational inference Using variational inference to find an approximate posterior distribution q(C, s, L) = q(C)q(s)q(L) that factorises over C, s, and L does not work in this case, because of the nature of the joint probability distribution over these variables. In the task considered here if C = 1 and L = i then s i = 0, and no other values for s i are possible. For the reasons discussed in the main text (see "Theoretical analysis"), the approximate posterior must therefore assign zero probability to q(C = 1, L = i, s i ̸ = 0) = q(C = 1)q(L = i)q(s i ̸ = 0). This is only satisfied if q(C = 1) = 0, q(L = i) = 0 or q(s i ̸ = 0) = 0. This line of reasoning applies to all item locations i. Hence, either q(C = 1) = 0, or one of q(L = i) = 0 and q(s i ̸ = 0) = 0 is satisfied at each location i. Both of these options lead us to an implausible approximation of the posterior distribution.</p><p>Importance sampling The importance sampling observer approximates the ratio used by the Bayesian observer in (19) by computing approximate integrals as follows,</p><formula xml:id="formula_93">d = log p(x|C = 1) p(x|C = 0)<label>(33)</label></formula><p>= log p(x|s)p(s|C = 1)ds p(x|s)p(s|C = 0)ds (34)</p><formula xml:id="formula_94">≈ log 1 Ns Ns i=1 p(x|s (i,C=1) ) 1 Ns Ns i=1 p(x|s (i,C=0) ) ,<label>(35)</label></formula><p>where N s is the number of samples that this observer uses, and s <ref type="bibr">(i,C=Ψ)</ref> denotes the ith sample from the distribution p(s|C = Ψ). p(s|C = 0) can be computed from (14), using that the stimuli at each location, s i , are conditionally independent of the other stimuli. To find p(s|C = 1) we have to additionally take the location variable, L, into account, p(s|C = 1) = N l=1 p(s|L = l, C = 1)p(L = l|C = 1) , and then we can use (11), (13), (15). As before, N is the number of items in a trial.</p><p>(35) can be rewritten in a way that is easier to evaluate. To do so, we will first find an expression for log p ) .</p><p>Joint posterior sampling The joint posterior sampling observer uses the decision variable (5), and draws samples from the prior over C and s. For the visual search experiment we also have the location variable, L, and so the observer draws samples from the prior over C, s and L, p(s, L, C) = p(s|L, C)p(L|C)p(C) .</p><p>Following Metropolis-Hastings sampling, if we draw a proposal sampleC,L,s, the probability of accepting it and transitioning away from our current state, C (i) , L (i) , s (i) (superscript in brackets denotes sample number) is Von Mises approximate sampling To make repeated sampling from the von Mises distribution feasible -something required to implement the above visual search observer models -we employed various approximations to sampling from this distribution.</p><formula xml:id="formula_96">p(accept) = min 1, p(s,L,C|x)p(s (i) , L (i) , C (i) ) p(s (i) , L (i) , C (i) |x)p(</formula><p>Usually we used an approach inspired by sampling-importance-resampling, as described in <ref type="bibr" target="#b7">Bishop (2006)</ref>. At evenly spaced angles (6284 angles), we evaluated the von Mises probability density function. We then sampled from the evenly spaced angles, using the values of the probability density function as weights, to determine the probability each angle was sampled.</p><p>We used other approaches in specific cases. For mean µ = 0, and concentration parameter κ = 1.5, we sampled with replacement from a predrawn pool of 50000 values, that were themselves sampled from the von Mises. For κ = 0, we did not need to use an approximation, and simply sampled from the uniform distribution. For the computation of the optimal offset used by the optimal-criterion point estimate observer, we sampled directly from the von Mises without approximation.</p><p>Model comparison with additional point estimate observer Here we compare the models from the main text with the additional point estimate model, point estimate Ind. L (see above). The point estimate observer from the main text is here referred to as the point estimate Dep. L model. We fitted the models in the same way as in the main text (except this time only repeating the fitting from 20 different starting positions for each model and participant). As before, we can evaluate the models using the maximum log-likelihood obtained, because all models have the same number of parameters ( <ref type="figure" target="#fig_13">Figure 9</ref>). The point estimate Ind. L model appeared to perform slightly better than the point estimate Dep. L model. However, the mean maximum log-likelihood for the point estimate Ind. L model was lower than the mean for both the Bayesian and optimal point estimate models, leading to no change in the ordering of the models compared to the main text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Task and model fits for the visual search task<ref type="bibr" target="#b14">(Calder-Travis &amp; Ma, 2020)</ref>. A: Illustration of the task: Participants reported the presence or absence of a target Gabor patch oriented at 45 • clockwise from vertical, in a briefly presented display. B: Maximum log-likelihood found for each model, compared to the maximum log-likelihood of the Bayesian model. Dots represent the result for individual participant fits, and bars represent the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>d B = p(C = 1|x x x, y y y) p(C = 0|x x x, y y y) = p(x x x, y y y|C = 1)p(C = 1) p(x x x, y y y|C = 0)p(C = 0) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>C = 0 and i = −1 0 if C = 0 and i ̸ = −1 1 N if C = 1 and 1 ≤ i ≥ N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>x|s)P (s|L, C = 0)p(L|C = 0)] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>i )VM(0; 0, κ d (x i )) . Now consider p(x, s|L, C = 1) when L = i, p(x, s|L = i, C = 1) = p(x|s)p(s|L = i, C = 1) = p(x i |s i )p(s i |L = i, C = 1) j̸ =i p(x j |s j )p(s j |L ̸ = j, C = 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>max s p(x, s|L = i, C = 1) = VM(x i ; 0, κ) j̸ =i ρ(x j )VM(0; 0, κ d (x j )) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>cos (x j − s j )) 2πI 0 (κ) (38) = −N log (2πI 0 (κ)) + N j=1 κ cos (x j − s j ) .(39)With result (39) in hand, we can now rewrite (35) as follows,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Using the previous result (39) for log p(x|s) we have that,log p(x|s) p(x|s (i) ) = κ N j=1cos(x j −s j ) − cos(x j − s Model comparison for the visual search task, including the additional point estimate model. In this additional model L is considered independent of C, which slightly changes the dependence of the bias on the number of targets. As in the main text, the maximum log-likelihood is compared to the Bayesian observer results. Dots represent individual subjects. The bars represent the mean.giving, p(accept) = min 1, e κ N j=1 cos(xj −sj )−cos(xj −s (i) j )</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The same criticism applies to the computation of various other aspects of high dimensional distributions like expected values, variances, entropies, or expected rewards, but for brevity we will discuss things as if we always aim for a marginal distribution over some variable of interest.2 Point estimate observers could also choose the point estimate they evaluate based on different criteria than probability, but here we consider only optimisation based on the probability density.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Anecdotally, the participants in this task were mostly graduate student friends of the first author of the study, who might have been more motivated or knowledgeable than typical participants.4"support" of a distribution simply refers to the set of all possible outcomes, i.e. the set of all possibilities with non-zero probability</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">By including the prior probabilities and treating the two response categories as the models to be compared.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Will Adler, Shan Shen, and Yanli Zhou for sharing the data for their respective experiments, which made this project possible.</p><p>This study was funded in part by the German Research Foundation (DFG) through grant SCHU 3351/1-1 to HHS. JCT would like to thank the Grand Union ESRC Doctoral Training Partnership, and St John's College, Oxford, for DPhil studentship funding. W.J.M. was supported by grants R01EY020958 and R01EY026927 from the National Institutes of Health. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1834" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experience can change the &apos;light-from-above&apos; prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1057" to="1058" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing Bayesian and non-Bayesian accounts of human confidence reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1006572</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The physical limits of grating visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1915" to="1924" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marginalization in Neural Circuits with Divisive Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="15310" to="15319" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic Population Codes for Bayesian Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1142" to="1152" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spontaneous Cortical Activity Reveals Hallmarks of an Optimal Internal Model of the Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6013</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian just-so stories in psychology and neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="414" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian color constancy. JOSA A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1393" to="1411" />
			<date type="published" when="1997" />
			<publisher>Publisher: Optical Society of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive History Biases Result from Confidence-weighted Accumulation of Past Choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2189" to="2206" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal defocus estimation in individual natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="16849" to="16854" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining the effects of distractor statistics in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calder-Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision making under uncertain categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eyetracking reveals multiple-category use in induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1050" to="1067" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Remembrance of inferences past: Amortization in human hypothesis generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A theory of learning to infer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="441" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic modulation of decision biases by brainstem arousal systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Colizoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Kloosterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knapen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nieuwenhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23232</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian Spiking Neurons I: Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deneve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="117" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading population codes: a neural implementation of ideal observers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deneve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="740" to="745" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="11090" to="11095" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual search: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="14" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Humans integrate visual and haptic information in a statistically optimal fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">6870</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Irrationality of Categorical Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="19060" to="19070" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The free-energy principle: A unified brain theory?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<editor>e1000211. Friston, K.</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>PLoS Computational Biology</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predictive coding under the free-energy principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="1211" to="1221" />
			<date type="published" when="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequential ideal-observer analysis of visual discriminations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">267</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contributions of ideal observer theory to vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="771" to="781" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">What does the free energy principle tell us about the brain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07945</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>q-bio</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6245</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A probabilistic approach to demixing odors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barthelmé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">F</forename><surname>Mainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="106" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Signal detection theory and psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>Wiley</publisher>
			<pubPlace>New York; London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal Predictions in Everyday Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="767" to="773" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Perceptual Decision-Making as Probabilistic Inference by Neural Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haefner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The curse of dimensionality for numerical integration of smooth functions II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian fundamentalism or enlightenment? on the explanatory status and theoretical contributions of bayesian models of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="169" to="188" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">What&apos;s new in psychtoolbox-3? Perception, 36</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ECVP Abstract Supplement</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: Principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mechanisms of color constancy under nearly natural viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="312" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian integration in sensorimotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">6971</biblScope>
			<biblScope unit="page" from="244" to="247" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Point-estimating observer models for latent cause detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1009159</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust averaging protects decisions from noise in neural computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herce Castañón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vandormael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1005723</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. The Behavioral and brain sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The anchoring bias reflects rational use of cognitive resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="349" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Individual choice behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Post-decision biases reveal a self-consistency principle in perceptual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>eLife, In press</publisher>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Organizing probabilistic models of perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bayesian inference with probabilistic population codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1432" to="1440" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural coding of uncertainty and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jazayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Behavior and neural basis of near-optimal visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="783" to="790" />
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian decision theory as a model of human visual perception: Testing Bayesian transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="155" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<idno>No. MSR-TR-2005-173</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian sampling in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno-Bote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="12491" to="12496" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Decision noise: An explanation for observed violations of signal detection theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Weidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="494" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reasoning with uncertain categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="117" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Uncertainty in category-based induction: When do people integrate across categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="276" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cue combination on the circle and the sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visual search for singleton feature targets within and across feature dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Optimal eye movement strategies in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Najemnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="issue">7031</biblScope>
			<biblScope unit="page" from="387" to="391" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MCMC Using Hamiltonian Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Markov Chain Monte Carlo</title>
		<editor>S. Brooks, A. Gelman, G. Jones, &amp; X.-L. Meng</editor>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">20116022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ix. on the problem of the most efficient tests of statistical hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009</idno>
		<ptr target="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London. Series A</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="289" to="337" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
	<note>Containing Papers of a Mathematical or Physical Character</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural variability and sampling-based probabilistic representations in the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orbã¡n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="530" to="543" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The psychophysics of visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1227" to="1268" />
			<date type="published" when="2000-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neuronal message passing using Mean-field, Bethe, and Marginal approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Markovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1889</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Probabilistic brains: knowns and unknowns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1170" to="1178" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Benefits of commitment in hierarchical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="622" to="639" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Numerical mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Suboptimality in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rahnev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Denison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Category-based predictions: Influence of uncertainty and feature associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology. Learning, memory, and cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="736" to="753" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Types of approximation for probabilistic cognition: Sampling and variational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bayesian Brains without Probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="883" to="893" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: Alternative algorithms for category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1144" to="1167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Uncovering mental representations with Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Shiffrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="106" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Constraining bridges between levels of analysis: A computational justification for locally Bayesian learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Data available at. Open Science Framework. Retrieved from dx</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calder-Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">file://localhost/opt/grobid/grobid-home/tmp/dx.doi.org/10.17605/OSF.IO/X8Q6J</idno>
		<ptr target="doi.org/10.17605/OSF.IO/X8Q6J" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A detailed comparison of optimality and simplicity in perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="480" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Exemplar models as a mechanism for performing Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="464" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Imperfect Bayesian inference in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stengård</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1006465</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A Bayesian Model of Conditioned Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt, D. Koller, Y. Singer, &amp; S. T. Roweis</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Combining priors and noisy visual cues in a rapid pointing task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tassinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="10154" to="10163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Human inference reflects a normative balance of complexity and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tavoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pizzica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1153" to="1168" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Generalization, similarity, and Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Feature analysis in early vision: Evidence from search asymmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gormican</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Economic irrationality is optimal during noisy decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3102" to="3107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Choice history biases subsequent evidence accumulation. eLife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>De Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Opheusden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03985</idno>
		<title level="m">Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">One and Done? Optimal Decisions From Very Few Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="637" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">An Economist&apos;s Perspective on Probability Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vulkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Note on the Consistency of the Maximum Likelihood Estimate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="595" to="601" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Ensemble Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="129" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The psychometric function: I. Fitting, sampling, and goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1293" to="1313" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An internal model for sensorimotor integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="issue">5232</biblScope>
			<biblScope unit="page" from="1880" to="1882" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Uncertainty is maintained and used in working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Do perceptual biases emerge early or late in visual processing? Decision-biases in motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zamboni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ledgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schluppeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<date type="published" when="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Probabilistic interpretation of population codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="403" to="430" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">The role of sensory uncertainty in simple contour integration. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">350082</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="719" to="748" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
