<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Weighting of Time-varying Visual and Auditory Evidence during Multisensory Decision-making</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><forename type="middle">R M</forename><surname>Tuip</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Neuroscience</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution" key="instit1">Swammerdam Institute for Life Sciences</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology, Brain and Cognition</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Van Der Ham</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Neuroscience</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution" key="instit1">Swammerdam Institute for Life Sciences</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">A M</forename><surname>Lorteije</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Neuroscience</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution" key="instit1">Swammerdam Institute for Life Sciences</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology, Brain and Cognition</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Animal Welfare Body</orgName>
								<orgName type="institution">Radboud University/UMC</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Van Opstal</surname></persName>
							<email>f.vanopstal@uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Swammerdam Institute for Life Sciences, Center for Neuroscience</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="department" key="dep3">Rosanne R.M. Tuip) and Department of Psychology, Brain and Cognition</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Science Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country>The Netherlands (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Filip van Opstal)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 129</addrLine>
									<postCode>1001 NK</postCode>
									<settlement>Amsterdam</settlement>
									<country>The Netherlands (</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Weighting of Time-varying Visual and Auditory Evidence during Multisensory Decision-making</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multisensory integration</term>
					<term>evidence accumulation, sensory weighting</term>
					<term>temporal integration</term>
					<term>perceptual decision-making</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Perceptual decision-making in a dynamic environment requires two integration processes: integration of sensory evidence from multiple modalities to form a coherent representation of the environment, and integration of evidence across time to accurately make a decision. Only recently studies started to unravel how evidence from two modalities is accumulated across time to form a perceptual decision. One important question is whether information from individual senses contributes equally to multisensory decisions. We designed a new psychophysical task that measures how visual and auditory evidence is weighted across time. Participants were asked to discriminate between two visual gratings, and/or two sounds presented to the right and left ear based on respectively contrast and loudness. We varied the evidence, i.e., the contrast of the targets and amplitude of the sound, over time. Results showed a significant increase in performance accuracy on multisensory trials compared to unisensory trials indicating that discriminating between two sources is improved when multisensory information is available. Furthermore, we found that early evidence contributed most to sensory decisions. Integration of unisensory information during audiovisual decisionmaking dynamically changed over time and with difficulty. A first epoch was characterized by both visual and auditory integration, during the second epoch vision dominated and the third epoch finalized the integration period with auditory dominance. Late auditory dominance was especially apparent on hard trials. Our results suggest that during our task multisensory improvement is generated by a mechanism that requires cross-modal interactions but also dynamically evokes dominance switching.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Our brain combines information from the environment to form a coherent representation of the world. This implicates combining sensory signals originating from different sources.</p><p>Sensory evidence is not always instantaneously clear, but instead it can be noisy as it can consist of very subtle and sometimes even contradictory brief events that vary dynamically. For example, imagine a picnic with a group of friends in the park on a cloudy spring day.</p><p>When you are having a conversation with the person across from you, you must integrate the fragmented movements of the lips as belonging to a single origin while the scene constantly changes in light intensity. It is therefore not surprising that studies in the field of perceptual decision-making have impinged on the notion that we need to continuously accumulate sensory evidence across time <ref type="bibr" target="#b20">(Gold &amp; Shadlen, 2007;</ref><ref type="bibr" target="#b15">Drugowitsch et al., 2014;</ref><ref type="bibr" target="#b50">Ratcliff et al., 2016)</ref>.</p><p>Different strategies have been revealed for studying sensory evidence accumulation. A number of studies using fluctuating visual information (i.e., where the visual stimulus reliability changes over time) have demonstrated that observers tend to weight early sensory information more heavily than late information <ref type="bibr" target="#b25">(Huk &amp; Shadlen, 2005;</ref><ref type="bibr" target="#b27">Kiani et al., 2008;</ref><ref type="bibr" target="#b41">Nienborg &amp; Cumming, 2009;</ref><ref type="bibr" target="#b65">Zylberberg et al., 2012;</ref><ref type="bibr" target="#b43">Odoemene et al., 2018;</ref><ref type="bibr" target="#b5">Booras et al., 2021)</ref>. However, late sensory information integration strategies <ref type="bibr" target="#b8">(Bronfman et al., 2016;</ref><ref type="bibr">Cheadle et al., 2014;</ref><ref type="bibr" target="#b32">Levi et al., 2018)</ref> and flat weighting profiles <ref type="bibr" target="#b8">(Bronfman et al., 2016;</ref><ref type="bibr" target="#b43">Odoemene et al., 2018)</ref> have also been observed. These differences in weighting profiles might relate to task specifics and stimulus features. Early profiles are observed when information throughout the trial is equally reliable, while late profiles are related to instances where integrating early information is not sufficient to solve the trial and thus late integration is necessary <ref type="bibr" target="#b8">(Bronfman et al., 2016;</ref><ref type="bibr">Talluri et al., 2021;</ref><ref type="bibr" target="#b32">Levi et al., 2018)</ref>.</p><p>Besides dealing with noisy information, our brain receives sensory information originating from different modalities. In a study by <ref type="bibr" target="#b49">Raposo et al., (2012)</ref>, rats and humans had to integrate time-varying audiovisual information to discriminate between high and low rate events. They found that rate categorization was better on audiovisual compared to unisensory trials in both humans and rats. It is important to point out that the variable that subjects needed to estimate (i.e., rate) was dependent on time. Perceptual decisions, however, are not always of this nature and require the estimation of the quality of sensory information.</p><p>Estimations of visual features such as contrast and colour and the auditory features such as the loudness and tone are crucial to discriminate between real life events in space and time. The authors additionally showed that evidence integration in humans was characterized by an early weighting profile. However, the differential contribution of visual and auditory information on audiovisual trials was not investigated.</p><p>Visual and auditory information streams are often processed with unequal weights.</p><p>Visual dominance has been observed in numerous studies where participants relied more on a visual stimulus compared to an auditory stimulus during audiovisual decision-making tasks <ref type="bibr" target="#b47">(Pick &amp; Warren, 1969;</ref><ref type="bibr" target="#b11">Colavita, 1974;</ref><ref type="bibr" target="#b64">Welch &amp; Warren, 1980;</ref><ref type="bibr" target="#b3">Bertelson &amp; Radeau, 1981)</ref>.</p><p>How visual and auditory information individually contribute to audiovisual decisions over time, and how the quality of sensory information influences evidence accumulation, however, remains to be investigated. In this study we aimed to address these issues. We designed an experimental paradigm during which participants had to discriminate between two visual gratings, two sound sources, or a combination of both, based on contrast and loudness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>54 female and 24 male participants (mean age = 20.26, std = 2.52, age range = 18 -38) took part in this experiment. All participants were students at the University of Amsterdam and participated for course credits. They were recruited via the website of the Behavioural Science Lab. Participants were screened to exclude subjects with visual or auditory impairments, with the exception of corrected-to-normal vision and audition. They provided written consent and were naive regarding the experimental design and goal of the study. The study was approved by The Faculty Ethics Review Board of the Faculty of Social and Behavioural Sciences of the University of Amsterdam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli and Procedure.</head><p>Participants performed a two-alternatives forced-choice decision task designed using the Psychtoolbox library <ref type="bibr" target="#b7">(Brainard, 1997;</ref><ref type="bibr" target="#b46">Pelli, 1997)</ref> in MATLAB R2017a. The task consisted of 600 trials in total, comprising 51 blocks of four visual trials, 51 blocks of four auditory trials and 48 blocks of four audiovisual trials. The experiment started with two blocks of visual practice trials and two blocks of auditory practice trials (always in the order of visual, auditory, visual and auditory) ( <ref type="figure" target="#fig_0">Fig. 1A</ref>). These practice trials were introduced for participants to get familiar with the task, for example to respond within the maximum response time window of two seconds. After the practice blocks, visual, auditory and audiovisual blocks were presented in a random order.</p><p>Stimuli were presented on a 17.3 inch MSI Bravo gaming laptop with a screen refresh rate of 144 Hz which was gamma-corrected. Participants were at a viewing distance of roughly 60 cm from the centre of the screen. During visual trials, two Gabor gratings with different contrast intensities were shown on the screen on a grey background as depicted in <ref type="figure" target="#fig_0">Fig. 1B</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>The centre of the patches were 10 degrees to the left and right from the centre of the screen and had a random orientation each trial albeit both the same. Participants were instructed to press the 'f' key if the stimulus with the highest contrast (i.e., the visual target) was presented on the left side of the screen and the stimulus with the lowest contrast (i.e., the visual distractor) on the right side. They had to press the 'j' key if the visual target was presented on the right side of the screen and the visual distractor on the left side. After each key press participants were provided with response feedback, a grey screen with the text 'correct!' or 'incorrect!' depending on the outcome <ref type="figure" target="#fig_0">(Fig. 1E</ref>). During auditory trials, two pink noise bursts starting at 7 kHz and linearly decaying to 32 kHz were presented to the left and right ear using headphones with a sampling rate of 44.1 kHz (MD-5000DR, IMG stageline) <ref type="figure" target="#fig_0">(Fig. 1C</ref>).</p><p>Participants were asked at which side the sound was highest in amplitude (loudness), the auditory target, by pressing the same corresponding keyboard keys as during visual trials.</p><p>During audiovisual trials the auditory and visual stimuli were presented simultaneously and the visual target and auditory target were always on the same side ( <ref type="figure" target="#fig_0">Fig. 1D,E</ref>).</p><p>Trials could be of three difficulty levels: easy, intermediate and hard. The difficulty level was determined by changing the baseline intensity of the distractor stimulus while the baseline intensity of the target stimulus was always 60%. We used a 1-down-2-up staircase procedure on intermediate unimodal visual and auditory trials to determine the visual and auditory distractor baseline intensities for these levels (Garcia-Perez, 1998). We designed the procedure in a way to obtain an accuracy of ± 71% intermediate audiovisual trials. The staircase procedure started after the practice trials and continued until the end of the experiment. For the first two trials of each modality, the intensity level of the distractor stimulus was set at 80% of that of the target. From the third trial on, the intensity decreased with 1% if one unimodal trial was incorrect. This way, the difference between the target and distractor intensity (i.e., the evidence) was increased. If two consecutive unimodal trials were correct, the intensity increased with 1% to decrease the evidence. On easy trials, the baseline intensity of the distractor was multiplied with a factor of 0.5 and on hard trials with a factor of 1.5.</p><p>The contrast intensities of the visual stimuli and the amplitude of the auditory stimuli fluctuated every 48.6 ms (seven frames on a 144Hz monitor) over a baseline value <ref type="figure" target="#fig_0">(Fig.   1F</ref>,G). These time-varying stimulus intensities were included to retrospectively test which moments in time significantly contribute to the decisions. To avoid clicks when the sound amplitude increased or decreased during these fluctuations, the sound level gradually approached the intensity of the next fluctuation over the last 10 ms period. The change in visual contrast was abrupt. The fluctuation range was 14% from baseline intensity resulting in fluctuation intensities between 46% and 74% for the target stimulus. Depending on the performance accuracy of the participant and the difficulty level, the values of target and distractor across the fluctuations could be very close to each other, which meant that the fluctuations could cause the evidence for the distractor location to be stronger than evidence for the target location at random points in time <ref type="figure" target="#fig_0">(Fig. 1F,G)</ref>. This ensured that participants had to evaluate evidence and integrate it over time in order to make a correct decision. Moreover, the fluctuation onsets of the target and the distractor stimuli were simultaneous but the fluctuation intensities were calculated randomly (i.e. they were asynchronous).</p><p>We aimed to identify sensory weighting profiles during perceptual decision-making in which decision times were under the control of the participants. The task script, however, did not allow for the stimulus to discontinue when the participants had responded. To circumvent this issue, the trial length was determined by the reaction times (RTs) of the participants. Early in the task, from the fifth trial up to the 20 th , the average RTs of all of the previous trials was calculated. These trials could consist of trials from all modalities. Later in the task, after trial 20, the RTs of the 20 most recent trials were averaged. Subsequently, 65% was added to the average RT value with a lower limit of 500 ms and an upper limit of 2s to obtain the final stimulus length. Using this method, the trial length felt natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis.</head><p>Data of participants that experienced technical issues during the task or performed with an overall accuracy below 60% were removed from further analyses. This entailed excluding the data of three participants due to technical issues and of 11 participants that performed around chance level. The analyses were thus performed on the data of 64 participants.</p><p>We calculated the average accuracies per participant for trials of different modalities and difficulties within individual modalities to examine how modality and difficulty affected performance accuracy. The average accuracies were fed into in a 3 (Modality: Visual, Auditory and Audiovisual) × 3 (Difficulty: Easy, Intermediate, Hard) repeated measures ANOVA (rmANOVA). A similar rmANOVA was performed on the average RTs to test how modality and difficulty influenced the RTs. We performed post-hoc tests using the Holm method for multiple comparison corrections.</p><p>To investigate the dynamics of sensory integration and at which moments in time auditory and visual information contributed to the decision, we performed different generalized linear mixed-effects models (GLMs) for visual, auditory and audiovisual trials (MATLAB function glmfit with binomial distribution). As our task entailed discriminating the target from the distractor stimulus where the baseline as well as the fluctuation intensities of the two stimuli could be in close proximity <ref type="figure" target="#fig_0">(Fig. 1F,G)</ref>, the task could only be solved by evaluating the stimulus intensities relative to one another. We thus calculated the difference between the target and distractor intensity for every fluctuation sample and normalized these values using a min-max normalization for each participant and for every trial. We used these visual and auditory evidence values to subsequently model the probability to make a correct decision with predictor variables separately for the normalized evidence of the visual stimulus (Vev) and the auditory stimulus (Aev). We only included the evidence samples before the participant responded by taking the RT on each trial as a cut-off point. For each evidence sample with a time period t of 48.6 ms we used the following equation for visual trials:</p><formula xml:id="formula_0">= [1 + exp (−( 0 + 1, * , + , ))] −1 (equation 1)</formula><p>for auditory trials:</p><formula xml:id="formula_1">= [1 + exp (−( 0 + 1, * , + , ))] −1 (equation 2)</formula><p>for audiovisual trials:</p><formula xml:id="formula_2">= [1 + exp (−( 0 + 1, * , + 2, * , + , ))] −1 (equation 3)</formula><p>Where Y is the response accuracy of the trial (i.e., correct or incorrect), β0 is the intercept term and β1 and β2 reflect the weights of the normalized visual and/or auditory evidence for timepoint t on trial i for participant j. bj,t is a random-effects term comprising an intercept and slope for each participant j on each time point t that accounts for potential participant-specific variation in task performance.</p><p>We implemented the false discovery rate (FDR) method <ref type="bibr" target="#b2">(Benjamini &amp; Hochberg, 1995;</ref><ref type="bibr" target="#b12">Groppe, 2022)</ref> for multiple comparison correction of the p-values of the predictor coefficients (MATLAB function fdr_hb). To test whether auditory and visual evidence contributed to audiovisual decisions with similar weights over time, we tested the coefficients for similarity per time point (MATLAB function coefTest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Accessibility</head><p>All gathered data and all codes used for testing and the analyses in the current study can be accessed on OSF (https://osf.io/qznyu/files/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Performance is enhanced on multisensory trials.</p><p>Our first aim was to assess whether audiovisual stimuli elicited multisensory improvements in our task. A within-subjects 3 (Modality) × 3 (Difficulty) rmANOVA revealed a main effect of  <ref type="table">Table 2</ref>).  Early weighting profiles and sequential modality dominance on audiovisual trials.</p><p>The temporal dynamics of evidence integration during visual, auditory and audiovisual decision-making were determined by performing different GLMs. This provided us with Beta coefficients of the target and distractor differences, i.e. the evidence, per fluctuation time point. The coefficient value reflects the weight of the evidence. The significant coefficients (p &lt; 0.05, FDR corrected) that contribute significantly to the decision are marked with a coloured asterisks in the plots. We will use the term 'contribute' when we refer to coefficients that increase the probability of making a correct choice. We were also interested in the contribution of visual and auditory evidence on audiovisual trials relative to each other to reveal potential differential integration strategies based on modality dominance. Significant differences between visual and auditory coefficients weights (p &lt; 0.05, fdr corrected) are indicated with black asterisks.</p><p>Relative to the stimulus onset, early weighting profiles are apparent on auditory <ref type="figure" target="#fig_2">(Fig.   3A</ref>), visual ( <ref type="figure" target="#fig_2">Fig. 3B</ref>) and audiovisual <ref type="figure" target="#fig_2">(Fig. 3C</ref>) trials. Focusing on audiovisual trials, we observe that only the first sample of sound (i.e. the sound onset) contributes significantly to the behavioral report after which visual evidence is weighted exclusively.</p><p>Calculating the weights relative to the stimulus onset provides an accurate estimation of the contribution of the first samples. However, the contribution of the later samples are potentially underestimated as the RTs vary between trials and participants. Therefore, we sought to investigate whether evidence closer to the response is weighted less as suggested by the results of our stimulus locked analysis. <ref type="figure" target="#fig_2">Fig. 3D</ref>-F show the weighting profiles relative to the response. The up-ramping weights reflect the spread and underestimation of the weight of the first sample as an expected consequence of this alignment. On auditory trials participants weight auditory evidence up until around 200 ms before the response <ref type="figure" target="#fig_2">(Fig. 3D</ref>) while on visual trials this occurs until 300 ms <ref type="figure" target="#fig_2">(Fig. 3E)</ref>. Interestingly, on audiovisual trials we observe a significantly dominant visual weighting period from 500 up until 300 ms before the response followed by a significantly dominant auditory weighting period from 200 until 100 ms before the response <ref type="figure" target="#fig_2">(Fig. 3F</ref>). Auditory evidence contributes to auditory decisions until 100 ms before the response (red asterisks). (E) Visual evidence contributes to visual decisions until 300ms before the response (green asterisks). (F) On audiovisual trials, visual integration is largely dominant over auditory integration until 300 ms before the response after which auditory integration is dominant until the coefficient weights reach 0 around 100 ms before the response (black asterisks).</p><p>Overall we see early sensory weighting profiles during our task. In addition, these results indicate that when participants are presented with both visual and auditory information, integration can be split into three epochs. The first epoch is characterized by the weighting of the first visual evidence sample as well as the first auditory evidence sample which is most obvious when the coefficient weights are aligned to the stimulus onset. Aligned to the response, we observe that during the second epoch participants rely on visual information only until 300-400 ms before the response. Finally, auditory information is weighted during the third epoch until the coefficient weights reach zero around 200 ms before the response.  Next, we tested how sensory integration was affected by difficulty. For all difficulties we observed early weighting profiles during auditory, visual and audiovisual decision-making ( <ref type="figure" target="#fig_4">Fig. 4-6</ref>). The profiles aligned to the stimulus onset of each modality condition was similar for easy ( <ref type="figure" target="#fig_4">Fig. 4A-C)</ref>, intermediate <ref type="figure" target="#fig_6">(Fig. 5A-C)</ref> and hard ( <ref type="figure" target="#fig_8">Fig. 6A-B</ref>) trials, and similar compared to the profiles we observed for trials of all difficulties pooled <ref type="figure" target="#fig_2">(Fig. 3A-C)</ref>. On audiovisual trials, for example, only the first auditory evidence sample contributed to the audiovisual decision after which visual evidence continued to contribute for a short period.   Relative to the response, auditory weighting profiles and visual weighting profiles were also similar to the profiles we observed when we pooled trials of all difficulties and had the same characteristics on easy <ref type="figure" target="#fig_4">(Fig. 4D,E)</ref>, intermediate <ref type="figure" target="#fig_6">(Fig. 5D</ref>,E) and hard <ref type="figure" target="#fig_8">(Fig. 6D,E)</ref> trials.</p><p>Interestingly, the three epoch profile on audiovisual trials was most pronounced on hard trials <ref type="figure" target="#fig_8">(Fig. 6F</ref>). To elaborate, on hard trials the auditory evidence coefficients weights were significantly different from both zero and the visual evidence contribution 150 and 250ms before the response <ref type="figure" target="#fig_8">(Fig. 6F)</ref>. The significant higher contribution of auditory evidence compared to visual evidence strongly indicates the occurrence of late auditory dominance.   On easy audiovisual trials the final auditory epoch was associated with auditory coefficient weights significantly different from zero but similar to visual coefficients weight <ref type="figure" target="#fig_4">(Fig. 4F</ref>).</p><p>The auditory weights were similar to zero throughout intermediate audiovisual trials which indicates a lack of auditory contribution <ref type="figure" target="#fig_6">(Fig. 5F</ref>). These findings strongly suggest that audiovisual decision-making strategies are transformed by difficulty level. Visual evidence contributes to visual decisions until 300 ms before the response (green asterisks). (F) On audiovisual trials, visual integration is dominant over auditory integration response after which auditory integration is dominant until the coefficient weights reach 0 around 100 ms before the response (black asterisks).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This study investigated how time-varying visual and auditory evidence is weighted over time and used to discriminate between sensory events in the environment. Our analyses revealed three main characteristics in the temporal weighting profile. First, early sensory information contributes strongest to decisions across all modalities and difficulties. Second, the processing of auditory information is characterized by a highly contributing stimulus onset. Third, improved audiovisual decision-making is associated with sequential modality dominance during which early visual and auditory evidence is equally integrated followed by visual and auditory dominance switching.</p><p>The generalized mixed models revealed early weighting profiles where early sensory information contributes most heavily to decision-making both when unisensory and when multisensory information is available. The early weighting profiles are especially apparent when the coefficient weights were aligned to the stimulus onset which is reflected by the large coefficient weights of the first evidence samples <ref type="figure" target="#fig_2">(Fig. 3A-C, fig. 4A-C, fig. 5A-C, fig 6A-C)</ref>.</p><p>This conclusion is supported by the lack of a contribution of late evidence that is observed when aligning the weights to the response <ref type="figure" target="#fig_2">(Fig. 3D-F, fig. 4D-F, fig. 5D-F, fig 6D-F)</ref>. It has been demonstrated that different evidence integration strategies can be explained by specific differences in behavioural paradigms. Among important features that influence weighting profiles are the division of evidence during a trial <ref type="bibr" target="#b8">(Bronfman et al., 2016;</ref><ref type="bibr" target="#b32">Levi et al., 2018;</ref><ref type="bibr" target="#b49">Raposo et al., 2012)</ref> and choice expectation <ref type="bibr" target="#b5">(Booras et al., 2021;</ref><ref type="bibr">Talluri et al., 2021)</ref>. When stimulus information is equally informative throughout the trial <ref type="bibr" target="#b27">(Kiani et al., 2008;</ref><ref type="bibr" target="#b32">Levi et al., 2018)</ref> and the observer is able to report the decision whenever they have solved the task, like in the paradigm used here, there is no need to integrate late information after the decision has been made. In fact, one could argue that taking into consideration late information after a decision has already been made would be an inefficient use of mental load. When stimulus durations are extended and the moment to commit to a decision is postponed <ref type="bibr" target="#b8">(Bronfman et al., 2016)</ref> weighting profiles shift from early weighting to late weighting. In addition, providing only late informative evidence to participants leads to favouring the integration of late information close to the decision moment <ref type="bibr">(Talluri et al., 2021;</ref><ref type="bibr" target="#b32">Levi et al., 2018)</ref>. In this situation, early information would not reflect the state of the world relevant for the choice.</p><p>Previous studies that have regressed decision outcomes to stimulus features in the framework of signal detection theory (SDT) have speculated how weighting profiles relate to sensory integration mechanisms <ref type="bibr" target="#b20">(Gold &amp; Shadlen, 2007;</ref><ref type="bibr" target="#b27">Kiani et al., 2008;</ref><ref type="bibr">Okazawa et al., 2020;</ref><ref type="bibr" target="#b30">Levi &amp; Huk, 2020)</ref>. According to this theory weighting profiles of evidence accumulation most likely embody a combination of sensory integration and perceptual decision-making processes. The early information weighting profile could reflect a bounded accumulation process during which information is integrated until a decision bound has been reached and a decision has been made. Early information contributes heavily to the sensory integration process and the information that appears after the bound has been reached does not. In contrast, late weighting profiles resemble leaky accumulation processes that are supported by a neural circuit that 'forgets' or 'leaks' information during the integration process. It should be noted that a one on one comparison of weighting profiles and decisionrelated processes might not be without error as it can underestimate or ignore factors such as sensory weights, termination criterion of the decision, and the non-decision time <ref type="bibr">(Okazawa et al 2020)</ref>.</p><p>A closer examination of the timescale of auditory integration revealed a particularly large weight of the first auditory evidence sample and a sharp decrease to the weight of the second sample. This salient onset effect was not as evident for visual integration. One explanation could be that the auditory stimulus appeared in a silent background (i.e., a headphone covering the ears) compared to the visual stimulus which appeared in a background of light (i.e., a grey computer screen in a lit room). Salient auditory changes in the background can capture exogenous attention of the observer <ref type="bibr" target="#b23">(Huang &amp; Elhilali, 2020)</ref>.</p><p>This attentional capture might cause an increase in the synchronization between neural populates which modulates the neural representation of a target stimulus <ref type="bibr" target="#b17">(Elhilali et al., 2009)</ref>. Moreover, louder sounds increase neural gain by evoking higher gamma band responses compared to sounds of lower intensities <ref type="bibr" target="#b56">(Schadow et al., 2007)</ref>. On intermediate and hard trials the distractor stimulus was louder than on easier trials. Therefore, the overall volume level is higher during the stimulus epoch than during the pre-stimulus epoch on intermediate and hard trials compared to easy trials. The stimulus-driven attentional capture caused by loud(er) onsets of physically closely related stimuli could explain why the weight of the first sample is higher on auditory trials and increases with difficulty.</p><p>Audiovisual decision-making was characterized by a strategy comprising sequential modality dominance which depended on task difficulty. The onset of the auditory and visual evidence contributed equally to the decision after which participants relied only on visual evidence on all difficulties. These two epochs were followed by an integration period of auditory evidence which was dominant only on hard trials. This points to visual dominance for an extended period during audiovisual decision-making which is irrespective of difficulty.</p><p>The switch to auditory dominance right before the response, however, only occurs on hard trials when evidence is unclear. Visual dominance has been shown in numerous of other studies <ref type="bibr" target="#b47">(Pick &amp; Warren, 1969;</ref><ref type="bibr" target="#b64">Welch &amp; Warren, 1980;</ref><ref type="bibr" target="#b3">Bertelson &amp; Radeau, 1981)</ref>. However, this is the first study that shows that the relative weights of unisensory information streams can change over time and across task conditions. Future studies should test how dominance switching aids sensory integration and whether this is indeed affected by difficulty. For example, introducing a conflicting auditory target right before the response when auditory integration dominates, could reveal how crucial auditory integration is during this stage.</p><p>When late auditory integration is especially necessary when visual information is unclear, people might guide their decision by the auditory target on hard trials and chose accordingly even if it conflicts with the visual stimulus.</p><p>It is striking that even though the contribution of auditory information is generally reduced on audiovisual trials compared to auditory trials, the addition of auditory information significantly increased performance accuracy and shortened RTs close to those observed on auditory trials. These findings make us question how the brain dynamically processes information from different senses over time. The improved accuracy and shorter RTs on the one hand, and overall visual dominance on the other hand, argue against a neural circuit that processes both modalities completely separately where the decision is carried by the fastest modality (the auditory modality) as would be predicted by a race model <ref type="bibr" target="#b48">(Raab, 1962;</ref><ref type="bibr" target="#b63">Townsend &amp; Wenger, 2004)</ref>. It is more likely that visual and auditory information are combined during one of the different stages of sensory processing. The unisensory information streams could be processed separately first in early sensory areas and later be integrated to in regions as the superior colliculus <ref type="bibr" target="#b39">(Meredith &amp; Stein;</ref><ref type="bibr">1982)</ref> or in higher order cortical areas <ref type="bibr" target="#b22">(Holdstock et al., 2009</ref><ref type="bibr" target="#b29">, Laurienti et al., 2003</ref><ref type="bibr" target="#b52">, Rolls &amp; Baylis, 1994</ref> after which a motor action is generated by motor regions. The switching between dominant modalities could be mediated by dynamically changing connection strength within and between early sensory areas and frontal and motor regions as demonstrated by <ref type="bibr" target="#b24">Huang et al., (2015)</ref>. These authors showed that visual dominance is induced by top-down modulation from the frontal lobe. Auditory dominance was generated by increased connectivity between the auditory cortex and the sensorimotor areas. They also showed that the early sensory area that was functionally synchronized with the default mode network (DMN) lost the so-called multisensory competition. Such a neural circuit underlying switching between modalities could be involved during audiovisual decision-making in our paradigm as well. We should point out that the study by <ref type="bibr" target="#b24">Huang et al., (2015)</ref> was designed to study modality competition and as a result facilitated the process of multisensory separation rather than that of multisensory integration <ref type="bibr" target="#b36">(Meijer et al., 2019)</ref>. The multisensory improvement that we show in our study points towards some form of multisensory integration. Thus, the neural computations that underly audiovisual processing and decision-making during our paradigm could result in an integrated multisensory estimate of the target stimulus. Yet, another scenario would hold that the prominent onset of the auditory stimulus boosts visual processing and that a dominantly visual representation of the target emerges during early processing. Connections between early visual and auditory areas might be the crucial mediators of these early cross-modal effects. It has been demonstrated that a loud onset of an auditory stimulus is one of the most dominant features that is relayed from the primary auditory cortex (A1) to the primary visual cortex (V1) <ref type="bibr" target="#b13">(Deneux et al., 2019)</ref>. Additionally, temporally coincident auditory input may enhance the excitability of the visual cortex particularly after sound onset by increasing the connectivity between low-level visual and auditory areas <ref type="bibr" target="#b53">(Romei et al., 2007;</ref><ref type="bibr" target="#b55">Romei et al., 2009;</ref><ref type="bibr" target="#b33">Lewis &amp; Noppeney, 2010)</ref>. The visual representation could be refined by late auditory information right before the response in a higher order region. This final audiovisual interaction could be particularly necessary when the visual evidence is low and as a consequence the representation is not clear enough for a decision to emerge.</p><p>Our experimental design relied on audiovisual trials that were asynchronous. This required that the fluctuation direction of the contrast of the visual target and distractor did not mimic that of the sound changes of the auditory target and distractor <ref type="figure" target="#fig_1">(Fig. 2F,G)</ref>. These independent fluctuations allowed us to estimate the individual contribution of uncorrelated visual and auditory information. We hypothesized that participants integrate asynchronous fluctuations in a similar manner as synchronous fluctuations; that synchrony is not a confounding factor. To test this hypothesis we recruited additional participants to perform a control experiment (see Supplementary Material) where synchronous and asynchronous trials were presented <ref type="figure" target="#fig_0">(Fig. S1A,B)</ref>. We showed that audiovisual decision-making is not influenced by whether the auditory and visual stimuli are modulated synchronously over time <ref type="figure" target="#fig_0">(Fig.   S1C,D)</ref>. This suggests that unlike temporal and spatial synchrony of unisensory information <ref type="bibr">(Stein et al., 1988)</ref>, modulation synchrony does not affect multisensory processing. The multisensory improvement that we showed in our main experiment and the findings of our control experiment imply that discriminating between relevant and irrelevant events in space and time is more accurate when these events are constructed by more than one sensory system even when the fluctuations of sensory information are out of sync. These results both corroborate with and expand on the knowledge of the field of multisensory research that has extensively and elegantly shown how multisensory information improves stimulus detection <ref type="bibr" target="#b62">(Todd, 1912;</ref><ref type="bibr" target="#b38">Mercier &amp; Cappe, 2020)</ref>, stimulus localization <ref type="bibr" target="#b1">(Battaglia et al., 2003;</ref><ref type="bibr" target="#b61">Teder-Sälejärvi et al., 2005;</ref><ref type="bibr" target="#b0">Aller et al., 2015)</ref> and stimulus categorization <ref type="bibr" target="#b18">(Ernst &amp; Banks 2002;</ref><ref type="bibr" target="#b49">Raposo et al., 2012;</ref><ref type="bibr" target="#b35">Li et al., 2015;</ref><ref type="bibr" target="#b38">Mercier &amp; Cappe, 2020)</ref>.</p><p>To summarize, the results that we have demonstrated here suggest that early visual and auditory evidence is weighted most heavily during perceptual decision-making. Audiovisual decisions are generated by a mechanism that promotes cross-modal interactions while changing the gain of one modality over the other in a fast and dynamic way.</p><p>We hypothesized that synchrony did not affect audiovisual decision-making. To prove this null hypothesis we performed Bayesian statistic. First we calculated the accuracies of each participant on synchronous and asynchronous trials separately and per difficulty level. These values were then entered in a 2 (Synchronous vs. Asynchronous) × 3 (Difficulty: Easy, Intermediate, Hard) Bayesian repeated measures ANOVA (rmANOVA). A similar Bayesian rmANOVA was performed on the average RTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To reveal the influence of modulation synchrony, or lack thereof, during audiovisual discrimination we performed a 2 × 3 Bayesian rmANOVA on the accuracies. This showed that the data is most likely under a model with only the main effect of difficulty, BF10 = 1.32e+19. Both the model with the two main effects (i.e., difficulty and synchrony) and the model with the main effects and interaction effect were less likely, BF10 = 4.17e+18 and BF10 = 1.51e+18 respectively. Importantly, this analysis provided substantial evidence against a role for modulation synchrony: Compared to the null model, the data was 4.13 times less likely under a model with only the main effect of synchrony (BF01 = 4.13). The average accuracies of the different conditions are presented in <ref type="figure" target="#fig_0">Fig. S1C</ref>.</p><p>A similar analysis on the average RTs showed very similar results <ref type="figure" target="#fig_0">(Fig. S1D</ref>). Again the data were found to be most likely under a model with only a main effect of difficulty, BF10 = 344.87. A model with the two main effects and a model that included the interaction term were both less likely, BF10 = 59.85 and BF10 = 35.57 respectively. This analysis also revealed substantial evidence for the absence of a main effect of synchrony, BF01 = 5.88. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Design of the experimental task. (A) After practice blocks of visual and auditory trials, the task trials commenced which also included audiovisual trials, (B) On visual training trials, participants were requested to indicate whether the left or the right visual grating had the highest contrast. (C) During auditory trials participants had to indicate whether the left or right sound was louder. (D) On audiovisual trials, participants had to indicate at which side the sound and contrast were higher. (E) Visual contrast and loudness randomly fluctuated every 48.9ms around an average value which was fixed for the target stimuli, but differed for distractor values between the easy and intermediate and hard trials. Participants were required to answer as soon as they perceived the target side and received feedback regarding their answer after each trial. (F) Example of the contrast fluctuations of the visual target (bright green) and distractor (dark green) on an intermediate trial. (G) Example of the volume fluctuations of the auditory target (bright red) and distractor (dark red) on an intermediate trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Performance is enhanced on audiovisual trials. (A) the average accuracies and (B) the average RTs for visual trials (green), auditory trials (red) and audiovisual trials (blue) for the three difficulty levels. Error bars denote the 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visual and Auditory evidence accumulation over all difficulty levels. weighting profiles A-C are aligned to the stimulus onset, D-F to the response. (A) Early auditory evidence contributes to auditory decisions followed by some samples later in time (n=11840 trials) (red asterisks). (B) Early visual evidence contributes to visual decisions (n=12278 trials) (green asterisks). (C) The first auditory and visual sample contribute equally to audiovisual decisions (n=12117 trials)(red and green asterisks respectively). During a subsequent short period, until 300 ms, visual evidence is weighted significantly higher than auditory evidence (black asterisks). (D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Audiovisual integration strategies are affected by difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visual and Auditory evidence accumulation on easy trials. weighting profiles A-C are aligned to the stimulus onset, D-F to the response. (A) Early auditory evidence contributes to auditory decisions followed by some samples later in time (n=5925 trials) (red asterisks). (B) Early visual evidence contributes to visual decisions (n=6149 trials) (green asterisks). (C) The first auditory and visual sample contribute equally to audiovisual decisions (n=6061 trials) (red and green asterisks respectively). During a subsequent short period of around 100 ms visual evidence contributes significantly to the decision (green asterisks). (D) Auditory evidence contributes to auditory decisions until 150 ms before the response (red asterisks). (E) Visual evidence contributes to visual decisions until 25 0ms before the response (green asterisks). (F) On audiovisual trials, visual integration is dominant over auditory integration for a short period after which auditory evidence solely contributes up until the coefficient weights reach 0 around 200 ms before the response (black asterisks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visual and Auditory evidence accumulation on intermediate trials weighting profiles. 5A-C are aligned to the stimulus onset; 5D-F are aligned to the response. (A) Early auditory evidence contributes to auditory decisions followed by some samples later in time (n=2969 trials) (red asterisks). (B) Early visual evidence contributes to visual decisions (n=3055 trials) (green asterisks). (C) The first auditory and visual sample contribute equally to audiovisual decisions indicated (n=3026 trials) (red and green asterisks respectively). During a subsequent short period of around 100ms visual evidence contributes significantly to the decision (green asterisks). (D) Auditory evidence contributes to auditory decisions until 150 ms before the response (red asterisks). (E) Visual evidence contributes to visual decisions until 300 ms before the response (green asterisks). (F) On audiovisual trials, visual integration is dominant over auditory integration for a long discontinuous period after which the weights reach 0 around 300 ms before the response (black asterisks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Visual and Auditory evidence accumulation on hard trials. weighting profiles A-C are aligned to the stimulus onset, D-F to the response. (A) Early auditory evidence contributes to auditory decisions followed by one sample later in time (n=2946 trials) (red asterisks). (B) Early visual evidence contributes to visual decisions (n=3074 trials) (green asterisks). (C) The first auditory and visual sample contribute equally to audiovisual decisions indicated (n=3030 trials) (red and green asterisks respectively). During a subsequent short discontinuous period of around 100 ms visual evidence contributes significantly to the decision (green asterisks).(D) Auditory evidence contributes to auditory decisions until 150 ms before the response (red asterisks). (E)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure S1 .</head><label>S1</label><figDesc>Performance on audiovisual trials is not affected by fluctuation synchrony. (A) Example of the contrast fluctuations of the visual target (bright green) and the visual distractor (dark green) on an asynchronous intermediate trial (left) and an example of the volume fluctuation of the auditory target (bright red) and the auditory distractor (dark red) on an asynchronous intermediate audiovisual trial. (B) Example of the contrast fluctuations of the visual target (bright green) and the visual distractor (dark green) on a synchronous intermediate trial (left) and an example of the volume fluctuation of the auditory target (bright red) and the auditory distractor (dark red) on a synchronous intermediate audiovisual trial. (C) the average accuracies and (D) the average RTs for synchronous audiovisual trials and asynchronous audiovisual trials consisting of trials of three difficulty levels. Error bars denote the 95% credible interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The effects of modality and difficulty on performance accuracy are illustrated inFig. 2A. Post-hoc tests revealed that on all difficulty levels, performance was better on audiovisual compared to visual and auditory trials(Table 1).A similar 3 × 3 rmANOVA was carried out on the mean RTs on correct trials of each participant. Sphericity was violated (ε = 0.726 for Modality, ε = 0.765 for Difficulty) and therefore Greenhouse-Geisser corrected results are reported for the effect of Modality and Huyn-Feldt corrected results are reported for Difficulty. This analysis revealed a main effect of Modality, F(1.453, 91.524) = 16.583, p &lt; 0.001, a main effect of Difficulty, F(1.561, 98.339 = 21.298, p &lt; 0.001) but no interaction between the two factors. The effects of modality and difficulty on RTs are illustrated inFig. 2B. Post-hoc tests showed that RTs were larger on visual trials compared to auditory and audiovisual trials for all difficulty levels (see</figDesc><table><row><cell>Modality, F(2, 126) = 51.406, p &lt; 0.001, of Difficulty, F(2, 126) = 302.336, p &lt; 0.001, and a</cell></row><row><cell>significant interaction effect, F(4, 252) = 5.955, p &lt; 0.001.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .Modality Di culty Easy Intermediate Hard Accuracy (fraction correct trials) A B</head><label>12</label><figDesc>The results of the post-hoc tests for the rmANOVA of the effects of modality and difficulty on accuracy.Depicted are the modality comparisons per difficulty level, the degrees of freedom (df), the t statistic (t) and the p-values (p). The test results show that multisensory improvement is present on each difficulty level.The results of the post-hoc tests for the rmANOVA of the effects of modality and difficulty on RTs. Depicted are the modality comparisons per difficulty level, the degrees of freedom (df), the t statstic (t) and the p-values (p).The test results show that RTs are largest on visual trials of all difficulties.</figDesc><table><row><cell></cell><cell>Difficulty</cell><cell></cell><cell></cell></row><row><cell>Contrast</cell><cell>Easy</cell><cell>Intermediate</cell><cell>Hard</cell></row><row><cell>V vs. AV</cell><cell cols="3">t(63) = -8.569, p &lt; .001 t(63) = -6.699, p &lt; .001 t(63) = -3.873, p &lt; .01</cell></row><row><cell>A vs. AV</cell><cell cols="3">t(63) = -8.608, p &lt; .001 t(63) = -7.330, p &lt; .001 t(63) = -3.689, p &lt; .01</cell></row><row><cell></cell><cell>Difficulty</cell><cell></cell><cell></cell></row><row><cell>Contrast</cell><cell>Easy</cell><cell>Intermediate</cell><cell>Hard</cell></row><row><cell>V vs. AV</cell><cell>t(63) = 3.710, p &lt; .01</cell><cell>t(63) = 3.050, p &lt; .05</cell><cell>t(63) = 3.359, p &lt; .05</cell></row><row><cell>V vs. A</cell><cell>t(63) = 4.884, p &lt; .001</cell><cell>t(63) = 5.675, p &lt; .001</cell><cell>t(63) = 4.276, p &lt; .001</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>RRMT is supported by an Interdisciplinary Doctoral Agreement grant from the Institute for Interdisciplinary Studies of the University of Amsterdam. This work was further supported by the Amsterdam Neuroscience grant CIA-2019-01. We thank Anna van Harmelen for her contribution to the task code and Myrthe Griffioen for gathering the data of the main experiment.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Experiment 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>26 female and 10 male participants (mean age = 20.81, std = 4.70, age range = 18-47) who took part in this experiment were recruited via the website of the Behavioural Science Lab.</p><p>All participants had normal or corrected-to-normal vision and hearing and were students at the University of Amsterdam who participated for course credits. They provided written consent and were naive regarding the experimental design and goal of the study. The study was approved by The Faculty Ethics Review Board of the Faculty of Social and Behavioural Sciences (ERB) of the University of Amsterdam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioural Task.</head><p>The experimental design differs from the design of Experiment 1 in some aspects. During Experiment 2, participants were presented with 468 audiovisual trials with a 1:1 ratio of 'synchronous' and 'asynchronous' trials ( <ref type="figure">Fig. S1A,B)</ref> where visual and auditory fluctuation changes were similar or were determined in a random fashion respectively. Easy, intermediate and hard trials were randomly included in equal amounts. Baseline intensities for the difficulties and trial length were calculated using the same methods as in experiment 1.</p><p>Stimuli were presented on a 23 inch ASUS VG236H monitor with a screen refresh rate of 60 Hz which was gamma-corrected. Participants were at a viewing distance of roughly 60 cm from the centre of the screen. Auditory stimuli were presented in the same manner as during Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis.</head><p>Data of participants that experienced technical issues during the task or performed with an overall accuracy below 60% were removed from further analyses. This resulted in discarding data of four participants due to technical issues and of six participants that performed around chance level. The analyses were thus performed on the data of 26 participant for this control experiment.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A spatially collocated sound thrusts a flash into awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Noppeney</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnint.2015.00016</idno>
		<ptr target="https://doi.org/10.3389/fnint.2015.00016" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in integrative neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian integration of visual and auditory signals for spatial localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<idno type="DOI">10.1364/josaa.20.001391</idno>
		<ptr target="https://doi.org/10.1364/josaa.20.001391" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America. A, Optics, image science, and vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1391" to="1397" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal statistical society: series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-modal bias and perceptual fusion with auditoryvisual spatial discordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bertelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="578" to="584" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.3758/bf03207374</idno>
		<ptr target="https://doi.org/10.3758/bf03207374" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Booras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Mccormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Rhoads</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Change point detection with multiple alternatives reveals parallel evaluation of the same stream of evidence along distinct timescales</title>
		<idno type="DOI">10.1038/s41598-021-92470-y</idno>
		<ptr target="https://doi.org/10.1038/s41598-021-92470-y" />
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13098</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Psychophysics Toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-monotonic Temporal-Weighting Indicates a Dynamically Modulated Evidence-Integration Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Bronfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brezis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004667</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1004667" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheadle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Gardelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herce Castañón</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive gain control during human perceptual choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2014.01.020</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2014.01.020" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1429" to="1441" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human sensory dominance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Colavita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="409" to="412" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Groppe</surname></persName>
		</author>
		<ptr target="https://www.mathworks.com/matlabcentral/fileexchange/27418-fdr_bh)" />
	</analytic>
	<monogr>
		<title level="j">MATLAB Central File Exchange</title>
		<imprint>
			<date type="published" when="2022-01-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Harrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kempf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ceballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filipchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bathellier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Context-dependent signaling of coincident auditory and visual events in primary visual cortex. eLife, 8, e44006</title>
		<idno type="DOI">10.7554/eLife.44006</idno>
		<ptr target="https://doi.org/10.7554/eLife.44006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drugowitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Deangelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Klier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Angelaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal multisensory decision-making in a reaction-time task</title>
		<idno type="DOI">10.7554/eLife.03005</idno>
		<ptr target="https://doi.org/10.7554/eLife.03005" />
	</analytic>
	<monogr>
		<title level="m">eLife, 3, e03005</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Simon</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1000129</idno>
		<ptr target="https://doi.org/10.1371/journal.pbio.1000129" />
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Humans integrate visual and haptic information in a statistically optimal fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<idno type="DOI">10.1038/415429a</idno>
		<ptr target="https://doi.org/10.1038/415429a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">6870</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Forced-choice staircases with fixed step sizes: asymptotic and small-sample properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>García-Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0042-6989(97)00340-4</idno>
		<ptr target="https://doi.org/10.1016/s0042-" />
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1861" to="1881" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The neural basis of decision making. Annual review of neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.neuro.29.051605.113038</idno>
		<ptr target="https://doi.org/10.1146/annurev.neuro.29.051605.113038" />
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="535" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<title level="m">Signal detection theory and psychophysics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1966" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1969" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating visual and tactile information in the perirhinal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Holdstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hocking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Notley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhp073</idno>
		<ptr target="https://doi.org/10.1093/cercor/bhp073" />
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2993" to="3000" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes. eLife, 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.52984</idno>
		<ptr target="https://doi.org/10.7554/eLife.52984" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multisensory Competition Is Modulated by Sensory Pathway Interactions with Fronto-Sensorimotor and Default-Mode Network Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3760-14.2015</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3760-14.2015" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="9064" to="9077" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="10420" to="10436" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.4684-04.2005</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.4684-04.2005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3017" to="3029" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.4761-07.2008</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.4761-07.2008" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-modal sensory processing in the anterior cingulate and medial prefrontal cortices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Laurienti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Maldjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Susi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Burdette</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbm.10112</idno>
		<ptr target="https://doi.org/10.1002/hbm.10112" />
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="213" to="223" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interpreting temporal dynamics during sensory decisionmaking. Current opinion in physiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Huk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cophys.2020.04.006</idno>
		<ptr target="https://doi.org/10.1016/j.cophys.2020.04.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Strategic and Dynamic Temporal Weighting for Perceptual Decisions in Humans and Macaques. eNeuro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Katz</surname></persName>
		</author>
		<idno type="DOI">10.1523/ENEURO.0169-18.2018</idno>
		<ptr target="https://doi.org/10.1523/ENEURO.0169-18.2018" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="169" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Audiovisual synchrony improves motion discrimination via enhanced connectivity between early visual and auditory areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Noppeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="12329" to="12339" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.5745-09.2010</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.5745-09.2010" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatiotemporal Relationships among Audiovisual Stimuli Modulate Auditory Facilitation of Visual Target Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1068/p7846</idno>
		<ptr target="https://doi.org/10.1068/p7846" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="242" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Meijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pennartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Olcese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Lansink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in neurobiology</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.pneurobio.2019.01.004</idno>
		<ptr target="https://doi.org/10.1016/j.pneurobio.2019.01.004" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The interplay between multisensory integration and perceptual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cappe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2020.116970</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2020.116970" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">116970</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactions among converging sensory inputs in the superior colliculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Meredith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">4608</biblScope>
			<biblScope unit="page" from="389" to="391" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<idno type="DOI">10.1126/science.6867718</idno>
		<ptr target="https://doi.org/10.1126/science.6867718" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decision-related activity in sensory neurons reflects more than a neuron&apos;s causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nienborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Cumming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7243</biblScope>
			<biblScope unit="page" from="89" to="92" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<idno type="DOI">10.1038/nature07821</idno>
		<ptr target="https://doi.org/10.1038/nature07821" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual Evidence Accumulation Guides Decision-Making in Unrestrained Mice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Odoemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pisupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page" from="10143" to="10155" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.3478-17.2018</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3478-17.2018" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Psychophysical reverse correlation reflects both sensory and decision-making processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Okazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-05797-y</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-05797-y" />
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3479</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The videotoolbox software for visual psychophysics: transforming numbers into movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Pelli</surname></persName>
		</author>
		<idno>doi: 10.1163/ 156856897X00366</idno>
	</analytic>
	<monogr>
		<title level="j">Spat. Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="437" to="442" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sensory conflict in judgments of spatial direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Pick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="203" to="205" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Statistical facilitation of simple reaction times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">H</forename><surname>Raab D</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2164-0947.1962.tb01433.x</idno>
		<ptr target="https://doi.org/10.1111/j.2164-0947.1962.tb01433.x" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="574" to="590" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multisensory decision-making in rats and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Schrater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.4998-11.2012</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.4998-11.2012" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3726" to="3735" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diffusion Decision Model: Current Issues and History</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="281" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tics.2016.01.007</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.01.007" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gustatory, olfactory, and visual convergence within the primate orbitofrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Baylis</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.14-09-05437.1994</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.14-09-05437.1994" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5437" to="5452" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Occipital transcranial magnetic stimulation has opposing effects on visual and auditory stimulus detection: implications for multisensory interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Romei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Merabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="11465" to="11472" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<idno type="DOI">10.1523/JNEUROSCI.2827-07.2007</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2827-07.2007" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Preperceptual and stimulusselective enhancement of low-level human visual cortex excitability by sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Romei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thut</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2009.09.027</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2009.09.027" />
	</analytic>
	<monogr>
		<title level="j">Current biology : CB</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="1799" to="1805" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schadow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thaerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Herrmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stimulus intensity affects early sensory processing: sound intensity modulates auditory evoked gamma-band activity in human EEG</title>
	</analytic>
	<monogr>
		<title level="j">International journal of psychophysiology : official journal of the International Organization of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.ijpsycho.2007.04.006</idno>
		<ptr target="https://doi.org/10.1016/j.ijpsycho.2007.04.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Bronfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brezis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Donner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Choices change the temporal weighting of decision evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1152/jn.00462.2020</idno>
		<ptr target="https://doi.org/10.1152/jn.00462.2020" />
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1468" to="1481" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Effects of spatial congruity on audio-visual multimodal integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Teder-Sälejärvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Di Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hillyard</surname></persName>
		</author>
		<idno type="DOI">10.1162/0898929054985383</idno>
		<ptr target="https://doi.org/10.1162/0898929054985383" />
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1396" to="1409" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Reaction to multiple stimuli. New era printing Company</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Todd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A theory of interactive parallel processing: new capacity measures and predictions for a response time inequality series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wenger</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.111.4.1003</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.111.4.1003" />
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1003" to="1035" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Immediate perceptual response to intersensory discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">638</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Decision making during the psychological refractory period</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zylberberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ouellette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology : CB</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1795" to="1799" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cub.2012.07.043</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2012.07.043" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
