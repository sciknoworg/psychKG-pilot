<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Running head: STRUCTURED PRIORS IN HUMAN FORECASTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Quiroga</surname></persName>
							<email>francisco.quiroga@uc.cl.</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London Eric Schulz Harvard University Maarten Speekenbrink University College London Nigel Harvey University College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Experimental Psychology</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>London, UK; Eric Schulz, Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>26 Bedford Way</addrLine>
									<postCode>WC1H 0AP</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Speekenbrink</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Experimental Psychology</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>London, UK; Eric Schulz, Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Harvey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Experimental Psychology</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>London, UK; Eric Schulz, Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Running head: STRUCTURED PRIORS IN HUMAN FORECASTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T14:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Forecasting</term>
					<term>Time Series</term>
					<term>Gaussian Process</term>
					<term>Function Learning</term>
					<term>Intuitive Statistics</term>
					<term>Structured Learning</term>
					<term>Decision Making</term>
					<term>Judgments</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Forecasting is an increasingly important part of our daily lives. Many studies on how people produce forecasts frame their behavior as prone to systematic errors. Based on recent evidence on how people learn about functions, we propose that participants&apos; forecasts are not irrational but rather driven by structured priors, i.e. situationally induced expectations of structure derived from experience with the real world. To test this, we extract participants&apos; priors over various contexts using a free-form forecasting paradigm. Instead of exhibiting systematic biases, our results show that participants&apos; priors match well with structure found in real-world data. Moreover, given the same data set, structured priors induce predictably different posterior forecasts depending on the evoked situational context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Forecasting plays a crucial role in our daily lives. We decide whether to take an umbrella to London based on a weather forecast. We forecast our commute time to determine when to leave home. Forecasting is also vital in the business world. For instance, sales forecasts aid companies to manage their inventories, and event production companies predict attendance numbers to select a suitable venue. <ref type="bibr" target="#b24">Weller and Crone (2012)</ref> surveyed 173 companies from different sectors and found that 44% of their forecasts were based on human judgments aided by a statistical baseline. The rest of the forecasts were similarly divided into approaches that used pure statistics (29%) or pure human judgment (26%) 1 .</p><p>Psychological research on judgmental forecasting has advanced our understanding of different effects that arise when individuals produce forecasts from time series data <ref type="bibr" target="#b10">(Lawrence, Goodwin, O'Connor, &amp; Önkal, 2006)</ref>. <ref type="bibr" target="#b16">Reimers and Harvey (2011)</ref> highlight four well-established forecasting effects: (i) trend damping: participants underestimate linear trends, (ii) optimism: forecasts for untrended series are too high and trend damping is larger for downward facing trends, (iii) noise adding: participants add noise to their forecasts, and (iv) overestimation of autocorrelation. Frequently, these effects are also referred to as judgmental forecasting biases, under the assumption that participants' forecasts are either suboptimal or -at the least-require the aid of decision support systems (see <ref type="bibr" target="#b0">Armstrong &amp; Collopy, 1998)</ref>. However, other interpretations of these effects are also conceivable (cf., <ref type="bibr" target="#b6">Harvey &amp; Reimers, 2013)</ref>.</p><p>This paper aims to further assess where systematic patterns in human forecasting come from and whether they are systematically biased or the results of an adaptation to the real world. We base our account on the assumption that phenomena observed in human forecasts mirror people's experiences of patterns found in the real world. We show that participants' forecasts are driven by structured priors that track naturally occurring patterns in their environments and that these priors can account for varying behavior in different forecasting scenarios. <ref type="bibr">1</ref> The percentages in the original study do not add up to 100%, presumably due to rounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A model of structured priors</head><p>Our approach to assessing structured priors is inspired by <ref type="bibr" target="#b5">Griffiths and Tenenbaum's (2006)</ref> work on people's priors over durations and magnitudes. By asking people to produce forecasts for common phenomena such as a person's predicted lifespan or the number of lines in a poem, Griffiths &amp; Tenenbaum extracted participants' priors over different scenarios and found that the extracted forms matched, on average, those of a full-information Bayesian model.</p><p>We adopt a similar method, but instead of extracting priors over distributional shapes, our goal is to extract priors over functional shapes. More specifically, we will use a model based on recent evidence showing that human function learning can be well-explained by Gaussian Process regression <ref type="bibr" target="#b4">(Griffiths, Lucas, Williams, &amp; Kalish, 2009;</ref><ref type="bibr" target="#b11">Lucas, Griffiths, Williams, &amp; Kalish, 2015;</ref><ref type="bibr" target="#b20">Schulz, Tenenbaum, Duvenaud, Speekenbrink, &amp; Gershman, 2017)</ref>. It is interesting to note that the difference between function learning and judgmental forecasting turns out to be rather small. Whereas function learning is frequently assessed sequentially, i.e., presenting inputs and asking for predictions of the output one at a time, in judgmental forecasting generally the entire dataset is revealed at once and subjects are subsequently asked to produce forecasts for future time points. Nevertheless, many of the effects found in judgmental forecasting translate to findings in the domain of function learning. For example, when <ref type="bibr" target="#b2">DeLosh, Busemeyer, and McDaniel (1997)</ref> trained participants to learn an underlying quadratic function, they found that their average predictions fell between the true function and straight lines fitted to the closest training points. This behavior is similar to a form of (polynomial) trend damping as well as an overestimation of auto-correlation <ref type="bibr" target="#b3">(Eggleton, 1982)</ref>.</p><p>Recently, <ref type="bibr" target="#b20">Schulz, Tenenbaum, et al. (2017)</ref> showed that the way in which participants learn complex functions can be explained by the use of compositional kernels formed by combining simpler structural components (see also <ref type="bibr" target="#b18">Schulz, Tenenbaum, Duvenaud, Speekenbrink, &amp; Gershman, 2016)</ref>. We expand on these findings by assessing participants' structured priors in a both a "free form" and a traditional forecasting paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STRUCTURED PRIORS IN HUMAN FORECASTING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal description</head><p>We use Gaussian Process (GP) regression as a Bayesian framework to assess participants' forecasts (see <ref type="bibr" target="#b17">Schulz, Speekenbrink, &amp; Krause, 2017</ref>, for a tutorial on Gaussian Process regression). Formally, a GP is a collection of random variables, any finite subset of which are jointly Gaussian-distributed <ref type="bibr" target="#b15">(Rasmussen &amp; Williams, 2006)</ref>. A GP can be expressed as a distribution over functions. Let f : X → R denote a function over input space X that maps to real-valued scalar outputs. These functions are assumed to be random draws from a GP:</p><formula xml:id="formula_0">f ∼ GP (m, k),<label>(1)</label></formula><p>where m is a mean function specifying the expected output of the function for each input x, and k is a kernel (or covariance) function specifying the covariance between outputs at inputs</p><p>x and x :</p><formula xml:id="formula_1">m(x) = E[f (x)] (2) k(x, x ) = E [(f (x) − m(x))(f (x ) − m(x ))] .<label>(3)</label></formula><p>Intuitively, the kernel encodes an inductive bias about the expected shape of the underlying function <ref type="bibr" target="#b22">(Schulz, Tenenbaum, Reshef, Speekenbrink, &amp; Gershman, 2015)</ref>. We will use the form of this kernel to assess structured priors in human forecasting. To simplify exposition, we follow standard convention in assuming a prior mean of m(x) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional on observed data</head><formula xml:id="formula_2">D t = {x k , y k } t k=1 , where y k ∼ N (f (x k ), σ 2 )</formula><p>is a draw from the latent function with added noise, the posterior predictive distribution for a new input x * , is</p><formula xml:id="formula_3">Gaussian p(x * |D t ) = N (m t (x * ), v t (x * )</formula><p>) with mean and variance given by:</p><formula xml:id="formula_4">m t (x * ) = E[f (x * )|D t ] = k t * (K t + σ 2 I) −1 y t (4) v t (x * ) = V[f (x * )|D t ] = k(x * , x * ) − k t (K t + σ 2 I) −1 k t * ,<label>(5)</label></formula><p>where y t = [y 1 , . . . , y t ] , K t is the t × t matrix of covariances evaluated at each pair of observed inputs, and  <ref type="bibr" target="#b1">(Brehmer, 1974)</ref> and following recent compositional accounts of function learning <ref type="bibr" target="#b20">(Schulz, Tenenbaum, et al., 2017)</ref>, we will use three base kernels which can be combined by addition and multiplication to produce a rich set of compositional kernels. The base kernels</p><formula xml:id="formula_5">k t * = [k(x 1 , x * ), . . . , k(x t , x * )]</formula><formula xml:id="formula_6">are the linear kernel, k(x, x ) = (x − θ 1 )(x − θ 1 ), the periodic kernel, k(x, x ) = θ 2 4 exp − 2 sin 2 (π|x−x |θ 5 ) θ 2 6</formula><p>, and the radial basis function kernel,</p><formula xml:id="formula_7">k(x, x ) = θ 2 2 exp − (x−x ) 2 2θ 2 3</formula><p>. For tractability reasons, we will fix the maximum number of combined kernels to three and will not allow for repeated kernels (following <ref type="bibr" target="#b18">Schulz et al., 2016)</ref>. The complete set of resulting kernels is shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set-up</head><p>Participants were asked to produce forecasts for six different scenarios. This was done by placing points over empty plots where the x-axis covered a four-year period by clicking on the plotting area. These points were automatically connected by a single curve that was computed using the centripetal Catmull-Rom method, which has the feature of ensuring that the curve passes through all the given points <ref type="bibr" target="#b25">(Yuksel, Schaefer, &amp; Keyser, 2009)</ref>. Participants could remove points by clicking on them or by pressing a "undo" button. To avoid a curve from describing a non-injective function (i.e., having more than one value of f (x) for a single</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STRUCTURED PRIORS IN HUMAN FORECASTING</head><p>x), participants were asked to place their points at least one month away from each other. At the same time, participants were asked to ensure that their plots had at least one point in the first month and one in the last month (see also <ref type="figure">Figure 1</ref>). There were four conditions and six forecasting scenarios in total. Every participant encountered a balanced combination of the conditions and scenarios, totaling a number of 12 trials overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and procedure</head><p>Scenarios and conditions. The experiment consisted of four conditions (1+3): a Prior condition which was the same for each participant, and Posterior-Positive, Posterior-Stable, and Posterior-Negative conditions which were randomly sampled for each scenario and participant. There were six scenarios in total: participants had to forecast temperature, probability of rain, sales, gym memberships, hourly wage, and number of Facebook friends, all over 4 years in total. We intentionally chose six scenarios from relatively different contexts which included both frequently and less frequently experienced situations.</p><p>The plotting area on which participants drew their predictions consisted of four years on the horizontal axis. The range of the vertical axis depended on the scenario and were as follows: −10 to 40°C for temperature, 0 to 100% for rain probability, 0 to 5000 units for sales, 0 to 50 for gym members, 0 to 1000 for Facebook friends, and 0 to £50 for the hourly wage.</p><p>Stage I: Prior condition. During the first stage, every participant was sequentially presented with the six scenarios in random order. This stage was associated with the Prior condition only and designed to extract participants' structured priors before any contact with experimental data. We used a free-form forecasting paradigm in which participants were</p><p>shown an empty plotting area and instructions in text format (see <ref type="figure">Figure 1a</ref>). Each screen started with general instructions that the participant could show or hide at will. Additionally, each scenario was introduced with a single sentence explaining the context, for example:</p><p>"Please draw the temperature forecast for a large city".</p><p>Stage II: Posterior conditions. After participants had entered the predictions on the six scenarios in the Prior condition, the scenarios reappeared -in the same order-for the Posterior conditions. Participants had to produce predictions once again. However, the initial plotting areas were not empty anymore; five pre-established and inalterable data points were automatically displayed during this stage of the experiment. These points were equidistantly distributed on the first 11 months (i.e., from the first day of January to the last day of November of the first year; see <ref type="figure">Fig. 1b</ref>). The values of these points depended on the current Posterior condition and a noise group. The specific condition determined the slope of an underlying linear trend that could be either positive, negative, or stable (i.e., constant). The noise group determined the noise applied over the underlying linear trend. The occurrence was balanced so that each of the three Posterior conditions appeared twice to each participant (i.e., the posterior-positive condition appeared twice, and so did the posterior-stable and posterior-negative conditions).</p><p>The curves for two different scenarios under the same Posterior condition and noise groups were displayed to be visually equivalent; only their intercept with the y-axis differed (see Appendix B for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral results</head><p>Two participants were not included in the analysis. One consistently placed one point at the minimum value for the last month, and the other indicated browser problems.</p><p>We interpolated participants' submitted points using the same centripetal Catmull-Rom method that was used in the experiment. We did this because the interpolation was presented to the participants in real time during the experiment. Hence it is natural to include it as part of each participant's input (see <ref type="figure">Fig. 1</ref>). Moreover, applying the same interpolation method ensures getting the same points that participants saw, whereas other interpolation methods could produce different curves for the same dataset.</p><p>The data associated to the first 30 days (in the Prior condition) and last 30 days (in all conditions) of the four years were filtered out of the analysis to ensure that every selected day had a value for every participants' predictions. This was because, although participants were forced to place one point in the first month and one in the last month, they could do so in any day of those months. Hence, one participant's prediction might have her first value on the 1st</p><p>of January while another one might have hers on the 31st of January. Lastly, we used data points in steps of five days (e.g., 31, 36, 41). <ref type="figure" target="#fig_2">Figure 2A</ref> presents all the behavioral data collected in the Prior condition. We can see that some scenarios present strong periodicities in the Prior condition (e.g., Temperature, Rain) while others do not (e.g., Salary, Facebook friends). Moreover, participants visibly generated different time series for different scenarios, thereby indicating that they might have different prior expectations about the shape of the time series depending on the scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracting structured priors</head><p>We fitted Gaussian Process (GP) regressions, with each of the eleven possible compositional kernels, to every forecast produced by each participant in the six Prior condition scenarios using the GPFlow software <ref type="bibr">(Matthews et al., 2017</ref>). 2 In cases where the GP conditioning procedure failed to optimize (often due to problems with the Cholesky decomposition), a white noise kernel was added to the composition. If the GP failed to optimize again, it was flagged for removal. We also removed those log marginal likelihoods that were five standard deviations or more away from the mean of each scenario-kernel combination. Overall, 5.02% of the fitted GPs were excluded using this procedure.</p><p>For each participant's forecasts (i.e., for each participant-scenario-condition combination), we standardized the likelihoods produced by each of the GPs' kernel compositions to be in the range from 0 to 1 to approximate the posterior probabilities assuming equal prior probability for each kernel combination. Afterwards, we averaged the results, per kernel and scenario, over participants. We executed this process to obtain an approximate compositional prior over the different kernels, as currently there is no reliable way to reach actual distributions over combined kernels (but see Janz, Paige, Rainforth, van de Meent, &amp; Wood, 2016). The resulting approximated prior distributions are shown in <ref type="figure" target="#fig_2">Figure 2B</ref>.</p><p>We can see that the different scenarios lead to diverse characteristic forecasting patterns and distinct best fitting kernels as measured by the standardized likelihood. In the temperature, rain, and gym members scenarios, participants' prior forecasts were best modeled by a multiplicative combination of a radial basis function and a periodic kernel indicating that they assume the data to have a periodicity that is not always perfectly repeating. In the sales, gym membership, and Facebook friends scenarios, participants' priors were best captured by an additive combination of a linear and a radial basis function kernel.</p><p>This indicates that people might expect this data to have a trend, yet that this trend does not persist indefinitely but slows down over time.</p><p>Human priors track real-world structure</p><p>We found that the kernels that best described the data in the Prior condition depended on the scenario. To understand whether this might be due to a de facto match between a scenario's actual structure and participants' priors, we gathered real-world data and processed it.</p><p>We used four years of real-world data for each scenario, using proxies when direct data was unavailable. In particular, the temperature data was the daily temperature in London from 2013 to 2016. The rain data was the daily rainfall data in the city of London for the same period. We summarized the rain data by trimesters to estimate the probability of rain over time. The sales data was obtained from a report by the Office for National Statistics of the United Kingdom which includes all retail sales (excluding fuels). This time series was monthly for the years 2013 to 2016. The gym membership data was generated by exporting how many times people searched for the term "gym membership" on Google, from 2013 to 2016. For the salary data, we used the median annual earnings, as reported by The Hamilton Project 3 , of adults ranging from 25 to 28 years old (as per the question participants had to answer; see Appendix A). Finally, the Facebook friends data was exported from one of the author's actual Facebook profile, starting from when he was 25 years old. We processed every real-world dataset as if it was participants' data; the Catmull-Rom interpolation was calculated to produce daily values, and the different Gaussian Process kernels were fitted to the data. 4 <ref type="figure" target="#fig_2">Figure 2</ref> shows red rectangles around the kernels that best described the collected real-world data. The optimal kernel composition coincided for the participants and the the best fitting kernel for the real-world data was l + p × r, which has a linear component that the optimal kernel composition for the participants' data did not have. The Sales and</p><p>Facebook friends scenarios best fitting kernels in the real-world data were p + r and p × r,</p><p>respectively. In the case of Facebook friends, although the participants' best fitting kernel was l + r, the standardized likelihood for p × r was not statistically different.</p><p>Next, we assessed the rank correlation between each participant's prior distribution over compositions and the distributions derived from the real-world data for each scenario. The effect of priors on posterior forecasts</p><p>In the Posterior conditions, participants were shown 11 months of data and were asked to forecast the time series for the remaining three years. We expected participants to push through their ideas of how the world works from the Prior to the Posterior conditions, i.e., perform a sensible update of their structural expectations. If this was the case, then we expected to observe varying participants' predictions for data that is visually identical but presented as stemming from different scenarios. Moreover, the better a kernel performed in the Prior condition, the better it should be able to predict participants' forecasts in the Posterior conditions.</p><p>We fitted new GPs to the 11-month of data presented to the participants. Adhering to a Bayesian treatment, we informed these new GPs with the optimal hyper-parameters found in the respective GPs of the Prior condition. For example, the hyper-parameters that were found to maximize the fit of a GP with kernel p + r to the forecasts of Participant A in the Prior condition Temperature scenario were subsequently used as prior values of a new GP (with the same p + r kernel) that was fitted to the data that Participant A saw in the respective Posterior condition for the Temperature scenario. The prior distribution for each hyper-parameter was assumed to be Gaussian with a mean value equal to the optimal value found in the Prior condition, and a variance of 1. This procedure was performed for every scenario-participant-kernel combination.</p><p>Following the same data preparation method as in the Prior condition, the GPs that failed to optimize, and those with likelihoods five standard deviations or more away from the mean of each scenario-kernel combination, were removed. In total, these 1.66% of the Posterior estimations were removed this way.</p><p>We used the new GPs to produce predictions from month 12 onwards, in line with the participants' experimental task. Afterwards, we compared these predictions to each participant's actual forecasts and calculated the mean squared error between the models' and participants' predictions (see <ref type="bibr" target="#b14">Mozer, Pashler, &amp; Homaei, 2008</ref>, for a similar approach).  influenced by both prior assumptions as well as the incoming data, one would indeed expect a medium -and not a strong-match between the extracted structured priors and these structures' performance in the Posterior condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Conclusion</head><p>We proposed a novel experimental paradigm and a Bayesian structure search model to study the relationship between the forecasts that humans make before and after contact with experimental data. Our results revealed a good match between participants' structured priors and the structure found in real-world data. This indicates that participants' priors across a diverse set of forecasting scenarios do not reflect a manifestation of a behavioral bias, but rather result from an intelligent adaptation to their environment. Moreover, participants' priors are strong enough to induce predictable differences in their forecasts for visually similar time series data presented in different scenarios.</p><p>We are not the first to find evidence that people's forecasts might be representative of their environment. In fact, in the context of judgmental forecasting, <ref type="bibr" target="#b6">Harvey and Reimers (2013)</ref> showed that the trend damping effect can be a result of an adaptation to the environment. This evidence opposed two other hypotheses that indicated an anchoring bias or an experimental interference. We believe that our results strengthen the idea of adaptation over bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Currently, our experimental setup suffers from at least three shortcomings: (i) the set of presented scenarios could be larger, (ii) the ranges shown in the plots were pre-established and fixed, and (iii) the information in the Posterior condition was presented statically. First, we only assessed participants' priors across six different scenarios. Although we selected them from a relatively broad range of topics, humans are capable of producing forecasts in vastly more diverse and richer settings than those assessed here. Second, we acknowledge that, despite our best efforts, pre-established ranges will always exert an influence on participants'</p><p>forecasts. Finally, we approximated the underlying distributions over kernels by standardizing the marginal likelihood produced by each kernel composition for every participant. This rough approximation was chosen as current implementations providing fully Bayesian treatments of estimating posteriors over kernels converged unreliably and very slowly in our datasets. Thus, we intend to reassess our current data by calculating the true posterior over structures as soon as the required software to do so becomes available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>In future work, we intend to further broaden the scope of our proposal by assessing participants' behavior in larger and more diverse sets of forecasting scenarios while also adding more kernels to our grammar. One interesting question for follow-up studies could be to contrast unfamiliar with familiar scenarios thereby assessing how priors develop as participants gain experience with a scenario.</p><p>Additionally, we believe that the effect of presentation formats on priors in time series data should also be assessed. For example, <ref type="bibr" target="#b8">Kusev, van Schaik, Tsaneva-Atanasova, Juliusson, and Chater (2018)</ref> showed that presenting time series data sequentially instead of revealing all of the points at once improved the accuracy of participants' forecasts but did not have an effect on their estimations of the mean.</p><p>Finally, inspired by the results of <ref type="bibr" target="#b6">Harvey and Reimers (2013)</ref>, and considering that the RBF kernel has the characteristic of reverting the predictions to the mean, we hypothesize that our paradigm could eventually explain trend damping. In fact, we tested this hypothesis with our current setup yet found no conclusive evidence. Nevertheless, we believe that this is a natural step forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Using a Bayesian model of structure search, we have found that participants' priors in different forecasting scenarios are structural by nature, are well-aligned with the underlying structure of the environment and lead to predictable differences in forecasts for the same data when labeled as coming from different scenarios. These results enrich our understanding of how people generate forecasts by using both prior structural expectations and the encountered data. <ref type="table" target="#tab_1">Table 1</ref> Kernel combinations in the grammar of structured functions and their interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination Interpretation</head><p>Linear   The Y-intercept value for Stage II of each scenario were chosen as reasonable values by the experimenters. These values were 15 for temperature, 30 for rain, 2500 for sales, 25 for gym member, 500 for Facebook friends, 20 for hourly wage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise values</head><p>There were three noise groups of five elements each (see <ref type="table" target="#tab_1">Table B1</ref>). Every element was sampled from a Uniform distribution with minimum value 0 and maximum value 1. These random numbers were generated in Python 3.5.2 by random.random(), from the random.py library. Each noise group was then normalized to have a mean of zero and a standard deviation of 1. Finally, the three center elements of each group were added to the linear trend. The details of the last two steps are explained again further in this Appendix section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table B1</head><p>Noise groups used in the creation of the Posterior conditions' initial values. Values have been rounded to two decimals; complete multi-decimal values can be found on GitHub. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial values' calculation</head><p>The Y-intercept values, noise group, and posterior condition were integrated in such a way that when participants looked at two plots with the same posterior condition and noise group (e.g., posterior-positive and group 1, respectively), both plots would look equivalent (albeit shifted to have lower or higher values). However, as is clear by focusing on the values of the different plots in <ref type="figure">Figure B1</ref>, the curves are not quantitatively equivalent. <ref type="figure">Figure B1</ref>. Screenshots of the six scenarios under the same posterior group and noise group. It can clearly be seen that despite each scenario having different ranges in the Y-axis, the curves look the same.</p><p>The five points of the first year are described by the following formula:</p><formula xml:id="formula_8">points i = yintercept s + (i × slope) + (4 × scale × random ji )<label>(6)</label></formula><p>where:</p><p>i is the ordinal position of each point, i ∈ {0, 1, 2, 3, 4}</p><p>j is the noise group, j ∈ {0, 1, 2} </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Finally, we assessed</head><label></label><figDesc>the correlation between a kernel's fitting performance in the Prior condition against its predictive performance in the Posterior condition. The former was operationalized as the standardized likelihood computed in the section above. The latter was operationalized as a rank over the error of each kernel composition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>3B shows the results of this analysis. The average correlations between a kernel's prior likelihood and its error in capturing participants' forecasts was ρ = −0.23 (t(118) = −6.77, p &lt; 0.001) for the Temperature data, ρ = −0.25 (t(118) = −8.54, p &lt; .001) for the Rain data, ρ = −0.1 (t(118) = −3.37, p = .001) for the Sales data, ρ = −0.16 (t(118) = −4.6, p &lt; .001) for the Gym membership data, ρ = −0.14 (t(118) = −4.35, p &lt; .001) for the Facebook friends data. In the case of the Salary scenario, the correlation was not significantly different from zero at ρ = −0.01 (t(118) = −0.24, p = .81). Thus, we found a match between participants' structured priors and those structures' performance when describing their Posterior condition forecasts. These results indicate that participants priors extracted from the first stage continued to influence their predictions in the second stage. At the same time, this analysis also shows that participants can adapt the structure of their forecasts in light of new evidence. As posterior forecasts should always be</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>A: Results of Stage 1. Blue curves represent each participant's input while the black curves represent the mean value averaged over participants. Red curves represent real world data taken from related domains, mean-centered by using the empirical means. B: Standardized prior likelihoods over compositions, averaged over participants. Error bars represent the standardized 95% confidence interval of the means. Red borders indicate the best fitting composition for the real world data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Results of Stage II. A: Participants' actual forecasts for each scenario in the different conditions (blue: positive, green: stable, red: negative). B: Standardized likelihood in the Prior condition condition against its respective rank of error in the Posterior condition. Appendix B Initial values in the Posterior conditions Y-intercept value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is the covariance between each observed input and the new input x * . Mathematically, it is possible to create richly structured and interpretable kernels by combining different base components. For example, by summing kernels, we can model the data as a sum of independent functions. Inspired by previous research on functional rule learning</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>alongside their natural</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Linear × RBF Repeated pattern plus local deviations with increasing amplitude Linear × RBF × Periodic Slowly changing repeated pattern with increasing amplitudeFigure 1. Screenshots of the experiment's visual interface. A: Prior condition for the Temperature scenario. Participants could place as many points as they liked by clicking on the plotting area. B: Posterior condition with negative trend. Years 2 to 4 have been cut off the image but were presented to participants to enter their forecasts.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temperature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Likelihood</cell><cell>0.0 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="17">+ Periodic Linear + RBF RBF + Periodic Linear × Periodic Linear × RBF RBF × Periodic Linear + RBF + Periodic Linear trend plus local deviations plus repeated pattern Linear trend plus repeated pattern Linear trend plus local deviations Repeated pattern plus local deviations Repeated pattern with increasing amplitude Local deviations with increasing amplitude Slowly changing repeated pattern Linear + Periodic × RBF Linear trend plus slowly changing repeated pattern Y2 Y3 Y4 0 50 100 Y1 Y2 Y3 Y4 Rain 0 2500 5000 Y1 Y2 Y3 Y4 Sales l+p l+r p+r l×r l×p p×r l+p+r l+p×r l×r+p l×p+r l×p×r 0.0 0.5 1.0 l+p l+r p+r l×r l×p p×r l+p+r l+p×r l×r+p l×p+r l×p×r Likelihood 0.0 0.5 1.0 l+p l+r p+r l×r l×p p×r l+p+r l+p×r l×r+p l×p+r l×p×r Likelihood Periodic + Y1 Y1 Y2 Y3 Y4 l+p l+r p+r l×r l×p p×r l+p+r l+p×r l×r+p l×p+r l×p×r 0 25 50 0.0 1.0 Gym members 0.5 Likelihood</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Salary</cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell>Likelihood</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Y1</cell><cell>Y2</cell><cell>Y3</cell><cell>Y4</cell><cell>l+p</cell><cell>l+r</cell><cell>p+r</cell><cell>l×r</cell><cell>l×p</cell><cell>p×r</cell><cell>l+p+r</cell><cell>l+p×r</cell><cell>l×r+p</cell><cell>l×p+r</cell><cell>l×p×r</cell></row><row><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FB Friends</cell><cell>500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Y1</cell><cell>Y2</cell><cell>Y3</cell><cell>Y4</cell><cell>l+p</cell><cell>l+r</cell><cell>p+r</cell><cell>l×r</cell><cell>l×p</cell><cell>p×r</cell><cell>l+p+r</cell><cell>l+p×r</cell><cell>l×r+p</cell><cell>l×p+r</cell><cell>l×p×r</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) s is the scenario, s ∈ {temperature, rain, ...} (9) c is the condition, c ∈ {posterior-positive, ...} element of noiseGroup j ) − mean(noiseGroup j ), otherwise</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(10)</cell></row><row><cell></cell><cell></cell><cell cols="4">noiseGroup j is the set of values corresponding to j</cell><cell>(11)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>yintercept s =</cell><cell>                      </cell><cell cols="2">15, 30, 2500, if s = sales if s = temperature if s = rain</cell><cell>(12)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>                      </cell><cell cols="2">30, 20, 500, if s = F acebook f riends if s = gym members if s = wage</cell></row><row><cell></cell><cell></cell><cell cols="4">slope = scale × slopeScale c</cell><cell>(13)</cell></row><row><cell></cell><cell></cell><cell cols="4">slope = 0.05 × range(scenario)</cell><cell>(14)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2"></cell></row><row><cell></cell><cell></cell><cell>slopeScale =</cell><cell cols="2">        </cell><cell>1, 0,</cell><cell>if c = posterior-positive if c = posterior-stable</cell><cell>(15)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">        </cell><cell>−1, if c = posterior-negative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>random ji =</cell><cell>   </cell><cell>0,</cell><cell></cell><cell></cell><cell>if i ∈ {0, 4}</cell></row><row><cell></cell><cell>   </cell><cell>(i</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The optimization time of each GP took approximately 1 minute on average on a personal computer (Intel(R) Core(TM) i5-6200U at 2.30 GHz, 8GB of RAM, running Python 3.6.1). This lead to an overall runtime of 11 hours for the 714 combinations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.hamiltonproject.org 4 All data can be found online: https://git.io/vA86Q</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STRUCTURED PRIORS IN HUMAN FORECASTING</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>The following are the questions that participants encountered during the first stage of the experiment, depending on the scenario.</p><p>• Please draw the temperature forecast for a large city.</p><p>• Please draw the sales forecast for a large company.</p><p>• Please draw a graph showing the number of total Facebook friends of a 25 year old male.</p><p>• Please draw the probability of a rainy day for a large city.</p><p>• Please draw the number of total gym members of a small gym.</p><p>• Please draw the hourly wage of a 25 year old male.</p><p>The following sentence was added to the second stage (i.e., the Posterior conditions) of the experiment: given the information for the first year.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Integration of statistical methods and judgment for time series forecasting: Principles from empirical research. Forecasting with Judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collopy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="269" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hypotheses about relations between scaled variables in the learning of probabilistic inference tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brehmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Performance</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extrapolation: The sine qua non for abstraction in function learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Delosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="968" to="986" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intuitive time-series extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Eggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="page" from="68" to="102" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling human function learning with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kalish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal predictions in everyday cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="767" to="773" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trend damping: Under-adjustment, experimental artifact, or adaptation to features of the natural environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reimers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">589</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06863</idno>
		<title level="m">Probabilistic structure discovery in time series data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsaneva-Atanasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juliusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive anchoring model: How static and dynamic presentations of time series influence judgments and predictions</title>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="77" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Judgmental forecasting: A review of progress over the last 25 years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Önkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="493" to="518" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A rational model of function learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kalish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1193" to="1215" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPflow: A Gaussian process library using TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal predictions in everyday cognition: The wisdom of individuals or crowds?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Homaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1133" to="1147" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sensitivity to autocorrelation in judgmental time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1196" to="1214" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A tutorial on gaussian process regression: Modelling, exploring, and exploiting functions. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">95190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probing the compositionality of intuitive functions</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="3729" to="3737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional inductive biases in function learning</title>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="44" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the perceived predictability of functions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 37th Annual Conference of the Cognitive Science Society</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Supply chain forecasting: Best practices &amp; benchmarking study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Crone</surname></persName>
		</author>
		<ptr target="http://www.lancaster.ac.uk/lums/forecasting/material/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the parametrization of Catmull-Rom curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
