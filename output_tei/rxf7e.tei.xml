<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where does value come from?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keno</forename><surname>Juechems</surname></persName>
							<email>keno.juchems@psy.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Walton Street</addrLine>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Summerfield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Walton Street</addrLine>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Where does value come from?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>value</term>
					<term>reward</term>
					<term>goal-directed decision-making</term>
					<term>medial prefrontal cortex</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The computational framework of reinforcement learning (RL) has allowed us to both understand biological brains and build successful artificial agents. However, in this article we highlight open challenges for RL as a model of animal behaviour in natural environments. We ask how the external reward function is designed for biological systems, and how we can account for the context sensitivity of valuation. We argue that rather than optimizing receipt of external reward signals, animals track current and desired internal states and seek to minimise the distance to goal across multiple value dimensions. Our framework can readily account for canonical phenomena observed in the fields of psychology, behavioural ecology, and economics, and recent findings from brain imaging studies of value-guided decision-making.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>What is it that animals seek to achieve by their behaviour? For evolutionary biologists, there is a simple answer to this question -that animal behaviour has evolved to maximise reproductive fitness, i.e. the propensity to produce offspring that will carry forth our genes. However, for those studying the behaviour of humans and animals in the fields of ethology, economics, psychology and neuroscience, this answer is only partly satisfying. Critically, it stops short of specifying how animals can learn which behaviours will increase or decrease their fitness. Instead, behavioural scientists propose that the environment furnishes reinforcement signals that indicate the likely costs or benefits of an action and assume that these signals can be directly sensed by the agent, allowing them to behave in ways that maximise utility over the short or long term. This is sometimes called the "reward hypothesis" <ref type="bibr" target="#b0">[1]</ref>. Over the past half century, the fields psychology and artificial intelligence (AI) have jointly developed a normative theory, known as reinforcement learning (RL), that is founded on the reward hypothesis <ref type="bibr" target="#b2">[3]</ref>. RL conceives of each encounter with the world as involving three distinct stages: (i) an agent receives sensory observations from the environment; (ii) the agent takes actions that influence its future states; and (iii) the agent receives a scalar reward signal that is emitted by the environment, and that is processed by the agent via a dedicated input channel <ref type="figure" target="#fig_0">(Fig. 1a</ref>). This conceptualisation lends itself naturally to modelling the canonical laboratory paradigm for operant behaviour, whereby an animal receives a sensory stimulus (e.g. a visual signal or tone), takes an action (e.g. presses a button or licks a spout) and receives a positive or negative reinforcer (e.g. liquid, food, or money). The framework successfully accounts for a wide array of habit-based and goal-directed behaviours in humans and other animals <ref type="bibr" target="#b3">[4]</ref>, and prominently explains the neural signals that accompany the expectation and delivery of reinforcers <ref type="bibr" target="#b4">[5]</ref>. RL has furnished the canonical computational framework for understanding valueguided decision-making in humans and other primates, and has been widely used to explain how different brain regions participate in valuation and choice <ref type="bibr" target="#b2">[3]</ref>. Moreover, RL models are currently offering some of the most exciting advances in AI research, producing artificial agents that can behave intelligently in complex, dynamic environments such as board and video games, thereby providing a proof of concept that intelligent behaviour can emerge de novo under the assumptions of the reward hypothesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The reward paradox</head><p>The RL framework offers a useful set of computational tools for understanding animal learning and behaviour. However, as a theory of biological intelligence, it has a major shortcoming that often goes unacknowledged: it is unclear who designs the external reward function against which behaviour is optimised. In machine learning, the reward function for an RL model is hand-designed by the researcher. For example, when training an autonomous vehicle, she might specify that it needs to reach its destination as quickly as possible whilst obeying the traffic laws, by assigning benefits and costs for desired and undesired behaviours. However, in natural environments, no external entity exists that can directly quantify the consequences of each action, like the points that are awarded in a video game for completing levels or shooting monsters <ref type="figure" target="#fig_0">(Fig. 1b)</ref>. Nor is it obvious that biological systems have a dedicated channel for receipt of external rewards that is distinct from the classical senses. Rather, rewards and punishments are sensory observations -the taste of an apple, the warmth of an embrace -and so stimulus value must be inferred by the agent, not conferred by the world. In other words, rewards must be intrinsic, not extrinsic (Box 1). We call this "the reward paradox". A. Schematic representation of the RL framework. Top: a typical "bandit" task in which participants choose between options (A and B) with uncertain reward probability, incurring financial outcomes. Below: in the RL framework, an agent receives observations (green), takes an action (blue) and receives feedback in the form of reward (red). The numbers highlight some assumptions of the RL framework that are poorly suited to explaining motivated behaviour in biological organisms (grey box below). B. The RL framework applied to video games (here, Ms. Pac-Man). The agent acts in order to maximise game score, which is a scalar quantity. Unlike humans playing the game, the agent receives the score differential directly (e.g. points for eating pellets) as reward signal via a dedicated input channel. C. Schematic of model-based RL, where the agent searches through a tree of possible future states and actions, sometimes stopping if no reward is expected from this branch of the search tree, and ultimately choosing the trajectory through the tree that leads to the highest return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-dependent valuation</head><p>To circumvent the reward paradox, it is sometimes assumed that the reward function has been conveniently designed by evolution. In other words, reward computation may rely exclusively on fast, mandatory, phylogenetically prespecified mechanisms that automatically convert sensory observations into hedonic signals that act as a proxy for "external" rewards. Indeed, there is no doubt that dedicated mechanisms exist for those physiological drives that lie at the bottom of our "hierarchy of needs" <ref type="bibr" target="#b7">[8]</ref>, such as hunger or fatigue. These needs are regulated by projections from the hypothalamus to neural systems that respond to rewards, including dopaminergic neurons in the ventral tegmental area <ref type="bibr" target="#b8">[9]</ref>. On this basis, proponents of RL models have dubbed dopamine neurons the "reward retina" as if they were responsible for directly sensing reinforcement from the environment <ref type="bibr" target="#b9">[10]</ref>. However, one challenge for this perspective is that the reward value of a stimulus depends on the context in which it occurs. In other words, value depends on both the agent's internal state and the external environment. Intuitively, the value a potentially rewarding stimulus (e.g. a hamburger) will depend on the agent's ongoing bodily needs (am I hungry?). In the RL framework, there is no natural way to account for this context sensitivity. In fact, RL models assume that reward is a purely hedonic signal that is used solely to update the agent's value function. Reward is thus not fungible -that is, once acquired, it is not exchangeable for other assets, and cannot be used in a way that would alter future observations or facilitate future actions. For example, when RL agents are learning to play video games, the rewards they receive (i.e. game score differential) evaporate instantly without providing future opportunities for disbursement within the game. This is wholly unlike natural settings, where rewards are typically accumulated because they lead to persistent changes in state. For example, bodies act as repositories for energetic resources that are harvested by the agent during trophic behaviours, just as bank accounts are used to store for income acquired in exchange for work. The agent can thus choose to exchange one stored asset for another that is needed, the defining feature of economic behaviour in the marketplace. In the laboratory, rewards may be implicitly accumulated as experimental animals or human participants become more sated or wealthy as a result of the liquid rewards or financial incentives that they receive for performing the task, but these changes in state are typically considered to be nuisance variables by the experimenter. For example, when a monkey working for liquid reward is no longer thirsty enough to work, the researcher will typically terminate the experiment. To fully account for biological behaviour, our models of value learning need to account for the internal state of the organism.  <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. For these agents, the cost function is populated by additional terms that encourage the agent to pursue adaptive behaviours even in the absence of reward. Many of these additional terms draw inspiration from psychological theory, which has long noted that humans and other animals often pursue activities for their own sake (e.g. a kitten furiously chasing a piece of string, or a commuter completing the crossword on her journey to work). For example, one key intrinsic motivation may be to explore the world in a structured fashion, satisfying curiosity and avoiding boredom <ref type="bibr" target="#b13">[14]</ref>. This allows agents to seek out experiences that may accelerate learning without directly enhancing reward, a phenomenon known as active learning. Other intrinsic motivation signals may encourage us to make actions with predictable consequences, or that exert maximal control over the environment or the behaviour of conspecifics, or those that minimise discrepant or dissonant inputs (see <ref type="bibr" target="#b10">[11]</ref> for a review). Augmenting the reward function with these intrinsic motivational signals often leads to faster learning and more stable patterns of behaviour in artificial agents <ref type="bibr" target="#b14">[15]</ref>.</p><p>However, in most reports, intrinsic motivation signals supplement (rather than replace) rewards from the external environment, leaving the thorny question of where rewards come from only partially answered. Other approaches that avoid any external feedback by using purely unsupervised or self-supervised methods -where the agent is optimised to maximise information gain or reduce sensory surprise -hold undoubted promise but</p><p>have not yet been observed to yield complex behaviours in dynamic environments that resemble the natural world <ref type="bibr" target="#b15">[16]</ref>. example dimensions (labelled satiety and hydration). The agent's current state is denoted , candidate next states ′ and the desired state (setpoint) * . Offers in the (x,y) dimension are shown in green. In the leftmost panel, the agent is sated but thirsty, as in a laboratory experiment where a monkey is working for liquid. An action reduces the distance between the current and desired state ‖ − * ‖ (where indicates the Minkowski distance; we use this here for simplicity, but note that a more general, two-parameter version depicted in panel C is needed to account for non-linearities in valuation, see Box 2 for details). Notice that the reduction in distance is proportional to what would (in the RL framework) be construed as an external reward signal. In the middle panel, an agent who is more thirsty than hungry is offered an equivalent quantity of food (0,1) or water (1,0).</p><p>Water reduces distance to goal (Euclidean, = 2) more than food and is thus preferred. In the rightmost panel, the agent is both very hungry and very thirsty. An offer comprising a bundle of both food and water (1,1) is preferred over a large quantity of food (0,2). C. Predictions for the full model with two parameters, p and q, governing non-linearities. The model can reproduce the ubiquitous finding of diminishing marginal utility (left panel). As p and q increase, distance estimates depend more strongly on just one dimension, indicating that maximum weight is given to the currently most needed asset (middle panel). Intermediate values p and q can reproduce one of the central findings of Juechems et al., 2019: that choices are both driven by offers and needs when assets need to be kept in equilibrium (see also <ref type="figure" target="#fig_3">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The value of internal states</head><p>A different theory has been invoked to understand how internal states motivate behaviour -one which assumes that animals seek to keep internal drives in equilibrium <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. This theory builds on early models of homeostatic function <ref type="bibr" target="#b18">[19]</ref> and found its first prominent expression in the 1940s with Hull's Drive Reduction Theory <ref type="bibr" target="#b19">[20]</ref>. These homeostatic models argue that motivated behaviours seek to restore imbalance among different internal needs, such as warmth, satiety, and hydration ( <ref type="figure" target="#fig_1">Fig. 2a)</ref>. A central idea is that neural circuits encoded desired states or physiological "setpoints", and drives result from the disparity between current state and these setpoints. Thus, rewards are multidimensional quantities signalling the drive reduction that occurs when an imbalance among the organism's multiple internal needs is redressed. Pure drive reduction theories fell from favour with the realisation that animals will work for rewarding stimuli when their drives are satisfied, i.e. purely for their hedonic value <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, which is important for understanding some psychological disorders, such as addiction. However, the incentive value of a stimulus nevertheless clearly depends on the internal state of the organism <ref type="bibr" target="#b22">[23]</ref>. For example, the rate of approach towards food depends on hunger levels, and adapts immediately according to internal levels of satiety irrespective of prior training <ref type="bibr" target="#b23">[24]</ref>. Similarly, food items that have been administered during states of food deprivation are preferred over control items during a later test when the animal is sated, suggesting they have acquired greater value <ref type="bibr" target="#b24">[25]</ref>. Neither of these phenomena are readily accounted for by the RL framework. Structure in ongoing or spontaneous brain states is a major driver of evoked neural activity <ref type="bibr" target="#b25">[26]</ref>, and neural signals across the brain may encode endogenous variation in internal motivational states. For example, genetic labelling can be used to identify neurons in the median preoptic nucleus of hypothalamus that become active when a mouse is thirsty, and optogenetic stimulation of these neurons provokes drinking behaviour even in hydrated mice <ref type="bibr" target="#b26">[27]</ref>. Using large-scale recording methods to measure firing rates in multiple brain regions, every region tested (including neocortical regions such as the OFC) contained a substantial fraction of neurons that responded to thirst levels in the baseline period before a cue for water delivery was presented <ref type="bibr" target="#b27">[28]</ref>. In other words, information about the animal's internal state of hydration is broadcast across the brain, in a way compatible with drive reduction theories. Thus, a more general framework is still needed in which value is computed from a combination of internal state and learned stimulus value <ref type="bibr" target="#b18">[19]</ref>. This has been largely overlooked in cognitive theories of value learning which focus instead on the RL framework, where reward is wholly determined by the external reward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goals as cognitive setpoints</head><p>In this Opinion we make a connection between homeostatic models of reward, which have emphasised balance among basic drives and interoceptive processes, and recent work on value-guided decisionmaking in humans and other primates <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. We argue that motivated behaviour in cognitive, economic or social settings can similarly be understood as seeking to restore a balance among competing "setpoints" that correspond to higher-order goals. Unlike currently popular theories of value-guided choice, which are largely inspired by RL models <ref type="bibr" target="#b2">[3]</ref> or expected utility theory from economics <ref type="bibr" target="#b31">[32]</ref>, our view emphasises the multidimensional nature of value signals, the dependence of motivated behaviour on the internal state of the agent, and the desire to explicitly realise goal states (constraint satisfaction) rather than maximise reward. In what follows, we show how this theory can explain some otherwise puzzling behavioural and neural findings in the literature on value-guided decision-making and economic choice.</p><p>Motivated behaviour is intrinsically goal-directed in nature. Animals often appear to be driven by the attainment of discrete states, and the avoidance of others. This is particularly salient in humans, who will work tirelessly to complete tasks whose only apparent reward is the satisfaction of task completion itself, for example when seeking to win a race, solve a puzzle, or climb a mountain <ref type="bibr" target="#b32">[33]</ref>. Indeed, this purposive, self-consistent property of human behaviour is so powerful that humans will seek to complete tasks even when the outcome is known to be a net loss, a phenomenon known as the sunk cost fallacy <ref type="bibr" target="#b33">[34]</ref>. Although the RL framework embraces goal-directed behaviours, it does so under the assumption that animals explicitly learn the transition probabilities between states and use mental tree search to identify the most rewarding trajectories, which then guide "model-based" behaviours (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). Thus, for RL, goals are not the objects of computation themselves but emerge as a by-product of policies which optimise expected future reward <ref type="bibr" target="#b34">[35]</ref>. RL also offers no ready way to understand how abstract goals modulate rewards (a hamburger is worth less to a vegetarian than an omnivore). Similarly, affective states that may be potent modulators of value are notably absent from the RL framework, as when a hamburger is left uneaten by someone nervous before an exam. A complete theory of motivation needs to account for how value depends on goals and affective states as well as basic needs and drives. We argue that during learning, animals form new setpoints pertaining to cognitive goals. For example, humans might represent current and desired states on axes pertaining to financial stability, moral worth, or physical health as well as hunger, thirst or temperature. A full computational model of this process is beyond the scope of the current article, but we offer a theory sketch in Box 2. This sketch, which draws on extant models of homeostatic function <ref type="bibr" target="#b17">[18]</ref>, proposes that current states and goals are encoded in a multidimensional "value map". Motivated behaviour can then be seen as an attempt to minimise the maximum distance to setpoints in this value space. Agents commit to policies which focus on purposively driving the current state towards setpoints on a particular goal dimension, such as caching resources, building a shelter, obtaining a mate, or enhancing professional status. In doing so, their ultimate goal is to maintain equilibrium among all goal states, achieving what might be popularly characterised as a state of "wellbeing". We call this view "goal equilibrium theory". In goal equilibrium theory, reward emerges naturally as a consequence of the computations required for learning, rather than being furnished by the external environment. This allows us to sidestep the "reward paradox" highlighted above. To illustrate, consider an animal whose internal state is currently described by parameters and who seeks to achieve a desired state * . Let us assume that the agent has a biologically plausible functional form, such as a neural network. In order to calculate the gradients that will allow network weights to be optimised for future behaviour, it is necessary to compute the loss term Δ(‖ * − ‖ ), which indicates the change in distance between a current and desired state that is afforded by any action. Now consider a typical laboratory study in which a single reward dimension is relevant, for example because the experimenter reduces prior liquid intake and offers only water in exchange for behaviour. For a thirsty animal, this loss term is now mathematically identical to what one would construe as "reward" in the RL framework -i.e. the volume of liquid administered on a trial <ref type="bibr" target="#b17">[18]</ref>. This equivalence will break down as tends towards * , but by this point the researcher will typically have ended the experiment because the animal is fully hydrated. Thus, goal equilibrium in this experiment can be thought of as subsuming the ubiquitous case were value is artificially constrained to be unidimensional. As an aside we note that from the agent's perspective, multiple value dimensions may continue to be relevant even during performance of a "unidimensional" value task. For example, a human participant might be tempted to neglect the task in favour of checking their social media account, in order to enhance social capital. However, this individual would most likely be excluded from the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box 2: Goal equilibrium theory</head><p>Goal equilibrium theory relies on three key intuitions. First, sensory observations, are mapped onto a persistent internal state :</p><formula xml:id="formula_0">= × + −1 × Eq.1</formula><p>Where is a matrix mapping the chosen option onto . The internal state, , exhibits dynamics whereby it will change (decay) in the absence of any input, e.g. credit card debt will increase if it is not repaid, just as thirst will increase if not slaked. These dynamics are captured by the matrix , which governs the intrinsic changes over time.</p><p>Note that this implies that the correspondence between internal and external states must be learned, i.e. our preferences are not simply given to us but acquired with experience, and that we might be uncertain about whether we are hungry, tired or happy. This relates to the framework of "active inference", under which reward values are inferred rather than being directly conferred <ref type="bibr" target="#b16">[17]</ref>. We conceive of this internal state space as a multidimensional map whose dimensions variously pertain to both physiological states (e.g. satiety) and cognitive goals (e.g. social status). Internal states are subject to momentum via recurrent connections so that states (or moods) may persist in the face of a change in external circumstance.</p><p>Second, the agent represents a set of desired (or aversive) points * on this space, allowing it to estimate the distance (and contributing features) of its current position to the goal state. In doing so, the agent needs to combine across the relevant goal dimensions and weight these according to their relative contribution to this distance by computing the Minkowski distance:</p><formula xml:id="formula_1">+1 = ‖ +1 − * ‖ = √∑| +1 − * | , = Eq.2</formula><p>The parameter allows the agent to prefer bundles that only consist of one single asset ( &lt; 1), be indifferent between all bundles ( = 1) or prefer mixed bundles ( &gt; 1). The model is thus well placed to account for commonly observed indifference curves between two or more economic goods. If we further assume that there are two non-linearities, p and q, rather than just k, our model can further account for diminishing marginal utility, <ref type="figure" target="#fig_1">Fig. 2c</ref>. This computational step is directly inspired by a proposal in ref <ref type="bibr" target="#b17">[18]</ref>.</p><p>Finally, where, then, is value (or: reward) in our framework? Value arises naturally from our framework when the agent compares its current distance to its desired state to its previous distance, which summarizes that the agent has progressed on at least one of its goals since the last time-point:</p><formula xml:id="formula_2">+1 = − +1 Eq.3</formula><p>Value is thus a summary of whether the agent is approaching or retreating from its goals. This model encapsulates situations in which multi-attribute objects need to be integrated into a single reward signal, such that = × if = 0 and k = 1 in Eq 2, where * can account for reference points such as running averages over past choices. Goal equilibrium theory readily incorporates other accounts of reference-dependent evaluation by adjusting how the agent computes * . For instance, it may do so by using a running average over past options and anchor its behaviour to this reference. When &gt; 1, simply assuming that other drives exist that cannot currently be satisfied immediately gives rise to the ubiquitous phenomenon of diminishing marginal utility. In other words, as the agent acquires the currently relevant asset (such as liquidreward) its attention will progressively tend towards other relevant assets, diminishing the contribution of liquid reward to its well-being <ref type="figure" target="#fig_1">(Fig. 2c</ref>). Our view relates to an earlier proposal that animals are motivated to satisfy a "current concern" and understands disorders of mental health as disrupted goal selection, pursuit and appraisal <ref type="bibr" target="#b35">[36]</ref>. A fleshed out computational theory of this process will require the specification of how agents learn the relationship between individual states and the degree of goal realisation. One model proposes a distinction between "primary" and "learned" value, where the former resembles the traditional reward signal in the RL framework and the latter quantifies the eligibility of states for goal realisation <ref type="bibr" target="#b32">[33]</ref>. A very appealing aspect of this framework is that it provides a natural way to understand the affective states that pervade our everyday mental landscape, including satisfaction (goal completion), frustration (goal obstruction), and disappointment (goal abandonment), which have largely eluded computational description thus far <ref type="bibr" target="#b32">[33]</ref>.</p><p>A map of internal state in the OFC For valuation to depend on the internal state of the agent, these states must be explicitly represented in brain signals. However, most neural studies of value-guided choice have used a paradigm in which the animal or human participant makes a succession of unrelated, independent choices between food items or monetary gambles. This paradigm is not well suited to measuring how variation in internal state (how sated am I right now? How much wealth have I accumulated?) might be coded in neural circuits. However, recently researchers have begun to employ more complex paradigms that may shed some light on this issue. At the neural level, the integrity of the orbitofrontal cortex (OFC) is critical for goal-driven reward behaviour <ref type="bibr" target="#b36">[37]</ref>. Interestingly, a recent theory proposes that the OFC constitutes a "map" of both observable and latent states within a task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>, similar to the framework proposed here (c.f. Eq.1 in Box 2). Moreover, OFC may code endogenous (internal) as well as exogenous (external) value, as firing rates <ref type="bibr" target="#b38">[39]</ref> and BOLD signals <ref type="bibr" target="#b39">[40]</ref> can be choice-predictive even in the baseline period prior to stimulus onset. An emerging theory suggests that OFC encodes affective states that pertain to ongoing positive or negative value, i.e. moods or "momentum" in internal value states <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Together, these findings imply that OFC is a likely candidate for representing ongoing estimates of position on the value map. One recent study directly tested whether BOLD signals in OFC tracked the internal accumulation of financial resources across an episode in a way consistent with the ongoing internal state representation proposed here <ref type="bibr" target="#b42">[43]</ref>. Participants performed an economic task that involved gambling for financial incentives over successive, independent trials. For a rational agent performing this task, there is no imperative to track the accumulated level of payoff incurred by choices -to maximise return, one simply needs to select, on each trial, the gamble with the highest expected value. However, when humans performed this gambling task over multiple discrete episodes, each signalled by a prominent contextual cue that was structurally irrelevant to the task, brain activity in the medial OFC tracked the level of accumulated resources (or "wealth") over the context <ref type="figure" target="#fig_2">(Fig. 3)</ref>. This "value accumulation" signal occurred even though the accumulated rewards were never overtly signalled to participants, nor were they instructed or incentivised to calculate their ongoing wealth. Another brain imaging study revealed that a nearby region of medial prefrontal cortex tracks internal states during wealth accumulation <ref type="bibr" target="#b43">[44]</ref>. Participants were asked to choose among four alternatives (bandits) that either paid or charged an uncertain sum of money. Critically, each selected bandit was fungible -it continued to incur financial costs or benefits across a short block of trials, as if the agent had purchased that asset and was continually profiting (or otherwise) from its possession ("rule in" condition). In a different condition, the bandits began in the agent's possession and decisions had to be made about which (if any) bandits to "rule out". This allowed us to assess how neural encoding of value depended on whether an asset was being bought or sold. Critically, BOLD signals in the ventromedial prefrontal cortex (vmPFC) encoded the cumulative expected value of assets across the block <ref type="figure" target="#fig_2">(Fig. 3)</ref>, and only coded for momentary rewards when decisions were made with future consequences for reward (e.g. when a bandit was selected in the "rule in" condition, or not selected in the "rule out" condition). Together, these studies suggest that the reward system tracks level of accumulated resources over time, for example so that future decision policies can adapt according to the current internal state, in a way that is compatible with goal equilibrium theory.</p><p>Budget rules: aspiration and avoidance points The natural world is structured in such a way that some states are critical for survival or have substantial impact on long-run future outcomes. For example, a student might work hard to pass his exams in the knowledge that it will open up interesting career opportunities. These states are often attained when accumulated resources reach, or fall below, a critical threshold. Behavioural ecologists have argued that animals' risky foraging behaviour adapts to satisfy a "budget rule" that seeks to maintain energetic resources at aspirational levels that safely offset future scarcity. For example, birds make risky foraging choices at dusk in order to accrue sufficient energy to survive a cold night <ref type="bibr" target="#b44">[45]</ref>. This view is neatly accommodated within the framework proposed here, in that the aspiration level reflects the setpoint against which current resource levels are compared, and the driver of behaviour is the disparity between current state and goal. Reprinted with permission [pending] from refs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Recent work with brain imaging <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> suggests that this goal distance signal might be computed in a different medial prefrontal cortex region, the dorsal anterior cingulate cortex (dACC). Symmonds <ref type="bibr" target="#b45">[46]</ref> asked participants to make a series of gambles which only paid out if a critical threshold or aspiration point was reached. BOLD signals in the vmPFC scaled with the expected future return over a series of gambles, similar to <ref type="bibr" target="#b43">[44]</ref>, whereas dACC coded for the aspiration point itself <ref type="figure" target="#fig_2">(Fig. 3)</ref>. Using a very similar design, Kolling <ref type="bibr" target="#b46">[47]</ref> asked human participants to accept or reject monetary gambles over a short block, with cumulative earnings increased by a multiplier if they reached a fixed aspiration level. BOLD signals in the dACC coded for number of steps to goal, as well as the discrepancy between accumulated return and the aspiration level, scaled by the distance to the end of the block. This signal, which the authors call "risk pressure", seems to be signalling the agent's current position with respect to a goal, rather than merely the arrival of a punctate reward. In another study involving planning in a virtual subway environment, dACC BOLD signals coded for the distance to goal in number of context switches <ref type="bibr" target="#b47">[48]</ref>. Indeed, the dACC is one brain region where single-cell responses gradually build up in association with a series of actions that precede reward delivery <ref type="bibr" target="#b48">[49]</ref>, and where lesions provoke failures of task persistence <ref type="bibr" target="#b49">[50]</ref>, as if this region participated more generally computing the disparity signal * − in the service of goal-directed behaviour. Participants managed virtual zoos in which they needed to accumulate as many lions and elephants as possible.</p><p>Starting out from an empty zoo, participants were offered varying amounts of lions and elephants to acquire.</p><p>Importantly, reward was calculated as the lower tally of animals currently owned within the zoo (e.g. when participants owned 2 lions and 6 elephants, they earned 2 units of reward). This enforced a rule by which the two tallies should be held approximately in equilibrium across time. Participants completed five zoos per block with their respective length indicated as a countdown in the centre of the screen. B. Logistic regression predicting a "redress" choice -i.e. whether participants chose to increase the lower tally of animals currently in the zoo (as given by the feedback screen). Participants placed approximately equal weight on the offer and goal (tally) differences, as in the simulation in <ref type="figure" target="#fig_1">Fig. 2c</ref>. C. Indifference map over the two assets (lions and elephants) and three key neural correlates. Regions in the dACC and vmPFC encoded the offer difference, goal difference, and the redress (the decrease in distance to goal between trials). Reprinted with permission [pending] from ref <ref type="bibr" target="#b1">[2]</ref>.</p><p>The multidimensional nature of value In the standard RL framework, each state-action pair is associated with a scalar value, often called a Qvalue <ref type="bibr" target="#b0">[1]</ref>. In other words, most RL models assume that value is a unidimensional quantity (although "multi-objective" RL models relax this assumption <ref type="bibr" target="#b50">[51]</ref>). This seems incongruous when considering the challenges that animals face in natural environments, which involve jointly satisfying many different constraints (for example, maintaining levels of satiety, hydration, warmth and social capital). Rewards take many different forms -from food items to social and sexual signals to secondary reinforcers such as money. A popular view in the hybrid field of neuroeconomics is that neural signals allow potentially incommensurable rewards to be encoded in a single neural "common currency" <ref type="bibr" target="#b30">[31]</ref>. For example, single cells in OFC code for the "offer value" of different liquid rewards irrespective of their taste <ref type="bibr" target="#b51">[52]</ref>. The existence of a common neural code for different assets allows economists to understand rational decision-making in consumer settings, which might involve choosing between a meal in a restaurant or tickets to the theatre. However, goal equilibrium theory suggests a different perspective, whereby assets are explicitly encoded along different dimensions (needs) in the value map, and decisions are made according to whichever need is currently greatest. Without further assumptions, goal equilibrium theory predicts that animals will treat different assets as if they are imperfect substitutions for one another. Multidimensional prospects (or "bundles") should be preferred if they contain a mixture of different assets, because this allows multiple drives to be satisfied simultaneously <ref type="figure" target="#fig_1">(Fig. 2b)</ref>. For example, it is better to be neither too hungry nor too thirsty than to be starving or dehydrated. Several longstanding empirical results support this view. First of all, in early conditioning experiments it was observed that a cue that is associated either simultaneously or successively with multiple assets -such as food and water -has greater value that an equivalent cue associated with only a single asset (or drive) <ref type="bibr" target="#b52">[53]</ref>. Finally, a preference for compromise decisions is also apparent from studies of so-called "decoy" effects in multi-attribute, multi-alternative choices. Faced with a choice between two iso-preferred items that have complementary costs and benefits (e.g. an expensive but high quality consumer product A, or a cheaper but lower quality product B, the introduction of a "decoy" that is higher in price/quality than A will bias preferences towards A, whereas an item that is lower in price/quality than B will bias choices towards B, as if participants had an intrinsic preference for the compromise solution <ref type="bibr" target="#b53">[54]</ref>. This preference is predicted by the goal equilibrium model by assuming that agents aspire to high quality and low price simultaneously, rendering the agent's indifference curve over options convex <ref type="figure" target="#fig_1">(Fig. 2b)</ref>. In natural environments we are faced with choices between offers of different quantity and quality, in variable states of need. In such scenarios, our model predicts that both goal difference (the relative level of two internal resources A and B) and offer difference (the relative quality of offer of A and B) will predict choices, <ref type="figure" target="#fig_1">Fig. 2c</ref>. This prediction was tested in a recent study in which participants made decisions that directly trade off multiple assets. Juechems <ref type="bibr" target="#b1">[2]</ref> asked participants to perform a task that involved managing a virtual zoo containing lions and elephants. On each trial they populated the zoo by choosing among offers of variable numbers of each animal, and the financial success of their zoo was proportional to the minimum number of either lions or elephants present on each trial (which was not directly observed), <ref type="figure" target="#fig_3">Fig. 4a</ref>. Each choice was classified according to whether it "redressed" the imbalance in cumulative lions and elephants and decisions to "redress" were predicted by both goal difference and offer difference, <ref type="figure" target="#fig_3">Fig. 4b</ref>. Critically, BOLD signals in the vmPFC signalled the level of redress incurred by a choice -the extent to which a choice brought the numbers of animals into equilibrium, but independent of the total animal numbers and thus of any overall financial return. Interestingly, the dACC coded for the level of imbalance among assets, i.e. the pressure to redress this imbalance, reminiscent of the "risk pressure" signal described above <ref type="bibr" target="#b46">[47]</ref>, <ref type="figure" target="#fig_3">Fig. 4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding Remarks</head><p>Our contribution here is twofold. Firstly, we highlight some challenges for RL as a computational theory of motivated behaviour in biological agents. The RL framework assumes that rewards originate in the environment, that they are sensed by dedicated input channels, that they are not fungible, and (typically) unidimensional. Secondly, we propose an alternative framework, "goal equilibrium theory" that treats reward as the by-product of computing distance to largely self-defined (intrinsic) goals. In doing so, we acknowledge a debt to theories of motivation past <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> and present <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>. Although our theory is preliminary, its readily accounts for commonly observed effects in human and animal behaviour, such as reward-maximization, reference-dependence, diminishing marginal utility, and convex indifference between goods. Empirical tests of the theory's wider predictions will require tasks (or field observations) that involve accumulation of multiple assets. Such tasks are rare in the literature concerning value-guided choices, and where multiple value dimensions inadvertently come into play, they are often even discarded as nuisance variables. Goal equilibrium theory may provide a framework in which to probe goal-directed behaviour using richer tasks which more closely emulate the fact that human (as well as animal) behaviour is driven by multiple, competing goals. How does the notion of a "value map" proposed here relate to theories proposing cognitive maps in other domains, such as spatial navigation and conceptual spaces?</p><p>Distance to goal depends on policy. How is the agent's policy factored into the goal distance computation?</p><p>What is the precise division of labour between the vmPFC and dACC in representing internal states and computing goal distance?</p><p>Goal realisation may be uncertain. How is the probability of goal attainment, i.e. the "feasibility function"</p><p>represented in neural circuits?</p><p>Can we capture a wide range of psychiatric disorders, such as addiction or mood disorders, using goal equilibrium theory by adjusting how much distinct goals compete? For example, could a lowered tendency to discard unrealistic goals account for sustained negative mood?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights -</head><p>The computational framework of Reinforcement Learning has allowed us to make great strides in understanding a wide range of motivated behaviours, as well as permitting us to build intelligent artificial agents -However, RL does not explain the dependence of value-guided choices on ongoing needs, affective states, or abstract goals.</p><p>-Recent neural evidence suggests that areas in the medial prefrontal cortex represent internal value states and distance to intrinsic goals in a way that is not readily explained by the RL framework -We propose that reward emerges from the more general mechanism of computing distance to intrinsic goals and that this framework can explain a range of behavioural and neural data from value-guided decision tasks in humans and other animals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A. Schematic representation of the RL framework. Top: a typical "bandit" task in which participants</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A. Schematic of Drive Reduction Theory. Behaviours are initiated to reduce drives, which in turn are provoked by needs that arise following homeostatic imbalance. B. Illustration of the Value Map, here in two</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Cortical areas on the medial surface correlating with diverse aspects of internal state and goal distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Neural signals for computing goal equilibrium. A. Experimental task used in Juechems et al., 2019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Box 3 .</head><label>3</label><figDesc>Outstanding questions.How do we form and learn about new goals during development and maturation, and how are these incorporated into the value map?</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Network for Computing Value Equilibrium in the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Juechems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Medial Prefrontal Cortex. Neuron</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="977" to="987" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decision theory, reinforcement learning, and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Affect Behav Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="53" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Goals and habits in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="337" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural substrate of prediction and reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5306</biblScope>
			<biblScope unit="page" from="1593" to="1602" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="562" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theory of human motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Maslow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="370" to="96" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anatomical substrates of orexin-dopamine interactions: lateral hypothalamic projections to the ventral tegmental area</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Deutch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neuronal Reward and Decision Signals: From Theories to Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiol Rev</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="853" to="951" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What is Intrinsic Motivation? A Typology of Computational Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intrinsic and Extrinsic Motivations: Classic Definitions andNew Directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Deci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human Orbitofrontal Cortex Represents a Cognitive Map of State Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Schuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1402" to="1412" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards a neuroscience of active sampling and curiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="758" to="770" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Learning for Reward Designto Improve Monte Carlo Tree Search in ATARI Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The free-energy principle: a unified brain theory?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="165" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active Inference, homeostatic regulation and adaptive behavioural control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pezzulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog Neurobiol</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Homeostatic reinforcement learning for integrating reward collection and physiological stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keramati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gutkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Elife 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motivation concepts in behavioral neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Berridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiol Behav</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="209" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Principles of Behavior: An Introduction to Behavior Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Hull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943" />
			<pubPlace>Appleton-Century-Croft</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reward effects of food via stomach fistula compared with those of food via mouth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comp Physiol Psychol</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="555" to="564" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative and Physiological Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="419" to="427" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The ecological rationality of state-dependent valuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="123" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shifts in deprivation level: different effects depending on the amount of preshift training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Mollenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">State-dependent learning and suboptimal choice: When starlings prefer long over short delays to food</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pompilio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="571" to="578" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="issue">5283</biblScope>
			<biblScope unit="page" from="1868" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Thirst-associated preoptic neurons encode an aversive motivational drive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="1149" to="1155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Thirst regulates motivated behavior through modulation of brainwide neural population dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frontal cortex and reward-guided learning and decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Rushworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1054" to="69" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for studying the neurobiology of value-based decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="545" to="56" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The neurobiology of decision: consensus and controversy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="733" to="778" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Decision, Uncertainty and the Brain: : The Science of Neuroeconomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>O'reilly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.7591</idno>
		<title level="m">Goal-Driven Cognition in the Brain: A Computational Framework</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Postdecision dissonance at post time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Inkster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="319" to="323" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1704" to="1715" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Consequences of Commitment to and Disengagement from Incentives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of the orbitofrontal cortex in the pursuit of happiness and more specific rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="issue">7202</biblScope>
			<biblScope unit="page" from="340" to="344" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orbitofrontal cortex as a cognitive map of task space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="79" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neuronal origins of choice variability in economic decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Padoa-Schioppa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1322" to="1358" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural mechanisms underlying contextual dependency of subjective values: converging evidence from monkeys and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abitbol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2308" to="2328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mood as Representation of Momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neuro-computational account of how mood fluctuations arise and affect decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vinckier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1708</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ventromedial Prefrontal Cortex Encodes a Latent Estimate of Cumulative Reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Juechems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="705" to="714" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural mechanisms of economic commitment in the human medial prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsetsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Elife 3</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Energy budgets, risk and foraging preferences in dark-eyed juncos (Junco hyemalis)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caraco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Ecol Sociobiol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="213" to="217" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A behavioral and neural evaluation of prospective decision-making under risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Symmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="14380" to="14389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiple neural mechanisms of decision making and their competition under changing risk pressure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balaguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="903" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Anterior cingulate: single neuronal signals related to degree of reward expectancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shidara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Richmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">5573</biblScope>
			<biblScope unit="page" from="1709" to="1720" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimal decision making and the anterior cingulate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kennerley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="940" to="947" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiobjective Reinforcement Learning: A Comprehensive Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="385" to="398" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neurons in the orbitofrontal cortex encode economic value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Padoa-Schioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Assad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="issue">7090</biblScope>
			<biblScope unit="page" from="223" to="229" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Secondary reinforcement and multiple drive reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Wike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barrientos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative and Physiological Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="640" to="643" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Choice Based on Reasons: The Case of Attraction and Compromise Effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simonson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="158" to="174" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
