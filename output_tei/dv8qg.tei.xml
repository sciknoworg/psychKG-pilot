<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning and choosing in an uncertain world: An investigation of the explore-exploit dilemma in static and dynamic environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-13">August 13, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
							<email>d.navarro@unsw.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
							<email>ben.newell@unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Schulze</surname></persName>
							<email>cschulze@mpib-berlin.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning and choosing in an uncertain world: An investigation of the explore-exploit dilemma in static and dynamic environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-13">August 13, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.cogpsych.2016.01.001</idno>
					<note type="submission">Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>decision making</term>
					<term>dynamic environments</term>
					<term>explore-exploit dilemma</term>
					<term>decisions from experience</term>
				</keywords>
			</textClass>
			<abstract>
				<p>How do people solve the explore-exploit trade-off in a changing environment? In this paper we present experimental evidence from an &quot;observe or bet&quot; task, in which people have to determine when to engage in information-seeking behavior and when to switch to reward-taking actions. In particular we focus on the comparison between people&apos;s behavior in a changing environment and their behavior in an unchanging one. Our experimental work is motivated by rational analysis of the problem that makes strong predictions about information search and reward seeking in static and changeable environments. Our results show a striking agreement between human behavior and the optimal policy, but also highlight a number of systematic differences. In particular, we find that while people often employ suboptimal strategies the first time they encounter the learning problem, most people are able to approximate the correct strategy after minimal experience. In order to describe both the manner in which people&apos;s choices are similar to but slightly different from an optimal standard, we introduce four process models for the observe or bet task and evaluate them as potential theories of human behavior.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A defining characteristic of decision making under uncertainty is that people lack definitive evidence to motivate their choices, due to ambiguity, insufficient expertise or missing information. In many laboratory tasks, people are simply given a partial specification of the problem and asked to make choices. In real life, however, people are often required to expend time and effort acquiring the information needed to guide their choices. This creates a dilemma: the actions that return the most information about the world are not necessarily the same as those actions the greatest immediate reward. This disconnect plays out in many real world scenarios:</p><p>• Workers on online marketplaces (e.g., Amazon Mechanical Turk) typically do not get paid until they complete a task, and cannot know their hourly wage until later when they are paid by their employer. The action that yields rewards is to do tasks, but the action that yields most information is to read reviews (e.g., on Turkopticon) of potential employers.</p><p>• Grant agencies have limited budgets with which to fund projects. The actions (monetary allocations) that yield greatest information are to employ officers to solicit grant applications, review progress reports, and check audit trails, but the ones that yield rewards are those that allocate money directly to projects.</p><p>• Manufacturing processes that generate a stream of outputs (e.g., factory productions lines) provide rewards for the company when those goods are sold to consumers, but only if the goods are not faulty.</p><p>Allocating resources to quality control processes (product testing) produces more information about the goods, but at the cost of taking resources away from the production line itself.</p><p>In these and many other scenarios, there is an inherent tension between selecting actions that maximize immediate rewards and actions that maximize immediate information gain. In an ideal world, a decision maker would not be forced to choose between information and reward, but this rarely occurs in practice. The decision maker either has resource constraints (e.g., funding agencies are often very short on money), time constraints (e.g., online workers cannot devote attention to two things at once) or physical constraints (e.g., quality control processes often require destructive tests -measuring tensile strength of a steel bar, for instance) that ensure that there is some trade off involved. In the long run, of course, information eventually works its way back to the decision maker: the online worker gets paid, the agency finds out which projects worked, the manufacturer learns which product lines had to be recalled. In the short term, however, this delayed feedback means that the decision maker must find some way to balance the search for information against the need to generate rewards. It is almost never a wise idea to forego all information-rich actions in the hope that when the rewards (and hence feedback) eventually arrive, one's reward-seeking actions will turn out to have been good ones. Our focus in this paper is on a laboratory task which shares some of the fundamental features of these situations. The task presents a very clear distinction between information that is obtained via observationbut is not associated with any immediate reward -and actions that can lead to rewards but for which only delayed feedback about the (non)occurrence of a reward is available. Specifically we examine the "observe or bet" task introduced by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref>, in which the decision maker has a number of options available, each of which may yield rewards or losses. On each trial, she may choose to observe the state of the world, in which case she gets to see what rewards each option provided, but receives no reward nor suffers any losses. Alternatively she may pick one of the options (i.e., bet on it) and receive the rewards/losses associated with that option at the end of the task. However, she receives no information at the time of the choice: the outcomes are hidden from her. By separating information from reward so cleanly, the task provides a very pure means by which to assay the explore-exploit dilemma (e.g., <ref type="bibr" target="#b26">Mehlhorn et al., 2015;</ref><ref type="bibr" target="#b5">Cohen et al., 2007;</ref><ref type="bibr" target="#b14">Hills et al., 2015)</ref>.</p><p>Moreover, we are interested in how people deal with this very stark form of an explore-exploit dilemma when there is some possibility that the world can change (cf. <ref type="bibr" target="#b19">Knox et al., 2011;</ref><ref type="bibr" target="#b10">Gureckis &amp; Love, 2009;</ref><ref type="bibr" target="#b6">Daw et al., 2006;</ref><ref type="bibr" target="#b52">Speekenbrink &amp; Konstantinidis, 2014</ref>. This combination of static and dynamic environments and a task in which exploration and exploitation are pure and clearly defined enables us to go beyond most existing work on more typical bandit problems. In standard tasks the actions that yield information and the actions that yield rewards are often conflated (e.g., <ref type="bibr" target="#b42">Robbins, 1952;</ref><ref type="bibr" target="#b54">Steyvers et al., 2009;</ref><ref type="bibr"></ref> though see <ref type="bibr" target="#b46">Sang et al., 2011</ref> for an exception) thus making it difficult to assess how people balance the relative value of these actions under assumptions about stable or changing environments. Our approach allows us to examine how environmental dynamics affect how people decide to start, stop and possibly restart searching for information. The observe or bet task is an example of a sequential decision making problem that has analogs in statistics <ref type="bibr" target="#b62">(Wald, 1947)</ref> and machine learning <ref type="bibr" target="#b17">(Kaelbling et al., 1998)</ref>. Typical discussions of the inference problem suggest that the behavior of a rational agent is remarkably simple. One should make a series of observations at the beginning of the process. Once the amount of evidence accrued reaches a critical threshold, the agent should shift whole heartedly to reward taking, picking the best observed option and betting on it with every subsequent action. The optimal strategy bears a very close resemblance to standard sequential sampling models of simple choice problems (e.g., <ref type="bibr" target="#b40">Ratcliff, 1978;</ref><ref type="bibr" target="#b60">Vickers, 1979;</ref><ref type="bibr" target="#b41">Ratcliff &amp; Smith, 2004)</ref>. In fact, because the "right" thing to do in these kinds of problems tends to be "explore first, then exploit", there are a number of decision making problems in which the explore-exploit problem is framed in terms of an "optimal stopping" problem in which the problem that the learner has to solve is to work out how much information is required before switching from exploration to exploitation (e.g. <ref type="bibr" target="#b21">Lee, 2006;</ref><ref type="bibr" target="#b57">Todd &amp; Miller, 1999)</ref>.</p><p>Nevertheless, human participants given an observe or bet problem frequently fail to adopt this strategy <ref type="bibr" target="#b58">(Tversky &amp; Edwards, 1966;</ref><ref type="bibr" target="#b39">Rakow et al., 2010)</ref>. Typically, they start collecting a critical mass of information as the rational strategy suggests, but then fail to fully commit to a decision. Although people start to place bets, mostly but not exclusively on a single preferred option, they frequently "switch back" into exploratory behavior, forgoing rewards to collect new observations quite late in the task. Similar results are found in related tasks <ref type="bibr" target="#b46">(Sang et al., 2011)</ref>. This violates a basic assumption of existing rational analyses of the task: if the decision maker really believed that more information might be useful, he or she should have collected it at the beginning when it would have been most useful.</p><p>Why do people do this? One possibility is simply that people are unable to understand or produce rewardmaximizing behavior, as suggested in early work on the problem <ref type="bibr" target="#b58">(Tversky &amp; Edwards, 1966)</ref>. However, another possibility is hinted at in the original data set collected by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref>: when people were told (falsely) that the reward distribution might change over time, they tended to make many more observations than when the reward distribution was expected to be static. Perhaps people's behavior in the static task reflects the fact that most real world problems are dynamic. Consider the three examples listed at the start of the paper. Online workers need to continually assess potential employers to see if their jobs still represent good value for effort. Grant agencies need to adjust funding policies in response to changing political environments and various other factors. The inputs to a production line can change over time (e.g., sources of raw materials shift), potentially requiring a change in the quality control process. In general, choices that were a good idea in the past may no longer be good ones in the future. Consequently, strategies that are optimal in an unchanging world may be highly maladaptive in a dynamic world. If people are accustomed to making choices in a dynamic environment and use strategies that are appropriate to such an environment, studying their behavior using static tasks may be highly misleading.</p><p>Human behavior in dynamic environments has been studied with respect to several problems, including bandit tasks <ref type="bibr" target="#b6">(Daw et al., 2006;</ref><ref type="bibr" target="#b52">Speekenbrink &amp; Konstantinidis, 2014</ref>, decision making biases <ref type="bibr" target="#b1">(Ayton &amp; Fischer, 2004)</ref>, prediction tasks <ref type="bibr" target="#b3">(Brown &amp; Steyvers, 2009;</ref><ref type="bibr" target="#b33">Osman &amp; Speekenbrink, 2012)</ref>, risky choice <ref type="bibr" target="#b38">(Rakow &amp; Miler, 2009)</ref>, sequential effects <ref type="bibr" target="#b63">(Yu &amp; Cohen, 2008)</ref>, probability matching behavior <ref type="bibr" target="#b9">(Green et al., 2010)</ref> and categorization <ref type="bibr" target="#b29">(Navarro et al., 2013)</ref>, but to our knowledge there are no studies investigating how genuinely dynamic environments affect how people approach problems -such as the observe or bet task -in which information-generating actions are separated from reward-generating ones. 1 Nevertheless, the relevance seems obvious. Perhaps people's tendency to alternate between information search and reward taking is a consequence of the fact that most real world problems are dynamic. Intuitively it seems plausible to think that people's tendency to shift back and forth between information seeking and reward taking is a sensible strategy in a dynamic environment, but there are no rational analyses of the dynamic task that could be used to generate more detailed predictions than the rather trivial suggestion that "more observation" might be worthwhile. Given the absence of any normative standards or empirical data in this area, our goal in this paper is to provide both. The structure of this paper is as follows. We first describe a rational analysis of the observe or bet task that produces optimal decision policies for the static version of the problem as well as the dynamic one. We then describe two experiments that explore how people make choices in both versions of the task, as well as how people learn to adjust their decision strategies across repeated experience in the task. We then introduce four process models for the observe or bet task that are capable of making trial by trial predictions about human behavior and discuss their connection to the rational analysis of the task. We evaluate the process models as candidate explanations of human behavior, and use them to shed light on how people learn from experience across a series of observe or bet tasks, as well as to characterize the different strategies that people use in different environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Optimal decision policies</head><p>To motivate the experimental work, it is useful to understand what kind of behavior is consistent with optimal or near optimal decision making in the observe or bet task. In this section we discuss the key features of such an analysis, since the complete specification of the ideal observer model is somewhat technical (it is discussed in detail in the Appendix). We focus on a 50-trial observe or bet task, in which the learner knows that they have a total of 50 actions (observations or bets) that they can take before the task ends. To solve this problem effectively, the learner must strike the right balance between information collection and reward taking. Suppose that on any given trial there are two possible outcomes, A and B. For instance, in the original work by <ref type="bibr" target="#b58">(Tversky &amp; Edwards, 1966)</ref> the two outcomes a learner could have observed were "left light turns on" and "right light turns on". At any given point in time, the learner has access to a set of observations x t = (x 1 , . . . , x t ) where x i denotes what was observed on trial i, which might be A, B or ∅ (i.e., nothing). For instance, if the learner had observed an A on the first and second trials, observed a B on the third trial, but had placed a bet on the fourth trial, then the set of observations available after four trials would be written x = (A, A, B, ∅). Using these data, the learner needs to infer θ t , the probability that the outcome on trial t will be A. More precisely, on trial t the learner has some belief state b t that captures all their knowledge and uncertainty about the true value of θ t . If the learner is a Bayesian reasoner, then this belief state is identical to the posterior distribution over θ t , so we define b t := P (θ t |x t ). In a static environment where the outcome probabilities do not change over time, we can drop the subscript on θ. In such an environment, it is always the case that "yesterday's posterior is today's prior", and Bayes' rule prescribes a belief updating mechanism that takes the following form:</p><formula xml:id="formula_0">P (θ|x t ) ∝ P (x t |θ)P (θ|x t−1 )<label>(1)</label></formula><p>When the world can change, the simple aphorism relating priors to posteriors no longer holds, and the belief updating prescribed by Bayes' rule now requires the learner to take account of the possibility that something has changed in between the last trial and the current one,</p><formula xml:id="formula_1">P (θ t |x t ) ∝ P (x t |θ) P (θ t |θ t−1 )P (θ t−1 |x t−1 ) dθ t−1<label>(2)</label></formula><p>and for the purposes of the current paper we assume that the learner relies on a simple dynamic theory in which the value of θ is redrawn from the prior with probability α, but otherwise remains the same. This change model is a little simplistic, but it makes a good approximation to the actual change mechanisms used in our experiment, and it also has the advantage of being consistent with other change detection models in the literature (e.g., <ref type="bibr" target="#b5">Cohen et al., 2007;</ref><ref type="bibr" target="#b63">Yu &amp; Cohen, 2008;</ref><ref type="bibr" target="#b3">Brown &amp; Steyvers, 2009</ref>). 2</p><p>The critical thing to recognize is that the Bayesian analysis discussed above is only one component to learning a decision strategy. All that the Bayesian analysis tells us is what an ideal learner should believe about the task at each point in time: it does not say what that learner should do. The posterior distribution P (θ t |x t ) describes the belief state b t but it does not provide an explicit guide to action. To compute the optimal decision strategy, we employ methods from the reinforcement learning literature, which allow us to assign a utility to each belief state using Bellman's equations (see <ref type="bibr" target="#b2">Bellman, 1957;</ref><ref type="bibr" target="#b18">Kaelbling et al., 1996)</ref>:</p><formula xml:id="formula_2">u(b t ) = r(b t ) + max at bt+1 u(b t+1 )P (b t+1 |a t , b t )<label>(3)</label></formula><p>where r(b t ) describes the reward that the learner expects to receive on trial t, and P (b t+1 |a t , b t ) describes the probability that the next belief state will be b t+1 if the learner takes action a t on the current trial. Once the solutions to Bellman's equations are calculated (via dynamic programming methods such as the value iteration algorithm; e.g., <ref type="bibr" target="#b18">Kaelbling et al., 1996)</ref>, we can read off the optimal decision strategy by assuming that the agent selects the action a * that maximizes the expected utility of the next belief state. This optimal decision strategy is an example of a Markov decision policy (MDP), so-called because it satisfies the Markov property in which the action depends on the (belief) state of the learner. For more details on the dynamic programming involved see the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">What is the optimal strategy?</head><p>As it turns out, the optimal decision strategies for the static and dynamic versions of the observe or bet task are fairly straightforward to characterize. Although the belief state b t describes a complete distribution over θ t , for the purposes of working out what action to undertake the learner does not really need all that information: in the static task, it turns out that the optimal action is almost perfectly described in terms of n a − n b , the difference between the number of As and the number of Bs that the learner has observed so far. If this magnitude difference score exceeds a certain "evidence threshold", then the optimal action is to bet on whichever outcome has been more frequently observed. If the difference does not reach the threshold, then another observation should be made. The implications of this decision strategy are spelled out in <ref type="figure">Figure 1a</ref>, which plots the thresholds for a 50 trial observe or bet problem as a function of the trial number. At the Figure 1: Optimal decision policies for a static 50-trial observe or bet task (panel a) and a dynamic one with a change rate of 5% (panel b). The trial number is plotted on the x-axis, and the amount of evidence (number of As minus number of Bs) on the y-axis. The solid lines plot the decision thresholds, i.e. the amount of evidence required to justify betting. The dotted lines show the actual behavior of an agent following the policy. In a static task (panel a) the optimal decision policy leads to a pattern in which the learner observes until the evidence threshold is reached, and then bets thereafter. Black and white dots denote trials where the learner observed outcome A or B respectively, and grey dots corresponds to trials where the agent bets. When the learner believes the world can change (panel b), the evidence value actually corresponds to the estimated difference between the number of As and Bs since the last change, and it decreases over time unless observations are made. As a consequence the agent continues to make occasional observations later in the task.</p><p>start of the task, the optimal threshold implies that the learner needs to have observed 4 more As than Bs (or vice versa) to justify making a bet. However, the threshold value declines over time. On the final trial of the task it is never better to observe than to bet, as there is little point in collecting new information when the experiment is about to end. <ref type="bibr">3</ref> In practice, the behavior of an optimal agent in the static task agrees with the description given by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref>: the agent makes observations on every trial until the difference between the two outcomes exceeds a threshold, and then bets on the more frequently observed outcome for all subsequent actions. This behavior is illustrated by the dashed line in <ref type="figure">Figure 1a</ref>: each black dot denotes a trial on which the learner observed outcome A and each white denotes a trial on which outcome B was observed. Grey dots depict trials on which the learner chose to bet. At the start of the task, the agent makes a sequence of observations, and as a consequence the amount of evidence accrued jumps up and down as a function of the outcomes that the learner experiences. Eventually, the observed evidence level jumps above the threshold (solid black line) and the agent starts to bet. If the learner believes that the world is truly static, then placing a bet introduces no change in the learner's belief state, so the evidence accrued remains flat and the learner continues to bet for the rest of the task.</p><p>The optimal decision strategy for a learner who believes the world can change is similar in some respects, but strikingly different in others. Firstly, when the world can change the evidence is not characterized in terms of the total number of As and Bs observed since the start of the experiment. Rather, the critical quantity is the number of As and Bs observed since the last time the world changed. However, since the learner does not know precisely when the world changed, the actual quantity of interest is the learner's best guess as to how many As and Bs have been observed since the last change. More formally, the relevant quantity is the expected number of As and Bs observed since the unknown change point: the Appendix discusses how we compute this. The important consequence of this difference is that the evidentiary value of older observations declines over time: old data should only be included in the evidence tallies if the world has not changed since those observations were made. This is illustrated in <ref type="figure">Figure 1b</ref>. As before, the model begins by making a series of observations until a critical threshold is reached and then it starts betting, always placing bets on whichever outcome it believes to have been the more frequent one. However, because it receives no feedback during the betting phase the evidence total decreases, pushing the model's "confidence" back below the threshold. At that point the model starts to make observations again until the confidence gets pushed back above threshold. As <ref type="figure">Figure 1b</ref> illustrates, this means that the model starts the task by making a long run of observations, but then it tends to switch into a phase of alternating between runs of bets and runs of observations, always seeking to keep its confidence above the required level. Finally, there is a tendency for the task to end with a long run of bets, caused by the fact that the thresholds themselves start to collapse back towards zero right at the end of the task.</p><p>One additional difference between the static model and the dynamic model is worth highlighting: the evidence thresholds shown in the two panels of <ref type="figure">Figure 1</ref> are not identical. The thresholds in the dynamic world are lower than in the static one. This difference occurs for a theoretically important reason. In a static environment, it is perfectly sensible to require a lot of evidence to justify your decision making: the rewards will still be there when you switch from information collection to reward taking, so it does not hurt to be patient. In a dynamic world, you need to "strike while the iron is hot" because the world can change on you. A learner who patiently collects lots of evidence in a dynamic world can easily lose out because the world changes before he or she has the opportunity to take the rewards. This effect is illustrated in <ref type="figure">Figure 1</ref>. In the initial phase of observations, the dynamic model begins to bet after only seeing three observations. The static model sees the same three observations, but does not start betting because it requires more evidence to justify betting. As it turns out, the static model waits until eight observations have been made before it begins to bet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deriving experimental predictions using the optimal policy</head><p>Finding the optimal policy even for a simple task like the 50 trial observe or bet task is computationally expensive, and it seems highly unlikely that human participants are able to do so. However, this does not preclude the possibility that people are able to reason about the task sufficiently well to be able to devise a strategy that approximates it fairly well. Given this, it is useful to give careful consideration to the qualitative differences between what the model predicts people should do in the static task, and what it predicts they should do in the dynamic task. If people really are sensitive to the possibility that the world might change, then at a minimum we should expect people's strategies to shift in a manner that qualitatively agrees with the shifts made by an ideal observer when the reward structure is dynamic. Figure 2: Panel a: Results of simulating the optimal decision policy in a static world (solid lines) and a dynamic world (dashed lines). Each plot was constructed from 5000 simulated experiments, and shows the proportion of trials on which the agent makes an observation, broken down by trial number. For the static world, the true change rate was always fixed at 0%. For the dynamic world, the change rate varied randomly between 2% and 18%. Panel b plots the difference between the two curves, highlighting the qualitatively important prediction: observation rates in the dynamic world should be substantially higher late in the task, but slightly lower early on.</p><p>With that in mind, we ran simulations using the two versions of the model. For the static model, we generated 5000 random outcome sequences, each consisting of 50 trials, in which the true value of θ (i.e., the probability that the outcome will be A) was 0.75 and applied the MDP model to that sequence. For the dynamic model the simulation was similar, except that for each sequence we also selected a random change rate parameter α (between 2% and 18%) for the model and for the data sequence. 4 These simulations allowed us to count how often the model chose to make an observation, broken down by trial number. The results are shown in <ref type="figure">Figure 2a</ref>. In a static environment (solid line), the observation rate drops sharply over the first few trials, and then continues to gradually decline towards zero throughout the task. In the dynamic version of the simulation (dashed line) the initial decline is even sharper: as suggested above, the optimal policy in the dynamic task genuinely does favor an earlier shift from observation to betting. However, because the dynamic task also motivates the decision maker to shift back and forth between observation and betting, the overall observation rates plateau through the middle phase of the task. It is only over the last few trials that the model shifts to a pure betting strategy, because the approaching deadline removes much of the motivation for continued information search.</p><p>An important point to recognize is that, while the simulations rely on specific parameter choices, the qualitative characteristics of the curves in <ref type="figure">Figure 2a</ref> are more or less invariant: they emerge as a natural consequence of the solution to the decision problem as illustrated in <ref type="figure">Figure 1</ref>. Given this, while we might not expect human performance to precisely match the observation curves in <ref type="figure">Figure 2a</ref>, we should expect that differences between the two versions of the task to be robust. Specifically, if we plot the differences between the two curves, as shown in <ref type="figure">Figure 2b</ref>, we obtain a very striking empirical prediction about how human performance should differ in the two versions of the task: for the first few trials of the experiment, there should be a widening gap between the two conditions in which people actually observe more in the static task than the dynamic task. This difference should then rapidly reverse, and for most of the task there should be more observation in the dynamic condition. Finally, the difference tails away towards the end of the experiment. The difference curve in <ref type="figure">Figure 2b</ref> represents a much stronger prediction than the obvious intuition that "people should explore more in a dynamic environment". Each aspect of this prediction ties to fundamental characteristics of the MDP analysis: the initial part of the curve reflects the need to strike while the iron is hot in a changing world, the middle part reflects the need to constantly monitor a changing world, and the tailing away at the end reflects the fact that the oncoming deadline removes the value of exploratory action in both environments. Given the quite unusual shape of the curve in <ref type="figure">Figure 2b</ref> and its tight links to the underlying rational analysis of the task, this represents a strong prediction. In the next section, we present an experiment designed to test this prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment 1</head><p>3.1. Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Participants</head><p>Data from 614 USA-based participants recruited via Amazon Mechanical Turk are reported: 56 participants were excluded for not making a single observation at any point in the experiment. A total of 246 participants self-identified as female, 363 as male, and 4 selected other/undisclosed. The average reported age was 33.2 years (sd = 15.8). The experiment took approximately 10 minutes, and all participants were paid $US0.60 regardless of their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Procedure</head><p>The experiment was delivered online via a dedicated website. At the start of the experiment, participants were told they were taking part in a guessing game that involved "blox machines". The image of a blox machine showed two lights, one red and one blue. The instructions explained that on each trial of the experiment one of the two lights would turn on, but they would not necessarily be shown which light turned on. Each blox machine has a bias, explained using the following text:</p><p>The lights turn on and off in an irregular way, but with a bias. Some blox machines are biased to turn on the blue light more often, and others are biased to turn the red light more often.</p><p>Participants were randomly assigned either to a static condition or a dynamic condition. In the dynamic condition, it was explained that the bias for any given machine is not fixed:</p><p>A blox that used to be biased towards red can sometimes change its bias to blue, and vice versa. This doesn't happen all the time, but you should expect to see it happen a few times during this task.</p><p>No additional details were given to participants about how strong the bias is, nor were participants in the dynamic condition given any other information about how often changes might occur. <ref type="table">Table 1</ref>: Example sequences used in the experiment. This particular pair of sequences corresponds to Game 2, Version 1. The two sequences are identical up to the change point (marked with the vertical line), which occurs on trial 17 in this instance. Prior to the change point, Option A (black dots) occurs more frequently. After the change point, in the static condition this trend continues, with black remaining the more common outcome. In the dynamic condition, all outcomes after the change point are reversed, so Option B (white dots) is the better choice after trial 17. Participants were not told the specific nature of these changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static:</head><p>•</p><formula xml:id="formula_3">••••••••••••••• | •••••••••••••••••••••••••••••••••• Dynamic: •••••••••••••••• | ••••••••••••••••••••••••••••••••••</formula><p>The instructions then explained that on each trial they could choose one of three actions: "guess blue", "guess red", or "observe", each corresponding to distinct buttons on the screen. 5 If they observed, they would be shown which one of the two lights turned on, but would forgo any opportunity to receive any points. If they guessed, the experiment would indicate that their guess had been recorded, but did not give any indication as to whether the guess was correct. The instructions explained that each correct guess would earn 1 point, and each incorrect guess would lose 1 point, and that the object of the game was to maximize the number of points gained across a total of 50 trials (the trial number was shown on screen). They were told that they would play a series of 5 such games, each with a different blox machine. To capture the "delayed feedback" idea, participants received feedback about their performance between games. The feedback given was quite detailed: at the end of each game participants were shown a graphical representation showing exactly which action they took on all 50 trials, as well as what the blox machine did on each trial. It showed which trials they won points for and which they lost points for, as well as giving them a final score. At the end of the experiment participants were given a completion code to claim payment via Amazon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Materials</head><p>The sequence of outcomes produced by the blox machine was fixed ahead of time. There were 2 betweensubject conditions (static and dynamic), 3 between subject "versions" (discussed below) and 5 within-subject games, yielding a total of 30 distinct outcomes sequences listed in the Appendix. For the static condition the sequences were generated pseudo-randomly. Across the 15 sequences the average proportion of high frequency outcomes was 78%. Red was the high frequency outcome for 7 of the sequences, and blue for the other 8.</p><p>The stimulus sequences in the dynamic condition were constructed from the static sequences via the following procedure. A "change point" was selected (constrained to lie between trial 15 and trial 35): all outcomes prior to the change point were exactly identical to their counterpart in the static sequence, and all outcomes after the change point were reversed. An example of a sequence produced in this fashion is shown in <ref type="table">Table 1</ref>. For all three versions of the dynamic task, 4 of the 5 sequences were altered in this fashion, and one was left unchanged: given 4 changes across 250 trials, the empirical change rate in the task was 1.6%. Across the 12 altered sequences, the average rate of the high frequency outcome prior to the change points was 75%. After the change points, the rate of the now low frequency outcome fell to 21%.</p><p>The location of the change point in every game is listed in <ref type="table">Table 2</ref>. As the table makes clear, participants in Version 1 of the dynamic condition did not see any changes during Game 1, whereas in Version 2 it was Game 3 that did not contain a change point, and in Version 3 it was Game 5. In these cases the stimulus sequence was identical in the static and dynamic versions of the task, allowing us to examine the "pure" effect of expectations about change independent of any effects of the stimulus sequence itself. <ref type="table">Table 2</ref>: Trial number upon which the change occurred, in all three versions of the dynamic condition. Note that in all three versions there is one game in which no change actually occurred. In these cases the sequence of outcomes in the dynamic condition was identical to the sequence used in the static condition. Throughout the results, we refer to Option A as the high frequency outcome in a static sequence, regardless of whether that corresponds to the red or blue light. In the dynamic condition Option A refers to the option that is initially high frequency, but later (in 4 of 5 cases) becomes low frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Observation behavior at the aggregate level</head><p>The proportion of participants making observations on each trial of the experiment is shown in <ref type="figure">Figure 3</ref>, broken down by condition and game number and averaged across the three different versions. The results follow the same basic profile predicted on the basis of the optimal model shown in <ref type="figure">Figure 2a</ref>. In both conditions, people almost always observe on the first trial (88% for the static condition, 84% for the dynamic condition). In the static condition the observation rate consistently declines throughout the task, and by the final trial the rate of observation has fallen to 3%. In the dynamic condition, however, the observation rate stabilizes throughout the middle portion of the task. From trials 15 to 35, the average observation rate was 23%, and displayed almost no variation over that time frame: the highest observation rate over that range was only 27% (on trial 21) and the lowest was 20% (trial 35). However, with the end of task acting as a deadline that limits the value of new observations even in the dynamic condition, the observation rate declined over the last few trials of the experiment, with observation rates on the final trial being 12%.</p><p>As noted in the introduction, the MDP analysis of the observe or bet task makes quite strong predictions about the differences between the two conditions. It does not merely predict higher observations rates in the dynamic condition. Rather, if the demands of different environments are genuinely placing a strong constraint on people's decision making strategies in the task, then the difference between the observation rates across conditions should have the same qualitative character as the curve shown in <ref type="figure">Figure 2b</ref>. We formalize the prediction in <ref type="figure">Figure 2b</ref> using a three segment Bayesian regression model implemented in JAGS <ref type="bibr" target="#b36">(Plummer, 2003)</ref>: the first and second segments are linear, while the third is quadratic. The edges of the different segments are treated as latent variables, as are the regression coefficients themselves. The formal specification of the data analysis model is given in the Appendix.</p><p>Applying this model to the difference data shows a high degree of agreement. The model fits are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The solid line shows the estimate of the regression function, the shaded regions depict Bayesian 95% credible intervals for the difference at each trial, and the dotted lines show 95% credible intervals for the raw data. The difference between conditions very closely agrees with the qualitative prediction made by the MDP analysis, especially for Games 2-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">The effect of expectations about change</head><p>For the most part, the differences in <ref type="figure">Figure 3</ref> reflect both the participant strategy choices and the interaction between those strategies and the environment. A more direct measure of how people's expectations influence behavior can be obtained by looking at those cases where the static and dynamic conditions used identical sequences: Game 1 of Version 1, Game 3 of Version 2 and Game 5 of Version 3. The observation Game 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Game 2</head><p>Game 3 Game 4 Game 5 Trial Number Proportion of Observe Actions <ref type="figure">Figure 3</ref>: Average proportion of observe actions, broken down by trial number, game number and condition. In all cases, participants observe with high probability during the first few trials. Consistent with the qualitative predictions made by the rational analysis, in the static condition the observation rate continues to decline to near zero, whereas in the dynamic condition it tends to plateau at a constant level until near the very end of the task. profiles for these cases are plotted in <ref type="figure">Figure 5</ref>, and the differences between conditions and corresponding regression analyses are plotted in <ref type="figure">Figure 6</ref>. As is clear from inspection of these plots, the effect that we observe on Games 3 and 5 is essentially unchanged from the corresponding plots in <ref type="figure" target="#fig_3">Figures 3 and 4</ref>. Once people have some familiarity with the task the different observation profiles are heavily driven by their beliefs about the changeability of the environment. Looking at the profiles for Game 1 in Figures 5 and 6 reveals a subtly different story. In this case participants have no prior experience with the task, no knowledge about when the changes might occur or what form they might take (in the dynamic condition), and as it turns out the actual sequence of observations contained no changes at all regardless of condition. The only difference in this instance is the cover story. The results show a greatly attenuated version of the effect: in the dynamic condition people do make more "late" observations, but there is no equivalent tendency to make more observations in the static condition early in the task. That being said, it is difficult to know how to interpret this pattern: although we included this case in order to have a direct analog of the dynamic condition from <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref>, it is a somewhat odd condition to consider at all, because it is somewhat deceptive. In the dynamic condition participants were told to expect changes, yet in the very first task they undertook there were none to be found.</p><formula xml:id="formula_4">q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Observation strategies at the individual subject level</head><p>A second way to examine model predictions is to consider the behavioral patterns produced by the MDP analysis at an individual subjects level when the task is assumed to be static (i.e., <ref type="figure">Figure 1a</ref>) versus when it is assumed to be dynamic (i.e., <ref type="figure">Figure 1b</ref>). In the static environment, the optimal strategy for any given instance of the observe or bet task is to have all observations precede all bets. We refer to this as the strict "front loading" strategy, in which all observations come first. The smooth curves shown at the aggregate level arise primarily because of variability in the specific sequence of outcomes. In the dynamic environment, this pattern does not hold. Instead, it is expected that observations be distributed throughout the task even at the individual subject level.</p><p>To what extent is this prediction satisfied by human participants? <ref type="figure">Figure 7</ref> plots the proportion of subjects who follow the strict front loading strategy, broken down by condition and game number. When the environment is dynamic, almost no-one uses this strategy: the proportion of front loading ranges from 5% to 10%, with no discernible pattern across games. The static conditions show a marked contrast: even on Game 1, the proportion of subjects who strictly front load the observations is 24%. As participants become familiar with the task, this proportion rises rapidly, so that by Game 5 fully 62% of participants are behaving exactly in accordance with the strict front loading strategy.</p><p>The analysis depicted in <ref type="figure">Figure 7</ref> adopts a very strict definition of the front loading strategy. An alternative view is to characterize the observation behavior in terms of the proportion of observations that occur during the first half of the task. Under a front loading strategy, this quantity is expected to be fairly close to 100%. This weaker criterion is almost always satisfied in the static condition: the median value for the first observe or bet game is 87.5%, and is 100% for all subsequent games. Compare this to the corresponding numbers for the dynamic condition, in which participants spread their observations throughout the task. The median proportion of first half observation did not change much across games, ranging from 57% to 60%, and (apart from a small group of subjects who front loaded all their observations) did not vary greatly among individuals. In almost all cases, people made almost as many observations in the second half of the task as they did in the first half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Betting strategies</head><p>Up to this point we have focused the discussion on how people choose to allocate their observations, because this is where the MDP analysis makes critical predictions. Nevertheless, it is also instructive to Trial Number</p><formula xml:id="formula_5">Game 1, Version 1 Game 3, Version 2 Game 5, Version 3 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.</formula><p>Proportion of Observe Actions <ref type="figure">Figure 5</ref>: The effect of expectations about change. Human observation rates in those instances when no changes occurred in the dynamic condition, and thus the stimulus sequence was identical in both the static and dynamic conditions. The pattern of results is consistent with the overall pattern in <ref type="figure">Figure 3</ref>, illustrating that people's expectation that changes can occur are sufficient to drive a difference in performance. consider the strategies that people use to make bets. To give an illustrative example, <ref type="figure">Figure 8</ref> plots the proportion of subjects choosing to bet, plotted across all 50 trials for one specific outcome sequence (Game 2, Version 1). In the static condition, the proportion of subjects betting on the better option (Option A) starts at zero and then rises rapidly, levelling out at about 80% of all actions. A similar pattern is seen for Option B, but it levels out at about 15% of actions taken. The remaining 5% of actions are observations. In the dynamic condition, a similar pattern is observed at the beginning, but after the change occurs on trial 17, the betting proportions shift, and people begin to allocate more bets to Option B. Although this figure only plots the data from a single outcome sequence, it is a very typical one. <ref type="bibr">6</ref> Examining the responses for the static condition suggests a choice pattern much in line with frequent findings from typical probability learning tasks, and other investigations of the observe or bet task: with some prior task experience (Game 2 and beyond), people over-match when allocating their bets (e.g., <ref type="bibr" target="#b48">Shanks et al., 2002;</ref><ref type="bibr" target="#b39">Rakow et al., 2010</ref>). If we consider only those trials on which participants placed a bet (i.e., ignore observations), <ref type="figure">Figure 8</ref> implies that by the end of the task participants were allocating about 85% of their bets on Option A. 7 Given that the proportion of Option A outcomes in this particular sequence is <ref type="bibr">6</ref> To draw the curves plotted in <ref type="figure">Figure 8</ref> we estimated a simple exponential rise to asymptote function for the static condition. The formula for the curve is y(t) = a(1 − e −bt ) where a is the asymptote and b is the rate of change. For the dynamic condition we estimated a two-stage model. Prior to the change point c, we again use the function y(t) = a 1 (1 − e −b 1 t ). For trials t &gt; c after the change point, the curve is an exponential shift-to-asymptote but with a new asymptote and new rate, giving y(t) = y(c) + (a 2 − y(c))(1 − e −b 2 (t−c) ) as the function. There is no theoretical reason for these choices, and other possibilities (e.g., logistic) exist. Moreover, unlike the observation profiles in <ref type="figure" target="#fig_3">Figures 4 and 6</ref> we did not have any genuinely a priori hypotheses about the form of the betting curves, and as such we felt it was inappropriate to convert our post hoc curve fitting exercise into a fully-fledged regression model. We fit these curves to the data solely for the purpose of improving the readability of the figure, not to make a substantive claim about human cognition, though we note in passing that this family of curves seems to work quite well across most games and versions.  <ref type="figure">Figure 8</ref>: Proportion of people betting on the different options for a specific stimulus sequence (Game 2, Version 1: see <ref type="table">Table 1</ref>). In both plots, "Option A" refers to the more-probable outcome on trial 1. For the dynamic condition, Option A is the better option for the first 17 trials, and Option B is better for the rest of the task. The pattern of bets shown here is grossly typical of all games and sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static Condition</head><formula xml:id="formula_6">Dynamic Condition q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q</formula><p>only 75%, it would appear that participants did not follow a pure probability matching strategy either (see <ref type="bibr" target="#b61">Vulkan, 2000</ref>, for a review). In other words, while participants did not strictly maximize when placing their bets, they adopted betting strategies that outperformed a simple matching heuristic. However, when we consider the dynamic condition, it is far less clear from this "two part" curve what we should conclude about whether participants were probability matching. Once again, however, we can turn to the MDP analysis presented in the introduction to assist in understanding participant behavior. As illustrated in <ref type="figure">Figure 1</ref>, the critical characteristic of the betting strategy adopted by a purely rational agent in this task is internal consistency within a run of bets. That is, if a participant makes a sequence of bets with no observations in between them, a rational agent should place all of those bets on the same option. An optimal decision policy for this task does allow the agent to switch between options, but the MDP model predicts that this can only happen when the agent makes new observations and updates his or her beliefs accordingly. An example of this is shown in <ref type="figure">Figure 1b</ref>: the first three runs of bets made by the MDP model all occur because the evidence has pushed the model's "confidence" above the threshold, and so all bets in those runs are all on the same option. In contrast, the fourth run of bets occurs when new observations push the model to believe the other option is superior, and so all bets within that run are placed on that option. At a qualitative level, this kind of belief change is what underpins the empirical reversal shown on the right hand side of <ref type="figure">Figure 8</ref>: that is, in a dynamic world participants use their observations to guide belief changes and therefore reversals in their betting strategies. However, the MDP analysis does not merely predict that people will reverse their betting strategies when the world changes: it also makes the prediction that every run of bets should be placed on the same option.</p><p>Not surprisingly, only a minority of participants adopt betting strategies that are perfectly consistent with this strict internal consistency requirement. Strict internal consistency was satisfied only in 28% of games in the static condition, and in 21% of games in the dynamic condition. To get a finer grained perspective on the individual behavior, we can disaggregate the data further and look at individual betting runs. When we do so it turns out that there is a remarkably high proportion of consistent runs. <ref type="figure" target="#fig_6">Figure 9</ref> plots the number of consistent betting runs (black bars) against the total number of runs (grey bars), broken down by run length and condition. Irrespective of how long the run of bets is and what condition we consider, it is clear that the vast majority of betting runs are in fact perfectly consistent. To give a sense of just how unlikely this is, the white error bars plot the 95% credible interval for the number of consistent runs we would expect if people adopted a probability matching strategy within betting runs. Given the massive overrepresentation of consistent betting runs at almost every run length, it seems safe to conclude that, at a bare minimum, participants betting behavior reflected some form of overmatching strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>The manipulation produced almost exactly the predicted effect with regards to observation profiles. A surprising proportion of people follow a strict front-loading strategy for their observations in the static condition, while almost nobody does in the dynamic condition. The betting strategies are less straightforward to characterize, and only a minority of participants match the strict internal consistency criterion. Nevertheless, at the level of individual betting runs there is a remarkably high proportion of perfectly consistent betting streaks regardless of how long the run is, a pattern that is grossly inconsistent with the usual probability matching effect. Thus, while not strictly optimal in allocating bets, people were very clearly overmatching to the sequence.</p><p>A critical point to make about all of these results is that none of them make any sense if (for instance) the tendency to make occasional non-front loaded observations when given static sequences is solely due to cognitive limitations. For instance, if the reason such observations are made is due to memory limitations, 8 why should they become more common in the dynamic condition? If people were not sensitive to some notion of what counts as an optimal decision policy, why do we see the proportion of observations decline sharply in the last few trials of the task? Perhaps most remarkably, if participants were not highly sensitive to the value of evidence in different environments, why do they observe less often in the first few trials of the dynamic condition and more often thereafter. The MDP analysis explains the asymmetry for the first few trials quite elegantly: formally, the effect is caused by the subtle lowering of the thresholds shown in <ref type="figure">Figure 1</ref> when the change rate is increased. But the deeper reason for why this threshold shift occurs is the "strike while the iron is hot" effect discussed earlier: if the world can change on you, it is a bad idea to collect too much evidence because it might change before you have the opportunity to exploit the knowledge it provides. Later on, the difference between conditions reflects the obvious "keep your eyes peeled" strategy: if the world can change on you, it is occasionally necessary to shift back to exploratory behavior even after the initial explore to exploit shift has occurred, because when the world changes a previously good strategy can become bad.</p><p>One particularly interesting finding with regards to observation strategies is that people did not properly front-load their observations in the first game in the static condition, as one might expect if they were adopting a near optimal strategy (cf. <ref type="bibr" target="#b39">Rakow et al., 2010;</ref><ref type="bibr" target="#b58">Tversky &amp; Edwards, 1966</ref>). On the one hand, the steep rise in front loading rates shown in <ref type="figure">Figure 7</ref> suggests that people can and do learn the optimal strategy (or something very much like it) from experience, to the point that by the end of a 10 minute task almost two-thirds of participants followed a strict front loading strategy. On the other hand, this raises the question of why only about a fifth of participants were able to do so on the first attempt.</p><p>Two possibilities suggest themselves. The first possibility is distrust. If participants did not completely trust the cover story presented to them, then it would be foolish to commit to a strict front loading strategy on the first game. More formally, an agent who is uncertain about whether the world can change is obligated to act as if change is a real possibility. In the observe or bet task this means that the agent needs to make observations later in the task in order to infer whether the world can change. Given the number of psychology experiments that involve deception, and (more importantly) the number of real life scenarios when the reality does not match the textbook description, one might forgive participants for bringing a certain degree of skepticism to the task.</p><p>The second possibility is that participants require trial-by-trial experience in the task before they can fully appreciate the implications of the described problem. That is, in order to hit upon the right strategy (front loading) they need to experience the differential effect of distributing versus concentrating observation trials (e.g., <ref type="bibr" target="#b32">Newell &amp; Rakow, 2007;</ref><ref type="bibr" target="#b30">Newell et al., 2013)</ref>. To a statistically trained reasoner who finds themselves in the static condition, it might be perfectly obvious on the basis of the simple verbal description we provided that the output of the blox machine can be characterized as a Bernoulli sequence with unknown success probability, 9 and such a reasoner might indeed be able to infer that observations should precede actions in all cases when this assumption holds. However, this strikes us as a very abstract form of reasoning, and it is not at all clear that we should expect people to be able to reason optimally about a task with which they have no direct experience.</p><p>So which is it? Do people require direct experience of the observe or bet task in order to construct an approximately optimal strategy, or do they need to be convinced to trust the experimenter in order to be willing to adopt it? It is this question that motivates Experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment 2</head><p>To tease the two candidate explanations apart, we created a version of the task designed to inspire as much trust as we possibly could. In Experiment 2, we ran the study in the lab, using a physical deck of playing cards to generate random outcomes instead of using a computer to generate (easily faked) outcomes. The task allowed participants to verify that the deck of cards was composed either of 70% red cards or 70% black cards, and to verify that the decks were properly shuffled, but did not allow them to know whether their deck was mostly red or mostly black. Given this, the participants should have no real reason to distrust the scenario, and so if distrust is the explanation, the front loading strategy should be adopted immediately. In contrast, if a lack of trial by trial experience is the explanation, participants would not be expected to front load their observations when doing the task, but they should be expected to realize at the end of the task that their strategy was incorrect.</p><p>4.1. Method 4.1.1. Participants 30 undergraduate students (16 female, 14 male) from the University of New South Wales participated in this experiment in exchange for course credit. Additionally, participants could earn a performance based monetary payoff. The average age of participants was 18.8 (sd = 1.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Materials and Procedure</head><p>In order to maximize trust in the random nature of the sequence, the following procedure was used. Participants were seated at a table across from the experimenter and presented with two decks of 100 cards each, one consisting of only red cards (hearts), the other of only black cards (spades). Participants were given a chance to verify the composition of both card decks before the experimenter visibly exchanged 30 cards between the decks, so that each deck was biased towards either red or black to the same magnitude (70:30). Both decks were then thoroughly randomized using a shuffling machine and placed in transparent containers. These containers were then placed in a larger box. The experimenter shuffled this box with both containers, before the participant randomly picked one of the containers and hence the deck it contained to play with in the guessing game.</p><p>After a deck was selected, it was placed face down on a table and the card guessing game commenced. On each trial participants could choose to either observe the top card in the deck (but receive no money) or to guess the color of that card without being shown what it was. With each choice, one card was moved forward, either face down or up, depending on whether the participant had opted to observe that particular card. Cards were not replaced back in to the deck. 10 At the end of the game, participants were informed Bernoulli sequences, though admittedly the deviations are small. The larger point remains though: real life is far messier than any description of it might imply.</p><p>10 The sampling without replacement scheme used in Experiment 2 differs from the sampling with replacement scheme in Experiment 1, but while that does have some implications for computing optimal strategies the differences are minor unless the agent makes a very large number of observations. about their guessing success and payoffs. To help ensure participants were motivated, they were paid on the basis of performance in addition to receiving course credit. For each correct guess, they received AUD$0.10; for each incorrect guess, a loss of AUD$0.10 was incurred. Negative total scores resulted in no payoffs. <ref type="bibr">11</ref> Following the card guessing game, participants were asked to complete a short questionnaire querying their knowledge of the card deck composition and their strategy use during the game. In particular, it asked them to consider four hypothetical choice strategies for the card game: (a) never observe, always guess a color; (b) observe for a few trials at the start of the game; then always guess a colour for the rest of the game; (c) going back and forth between observing and guessing throughout the entire game; and (d) never guess, always observe the card. They were asked to indicate which of these strategies best describes what they did, which one they think would have made the most money, and which one they would follow if they were given the opportunity to do the task again (cf. Koehler &amp; James, 2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The raw data for the experiment are plotted in <ref type="figure" target="#fig_8">Figure 10</ref>, and are consistent with the results for Game 1 of the static condition in Experiment 1, though the data are understandably noisier given the much smaller sample size. Only 5 of the 29 participants (17%) followed the strict front loading strategy, similar to the 24% of participants who did so in Game 1 of the static condition earlier. The median proportion of first half observations was 80.6%, which is broadly consistent with the 87.5% figure from the previous experiment, though the two are not perfectly comparable. Given the different number of trials (100 versus 50) and the fact that a big "spike" of observations is expected right at the beginning, the slightly lower proportion in Experiment 2 makes sense. Similarly, the betting proportions shown in <ref type="figure" target="#fig_8">Figure 10</ref> are broadly similar to those shown in the left hand panel of <ref type="figure">Figure 8</ref>. Overall, the pattern of behavior during the task itself does not show any substantial differences from the previous experiment, which provides suggestive evidence that the measures taken to increase participants' trust in the static world assumption made no difference to performance.</p><p>The strongest evidence comes from the post-task questionnaire, the results of which are plotted in <ref type="figure">Figure 11</ref>. Although only 5 of 30 participants (17%) front loaded all observations in the task, there were 12 participants (40%) who indicated that the front loading strategy was the one they had followed. This discrepancy presumably reflects the fact that many people front loaded most observations but not all of them. The critical finding is that a much larger proportion of participants agreed that the front loading strategy would have won the most money (22 of 30, or 73%) and indicated that they would follow such a strategy if given the opportunity to do so next time (23 of 30, or 77%). To check that these results do not simply reflect chance variation in a small sample, we ran Bayesian tests for the equality of proportions under an assumption of uniform priors over proportions: the Bayes factor of 1871:1 in favor of the alternative model is strong evidence that the difference between the 40% of people who say they front loaded and 77% of people who say they would do so next time is not due to chance.</p><p>Moreover, of the 12 subjects who said they front loaded their observations during the task, only 1 (8%) indicated that they would switch to a distributed observation strategy. In contrast, of the 16 subjects who indicated that they had distributed their observations during the task, 12 (75%) indicated that they would switch strategies. Again, we apply a Bayesian test for equality of proportions, and obtain odds of 22467:1 in favour of the hypothesis that people were more willing to switch from distributed observations to a front loading strategy than vice versa.  The left panel shows the proportion of participants observing on each trial, the middle plots the proportion betting on the high probability color (Option A), and the right panel plots the proportion betting on the low probability color (Option B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>In Experiment 2, where we provided people with very obvious evidence that the outcome sequences were not rigged in any way, we found no substantial deviations from the strategies that people adopted in Experiment 1. On the other hand, in the post experimental questionnaire, we found very strong evidence that those participants who had not initially followed the optimal strategy were able to recognize that they had not done so, and correctly identified what strategy they should follow next time. This is entirely consistent with the fact that in Experiment 1, where participants were allowed to play repeated games, this is exactly the shift that happened. Taken together, the results suggest that the need for trial by trial experience, rather than distrust, is the correct explanation. People do not necessarily infer the right strategy purely on the basis of a verbal description: they require direct experience of the task in order to do so (cf <ref type="bibr" target="#b32">Newell &amp; Rakow, 2007;</ref><ref type="bibr" target="#b30">Newell et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Four Process Models for the Observe or Bet Task</head><p>At the start of the paper we presented the MDP analysis as a rational analysis of the observe or bet task, and used it to motivate testable predictions about how people's behavior should differ in the static and dynamic versions of the problem. Inspired by the MDP model we now introduce a simple approximation to the rational model that can be used to investigate human performance on the observe or bet task in more detail. The model is fairly straightfoward, and consists of a simple evidence accrual rule and a simple but noisy decision rule.  <ref type="figure">Figure 11</ref>: Actual and reported strategies for how observations should be distributed, along with participants' judgments as to which strategy maximizes profits and which would be followed next time. Not shown is the one participant who responded with a "no observation" strategy to all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evidence Accrual Rules</head><p>The learner maintains an evidence tally that starts at zero. Every time an outcome is observed the tally either increases or decreases by 1. Formally, we let x t = 1 if the learner observes outcome A on trial t, x t = −1 if the learner observes outcome B, and x t = 0 if nothing is observed because the learner chose to bet on trial t. In the simplest version of the model, the total amount of evidence available at the end of trial t is given by the simple rule:</p><formula xml:id="formula_7">e t = x t + e t−1<label>(4)</label></formula><p>In essence, the evidence tally after t trials is simply the number of observed A outcomes minus the number of observed B outcomes. However, there are good reasons to think that people might weight recent observations more heavily than older ones. The most obvious reason is that, in the dynamic condition in particular, people might have reason to believe that the world has changed and old observations are no longer relevant. Alternatively, people might simply fail to remember older observations. Consistent with the "constant probability of change" dynamic model used in our MDP analysis, we propose a simple heuristic in which older evidence decays exponentially, losing some proportion α of its evidentiary value on each trial: 12</p><formula xml:id="formula_8">e t = x t + (1 − α)e t−1<label>(5)</label></formula><p>Because α governs the rate at which the evidentiary value of observations diminishes over time we refer to it as the evidence decay parameter. When α = 0 there is no evidence decay and the evidence accumulation process is equivalent to Equation 4. At the other extreme when α = 1 all older evidence is completely discarded, and the learner relies solely on the most recent event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Noisy Responses</head><p>What action does the learner take on trial t given that they have evidence e t ? According to the fully optimal MDP analysis, participants should follow the decision strategy outlined in <ref type="figure">Figure 1</ref>. That is, the learner has a "decision threshold" d t that determines when to observe and when to bet. If the evidence available meets this threshold (i.e., |e t | ≥ d t ) then the learner should place a bet, but if it does not (i.e., |e t | &lt; d t ) then they should make another observation. The heuristic model is essentially a noisy version of the same rule. To capture this, we assume normally distributed noise terms n t , where the noise on each trial is generated from a normal distribution with mean 0 and standard deviation σ. Given these assumptions, the probability that the learner bets on trial t is given by:</p><formula xml:id="formula_9">P (bet) = P (|e t + n t | ≥ d t ) = P (e t + n t ≥ d t ) + P (e t + n t ≤ −d t ) = Φ e t − d t σ + Φ −e t − d t σ<label>(6)</label></formula><p>In this expression, Φ(•) is the cumulative distribution function for a standard normal, and because σ governs the amount of noise in the decision process we refer to it as the response stochasticity parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Decision Thresholds</head><p>The final ingredient in the model is the form of the decision threshold itself. The simplest version of the model assumes that the decision threshold is a constant value, d. Despite its simplicity, this is at odds with the predictions of the MDP model. As <ref type="figure">Figure 1</ref> illustrates, the rational analysis of the observe or bet task predicts that, at a minimum, the decision threshold d t is not constant and should decline over time. A simple version of this is instantiated by assuming that the decision thresholds are initially flat, but after some "critical" trial they start to decline towards some final value (which may not be zero). We do not claim that this "piecewise linear" model is especially realistic, but it captures the key qualitative idea in a fairly simple way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluating Four Models of the Task</head><p>The previous discussion suggests four versions of the model. In the full version of the model we allow for the possibility that evidence might decay and allow for declining thresholds using the piecewise linear model. Three alternative models could be produced by removing one or both of these properties. Curiously, all four versions of the model can be viewed as variations on sequential sampling models used in the simple decision making literature <ref type="bibr" target="#b41">(Ratcliff &amp; Smith, 2004)</ref>. For instance, if the evidence tally does not decay and the decision threshold is constant, we obtain a random walk model that is similar in spirit to the highlysuccessful Weiner diffusion model <ref type="bibr" target="#b40">(Ratcliff, 1978)</ref>. If the decision threshold remains constant and evidence can decay, the resulting model is more closely related to the Ornstein-Uhlenbeck diffusion model and other leaky accumulators <ref type="bibr" target="#b51">(Smith, 2000;</ref><ref type="bibr" target="#b59">Usher &amp; McClelland, 2001)</ref>. In other contexts researchers have argued that when decision deadlines exist the threshold should decline over time in a fashion similar to the behavior of the MDP model <ref type="bibr" target="#b7">(Frazier &amp; Yu, 2007)</ref>, a topic that has been somewhat contentious in the literature on simple decisions (see, e.g., <ref type="bibr" target="#b47">Shadlen &amp; Kiani, 2013;</ref><ref type="bibr" target="#b11">Hawkins et al., 2015)</ref> In light of the existing evidence that the "sequential sampling" framework can be productively applied as models of strategic "high level" decision making tasks <ref type="bibr" target="#b23">(Lee &amp; Cummins, 2004;</ref><ref type="bibr" target="#b22">Lee &amp; Corlett, 2003;</ref><ref type="bibr" target="#b24">Lee &amp; Dry, 2006;</ref><ref type="bibr" target="#b31">Newell &amp; Lee, 2011;</ref><ref type="bibr" target="#b25">Lee et al., 2014)</ref>, there are good a priori reasons to think that all four models are psychologically plausible. However, only the full model captures all of the qualitatively important principles embodied by the MDP analysis. If it is truly the case that people are sensitive to the structure of the observe or bet task, we should find that the full model is necessary to account for our data. On the other hand, if people are simply relying on more general strategies, there is no reason to expect this result.</p><p>We implemented all four models as hierarchical Bayesian models in JAGS, and estimated model parameters using the data from Experiment 1 (see Appendix for details). As it turns out, the full model provides a good fit to the data but the other three models all show theoretically-significant qualitative failures. To illustrate the basic problem, <ref type="figure">Figure 12</ref> plots the posterior predictive values (i.e., model fit) for the observation probabilities in Game 3 of the dynamic condition. 13 Each panel corresponds to one of the four models. When the model is allowed to incorporate declining thresholds (top row) it is able to capture the way in which the observation rate falls off over the last 10 trials of the experiment, whereas models with fixed thresholds (bottom row) cannot do so. When the model does not allow for evidence decay (right panels) it cannot simultaneously capture the very high observation rates in the first few trials and the moderate observation rate through the middle of the experiment. When evidence decay is allowed (left panels) it can do so. Notwithstanding the fact that the full model is necessarily the most complex of the four, there is clear evidence that this complexity is necessary. The three other models all show failures to capture theoretically important trends in the data. This intuition is quantified in <ref type="table">Table 3</ref>, which reports model fits separately for the early trials (1-10), middle trials (11-40) and end trials (41-50). Consistent with the predictions of the MDP analysis, the only process model that consistently provides acceptable fits to the observation profiles at every stage of the task is one that allows for evidentiary decay and declining thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Model Performance at the Aggregate and Individual Subject Level</head><p>Before discussing the parameter estimates that emerge from the full model, it is useful to get a sense of how well the full model works. To that end, <ref type="figure">Figure 13</ref> plots the model fits for all five games in both conditions of Experiment 1. 14 As these plots make clear, the model fits at an aggregate level are in very close agreement with the empirical data. There is a slight tendency for the model to underestimate the amount of observation that takes place on the first two trials in the dynamic condition, but this is the only systematic failure. Trial Number</p><formula xml:id="formula_10">Evidence Can Decay No Evidence Decay q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q</formula><p>Proportion of Observe Actions <ref type="figure">Figure 12</ref>: A visual illustration of why the model requires both the declining thresholds and evidence decay. Grey dots plot the proportion of subjects observing on any given trial for Game 3 in the dynamic condition. The solid lines show the posterior predictive probabilities (i.e., Bayesian model fit) when all four versions of the heuristic model are fit to individual-subject level data. <ref type="table">Table 3</ref>: Quantitative measures of the performance of four different models. The table reports sum squared errors (SSE) for all four models separately for the first 10 trials, the middle 30 trials, and the final 10 trials. A fit is deemed "adequate" (and shown in italics) if for a given condition and trial block if it is not more than 50% worse than the best performing model. The only model that consistently provides adequate fit is the model that allows declining thresholds and evidence decay (shown in bold).  Trial Number</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Specification</head><formula xml:id="formula_11">q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.</formula><p>Proportion of Observe Actions <ref type="figure">Figure 13</ref>: Observation probabilities estimated when the full version of heuristic model (black lines) is fit to the data from Experiment 1 (grey dots). The model is fit at an individual subject level (see Appendix) and the curves reported here are aggregated across individuals.</p><p>Moving beyond comparisons at the aggregate level, our approach allows us to go further and examine how well the model captures the behavior of individual subjects. Specifically, we can use the estimated model parameters to construct an "evidence accumulation plot" similar to <ref type="figure">Figure 1</ref> for every participant and every game. The Bayesian data analysis allows us to reconstruct a best estimate of the decision threshold d t for each person, as well as estimates of the evidence decay parameter α and the response stochasticity σ. When combined with the empirical data we can estimate the subjective evidence e t available to each person on every trial. Not every participant is captured well by the model, but given the nature of the data (i.e., each game is just a binary sequence of 50 observe/bet choices) a remarkably large number of them are.</p><p>To illustrate the general patterns of variability that the model identifies, eight examples of these estimates are plotted in <ref type="figure" target="#fig_3">Figure 14</ref>. 15 As this plot illustrates there are a range of individual subject strategies that are captured by the model. For instance, panels (a), (b), (c) and (e) all show a participant engaging in a pure "front loading" strategy. Participants (a), (b) and (c) all show variations on this strategy: differences in decision threshold d t produce differences in the number of trials for which the particpant observes (4, 8 and 15, respectively) before switching to a betting-only phase. Because these participants never switch back to observations, the model estimates very small values of α and σ. Moreover, although this strategy was widespread in the static condition, there are some examples of it (e.g., panel (e)) that appeared -somewhat inappropriately -in the dynamic condition.</p><p>Many participants adopted a strategy that involved frequent switching between betting and observation,  <ref type="figure" target="#fig_3">Figure 14</ref>: Reconstructions of individual subject strategies. Each plot depicts the strategy of a single participant in Game 2 Version 1. In each plot, the solid black lines plot the decision threshold dt. However, because responses are assumed to be noisy, the decision threshold is "soft": the black line plots the point at which people are equally likely to bet as to observe. To represent the amount of noise, the dotted lines plot one standard deviation σ above and below the decision threshold. Black circles denote trials where the participant observed an A outcome, white circles denote trials where they observed a B outcome, and grey circles indicate a bet. Note that in the dynamic condition the change event occurred on trial 17.</p><p>as illustrated in panels (d), (f), (g) and (h). As noted earlier, this was far more common in the dynamic condition than in the static condition. Because it is somewhat difficult to predict precisely when people make these switches, the values of σ do tend to be larger in this case, as illustrated by the wider bands plotted in dotted lines. However, it is also notable that the model does tend to extract meaningful patterns from the switches. For instance, consider the participants plotted in panels (d) and (f). Although the participant in panel (f) makes more observations (20 to 15), this is almost entirely due to the fact that panel (f) shows data from the dynamic condition, and the additional observations occur around trial 20-25 shortly after the change point that occurred on trial 17. As the plot makes clear, at this point the participant made a sequence of 6 consective observations that allowed them to detect the change and pushed the evidence tally from below the lower threshold to above the upper threshold. In contrast, the behavior shown in panel (d) comes from the static condition: although the participant periodically made some observations, the evidence tally tended to remain below the lower threshold throughout the entire task. A final comparison worth noting is between the data sets shown in panel (g) and panel (h). In both cases the model estimates a very high evidentiary decay parameter (α = .25), but the decision thresholds are very different in both cases. In panel (g), the participant has very low thresholds, and so what we see is that this person makes one observation followed by a short betting run, followed by another observation and a short betting run, and so on. There is only one occasion where this person made consecutive observations. In contrast, the very high thresholds used by the participant in panel (h) leads them to make 37 consecutive observations at the start of the task, followed by an alternating cycle of bets and observations across the last few trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Parameter Estimates</head><p>An interesting picture emerges when we look at the distribution of parameter estimates. For instance, although the heuristic model incorporates the idea that the evidentiary value of observations can decay over time, it is agnostic as to the reason why this occurs. According to the MDP model, the evidentiary value decays because the learner believes that the world might have changed, and as such it makes the strong prediction that the evidence decay rate α should be high in the dynamic condition and very low in the static condition. As <ref type="figure" target="#fig_11">Figure 15a</ref> shows, this is precisely what happens. Not only that, there is a modest learning effect: in the static condition the median value of α is 0.034 on Game 1, which then declines to 0.014 on Game 2 and stays roughly constant for the remainder of the experiment. In the dynamic condition, on the other hand, median α for Game 1 is 0.125, and rises gradually across games until it reaches a value of 0.180 by Game 5.</p><p>It is worth noting that this pattern is not an artifact of people's responses becoming less variable across games in the static condition and more variable in the dynamic condition. Indeed, as <ref type="figure" target="#fig_11">Figure 15b</ref> illustrates, the response stochasticity parameter σ shows a somewhat different pattern. Although it does show a decline across games in the static condition, it does not show any substantial trend across games in the dynamic condition.</p><p>Another respect in which people's performance mirrors the qualitative predictions of the MDP model emerges when we look at the estimated thresholds (averaged across participants) for each game and condition. In <ref type="figure">Figure 1</ref> we saw that the optimal MDP policy predicts that the decision thresholds should be lower in the dynamic condition than in the static condition. As <ref type="figure">Figure 16</ref> illustrates, this is exactly what we find. For all five games, the thresholds in the static condition start out much higher than their counterparts in the dynamic condition, but decline much faster so that by the final trial they are very similar. Error bars depict 95% credible intervals constructed using the Bayesian bootstrap <ref type="bibr" target="#b44">(Rubin, 1981)</ref>. <ref type="figure">Figure 16</ref>: Evidence thresholds inferred for participants in Experiment 1. Plots are averaged across participants and across all three versions of the task. Consistent with the predictions made using the MDP model, the evidence thresholds are consistently higher in the static condition than in the dynamic condition, especially early in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">General Discussion</head><p>Making good decisions in everyday life requires people to make an exploration-exploitation trade-off. On the one hand, we cannot make good choices if we do not explore our environment to learn its structure. On the other hand, we cannot gain rewards if we do not exploit our knowledge of this structure to guide our actions and make rewarding choices. The observe or bet task introduced by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref> represents a very clean experimental tool for investigating how people solve this problem.</p><p>Our MDP analysis of this task motivated novel predictions and novel process models for this particular kind of (under-researched) "pure" explore-exploit dilemma in static environments as well as in dynamic ones. The conclusion offered by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref> -that participants failed to adopt an appropriate strategy -is at odds with our overall interpretation, and yet the original paper does in some ways foreshadow our findings. In their work Tversky and Edwards included a condition in which participants were (falsely) told that the outcome probabilities might change, and they noted that people did tend to make more observations as a consequence. However, as our analysis of the task indicates, there is a lot more subtlety to the task than this rather trivial result suggests. Optimal behavior in a dynamic environment does not simply involve making more observations in order to monitor a changing world; it also requires the learner to lower their evidence thresholds in order to "strike while the iron is hot". Similarly optimal behavior in short tasks with known time horizons (like the 50 trial observe or bet problems) requires that people be sensitive to the short time horizon for the problem, so evidence thresholds should decline over time. In all these respects, people show a striking qualitative agreement with the MDP model.</p><p>As noted in the introduction the specific combination of a task in which exploration and exploitation are cleanly separated (cf. <ref type="bibr" target="#b46">Sang et al., 2011)</ref> and the use of static and dynamic environments allows for insights that go beyond existing investigations of standard bandit problems. Our conclusions about the remarkable (qualitative) optimality of participants is based both on the simple data analysis (e.g., <ref type="figure" target="#fig_6">Figures 3, 7 and 9</ref>) and from fitting a process model to the aggregate and individual data (e.g., <ref type="figure" target="#fig_3">Figures 12 and 14)</ref>. Both types of analyses point to the same conclusion: although people might not behave in a strictly optimal manner, they show an acute sensitivity to the dynamics of the environment, and they show rapid improvement in selecting the correct action (observe or bet) as they gain experience in the task.</p><p>These findings are broadly consistent with more standard tasks in which either exploration and exploitation are conflated (to some degree), or static and dynamic tasks are examined in isolation. For instance, there is evidence for something akin to the front-loading we observed in an exclusively static bandit task: <ref type="bibr" target="#b64">Zhang &amp; Yu (2013a)</ref> present evidence suggesting that people tend to engage in more exploratory actions early in a bandit problem, though this conclusion was based on a heuristic that defined "exploratory" as "switching options after a win". Similarly, Zhang &amp; Yu (2013b) adopt a modeling approach for bandit problems that incorporates a "forgetfulness" parameter that has similarities to our approach to handling evidentiary decay. Relatedly, in dynamic versions of the bandit problem (i.e., "restless" bandits) there is evidence that people continue to switch between options throughout the task (e.g., <ref type="bibr" target="#b52">Speekenbrink &amp; Konstantinidis, 2014</ref>, consistent with what we find in the dynamic observe or bet task. Moreover, formal analysis of what people should do in a dynamic variant of the standard bandit problem has found people's behavior consistent with an ideal actor model in some but not all respects: although decision makers update beliefs in an optimally reflexive manner, they fail to utilize these beliefs fully for optimal long-term planning <ref type="bibr" target="#b19">(Knox et al., 2011</ref>; see also <ref type="bibr" target="#b10">Gureckis &amp; Love, 2009</ref> for a similar conclusion regarding the sub-optimality of long-term strategies).</p><p>This overall conclusion that participants can approximate optimal solutions but fall short in various (instructive) ways echoes our own findings. For example, one systematic violation of the MDP prediction found here was that in both experiments, people seem to show larger departures on the first observe or bet task they encounter. In the static condition, only a minority of participants realized that front loading observations was the optimal strategy for the task, consistent with the original results by <ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref> and those of <ref type="bibr" target="#b39">Rakow et al. (2010)</ref>. However, it turns out that this special case is actually the exception and not the rule. When we explicitly asked people what they had learned from a single observe or bet game (Experiment 2) almost everyone agreed that front loading was the optimal strategy and indicated that this is what they would do in future games. Moreover, when they were given the opportunities to play repeated games (Experiment 1) people did in fact make this shift, even though they were given no explicit prompt to consider alternative strategies. This latter finding resonates with <ref type="bibr" target="#b39">Rakow et al. (2010)</ref> who found that across six consecutive 100 trial games of the static observe-or-bet task, participants slowly learned to distribute their observations in a closer-to-optimal fashion, and with the work of <ref type="bibr" target="#b46">(Sang et al., 2011)</ref> who also found that people learned with experience in a related task. Interestingly, the improvement in the Rakow et al. study emerged despite withholding accuracy feedback until the completion of all six games. This contrasts with Experiment 1 in which detailed feedback was provided at the end of each game. The relatively modest improvements in front loading behavior observed by <ref type="bibr" target="#b39">Rakow et al. (2010)</ref>, in contrast to the clear jump in performance between Game 1 and Game 2 of Experiment 1 highlights the power of combining trial-by-trial experience with feedback in tasks of this kind <ref type="bibr" target="#b32">(Newell &amp; Rakow, 2007)</ref>.</p><p>How should we reconcile the fact that people show remarkable similarities to the behavior of an optimal agent, yet also fail to perfectly optimize their strategies? One possibility is that human decision makers simplify the complexity of the problem and thereby produce approximate solutions. The dynamic programming method that we used to compute optimal MDP solutions does not scale well to large real world problems, which makes it impractical for any real world agent (see, e.g., van Rooij et al., in press, for discussion). In contrast, the process models used later in the paper are comparatively simple, and in that sense are much more cognitively plausible. However, the heuristic models are very task specific: while the MDP approach can be generalized to other decision making tasks such as bandit problems, the same cannot be said for the process models. There are a number of more general possiblities for generating heuristics from the MDP approach. For instance, machine learning solutions to realistic sequential decision problems tend to rely on methods that produce approximate solutions rather than exact ones (e.g., <ref type="bibr" target="#b55">Thrun, 1999;</ref><ref type="bibr" target="#b34">Pineau et al., 2003;</ref><ref type="bibr" target="#b49">Silver &amp; Veness, 2010)</ref>.</p><p>Many of these approximate planning algorithms bear similarities to methods currently used by psychologists designing approximately rational cognitive models. For instance, particle filtering is a tool often used when approximating Bayesian learners (e.g., <ref type="bibr" target="#b45">Sanborn et al., 2010;</ref><ref type="bibr" target="#b3">Brown &amp; Steyvers, 2009)</ref>, and can be naturally extended to approximate optimal decision making <ref type="bibr" target="#b49">(Silver &amp; Veness, 2010)</ref>. These particle based methods are very similar to simple "mental simulation" models of sequential and multi-stage decision making (e.g., <ref type="bibr" target="#b15">Hotaling &amp; Busemeyer, 2012)</ref>, and represent a promising method for connecting the optimal solution to plausible planning algorithms.</p><p>A second perspective emerges when we consider that the most salient deviation from the optimal strategy occurs the first time people need to solve the static version of the task. Moreover the form of the departure is quite striking: by default, people adopt a strategy of repeated switching between observation and betting, a strategy that is far better suited to the dynamic task than the static one. One reason why we might expect to see this kind of systematic departure was hinted at in the introduction: so many problems in real life involve dynamic environments, and it makes sense that people's default strategies would be better suited to such a world. Relatedly, it appears that mistaking a changing world for a static one in our task is a "worse" failure than the reverse, in the sense that the rewards lost by observing too often in the static task turn out to be smaller than those lost by failing to detect changes in the dynamic one (see <ref type="bibr">Navarro &amp; Newell, 2014, for discussion)</ref>. If this asymmetry holds more generally, then a default strategy of acting as if changes are possible makes a good deal of sense.</p><p>A final perspective on this issue arises from the fact that the MDP model is itself a simplification. The MDP model relies on several assumptions that are sensible for the particular task we used in our experiments, but do not hold in all circumstances. For instance, the model expects to encounter abrupt changes in outcome probabilities rather than a gradual drift in the reward contingencies, and in the dynamic conditions it knows roughly how often such changes occur. In that sense it is similar in spirit to an ideal observer model that is given all relevant information about the task, but does not provide a completely general solution for how learners should solve explore-exploit problems in all dynamic environments. One the one hand, this does not greatly influence our central point, which is that people's behavior in this class of problems is -contrary to the conclusions offered by earlier work -entirely sensible and in broad agreement with the ideal observer for the task. On the other hand, noting these limits also opens up questions about what a normative standard for real world decision making would look like: human decision making needs to be robust across many different dynamic learning problems, and as such one might expect to see some departures between what people do and what the task-specific MDP model predicts.</p><p>More generally, these observations highlight the value of studying human behavior in changing environments. The problem, as we see it, is that human cognition is adapted either via evolution or prior learning to operate in changing and responsive worlds. This matters: as noted by many authors (e.g., <ref type="bibr" target="#b16">Jordan &amp; Russell, 1999)</ref>, the structure of the operating environment imposes strong constraints on how an intelligent agent can be built. This basic idea underpins both the "fast and frugal heuristics" literature (e.g., <ref type="bibr" target="#b50">Simon, 1956;</ref><ref type="bibr" target="#b8">Gigerenzer &amp; Brighton, 2009)</ref> as well as the "rational analysis" approach to building cognitive models (e.g., <ref type="bibr" target="#b0">Anderson, 1990)</ref>. From this viewpoint, one might argue that studying human cognition in an unchanging world can be like studying the aerodynamic properties of an octopus: technically possible, because water and air are both fluids, but frustrating and misleading because it ignores what the organism is designed to do.</p><p>However, while the belief state comprises a complete distribution over possible values of θ, the properties of the beta distribution mean that it is straightforward to compute the expected probability that the next trial will result in a "blue" outcome (i.e., expected value of θ) E[θ|n 1 , n 2 ] = n 1 + φ n 1 + n 2 + 2φ</p><p>Viewed as a reinforcement learning problem (i.e. a partially observable Markov decision policy, or POMDP), it is now possible to directly specify the expected reward that will be received on trial t + 1 given that the learner is in belief state b t on trial t and undertakes action a. Define a = 0 to be the "observe" action, a = 1 to be the "bet blue" action, and a = 2 to be "bet red" action. The expected reward r t+1 for moving out of belief state b t via action a is given by:</p><formula xml:id="formula_12">r a t+1 =            0 if a = 0 n1+φ n1+n2+2φ if a = 1 n2+φ n1+n2+2φ if a = 2</formula><p>Similarly, it is straightforward to define the transition probabilities that describe how the belief state will change as a function of the actions that the agent makes. If the agent bets (i.e., a = 1 or a = 2), then no observations are made available, so the belief state does not change (i.e., b t+1 = b t with probability 1). On the other hand, if the agent observed (i.e., a = 0) then b t+1 = Beta(φ + n 1 + 1, φ + n 2 ) with probability n1+φ n1+n2+2φ Beta(φ + n 1 , φ + n 2 + 1) with probability n2+φ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n1+n2+2φ</head><p>This defines a very simple POMDP for which we can infer the optimal policy using value iteration. Because the relationship between observations and beliefs is so simple, the size of the set of actually-reachable beliefs is the same as the size of the set of observation states (i.e., possible values for n 1 and n 2 ) so we don't need to do anything complicated. The utility of any given belief state b t on trial t is given by Bellman's equation. The only subtle thing to note is that, because rewards in the observe or bet task are given contingent on the previous action, the "immediate" reward for state b t is in fact determined by r a t+1 , the reward for moving into the next trial after taking action a. (One simply has to remember that even the final trial of the experiment generates a reward, even though no follow up action is possible). When written this way, the utilities must satisfy:</p><formula xml:id="formula_13">u(b t ) = max a r a t+1 + a u(b t+1 )P (b t+1 |b t , a)</formula><p>This is a slightly unusual way to write Bellman's equations, because the usual specification of the MDP problem assumes that the reward generated by any state is independent of the action that brought you into it, and this does not hold for the observe or bet task. The optimal policy shown in <ref type="figure">Figure 1a</ref> is computed using this specification of the problem. Strictly speaking, the decision policy is defined with respect to the two counts, n 1 and n 2 as well as the trial number t, but it turns out that the difference n 1 − n 2 and the trial number t are very nearly a sufficient statistic for determining the action, and for this reason the decision policies discussed in the main text define the policy in those terms.</p><p>Appendix A.2. Optimal behavior in the dynamic task</p><p>Computing optimal policies for dynamic environments is more difficult. A slightly simplified way of computing decision policies in the dynamic version of the task is outlined below. Suppose the learner believes that on every trial there is some probability that the outcome probability θ is redrawn, as per the generative model discussed at the beginning. That is, what happens if the agent sets α &gt; 0? Again, let's start with a slightly simpler case than the full problem facing the human participants. Suppose that the change points were actually announced to participants when they occurred. What this would mean is that the learner knows the true values for n 1 and n 2 , where n 1 and n 2 now refer to the number of observed "blue" and "red" outcomes since the most recent change. Under these conditions, the POMDP analysis proceeds exactly as before, except that the transition rule for the belief states when an observe action (a = 0) is made now becomes:</p><formula xml:id="formula_14">b t+1 =     </formula><p>Beta(φ + n 1 + 1, φ + n 2 ) with probability (1 − α) n1+φ n1+n2+2φ Beta(φ + n 1 , φ + n 2 + 1) with probability (1 − α) n2+φ n1+n2+2φ Beta(φ, φ) with probability α and when a bet action is made it becomes b t+1 = Beta(φ + n 1 , φ + n 2 ) with probability (1 − α) Beta(φ, φ) with probability α</p><p>In other words, the agent knows that on every trial it will be sent back to the beginning state (i.e. zero relevant observations) with probability α. This by itself has an effect on the optimal policy. It is this specification of the problem that leads to the policy shown in <ref type="figure">Figure 1b</ref> That being said, the actual belief state b t for a Bayesian learner in the dynamic task is somewhat more complicated than simply counting n 1 and n 2 . Because the actual change point is unobserved, the learner must maintain a full posterior distribution over n 1 and n 2 given the observations that have been made. This posterior distribution can be efficiently estimated using particle filtering, and we approximate the behavior of the optimal learner by choosing to bet only if the posterior expected value of n 1 − n 2 exceeds the relevant threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Data analysis model</head><p>One advantage to the MDP analysis is that it yields a genuine prediction about the experiment, one that is not especially dependent on free parameters. Specifically, it predicts a very distinctive profile for the difference in observation rates between conditions: an initial decline, followed by a sharp rise, followed by a curvilinear pattern. However, on its own the model does not go further than this qualitative prediction. It does not in any meaningful sense specify a complete probabilistic model for human responses on any given trial. One possibility when analyzing empirical data in light of such predictions is to augment the ideal observer model with additional assumptions about response generation (e.g., <ref type="bibr" target="#b27">Navarro et al., 2012;</ref><ref type="bibr" target="#b12">Hemmer et al., 2014)</ref> and then use Bayesian methods for our data analysis. Unfortunately, the complexity of the MDP model makes that infeasible in this case because of the difficulty even to extract a prediction from the model. An alternative possibility is to begin with the key qualitative constraint, and then construct a proper statistical model that embodies exactly that constraint. This idea has considerable merit in its own right: previous authors have argued that cognitive models are typically intended only to specify qualitative predictions <ref type="bibr" target="#b35">(Pitt et al., 2006)</ref>, and if so it makes sense to assess them in such terms. For the observation rates data, the "decline, then rise, then curve" prediction corresponds to the following non-linear regression Version 1, Game 1 00000100001000000100000011000000010000001001011000 Version 1, Game 2 00001010110000000010010000010000011000000100001110 Version 1, Game 3 11111011011111111111100011101111010110001101111011 Version 1, Game 4 00000000000101100010011110000000000000101011000000 Version 1, Game 5 11111111101111101101111110110011101111111110111110 Version 2, Game 1 00000001100100010110000010000001101000010010010000 Version 2, Game 2 00001010011000000100000000100110010110001000011000 Version 2, Game 3 00000001000001010000000100101010111000000000100100 Version 2, Game 4 01010110101100111110011111111111101110011111111111 Version 2, Game 5 10101111111111111111100111101111110100011111011111 Version 3, Game 1 01110101011110111111111111111101111011101111101111 Version 3, Game 2 11101111101111111111011111111010111011011110011111 Version 3, Game 3 00000000110110000000000000000010000000001010000000 Version 3, Game 4 11110111111111110111000111111101110110111111111111 Version 3, Game 5 11110010011111101001011011111111110111100111101111</p><p>The stimulus sequences used in the dynamic condition were as follows. Trials listed in italics correspond to events that occurred after the change point:</p><p>Version 1, Game 1 00000100001000000100000011000000010000001001011000 Version 1, Game 2 00001010110000001101101111101111100111111011110001 Version 1, Game 3 11111011011111111111100010010000101001110010000100 Version 1, Game 4 00000000000101100010011110001111111111010100111111 Version 1, Game 5 11111111101111101101100001001100010000000001000001 Version 2, Game 1 00000001100100010110000010000001100111101101101111 Version 2, Game 2 00001010011000011011111111011001101001110111100111 Version 2, Game 3 00000001000001010000000100101010111000000000100100 Version 2, Game 4 01010110101100111111100000000000010001100000000000 Version 2, Game 5 10101111111111111000011000010000001011100000100000 Version 3, Game 1 01110101011110111111110000000010000100010000010000 Version 3, Game 2 11101111101111110000100000000101000100100001100000 Version 3, Game 3 00000000110110000011111111111101111111110101111111 Version 3, Game 4 11110111111111110110111000000010001001000000000000 Version 3, Game 5 11110010011111101001011011111111110111100111101111</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The difference in observation probabilities between conditions, plotted separately for each game. Shaded area denotes the Bayesian 95% credible interval for the mean. Dotted lines show the 5th and 95th quantiles of the posterior predictive distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The difference in observation probabilities between conditions, plotted only for the special cases where the underlying sequences were identical in the static and dynamic conditions. Shaded area denotes the Bayesian 95% credible interval for the mean. Dotted lines show the 5th and 95th quantiles of the posterior predictive distribution. Proportion of participants who front load all of their observations, plotted separately by condition and game number. Error bars show 95% Bayesian credible intervals for an unknown proportion under a Jeffreys prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Number of consistent betting runs, broken down by run length and condition. The heights of the black bars depict the number of consistent runs, and the grey bars show the total number of runs of the corresponding run length. The white error bars show the expected 95% intervals for the number of consistent runs if participants were probability matching (at 80%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Response proportions for Experiment 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Median estimated values of the evidence decay parameter α (panel a) and the response stochasticity parameter σ (panel b) as a function of the game and condition, averaged across all three versions of the task and across all participants.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Moreover, we ran some additional simulations to verify that the qualitatively important characteristics of the model predictions are not sensitive to the specific choice of prior, and it appears to be true.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The "bumpy" look to these curves is a somewhat uninteresting consequence of the fact that (a) evidence is discretized in this task, since each observation provides a single bit of information, and (b) the full MDP analysis makes a subtle distinction between the evidentiary value of 3 As and 0 Bs, versus 13 As and 10 Bs, and so when the complete (high dimensional) decision policy is plotted in this simple fashion, a little information is lost. There are some cases when the difference on trial 23 is exactly 3 and the model bets, and others when it observes. There are particular points in the task where this variation is amplified: essentially, when it is unclear whether a threshold of 2 is better than a threshold of 3, this subtle effect gets amplified. Thus, instead of seeing a discrete drop in the threshold from 3 to 2, we obtain this smooth transition where some belief states make the jump down from 3 to 2 before others. Note that none of this subtlety is relevant to our experiments, and we mention it only because some readers may be puzzled by the bumpy shape of the curves. The important characteristic of the thresholds is that they start high and then collapse towards zero.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that these simulations are something of an approximation. In both cases the MDP model is assumed to know whether the environment is static or dynamic, and does not need to learn the rate of change. In real life this seems implausible, since people would have to infer this from their observations. However, our goal with these simplified simulations is to generate qualitative predictions that we will use to motivate experimental work, and from this perspective this simplification seems reasonable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The "two lights, three buttons" set up closely mirrors the physical device used by<ref type="bibr" target="#b58">Tversky &amp; Edwards (1966)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note thatFigure 8does not ignore observations: it plots the raw frequencies of the two bet options, and so the heights of the two curves do not sum to 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This is not to say that memory limitations have no role to play in understanding the task, and<ref type="bibr" target="#b39">Rakow et al. (2010)</ref> do find that memory capacity relates to performance in the static task. However, our results also highlight that there are good strategic reasons for people to avoid strict front loading.9 Obvious, but not strictly correct. A few of the sequences generated the first time were highly regular to the human eye, and we resampled them in order to avoid confusing participants: as such, the actual sequences used cannot be characterized as pure</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">It is worth noting that the rewards offered in Experiment 2 are far more performance based than in Experiment 1, but (to foreshadow the results somewhat) there is no evidence that participants performed better. If anything, participants in Experiment 1 performed better.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">The parameter in this model is denoted α by analogy with the corresponding parameter in the MDP analysis: since both produce an exponental decay in evidentiary value, it seemed sensible to label them identically. It is worth noting, however, that the MDP model explicitly asserts that this decay is caused by the learner's beliefs about the changeability of the environment, whereas the process model is more agnostic about why the decay occurs. However, as it will turn out, the estimates of the α parameter obtained by fitting the process model support the assumption made by the MDP model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">We note in passing that this procedure is slightly imperfect, and that a Bayes factor analysis would better quantify the strength of evidence for the four models. However, as the results show, the differences in performance between the models are very striking and in the interests of simplicity we forgo introducing this additional complexity.14 Although these plots are aggregated across the three versions of the task, we also confirmed that the model fits were good for each individual version. Similarly, although we estimated parameters separately across all games, versions and conditions, we did verify that parameter estimates tended to be stable within participant, as noted in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Although the decision thresholds used in the model are piecewise linear, what we have plotted inFigure 14is the posterior predictive value of the decision threshold dt at every trial. Because this averages across many different piecewise linear functions, the result can be a smoothed curve, though in most cases the result actually ends up looking fairly similar to a linearly decreasing threshold.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The research in this paper was partially supported via ARC grant DP150104206. DJN received salary support from ARC grant FT110100431 and BRN from ARC grant FT110100151. We thank Tim Rakow, Amy Perfors and Nancy Briggs for helpful comments. Preliminary version of the work was presented at the 36th Annual Conference of the Cognitive Science Society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. MDP analysis of the observe or bet task</head><p>The probabilistic model that underpins the observe or bet task is assumed to work as follows. On trial t the probability with which the blox machine turns on the blue light is denoted θ t , and this probability evolves according to the rule: θ t = θ t−1 with probability 1 − α θ new with probability α were α denotes the probability that a change occurs on any given trial. When a change occurs, the new outcome probability is drawn from a symmetric beta distribution θ new ∼ Beta <ref type="bibr">(φ, φ)</ref> and for the purposes of our analysis we fix φ = 1, corresponding to a uniform distribution over possible θ values.</p><p>Appendix A.1. Optimal behavior in the static task Consider the simpler case when the world is static (i.e., α = 0). At any given point in time t, the agent has made some number of observations n. Let n 1 denote the number of times a "blue" outcome has been observed, and let n 2 denote the number of times a "red" outcome has been seen. These observations do not allow the learner to fully identify the state of the world, since the actual probability θ remains unknown (we can drop the subscript t in this situation because θ never changes). Instead, what it determines is a belief state b t . If our agent is a Bayesian reasoner, then the belief state b t corresponds to the posterior distribution P (θ|n 1 , n 2 ). Because each observation is an independent Bernoulli trial with rate parameter θ, this belief state is: spline model:</p><p>An illustration of this regression function is shown in <ref type="figure">Figure B</ref>.17. In this plot, the two black dots mark the locations of the change points, c 1 and c 2 , which divide the regression function into three segments. The intercept is denoted b 0 , the slope of the first segment is b 1 , the slope of the second segment is b 2 , and the curvature of the third segment is b 3 . The general form of the regression function is inspired by the MDP analysis, but we set diffuse priors over the regression parameters. Priors over the b coefficients are normal with mean µ = 0 and precision τ = 10 −4 . The prior over the precision τ of the data is a gamma with shape and scale of 10 −3 . Priors over the location of change points are set to be similarly diffuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Additional details regarding the heuristic models</head><p>The main text describes the theoretically important aspects to the heuristic models, but does not describe the priors used in our JAGS implementation of these models. Our approach was as follows. First, we estimated model parameters independently for every distinct stimulus sequence (i.e., all five games, three versions and two conditions). The reason for doing so was partially convenience (the JAGS implementation becomes somewhat complicated otherwise) and partially to avoid placing strong priors over how these different cases should be related to each other. For any one case, however, we used a Bayesian hierarchical model with the following structure. If the response stochasticity parameter for the i-th subject is denoted σ i , then we can write our hierarchical model as follows:</p><p>For the evidence decay parameter α, in those models that allowed values other than α = 0, we adopted the following prior:</p><p>Depending on whether the model allowed declining thresholds or not, the decision threshold d t could be described in one of two ways. In both cases, there was an "initial value" for the decision threshold d 0 for which the following hierarchical prior was used:</p><p>If the model allowed declining thresholds we also included an "end value" for the decision threshold d 1 that indicated the evidence level required to justify betting on the final trial. The prior over this parameter was specified in the same way as the prior over the initial threshold:</p><p>Finally, in order to capture the idea that the threshold stays flat for some number of trials and only then begins to decline, we included a threshold-change parameter c that indicated the proportion of trials over which the initial flat region extends (e.g., if c = .2 then the threshold stays flat for the first 20% of trials; i.e., first 10 trials) and then declines linearly. The prior over c was specified as:</p><p>It should be noted that these priors were adopted primarily for convenience. We ran a few additional simulations to check that the model is robust to the choice of prior. Overall, we found that although there is some sensitivity to the precise model specification the theoretically important results are not sensitive to the prior: across many different versions of the prior we found that the full model (i.e., declining thresholds plus evidence decay) performed best, and that the parameter estimates across different games and conditions always showed the same qualitative pattern as those shown in <ref type="figure">Figure 15</ref>. Nevertheless, some degree of caution is warranted in regards to low-level details: it appears to be the case that some form of declining threshold is necessary to account for the data (as <ref type="figure">Figure 12</ref> shows, a fixed threshold does not work), but we suspect that the piecewise linear model that we have used in this paper is likely to be inaccurate. These caveats notwithstanding, it is noteworthy that the parameter estimates tended to be stable across games played by the same participant, suggesting that variability in model parameters does capture stable individual differences in performance. For instance, the average within-subject correlation between the α value between different games was r = 0.67, computed separately for each pair of games, and for each version and condition in order to avoid conflating the effect of experimental manipulation with the estimate of withinsubject correlation. Estimates for the initial value of the threshold d 1 were slightly less stable at r = 0.50, as were the final threshold values d 50 at r = 0.41. The least stable parameter was the response noise σ, at r = 0.38.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. List of sequences used in Experiment 1</head><p>In abstract terms, the stimuli in an observe or bet task can be characterized as a binary sequence of outcomes, with 0s corresponding to one outcome and 1s corresponding to the other. The stimulus sequences used in the static condition were as follows:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Adaptive Character of Thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The hot hand fallacy and the gambler&apos;s fallacy: Two faces of subjective randomness?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ayton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1369" to="1378" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Markovian decision process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<idno>DTIC Document</idno>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting and predicting changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When and why rare events are underweighted: A direct comparison of the sampling, partial feedback, full feedback and description choice paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Camilleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="933" to="942" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cortical substrates for exploratory decisions in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page" from="876" to="879" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential hypothesis testing under stochastic deadlines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt, D. Koller, Y. Singer, &amp; S. T. Roweis</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alterations in choice behavior by manipulations of world model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schrater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="16401" to="16406" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning in noise: Dynamic decision-making in a variable environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="180" to="193" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2476" to="2484" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moving beyond qualitative evaluations of Bayesian models of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="614" to="628" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decisions from experience and the effect of rare events in risky choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">U</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="534" to="539" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploration versus exploitation in space, mind, and society</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Redish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Couzin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Cognitive Search Research Group</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DFT-D: A cognitive-dynamical model of dynamic decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hotaling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="67" to="80" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The MIT Encyclopedia of the Cognitive Sciences</title>
		<editor>R. A. Wilson, &amp; F. C. Keil</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The nature of belief-directed exploratory choice in human decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probability matching and strategy availability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian model of human decision-making on an optimal stopping problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequential sampling models of human text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Corlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="159" to="193" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evidence accumulation in decision making: Unifying the &quot;take the best&quot; and the &quot;rational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">models. Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decision making and confidence given uncertain advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1081" to="1095" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modeling the adaptation of search termination in human decision making. Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="223" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unpacking the exploration-exploitation tradeoff: A synthesis of human and animal literatures. Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Braithwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hausmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="191" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sampling assumptions in inductive generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="187" to="223" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Information versus reward in a changing world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Conference of the Cognitive Science Society</title>
		<editor>P. Bello, M. Guarini, M. McShane, &amp; B. Scassellati</editor>
		<meeting>the 36th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning time-varying categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Vong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="917" to="927" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probability matching in risky choice: The interplay of feedback and strategy availability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Ravenzwaaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="329" to="338" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The right tool for the job? Comparing an evidence accumulation and a naive strategy selection model of decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="456" to="481" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of experience in decisions from description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1133" to="1139" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prediction and control in a dynamic environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Point-based value iteration: An anytime algorithm for POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global model analysis by parameter space partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Distributed Statistical Computing</title>
		<meeting>the 3rd International Workshop on Distributed Statistical Computing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Biased samples not mode of presentation: Re-examining the apparent underweighting of rare events in experience-based choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Demes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="168" to="179" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Doomed to repeat the successes of the past: History is best forgotten for repeated choices with nonstationary payoffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="985" to="1000" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The role of working memory in information acquisition and decision making: Lessons from the binary prediction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zougkou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1335" to="1360" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comparison of sequential sampling models for two-choice reaction time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="333" to="367" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Some aspects of the sequential design of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of Australian Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rational analysis, intractability, and the prospects of &apos;as if&apos;-explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwisthout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wareham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The Bayesian bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="130" to="134" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: Alternative algorithms for category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1144" to="1167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning near-optimal search in a minimal explore/exploit task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual conference of the cognitive science society</title>
		<editor>L. Carlson, C. Hoelscher, &amp; T. F. Shipley</editor>
		<meeting>the 33rd annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2800" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decision making as a window on cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="791" to="806" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A re-examination of probability matching and rational choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Shanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tunney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="233" to="250" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monte-Carlo planning in large POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, &amp; A. Culotta</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2164" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rational choice and the structure of the environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic dynamic models of response time and accuracy: A foundational primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="408" to="463" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Uncertainty and exploration in a restless bandit task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Conference of the Cognitive Science Society</title>
		<editor>P. Bellow, M. Guarini, M. McShane, &amp; B. Scassellati</editor>
		<meeting>the 36th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Uncertainty and exploration in a restless bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speekenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="351" to="367" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of human decision-making on bandit problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="168" to="179" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monte</forename><forename type="middle">Carlo</forename><surname>Pomdps</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. A. Solla, T. K. Leen, &amp; K. Müller</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1064" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">From pride and prejudice to persuasion: Satisficing in mate search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<editor>G. Gigerenzer, &amp; P. M. Todd</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page">287</biblScope>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Simple heuristics that make us smart</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Information versus reward in binary choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="680" to="683" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The time course of perceptual choice: The leaky, competing accumulator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="550" to="592" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Decision processes in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An economist&apos;s perspective on probability matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vulkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Sequential analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947" />
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sequential effects: superstition or rational behavior?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cheap but clever: Human active learning in a bandit setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Cognitive Science Society</title>
		<editor>M. Knauff, M. Pauen, N. Sebanz, &amp; I. Wachsmuth</editor>
		<meeting>the 35th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1647" to="1652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, &amp; K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2607" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
