<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decision from Experience Behavior Modeling (DEBM): an open-source Python package for developing, evaluating, and visualizing behavioral models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Yakobi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefim</forename><surname>Roth</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Human Services</orgName>
								<orgName type="institution">University of Haifa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decision from Experience Behavior Modeling (DEBM): an open-source Python package for developing, evaluating, and visualizing behavioral models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The last decade was characterized by an emphasis on enhancing reproducibility and replicability in the social sciences. To contribute to these efforts within the decision-making research field, we introduce DEBM (Decision from Experience Behavior Modeling)-an opensource Python package. The main goal of DEBM is to serve as a central collaborative pool of models and methods in the decision from experience domain. Specifically, it provides a convenient &quot;playground&quot; for developing models or experimenting with existing ones. DEBM includes many features such as multiprocessing, parameter estimation, visualization, and more. In this paper we cover the basic functionality of DEBM by simulating behavior using an existing model and given parameters, and recovering these parameters using grid search.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Nowadays, researchers are required not only to master their study domain, statistics, and methodology, but also to extensively write computer code. Even with current open science best practices (e.g., <ref type="bibr" target="#b6">Ince, Hatton, &amp; Graham-Cumming, 2012</ref>) that encourage scholars to publish their code on open access platforms, using it might be challenging as researchers in the social sciences and related fields often lack formal programming training or other software engineering skills (such as debugging and quality assurance, <ref type="bibr" target="#b7">Joppa et al., 2013;</ref><ref type="bibr" target="#b8">Merali, 2010)</ref>.</p><p>Furthermore, if any mistake is found in the code -correcting it in the original paper could prove a long and inefficient process (e.g., compared with code that is stored in a central repository).</p><p>The lack of formal programming training often results in a variety of software bugs for those who do try to program or modify others' code. These bugs can be roughly divided into two categories: those that stop code execution and throws an error message, and those that do not <ref type="bibr" target="#b0">(Axelrod, 2014)</ref>. The first set is less concerning as the user is aware of a problem in their code. Still, the latter type, which is the focus of this paper, may generate perfectly plausible output without alerting the user on any potential issues. A recent demonstration is reported in <ref type="bibr" target="#b2">Eklund, Nichols, and Knutsson (2016)</ref>, who found a major flaw in a popular statistics software used for analyzing functional magnetic resonance imaging (fMRI) data. Specifically, this bug inflated the false-positive rates, affecting thousands of published studies.</p><p>Software bugs are almost inevitable, and probably exist in most scientific software <ref type="bibr" target="#b11">(Soergel, 2015)</ref>. Soergel quotes an alarming estimate of 1.5%-5% errors in code that was written by professional programmers; it stands to reason that code written by graduate students without a formal training in programming, suffers from similar or even higher error rates. One might argue that the availability of data and code should solve this problem; Yet, if the code is run only by few other researchers, it is unlikely that such software bugs would be discovered and fixed <ref type="bibr" target="#b0">(Axelrod, 2014)</ref>.</p><p>Moreover, those with limited training in programming will likely find using others' code effortful. If those researchers encounter a piece of code in a published <ref type="bibr">(or under-review)</ref> manuscript, there must be a good reason for them to replicate or examine that code. A code repository that is actively maintained and used by a community of researchers, as we introduce here, could solve many of these issues. The rest of the paper is organized as follows: first, we present some motivating examples, illustrating how easily one can make code mistakes. Next, we introduce DEBM and demonstrate basic use cases.</p><p>How easy it is to make code mistakes From our own experience, code mistakes are easy to make and hard to detect, regardless of one's own level of expertise. To demonstrate how easily software bugs are made and set the ground for our rationale, we included some personal examples of code errors that were made in our lab.</p><p>1. Integer uniform distribution in R: When simulating a uniform distribution of integersfor example, between 1 and 10, we used the runif and round functions. That is, we generated continuous uniform numbers, and then rounded them to the nearest number. Although initially it may sound perfectly reasonable, this is an incorrect implementation that trims values at the two extremes <ref type="figure" target="#fig_0">(Figure 1</ref>). Moreover, due to the symmetrical nature of this mistake, the mean value of the distribution is correct, and only in later stages of our work this mistake was discovered.</p><p>2. Moving from verbal models to code: Recently, we coded a model that was verbally described in a research paper. When we tried to derive predictions from this model, our results did not make much sense. To investigate the issue, we asked a second researcher to code this model. After examining each other's' code, we noticed that we interpreted the model slightly differently. It was only after contacting the authors of the original paper that we found that both of our interpretations were different than the intentions of the authors. Modeling human decisions from experience (see a review in <ref type="bibr" target="#b3">Erev &amp; Haruvy, 2016)</ref> involves the following four stages: (a) simulating environments and behavior, (b) making predictions, (c) estimating the models and (d) comparing them. These steps usually translate into tens or hundreds of lines of code which deter many from trying to replicate others or writing their own models. To simplify modeling, we developed an open-source Python package with numerous built-in tools and models, including support for multiprocessing to significantly reduce processing time. We designed DEBM to be both flexible and easy to use for people with basic programming skills. To do so, DEBM consists of complimenting building blocks allowing researchers to run various models with minor code modifications. We chose Python because of its popularity for data modeling, while its readability and friendly syntax are aligned with our goal to make behavioral modeling approachable. In the next section, we introduce DEBM and demonstrate basic use cases with increasing complexity. Full documentation , more tutorials, as well as the list of models that are already integrated in DEBM can be found online at: https://git.io/JwHEu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using DEBM First steps</head><p>You should have Python installed on your machine, with the following packages: NumPy, Pandas, and Matplotlib. We recommend installing Anaconda (www.anaconda.com) which already includes Python and most of the popular packages. DEBM can be installed 1 using the command line by typing: pip install debm</p><p>In the next sections, we will replicate the predictions made in Erev and Roth (2014; Problems 1 and 2, <ref type="table">Table 2</ref>). In these problems, participants have to choose between two buttons, in a repeated 100 trials experiment (i.e., the "clicking paradigm", <ref type="bibr" target="#b1">Barron and Erev 2003</ref>; see Roth and Yakobi, 2022 for a review), as can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>. Each button produced outcomes according to a pre-defined distribution (unknown to the participants), and the participants are presented with the outcomes of the two buttons. They gain (or lose, if the outcome is negative) points according to the prospect they chose and get paid accordingly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defining prospects</head><p>Open your favorite IDE (e.g., Spyder, PyCharm) and start with importing the required packages.</p><p>from debm Import * # Import all modules from debm import numpy as np import pandas as pdNext, we will define the prospects. Problem 1 describes a 100-trial repeated choice task between a status-quo option (always zero) and a risky option (1 with 90%, -10 otherwise). The syntax in DEBM for creating a prospect follows:</p><p>Prospect(trials, fun, oneOutcomePerRun, *args, **kwargs)</p><p>Trials defines the number of trials, fun the function that generates the outcomes or an array of outcomes, oneOutcomePerRun is a Boolean variable defining whether the functions fun returns a single value per run, or the whole set of outcomes. Args, kwargs -Arguments for the function fun.</p><p>The following code creates the status-quo prospect:</p><formula xml:id="formula_0">StatQuo=Prospect(100,[0]*100,False)</formula><p>We defined a prospect with 100 trials, passed an array of one hundred zeros ('[0]*100')</p><p>representing the outcomes array, and passed False, stating that we passed the whole set of outcomes (oneOutcomePerRun=False). We named this prospect StatQuo. The first argument is the array of possible outcomes, the second set the number of outcomes to return, the third one states that numbers are drawn with replacement, and the last one defines the respective probabilities of each outcome.</p><p>In the following line of code, we define a prospect of 100 outcomes named B1, based on the function np.random.choice. The third argument (False) states that this function will return an array of the full length of the outcomes (100 in our case). The next four arguments will be passed directly to the function we specified (np.random.choice) each time new outcomes are generated.</p><p>B1=Prospect <ref type="formula">(100</ref> At this point, we should have two prospects in the memory, saved as StatQuo and B1,</p><p>representing the two choices participants faced in each round of the decision making task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting up models and making predictions</head><p>Erev and Roth (2014) suggest that participants' behavior can be captured by a Sample of K model. This model suggests that in each decision, people use some of their past experience.</p><p>Specifically, they sample with replacement K previous trials, and consider this sample's mean of each prospect in their decision. Thus, this model has one free parameter -Ƙ (Kappa), which determines the size of the sample. This model is part of the DEBM package and can be instantiated with the following syntax:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample_of_K(parameters, prospect_list, number_of_simulations)</head><p>The first argument is a dictionary of parameters, the second is an array of prospects, and the last is the number of simulations that will be used to make predictions. The following line of code defines a sample of K model with the arbitrary name sok1, with a Kappa of 5 and the two prospects we defined earlier. We chose to simulate choices 10000 times every time we generate predictions. We can now use sok1 to generate predictions. The following code generates predictions and saves them as an array in the new variable choices1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>choices1=sok1.Predict()</head><p>To print the mean over all trials, type print(choices1.mean(axis=0)). The results should be approximately 38% and 62% as in Erev and Roth ( <ref type="table">Table 2)</ref>.</p><p>The predictions could be also plotted over trials, or over blocks with the following lines of code:</p><p>sok1.plot_predicted() # Plot over all trials sok1.plot_predicted(4) # Plot over 4 block</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating models</head><p>In order to quantify how good a model fits actual behavior, we can calculate the mean squared deviation (MSD) 2 between the predictions and observed data. For that, we need a set of observed data to compare against our model's predictions. For our purposes, instead of using actual behavior data 3 , we will generate synthetic data using another model. In the following lines of code, we define a new model named simulated_agents, which is also based on the sample of K model. However, here we set Kappa to 10 (sample size of 10), generate predictions and save them in simulated_outcomes.</p><formula xml:id="formula_1">simulated_agents =Sample_of_K({'Kappa':10},[StatQuo,B1],10000) simulated_outcomes= simulated_agents.Predict()</formula><p>We will save the outcomes stored in simulated_outcomes in our original model (sok1) as the observed behavior:</p><p>sok1.set_obs_choices(simulated_outcomes)</p><p>In order to calculate the MSD for sok1, we will use the loss method. We specify the loss function and the scope of the loss calculation as follows:</p><p>Model.loss <ref type="bibr">(method, scope)</ref> where method could be MSD or LL (for log-likelihood), and the scope could be pw (prospect-wise -the loss function is calculated using the mean predictions of each prospect over all trials), or bw (bit-wise -the loss function is calculated over each trial and prospect). The following line of code outputs the prospect-wise MSD for the sok1 model by comparing the predictions of the model, and the stored "observed" outcomes (generated earlier using a second model):</p><p>sok1.loss('MSD','pw') 4</p><p>The relatively high MSD (~0.04) reflects the difference in Kappa between the original model (K=5) and the simulated behavior (K=10). Next, we will try to find an optimal Kappa that best captures the observed outcomes. Because in this case we generated the observed outcomes, we can expect to find the optimal Kappa to be 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model fitting</head><p>DEBM supports multiprocessing (i.e., utilizing more than one CPU in parallel for reducing computation time). The following line of code will set sok1 model to use four CPUs (or less, if your computer has less than four CPUs) in the fitting process:</p><formula xml:id="formula_2">sok1.mp=4</formula><p>For fitting, we will use the OptimizeBF method for finding the optimal Ƙ for our data.</p><p>OptimizeBF is using brute-force (grid-search) to find the best set of parameters, with the following syntax:</p><p>OptimizeBF(pars_dicts, progress_bar, loss_function, scope)</p><p>The first argument, pars_dicts, is a list of dictionaries. Each dictionary defines all the parameters of the model (in this example, only Ƙ), and all the dictionaries in the list are examined. For example, if we want to examine all Kappas from 1 to 50, pars_dicts should be: <ref type="bibr">[{'Kappa':1},</ref><ref type="bibr">{'Kappa':2},</ref><ref type="bibr">{'Kappa':3},</ref><ref type="bibr">…. {'Kappa':50}]</ref> We can use the helper function makeGrid to create this long list without manually setting the values:</p><p>pspace=makeGrid({'Kappa':range(1,51)})</p><p>The second argument of OptimizeBF is Progress_bar -a Boolean variable stating if we want to see a progress bar during the fitting process. Loss_function and scope are the two arguments we introduced before that are passed to the loss function (MSD or Log-likelihood, and bit-wise or prospect-wise, respectively).</p><p>The following line of code will find the best Kappa for our data, and store the fitting results as a dictionary in res1:</p><p>res1=sok1.OptimizeBF(pspace,True,'MSD','pw')</p><p>After the fitting is completed, res1 will contain the best Ƙ and the corresponding MSD, as well as the list of Kappas that were examined (i.e., 1-50) and their corresponding MSDs. The following line will print the optimal Kappa and its corresponding MSD:</p><formula xml:id="formula_3">print('The best Kappa is:',res1['bestp']['Kappa'], 'with an MSD of',res1['minloss'])</formula><p>The next line will plot the loss function over the different sets of parameters: Thus, we successfully recovered the behavior of the synthetic agents (which were simulated with a Kappa of 10).</p><formula xml:id="formula_4">resPlot(res1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In order to enhance reproducibility and replicability <ref type="bibr" target="#b9">(Munafò et al., 2017)</ref>  We demonstrated a basic use-case of DEBM: using an existing model for finding a best set of parameters for a given dataset (see <ref type="figure" target="#fig_5">Figure 4</ref> for an overview of the process and code). By working with synthetic data, rather than real human behavior, we also showed how to generate expected behavior for a given model and a given set of parameters. This can be used to assess parameter recoverability of a model (e.g., be generating many synthetic datasets with known parameters, and correlate these parameters with the fitted parameters).</p><p>Other DEBM features that are not covered here include creating correlated or dynamic prospects, creating your own model and embedding it in DEBM, individual differences, multigame fitting (i.e., when participant complete more than one decision task), exporting predictions to a CSV file, using other optimization methods, and more. These features -which are fully embedded or compatible with DEBM, are described and demonstrated in https://git.io/JwHEu. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Histograms of the incorrect (left) and correct (right) implementation of a uniform integer distribution in R.Left:  round(runif(100000, min = 1, max = 10)).Right: sample(1:10, 100000, replace=TRUE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>The Clicking paradigm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>For</head><label></label><figDesc>the risky option (1 with 90%, -10 otherwise), we cannot simply pass an array of outcomes because the outcomes are stochastic. Thus, we need to pass a function that generates random outcomes, such as NumPy's random choice module. The syntax is:np.random.choice(a, size=None, replace=True, p=None)The following command returns an array of 100 numbers drawn randomly from the distribution described above (1, 90%; -10 otherwise): np.random.choice([-10,1],100,True,[0.1,0.9])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 -</head><label>3</label><figDesc>The loss function for different K values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in the decisions from experience research field, we introduced DEBM -an open-source Python package. DEBM allows evaluating models and creating predictions and visualizations with a few lines of code, and embedding this code in research papers for an easy reproduction of results by other researchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 -</head><label>4</label><figDesc>The process of model fitting using DEBM</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If you are using Anaconda, you should install packages using the Anaconda Prompt. From the start menu, open Anaconda Prompt and type pip install debm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">DEBM can also calculate Log-Likelihood by replacing MSD with LL, see the online tutorial for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Data can be loaded from a CSV file or any other source, with the end goal of having a numerical array of the size nby p, where n is the number of trials, and p is the number of prospects. See online example for such cases: https://github.com/ofiryakobi/debm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the loss method uses the predictions stored in the model after running sok1.Predict().</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimizing bugs in cognitive neuroscience programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Axelrod</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2014.01435</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2014.01435" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Small Feedback-based Decisions and Their Limited Correspondence to Description-based Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.443</idno>
		<ptr target="https://doi.org/10.1002/bdm.443" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Decision Making</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Knutsson</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1602413113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1602413113" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">113</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning and the Economics of Small Decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haruvy</surname></persName>
		</author>
		<idno type="DOI">10.1515/9781400883172-011</idno>
		<ptr target="https://doi.org/10.1515/9781400883172-011" />
	</analytic>
	<monogr>
		<title level="m">The Handbook of Experimental Economics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximization, learning, and economic behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1073/pnas.1402846111</idno>
		<ptr target="https://doi.org/10.1073/pnas.1402846111" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The case for open computer programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham-Cumming</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature10836</idno>
		<ptr target="https://doi.org/10.1038/nature10836" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7386</biblScope>
			<biblScope unit="page">482</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Troubling trends in scientific software use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Joppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcinerny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>O'hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Emmott</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1231535</idno>
		<ptr target="https://doi.org/10.1126/science.1231535" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Error: Why scientific programming does not compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Merali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7317</biblScope>
			<biblScope unit="page">467</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A manifesto for reproducible science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Percie Du Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-016-0021</idno>
		<ptr target="https://doi.org/10.1038/s41562-016-0021" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On Attention to Information: The Checking Paradox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakobi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Experimental Finance</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rampant software errors may undermine scientific results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A W</forename><surname>Soergel</surname></persName>
		</author>
		<idno type="DOI">10.12688/f1000research.5930.2</idno>
		<ptr target="https://doi.org/10.12688/f1000research.5930.2" />
		<imprint>
			<date type="published" when="1000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
