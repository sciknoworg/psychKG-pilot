<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Journal Data Sharing Policies and Statistical Reporting Inconsistencies in Psychology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michèle</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Methodology &amp; Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<settlement>Tilburg</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Borghuis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Developmental Psychology</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<settlement>Tilburg</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coosje</forename><forename type="middle">L S</forename><surname>Veldkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Methodology &amp; Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<settlement>Tilburg</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Dominguez-Alvarez</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Ecorys, Rotterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Methodology &amp; Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<settlement>Tilburg</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Sociology</orgName>
								<orgName type="department" key="dep2">Faculty of Social and Behavioural Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<settlement>Utrecht</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelte</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Methodology &amp; Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<settlement>Tilburg</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Journal Data Sharing Policies and Statistical Reporting Inconsistencies in Psychology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2025-06-29T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Statistical errors</term>
					<term>data sharing</term>
					<term>journal policy</term>
					<term>meta-research</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we present three retrospective observational studies that investigate the relation between data sharing and statistical reporting inconsistencies. Previous research found that reluctance to share data was related to a higher prevalence of statistical errors, often in the direction of statistical significance (Wicherts, Bakker, &amp; Molenaar, 2011). We therefore hypothesized that journal policies about data sharing and data sharing itself would reduce these inconsistencies. In Study 1, we compared the prevalence of reporting inconsistencies in two similar journals on decision making with different data sharing policies. In Study 2, we compared reporting inconsistencies in psychology articles published in PLOS journals (with a data sharing policy) and Frontiers in Psychology (without a stipulated data sharing policy). In Study 3, we looked at papers published in the journal Psychological Science to check whether papers with or without an Open Practice Badge differed in the prevalence of reporting errors. Overall, we found no relationship between data sharing and reporting inconsistencies. We did find that journal policies on data sharing are extremely effective in promoting data sharing. We argue that open data is essential in improving the quality of psychological science, and we discuss ways to detect and reduce reporting inconsistencies in the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most psychological researchers use Null Hypothesis Significance Testing (NHST) to evaluate their hypotheses <ref type="bibr">(Cumming et al., 2007;</ref><ref type="bibr" target="#b24">Hubbard &amp; Ryan, 2000;</ref><ref type="bibr" target="#b42">Sterling, 1959;</ref><ref type="bibr" target="#b43">Sterling, Rosenbaum, &amp; Weinkam, 1995)</ref>. The results of NHST underlie substantive conclusions and serve as the input in meta-analyses, which makes it important that they are reported correctly. However, NHST results are often misreported. Several large-scale studies estimated that roughly half of psychology articles using NHST contain at least one p-value that is inconsistent with the reported test statistic and degrees of freedom, while around one in eight such articles contain a gross inconsistency, in which the reported p-value was significant and the computed p-value was not, or vice versa <ref type="bibr" target="#b10">Caperos &amp; Pardo, 2013;</ref><ref type="bibr" target="#b35">Nuijten, Hartgerink, Van Assen, Epskamp, &amp; Wicherts, 2016;</ref><ref type="bibr" target="#b47">Veldkamp, Nuijten, Dominguez-Alvarez, van Assen, &amp; Wicherts, 2014)</ref>.</p><p>In the medical sciences roughly one in three articles contains an inconsistent p-value <ref type="bibr" target="#b21">(Garcia-Berthou &amp; Alcaraz, 2004)</ref>, and in psychiatry about one in ten articles <ref type="bibr" target="#b8">(Berle &amp; Starcevic, 2007)</ref>.</p><p>There is evidence that inconsistent p-values are associated with reluctance to share data, especially when the inconsistencies concern statistical significance <ref type="bibr" target="#b52">(Wicherts et al., 2011)</ref>. Wicherts et al. speculated that it is possible that authors are reluctant to share data because they fear that other research teams will arrive at different conclusions, or that errors in their work will be exposed (see also <ref type="bibr" target="#b11">Ceci, 1988;</ref><ref type="bibr" target="#b23">Hedrick, 1985;</ref><ref type="bibr" target="#b44">Sterling &amp; Weinkam, 1990)</ref>. Along these lines, one may expect that if authors intend to make their data available from the start, they will double-check their results before writing them up, which would result in fewer inconsistencies in the final paper. <ref type="bibr">Wicherts et al.</ref> also offered the alternative explanation that the relation between data sharing and misreporting is caused by differences in the rigor with which data are managed; researchers who work more diligently in their handling and archiving of data are probably less likely to commit a reporting error.</p><p>In psychology, the availability of research data in general is already strikingly low <ref type="bibr" target="#b45">(Vanpaemel, Vermorgen, Deriemaecker, &amp; Storms, 2015;</ref><ref type="bibr" target="#b53">Wicherts, Borsboom, Kats, &amp; Molenaar, 2006)</ref>, although this problem is not limited to psychology (see e.g., <ref type="bibr" target="#b1">Alsheikh-Ali, Qureshi, Al-Mallah, &amp; Ioannidis, 2011)</ref>. This is a worrying finding in itself, since the availability of original research data is essential to reproduce or verify analyses. However, this problem becomes worse if data are even less likely to be shared if the research article contained statistical inconsistencies, because in these cases verification of the analyses is even more important. Over the past few years there has been increasing awareness that the availability of research data is essential for scientific progress <ref type="bibr" target="#b3">(Anagnostou et al., 2015;</ref><ref type="bibr">Nosek et al., 2015;</ref><ref type="bibr" target="#b50">Wicherts, 2011;</ref><ref type="bibr">Wilkinson et al., 2016)</ref>, and several journals have started to request authors to share their data when they submit an article (e.g., in PLOS and Psychological Science; see <ref type="bibr" target="#b9">Bloom, Ganley, &amp; Winker, 2014;</ref><ref type="bibr" target="#b29">Lindsay, 2017;</ref><ref type="bibr">respectively)</ref>.</p><p>We theorized that such journal policies on data sharing could help decrease the prevalence of statistical reporting inconsistencies, and that articles with open data (regardless of journal policy) contained fewer inconsistencies.</p><p>In this paper, we present three retrospective observational studies that investigate the relation between data sharing and reporting inconsistencies. Our two main hypotheses were that 1) journals that encourage data sharing will show a (larger) decrease in inconsistencies and gross inconsistencies compared to similar journals that do not encourage data sharing (an open policy effect), and 2) articles that are accompanied with open data have fewer inconsistencies and fewer gross inconsistencies than articles without open data (an open data effect). We compared inconsistency rates between two similar journals on decision making with different data sharing policies (Study 1), between psychology articles from journals from the open access publisher PLOS that requires open data and Frontiers that has less strict data sharing policies <ref type="bibr">(Study 2)</ref>, and between papers in the journal Psychological Science with and without Open Practice Badges (Study 3). Studies 2 and 3 are pre-registered and the relevant registrations can be found at https://osf.io/538bc/. Exploratory findings across the three studies are reported in a final results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1</head><p>In Study 1 we documented the prevalence of reporting inconsistencies in two similar journals on decision making that have different data sharing policies: the Journal of Behavioral Decision Making (JBDM; no data sharing policy) and Judgment and Decision Making (JDM; recommended data sharing). Furthermore, we compared the number of reporting inconsistencies in articles that actually did or did not include shared data, regardless of the journal they were published in. We hypothesized that JDM would show a (larger) decrease in inconsistencies and gross inconsistencies compared to JBDM after the introduction of the data sharing policy in JDM (open policy effect), and that articles that are accompanied with open data contain fewer inconsistencies and gross inconsistencies than articles that are not accompanied with open data (open data effect).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. We examined the relation between open data journal policy on statistical reporting inconsistencies in two similar psychological journals: JBDM (ISI impact factor in 2015: 2.768) and JDM (ISI impact factor in 2015: 1.856). Both journals focus on human decision processes and accept empirical research as well as theoretical papers. Furthermore, there is considerable overlap between their editorial boards: in 2015, seventeen researchers sat in the editorial boards of both JDM (51 members in total) and JBDM (125 members in total). A difference between the journals is that JDM is completely open access, whereas in JBDM the authors can pay a fee to make their article open access. The main difference of concern here, however, is that since 2011 JDM editors have started encouraging authors to submit their raw data at the time of review <ref type="bibr" target="#b7">(Baron, 2011)</ref>. <ref type="bibr">1</ref> When the articles are accepted, these data are subsequently published on the web site along with the articles. Before 2011, there was no explicit data policy in JDM. JBDM did not adopt a similar data sharing policy in the relevant years. 2</p><p>1 The data sharing recommendation of JDM states: "We encourage the submission of raw data at the time of review, and we include the data of accepted articles with the articles (unless this is for some reason difficult). We will also include stimuli, questionnaires, and code, when these are necessary to understand exactly what was done (again, unless this is difficult for some reason).", (http://journal.sjdm.org/). <ref type="bibr">2</ref> At the time of writing, JBDM actually did implement a data sharing policy: "Journal of Behavioral Decision Making encourages authors to share the data and other artefacts supporting the results in the paper by archiving it in an appropriate public repository. Authors should include a data accessibility statement, including a link to the repository they have used, in order that this statement can be published alongside their paper." (retrieved from http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1099-0771/homepage/ForAuthors.html, October 2017). We emailed JBDM's editorial office to ask when they</p><p>We downloaded the articles of JDM in the periods before and after their policy change, and we included articles from JBDM in the corresponding time periods. The first issue in JDM was published in 2006, and from April 2011 (Issue 3, 2011; corresponding to Issue 2 2011 of JBDM) onwards JDM started to implement the new data policy. We collected data in 2015, so we included papers up until the end of 2014 to include the most recent full year. Our final sample contained papers published in the years 2006 to February 2011 (T1), and in April 2011 to 2014 (T2). See <ref type="table" target="#tab_0">Table   1</ref> for the number of articles collected per journal and time period. We included all research articles and special issue papers from these periods in both journals, but no book reviews and editorials. All articles of JDM were HTML files, whereas all articles of JBDM were PDF files because no HTML files were available in T1. Procedure. For each article, we coded in which journal and time period it was published and whether the (raw) data were published alongside the articles. Published data files in matrix format with subjects in the rows (so no correlation matrices) as well as simulation codes and model codes were considered open data. The data had to be published either in the paper, an appendix, the journal's website, or a website with a reference to that website in the paper. Remarks such as "data are available upon request" were not considered open data (as such promises are often hollow; <ref type="bibr" target="#b28">Krawczyk &amp; Reuben, 2012)</ref>. Note that we did not assess whether any published data were also relevant, usable, and/or complete, which is by no means guaranteed <ref type="bibr" target="#b27">(Kidwell et al., 2016)</ref>.</p><p>We assessed the consistency of the reported statistical results through an automated procedure: an adapted version 3 of the R package "statcheck" (version 1.0.0; <ref type="bibr" target="#b17">Epskamp &amp; Nuijten, 2014</ref> an inconsistency, and incongruent p-values that possibly change the statistical conclusion from significant to non-significant (and vice versa) are marked as a gross inconsistency.</p><p>The program statcheck contains an automated one-tailed test detection: if the words "onetailed", "one-sided", or "directional" are mentioned somewhere in the article and a p-value would have been consistent if it was one-sided, it is counted as consistent. Furthermore, statcheck takes rounding of the reported test statistic into account. Take for instance the result t (48) = 1.43, p = .</p><p>158. Recalculation would give a p-value of .159, which seems incongruent with the reported pvalue. However, the true t-value could lie in interval [1.425, 1.435), with p-values ranging from .158</p><p>to .161, statcheck will count any p-value within this range as consistent. We assumed that all studies retained an overall alpha of .05. We also counted results reported as p = .05 as significant, since previous research showed that over 90% of the instances in which p = .05 was reported, the authors interpreted the result as significant <ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>. Finally, note that when erroneously only one of the three components of an NHST result (test statistic, degrees of freedom, or p-value) is adjusted to correct for multiple testing, post-hoc testing, or violations of assumptions, the result becomes internally inconsistent and statcheck will flag it as such. However, in an extended validity study of statcheck, we found that such statistical corrections do not seem to cause the high estimates of the general prevalence of inconsistencies (for details, see <ref type="bibr" target="#b36">Nuijten, Van Assen, Hartgerink, Epskamp, &amp; Wicherts, 2017)</ref>. For a more detailed explanation of statcheck, see <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref>, or the statcheck manual at http://rpubs.com/michelenuijten/statcheckmanual.</p><p>In <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref> we investigated the validity of statcheck and found that the interrater reliability between manual coding and statcheck was .76 for inconsistencies and .89 for gross inconsistencies. In an additional validity study, we found that statcheck's sensitivity (true positive rate) and specificity (true negative rate) were high: between 85.3% and 100%, and between 96.0% and 100%, respectively, depending on the assumptions and settings. The overall accuracy of statcheck ranged from 96.2% to 99.9%. For details, see Appendix A in <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref> and the additional validity study, see <ref type="bibr" target="#b36">(Nuijten et al., 2017)</ref>.</p><p>Using statcheck, we extracted 6,482 statistical results from 498 of the 764 articles (65.2%) that contained APA reported NHST results. Note that the conversion of articles to plain text files can be different for PDF and HTML files, which can cause statcheck to recognize or miss different statistical results. Since all articles for JBDM were PDF files, and all articles in JDM HTML files, we could not reliably compare overall inconsistency rates between the journals. However, since over time the file types for each journal stayed the same, we could compare change in inconsistencies over time between the journals. All tests in this study are two-tailed unless otherwise specified and we maintained an alpha level of .05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>General Descriptives. In total, we extracted 6,482 NHST results, which is on average 13.0</p><p>NHST results per article. On average, the articles in JBDM contained more NHST results than JDM articles (15.4 and 10.9 results, respectively). We found that on average 9.3% of the reported NHST results within an article was inconsistent and 1.1% grossly inconsistent. These inconsistency rates</p><p>are similar to what we found in previous research (9.7% and 1.4%, respectively; <ref type="bibr" target="#b35">Nuijten et al., 2016)</ref>.</p><p>Note that the general prevalence of inconsistencies can be estimated in several ways. A first way is to look at the complete set of NHST results, and calculate which percentage of these are inconsistent or grossly inconsistent. The downside of this method is that it does not take into account that results within one article may be statistically dependent. A second method is to calculate for each article which proportion of reported NHST results are inconsistent, and average this over all articles. The downside of this method is that articles with fewer results get as much weight in the calculations as articles with more results, whereas they contain less (precise) information. The third method is to use multilevel logistic models that estimate the probability that a single NHST result is inconsistent while including a random effect at the article level. The downsides of this method are that the assumption of normally distributed random effects may be violated and that the conversion of logits to probabilities in the tails of the distribution leads to inaccurate probability estimates. Taking into account the pros and cons of all these methods, we decided to focus on the second method: the average of the average percentage of inconsistencies within an article, which we call the "inconsistency rate". We retained this method throughout the paper to estimate the general Confirmatory analyses. Our first hypothesis was that JDM would show a larger decrease in (gross) inconsistencies than JBDM after the introduction of the data sharing policy in JDM. However, the mean prevalence of (gross) inconsistencies actually shows a pattern opposite to what we expected: the inconsistency rate increased in JDM after its open data policy from 9.7% to 11.0%, and the inconsistency rate decreased in JBDM from 9.1% to 7.0% (see <ref type="table" target="#tab_2">Table 2</ref>). For illustration purposes, we also plotted the inconsistency rates in both journals over time in <ref type="figure" target="#fig_1">Figure 1</ref>. The <ref type="figure">Figure   shows</ref> a drop in the inconsistency rate in JDM in 2013 onwards (two years after introduction of the data policy). However, there are only few inconsistencies in absolute sense in 2013 and 2014, which makes it hard to interpret this drop substantively; this decrease is in line with only random fluctuations from year to year. More details about the general trends in (gross) inconsistencies over time can be found in the Supplemental Materials at https://osf.io/5j6tc/. was inconsistent or grossly inconsistent (the "inconsistency rate").</p><p>We tested the interaction between journal and the period in which a paper was published with a multilevel logistic regression analysis in which we predicted the probability that a p-value was (grossly) inconsistent with Time (0 = before data sharing policy, 1 = after data sharing policy), Journal (0 = JBDM, 1 = JDM), and the interaction Time * Journal:</p><formula xml:id="formula_0">[( ) ] = 0 + 1 + 2 + 3 * + ,<label>( 1 )</label></formula><p>where subscript i indicates article, Time is the period in which an article is published (0 = published before JDM's data sharing policy, 1 = published after JDM's data sharing policy), Journal is the journal in which the article is published (0 = JBDM and JDM = 1), and is a random effect on the intercept b0. We included a random intercept because the statistical results are nested within article, which means there can be dependency in the inconsistencies within the same article.</p><p>The interaction effect was not significant (b = 0.37, 95% CI = [-0.292; 1.033], Z = 1.10, p = .273), which means that there is no evidence that changes in inconsistencies over time differed for the journals. Second, we looked at the change in the prevalence of gross inconsistencies, but these showed patterns opposite to those expected as well. The gross inconsistency rate stayed at 1.1% in</p><p>JDM after its open data policy, whereas the gross inconsistency rate in JBDM decreased from 1.4% to 0.6%. To test this finding, we performed the same multilevel logistic regression with Time, Journal, and Time * Journal as predictors, but this time we predicted the probability that a p-value was a gross inconsistency. Again, we included a random effect for article. In this analysis, too, we found that the interaction effect was not significant (b = 0.58, 95% CI = [-1.412; 2.580], Z = 0.57, p = .566), meaning that there is no evidence that any change in gross inconsistencies over time depends on journal.  <ref type="table" target="#tab_4">Table 3</ref>).</p><p>To test this pattern, we again fitted a multilevel logistic regression model in which we predicted the probability that a p-value was an inconsistency with Open Data <ref type="formula">(</ref>  Based on our analyses we found no evidence for our two hypotheses: JDM did not show a larger decrease in inconsistencies and gross inconsistencies than JBDM after the introduction of the data sharing policy in JDM. We also did not find that articles that are accompanied by open data contained fewer inconsistencies or gross inconsistencies than articles without open data, but this analysis is possibly confounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we investigated whether there is a relationship between recommended data sharing and statistical reporting inconsistencies, by comparing the number of inconsistencies over time in the journal JDM, which introduced a data sharing policy, and JBDM, that has no such policy.</p><p>We  <ref type="bibr" target="#b41">(Simonsohn, 2013;</ref><ref type="bibr" target="#b50">Wicherts, 2011)</ref>.</p><p>The main limitation of this study is its lack of power. Even though we downloaded a considerable number of articles for each cell in the design, statcheck did not retrieve statistics from every paper, and of the retrieved statistics only a small percentage was inconsistent, resulting in potentially underpowered regression analyses. Based on these data alone we cannot draw firm conclusions about the relation between data sharing and reporting inconsistencies. We therefore designed Studies 2 and 3 to obtain more power and thus more reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2</head><p>In Study 2 we compared the prevalence of inconsistencies and gross inconsistencies in be accompanied with open data. Their online policy on data availability states that "The data underlying the findings of research published in PLOS journals must be made publicly available.</p><p>Rare exceptions may apply and must be agreed to with the Editor." (https://www.plos.org/editorialpublishing-policies; retrieved October 2017). Furthermore, all submissions had to have an official Data Availability Statement explaining how the data were shared or why the data could not be shared. <ref type="bibr" target="#b9">(Bloom et al., 2014)</ref>. Not sharing data could affect the publication decision. The author guidelines of FP also state that data must be made available, but the guidelines are not as explicit as those of PLOS: FP does not require a standardized data availability statement, and it is not clear if not sharing data could affect the publication decision. <ref type="bibr">5</ref> We again hypothesized that the inconsistencies and gross inconsistencies in articles from PLOS would show a stronger decrease (or less strong increase) over time than in FP. Furthermore, we again hypothesized that data sharing (regardless of whether it was required) is associated with fewer inconsistencies and gross inconsistencies in an article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Preregistration. The hypotheses as well as the design and analysis plan were preregistered and can be found at https://osf.io/a973d/. The hypotheses, procedure, and power analysis were registered in detail, whereas the analysis plan was registered more generally, and consisted of the regression equations we intended to test. We followed our preregistered plan, except for one detail:</p><p>we did not preregister any data exclusion rules, but we did exclude one article from the analysis because it was unclear when it was received.</p><p>Sample. We downloaded all articles available in HTML from FP and all HTML articles with the topic "Psychology" from PLOS in two time periods to capture change in inconsistencies before and after the introduction of PLOS' requirement to submit raw data along with an article.</p><p>Articles from FP. We already had access to all FP articles published from 2010 to 2013 that were downloaded for the research in <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref>. On top of that, in the period of 9 to 15</p><p>June 2015 we manually downloaded all FP articles published from January 1st 2014 up until April 30th 2015. In total we had 4,210 articles published published from March 8th 2010 to April 30th</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2015.</head><p>For our sample, we selected only the research articles, clinical (case) studies, and methods articles (excluding editorials, retractions, opinions, etc.). We used systematic text searches in R to automatically select these articles, which resulted in 2,693 articles. Next, we also used systematic text searches in R to extract whether the articles were received before or after PLOS' data sharing policy 6 that came into effect March 1st 2014. 7 1,819 articles in the sample were received before the policy and 873 after the policy. One article was excluded because it was unclear when it was received. <ref type="table" target="#tab_7">Table 5</ref> shows the number of downloaded articles per period and journal.</p><p>Articles from PLOS. PLOS has the option of selecting articles based on the date they were received, which made it straightforward to download articles and categorize them in received before or after PLOS' data sharing policy. Using the R package rplos <ref type="bibr" target="#b12">(Chamberlain, Boettiger, &amp; Ram, 2014)</ref> we first automatically downloaded all PLOS articles with the subject "Psychology" that were received before March 1st 2014, which rendered 7,719 articles. Next, we downloaded all "Psychology" articles received after March 1st 2014, rendering 1,883 articles. We restricted this sample to articles that were published in the same time span that the FP articles were published, <ref type="bibr">6</ref> We could use systematic text searches because all research articles in FP have a standard header indicating the type of article. We included articles with the header "Original Research ARTICLE", "Clinical Trial ARTICLE", "Methods ARTICLE", and "Clinical Case Study ARTICLE", which resulted in 2,693 articles. We also wanted to extract whether the articles were received before or after PLOS' data sharing policy that came into effect March 1st 2014. In FP, this is also systematically indicated at the bottom of the article (e.g., "Received: 22 October 2010; Paper Pending Published: 10 November 2010; Accepted: 01 December 2010; Published online: 14 December 2010"). Because these dates were always reported in the same place and in the same way, we could use systematic text searches in R again to extract when the articles were received and published. <ref type="bibr">7</ref> The exact date at which the open data policy at PLOS was implemented is not entirely clear. In the editorial announcing the policy it was stated the policy was implemented at March 1 st 2014 <ref type="bibr" target="#b9">(Bloom et al., 2014)</ref>, but at the data availability web page, it was stated that the starting date was March 3 rd (http://journals.plos.org/plosone/s/data-availability). For our study we retained March 1 st .</p><p>which means that we excluded all PLOS articles published before March 8th 2010 (4 articles excluded) or after April 30th 2015 (376 articles excluded). Next, using systematic text searches in R we only selected the research articles from this sample, 8 rendering 7,700 articles from before the data sharing policy, and 1,515 articles from after the policy. The final sample size is described in <ref type="table" target="#tab_6">Table 4</ref>.  <ref type="formula">2016</ref>, we conducted a power analysis. We retained a baseline probability that a result in FP or PLOS was inconsistent of 6.4%. <ref type="bibr">9</ref> We concluded that we have a power of .80 if the decrease in inconsistencies in PLOS over time is 2 to 3 percentage points steeper than in FP. The full details of the power analysis including all R code have been included in the preregistration and can be found at https://osf.io/ay6sh/.</p><p>Procedure. We used statcheck version 1.0.2 <ref type="bibr" target="#b18">(Epskamp &amp; Nuijten, 2015)</ref> to extract all APA reported NHST results from the PLOS and FP articles. Due to feasibility constraints, we decided not to check all the downloaded articles for open data, but only the ones that statcheck extracted results from (1,108 articles from FP and 2,909 articles from PLOS 10 ).</p><p>For each downloaded article with detectable NHST results, we coded whether the (raw) data were available. Published data files in matrix format with subjects in the rows (so no correlation matrices) were considered open data. The data had to be published either in the paper, an appendix, or a website (with a reference to that website in the paper). Remarks such as "data are available upon requests" were not considered open data. Again, we did not assess whether any available data were relevant, usable, and/or complete.</p><p>Due to the large number of articles that needed to be coded with respect to data availability, we had seven coders: the six authors and a student assistant. We tested the coding protocol by assessing interrater reliability by coding 120 articles that were randomly selected from the full sample and calculating the intraclass correlation (ICC). In this set-up, every article was coded by two randomly selected coders. Per article three variables were coded. We coded whether the authors stated that the data was available (ICC(2,2) = .948), whether there actually was a data file available (ICC(2,2) = .861), and finally whether there was a URL linking to the data available (ICC(2,2) = .282). The last ICC was quite low. After further inspection of the coding it turned out that there was some confusion among coders whether a link to a data file that was also embedded in the article should be counted as a URL. Since this was not crucial for testing our hypotheses, we adapted the <ref type="bibr">10</ref> It is possible that our sample contained articles that contained reporting inconsistencies because those inconsistencies were the topic of investigation <ref type="bibr" target="#b5">(Bakker &amp; Wicherts, 2014;</ref><ref type="bibr" target="#b47">Veldkamp et al., 2014;</ref><ref type="bibr" target="#b52">Wicherts et al., 2011)</ref>. However, this sample is so small that it is unlikely to affect our general conclusions. protocol to only code two variables: whether the authors state that the data were available, and whether the data actually were available. The final protocol is available on https://osf.io/yq4mt/.</p><p>The total sample was coded for open data with the help of an extra student assistant, resulting in eight coders in total. As a final reliability check, 399 articles (approximately 10% of all articles with APA reported NHST results) were coded twice by randomly assigned coders. The interrater reliability was high: for the data availability statement the ICC(2,2) was .900 11 , and for whether the data was actually available the ICC(2,2) was .913 12 . Furthermore, the first author blindly recoded all cases in which a coder had added a remark, and solved any discrepancies by discussion. The first author also solved any discrepancies between coders when an article was coded twice. All coders were blind for the statcheck results, but not for the journal and time period in which the article was published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>General Descriptives. <ref type="table" target="#tab_7">Table 5</ref> shows the descriptive results per journal and time. It turned out that statcheck extracted NHST results from more articles than expected based on the data of <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref>. On average, 41.2% of the articles in FP and 31.6% of the articles in PLOS contained APA reported NHST results that statcheck could detect. This means that we obtained more power than expected based on our power analysis. Across journal and time, on average 13.0%</p><p>of NHST results in an article was inconsistent, and 1.6% was grossly inconsistent. The average percentage of inconsistencies within an article in FP increased over time from 13.1% to 16.2%, whereas the inconsistency rate in PLOS increased from 12.5% to 13.5%. The percentage of gross inconsistencies in FP increased slightly from 1.7% to 2.0%, and the gross inconsistencies in PLOS <ref type="bibr">11</ref> Discrepancies in coding data availability statements mainly arose in PLOS articles before they introduced the standardized data availability statements. There were also a few instances in which coders disagreed whether a statement such as "all relevant data are available" could be counted as a data availability statement.</p><p>12 Discrepancies in coding data availability mainly arose in cases where the shared data deviated from "standard" experimental data (e.g., in a meta-analysis or in genetic research), or when data about the stimuli were confused with collected data.</p><p>increased from 1.4% to 1.7%. The steeper increase in inconsistencies in FP as compared to PLOS seems to be in line with our hypothesis that an open data policy influences the inconsistency rates, but we will test this in the next section. For the sake of completeness, we also added a plot that shows the trends over time in the inconsistency rates per journal (see <ref type="figure" target="#fig_3">Figure 2)</ref>. Note that this plot shows the average inconsistency rates in the year the articles were published, not the years in which the articles were received. That means that even though some articles were published after PLOS introduced the data policy, they may have been submitted before the policy was implemented. Even so, the figure gives a good indication of the prevalence of (gross) inconsistencies in PLOS and FP over time. More details about the general trends in (gross) inconsistencies over time can be found in the Supplemental Materials at https://osf.io/5j6tc/. article that was inconsistent or grossly inconsistent (the "inconsistency rate"). Confirmatory analyses. For our first set of preregistered hypotheses we hypothesized that the probability that a result is inconsistent decreases more strongly in PLOS after they introduced a data sharing policy than in FP, where there was no data sharing policy (open policy effect). More specifically, we expected that there is a negative interaction effect of Time (0 = received before PLOS' data sharing policy, 1 = received after PLOS' data sharing policy) times Journal 13 (0 = FP, 1 = PLOS) on the probability that a result is inconsistent or grossly inconsistent. The raw probabilities of an inconsistency and gross inconsistency split up per time and journal can be found in <ref type="table" target="#tab_6">Table 4</ref>. We tested our hypotheses by estimating the following multilevel logistic models:</p><formula xml:id="formula_1">[( ) ] = 0 + 1 + 2 + 3 * + ,<label>( 2 )</label></formula><p>where subscript i indicates article, Time is the period in which an article is published (0 = received before PLOS' data sharing policy, 1 = received after PLOS' data sharing policy), Journal is the outlet in which the article is published (0 = FP and PLOS = 1), and is a random effect on the intercept b0.</p><p>We included a random intercept because the statistical results are nested within article, which means there can be dependency in the inconsistencies within the same article. We hypothesized that in both models the coefficient b3 is negative. We maintained an α of .05. We did not preregister that we would use one-tailed tests, so we tested our hypotheses two-tailed.</p><p>When predicting the inconsistencies, we found a significant interaction effect of Time * Journal in the predicted direction, b3 = -0.43, 95% CI = [-0.77; -0.085], Z = -2.45, p = .014. This indicates that the prevalence of inconsistencies decreased more steeply (or more accurately:</p><p>increased less steeply) in PLOS than in FP. This finding is in line with the notion that requiring open data as a journal could decrease the prevalence of reporting errors.</p><p>When predicting gross inconsistencies, we did not find a significant interaction effect of Time * Journal; b3 = -0.12, 95% CI = [-1.04; 0.80], Z = -0.25, p = .804. This means that there is no evidence that any change in gross inconsistencies over time depended on the journal in which the result was published. This finding is not in line with our hypothesis. Since we found no significant interaction effect, we (exploratively) tested the model again without the interaction effect to see if there is a main effect for Time and/or Journal. We found no evidence for a main effect of Time (b1 = without open data, respectively. These patterns are the opposite of what we expected. We tested whether there is a relationship between open data and the probability of a (gross) inconsistency by estimating the following two multilevel logistic models:</p><formula xml:id="formula_2">[( ) ] = 0 + 1 + ,<label>( 3 )</label></formula><p>where subscript i indicates article, Open Data indicates whether the data is published along with the article (0 = no open data, 1 = open data), and is a random effect on the intercept b0. We hypothesized that in both models the coefficient b1 is negative.</p><p>We found no effect of Open Data on the prevalence of inconsistencies (b1 = 0.06, 95% CI = [-0.16; 0.27], Z = 0.50, p = .617) or the prevalence of gross inconsistencies (b1 = 0.23, 95% CI = [-0.33; 0.79], Z = 0.79, p = .429). This finding is not in line with our hypothesis that articles accompanied by open data should have lower inconsistency rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we investigated the relation between required data sharing and statistical reporting inconsistencies using a larger dataset than in Study 1, by comparing the number of statistical reporting inconsistencies over time in open access articles. We compared psychology articles from journals in PLOS, which since March 2014 requires articles to be accompanied by open data, with articles in FP, which does encourage data sharing, but does not require it in the same strong terms as PLOS does. We hypothesized that PLOS would show a stronger decrease in (gross) inconsistencies than FP, and that p-values from articles accompanied by open data were less likely to be inconsistent. We found that the prevalence of inconsistencies over time increases less steeply in PLOS than in FP, which is in line with our hypotheses. However, we did not find evidence for our other hypotheses: there is no evidence that any change in gross inconsistency prevalence is different for PLOS and FP, and we also find no relationship between open data and p-value inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3</head><p>In Study 3, we examined the prevalence of reporting inconsistencies in the journal Psychological Science (PS). Before 2014, the policy of PS concerning data sharing was simply the general policy of the APA, which roughly states that data should be available upon request. From 2014 onwards, however, PS has started to award so-called "Open Practice Badges" in recognition of open scientific practices <ref type="bibr" target="#b16">(Eich, 2014)</ref>. "Open Practice Badges" is a collective term for three types of badges: Authors can earn an Open Data Badge, an Open Materials Badge, and a Preregistration</p><p>Badge. This simple intervention has proven to be very effective: the frequency of reported data sharing in PS increased almost ten-fold after introduction of the badges, compared to reported data sharing in PS before the badges, and data sharing in four comparable journals <ref type="bibr" target="#b27">(Kidwell et al., 2016)</ref>.</p><p>Furthermore, articles in PS with an open data badge had a much higher probability of actually providing the data (93.8%) than articles without a badge that promised data (40.5%; <ref type="bibr" target="#b27">Kidwell et al., 2016</ref> Badges have a lower probability to be inconsistent (Hypothesis 1) and grossly inconsistent (Hypothesis 2) than statistical results in PS articles published from 2014 onwards without an Open Practice Badge."</p><p>14 See https://osf.io/tvyxz/wiki/1.%20View%20the%20Badges/ for details. <ref type="bibr">15</ref> We note that <ref type="bibr" target="#b27">Kidwell et al. (2016)</ref> found that some articles that did share data did not receive an Open Data Badge, but it was unclear why. Conversely, there were also articles with an Open Data Badge that did not have available data. Even though these cases were rare, they indicate that having one of the Open Practice Badges is not necessarily a perfect indicator of open practice.</p><p>16 "Please note: Psychological Science uses StatCheck, an R program written by Sacha Epskamp and Michele B. Nuijten that is designed to detect inconsistencies between different components of inferential statistics (e.g., t value, df, and p). StatCheck is not designed to detect fraud, but rather to catch typographical errors (which occur often in psychology; see https://mbnuijten.com/statcheck/). We run StatCheck only on manuscripts that are sent out for extended review and not immediately rejected after extended review. Authors are informed if StatCheck detects any inconsistencies. Authors are welcome to run StatCheck before submitting a manuscript (http://statcheck.io/)." Retrieved from http://www.psychologicalscience.org/publications/psychological_science/ps-submissions#OPEN, October 2017.</p><p>These hypotheses concern an effect of open practices in general (including sharing materials and preregistration), but we were also interested in the effect of open data in particular on reporting inconsistencies. To that end we also focused on the Open Data Badges in specific, by testing the following two hypotheses (open data effects), as stated in the preregistration at https://osf.io/6nujg/:</p><p>"Statistical results in articles published in PS from 2014 onwards with an Open Data Badge have a lower probability to be inconsistent (Hypothesis 3) and grossly inconsistent (Hypothesis 4) than statistical results in articles published from 2014 onwards without an Open Data Badge."</p><p>Finally, we theorized that PS' policy to award open practice with badges has caused the journal to become known as a journal focused on open, solid science. Because of this, we speculated that after the installation of the badge policy in 2014, the articles submitted to PS were of higher quality, regardless of whether they actually received a badge or not. Therefore, we also hypothesized that (open policy effects), as stated in the preregistration at https://osf.io/6nujg/:</p><p>"Statistical results in articles published in PS before 2014 have a higher probability to be inconsistent (Hypothesis 5) and grossly inconsistent (Hypothesis 6) than statistical results in articles published in PS from 2014 onwards."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Preregistration. The hypotheses and analysis plan (including the full R code) of this study were preregistered. The preregistration can be found at https://osf.io/8j56r/. All elements of the preregistration were written up in a high level of detail. We followed our preregistered plan, except for one aspect of the analysis. We preregistered the R code for the intended analyses, but did not take into account convergence problems. Our solutions to deal with these problems were ad hoc.</p><p>Sample. To investigate the prevalence of inconsistencies and gross inconsistencies in PS, we looked at HTML articles published in PS from 2003 to 2016. We already downloaded the articles published from 2003 to 2013 in previous research, which resulted in a sample of 2,307 articles <ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>. In June 2016, a research assistant downloaded all HTML articles except editorials published from January 2014 up until May 2016, which resulted in 574 articles (see <ref type="table" target="#tab_9">Table   6</ref> for details).</p><p>Power Analysis. As we did in Study 2, we conducted power analyses for all hypotheses based on the number of downloaded articles and the results of <ref type="bibr" target="#b35">Nuijten et al. (2016)</ref>. We concluded that for hypothesis 1 and 3 we have 80% power if the probability of an inconsistency drops with about 50% after introduction of the badges (from .049 17 to .024), and if the probability of an inconsistency drops with about 25% for hypothesis 5 (from .049 to .036; see the preregistration for details). Furthermore, we concluded that we probably do not have sufficient power to detect predictors of a reasonable size of gross inconsistencies (hypotheses 2, 4, and 6). Consequently, we do not trust the test results on gross inconsistencies. However, we still reported the results of the multilevel logistic regression analyses of gross inconsistencies for the sake of completeness. The full details of this power analysis including all R code has been included in the preregistration and can be found at https://osf.io/xnw6u/.</p><p>Procedure. For the articles published from 2014 onwards a research assistant coded which (if any) badges accompanied the article. A detailed protocol (in Dutch) with instructions for the research assistant on which articles to download and how to code the open practice badges is available on OSF: https://osf.io/kktk5/. For full sample details, see <ref type="table" target="#tab_9">Table 6</ref>. We used statcheck version 1.2.2  to extract all APA reported NHST results from the downloaded PS articles and check them on internal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>General Descriptives. Of the 2,879 downloaded articles, 2,106 (73.2%) contained APA reported NHST results. In total, we extracted 20,926 NHST results, which is on average 9.9 NHST results per article. Per article we found that on average 9.3% of the reported NHST results was inconsistent and 1.1% grossly inconsistent. These inconsistency rates are similar to what we found in Study 1 and 2, and in previous research <ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>.</p><p>Hypotheses 1 &amp; 2: Open Practice Badges. Hypothesis 1 and 2 focused on whether the probability that a result is a (gross) inconsistency was lower if the article had one or more Open Practice Badges. We found that 574 articles were published in the period from 2014 onwards when PS started to award badges. In our sample, the probability that a result was inconsistent is slightly higher for articles with a badge (11.8%) than articles without a badge (9.7%), but the probability that a result was a gross inconsistency is equal in the two groups (see <ref type="table" target="#tab_10">Table 7</ref> for details). We tested hypothesis 1 and 2 with the following logistic multilevel models:</p><formula xml:id="formula_3">[( ) ] = 0 + 1 + ,<label>(4 )</label></formula><p>where subscript i indicates article, OpenPracticeBadge indicates whether an article had one or more of the three available Open Practice Badges (1) or not (0), and is a random effect on the intercept b0. We hypothesized that in both models the coefficient b1 is negative. We tested these hypotheses maintaining an α of .05, and we tested one-sided (b1 &lt; 0).</p><p>Consistent with our preregistered analysis plan, we took into account the possibility that the year in which the paper was published could cause a spurious relation between having a badge and the prevalence of (gross) inconsistencies: it is imaginable that a gradual change in research culture caused both the prevalence of open practice badges to increase and the prevalence of (gross) inconsistencies to decrease (although <ref type="figure" target="#fig_5">Figure 3</ref> does not seem to show such a trend in inconsistencies, see the next sections for more details). We therefore first intended to test whether there was an interaction effect between OpenPracticeBadge and Year on the prevalence of (gross)</p><p>inconsistencies. Due to convergence problems, we re-estimated this model by altering the number of nodes in the Gauss-Hermite quadrature formula to 0 and 0.9. The results of these analyses revealed no effect of the year in which an article was published. Therefore, we proceeded with fitting the originally hypothesized models. Based on our analyses, we found no evidence for an effect of OpenPracticeBadge on the probability that a result is inconsistent (b1 = -0.349, 95% CI = [-0.867; 0.169], z = -1.320, p = .093, one-tailed) or grossly inconsistent (b1 = -0.894, 95% CI = [-3.499;</p><p>1.711], z = -0.673, p = .250, one-tailed).</p><p>Hypotheses 3 &amp; 4: Open Data Badges. In Hypotheses 3 and 4, we looked at the relation between whether articles had an Open Data Badge or not and the probability that a result in that article was inconsistent. Of the 574 articles published in PS from 2014 onwards, 97 had an Open Data Badge and 477 did not. The average percentage of both inconsistencies and gross inconsistencies per article in this sample was higher in articles with an Open Data Badge than in articles without one (see <ref type="table" target="#tab_11">Table 8</ref> for details). We estimated the following logistic multilevel models to test Hypothesis 3 and 4:</p><formula xml:id="formula_4">[( ) ] = 0 + 1 + ,<label>( 5 )</label></formula><p>where OpenDataBadge indicates whether an article had an Open Data Badges (1) or not (0). We hypothesized that in both models the coefficient b1 is negative. We tested these hypotheses maintaining an α of .05, and we tested one-sided (b1 &lt; 0).</p><p>Similar to Hypotheses 1 and 2 and following the preregistration, we first tested the models including two extra control variables: in which year the article was published and whether the article had a badge other than an Open Data Badge. We included the latter control because we wanted to distinguish between effects of open practice in general and open data in particular. We first intended to test a three-way interaction between Open Data Badge, other badges, and year published, because if there would be a three-way interaction, any two-way interactions or main effects could not be interpreted. However, these models were too complex to fit and did not converge. We therefore continued to fit the models with three two-way interactions. Similar to hypotheses 1 and 2, we fit the models with the node-parameter set to 0 and 0.9. Based on these analyses, we continued to estimate the simple effects of the following model:</p><formula xml:id="formula_5">[ ] = 0 + 1 + 2 ℎ + 3 + 4 * + ,<label>( 6 )</label></formula><p>where we looked at the coefficients of the model when Year was centered on 2014, 2015, and 2016.</p><p>The results show that the negative relation between whether an article had an Open Data Badge and the probability that a result was inconsistent was stronger for articles published in 2014 than in 2015 or 2016 (see <ref type="table" target="#tab_12">Table 9</ref> for details). This finding would be in line with a scenario in which open data (badges) led to a lower prevalence of reporting inconsistencies in 2014, but that this effect decreased over time. Then, to predict the probability that a result was grossly inconsistent, we fitted a model including the two-way interactions to compare it with a model with only the main effects. However, the model with the two-way interactions was too complex to fit, and failed to converge. We therefore continued with the model with only main effects, which we again fitted with the node-parameter set to 0 and 0.9. We compared these models with a model with only Open Data Badge as a predictor and found that adding control variables did not significantly improve the model (χ 2 (2) = .531, p = .767). Based on the final model including only Open Data Badge as a predictor, we found that there was no significant relation between the probability that a result was grossly inconsistent and whether the article had an Open Data Badge or not (b = -.869, 95% CI = [-3.481; 1.743], z = -0.652, p = .257, one-tailed).</p><p>Hypotheses 5 &amp; 6: Time Period. For Hypothesis 5 and 6 we were interested if there was a change in the probability that a result was (grossly) inconsistent when PS started to award badges, so we looked at articles published in PS before and after 2014 when the badge system was introduced. In our sample, we had 2,305 downloaded articles from before 2014, and 574 articles from 2014 onwards. The prevalence of inconsistencies and gross inconsistencies was slightly higher in the second period (see <ref type="table" target="#tab_0">Table 10</ref> for details). We were interested in the difference in inconsistency rates before and after the introduction of the badges, but to sketch a more complete picture we also plotted the inconsistency rates per year (see <ref type="figure" target="#fig_5">Figure 3</ref>). This figure shows that there is a steep drop in the inconsistency rate in articles that were published after the Open Practice Badges were introduced, but this is not a consistent trend. More details about the general trends in (gross) inconsistencies over time can be found in the Supplemental Materials at https://osf.io/5j6tc/.  We tested our hypotheses using the following multilevel logistic models:</p><formula xml:id="formula_6">[( ) ] = 0 + 1 + ,<label>( 7 )</label></formula><p>where Period indicates the time period in which the article was published (0 = T1, published before 2014 and the badge policy; 1 = T2, published from 2014 onwards when the badge policy was installed). Again we included a random intercept to account for dependencies of results within articles. We hypothesized that in both models the coefficient b1 is negative. We tested this hypothesis maintaining an α of .05 using a one-sided (b1 &lt; 0) test.</p><p>Following the strategy from the previous hypotheses, we first intended to test the models controlling for possible effects of whether an article had any of the badges, and the specific year in which the article was published. Again, we first intended fit the models including a three-way interaction between Period, Badges, and Year, and in case there was no significant three-way interaction continue with a model with all two-way interactions, as we preregistered. However, we later realized that testing an interaction between Period and Badges does not make sense because badges were always awarded in T2. Similarly, any interaction between Year and Period also does not make sense, because all years up to 2014 were per definition T1 and from 2014 onwards T2. <ref type="bibr">20</ref> We therefore ran models with a main effect for Period and only one two-way interaction between Badges and Year. Including this two-way interaction did not improve the models, so we continued to fit the models including all main effects and compared them to the models with only Period as predictor. The models that included all main effects did not significantly improve in fit as compared to the models with only Period as predictor when predicting inconsistencies (χ 2 (2) = 2.244, p = .326) or gross inconsistencies (χ 2 (2) = 0.263, p = .877). We therefore proceeded with fitting the originally hypothesized models.</p><p>In line with our hypothesis, we found evidence that a result has a lower probability of being inconsistent when it was published from 2014 onwards (b1 = -0.204, 95% CI = [-0.424; 0.015], z = -1.823, p = .034, one-tailed). Note that this conclusion differs from the descriptives in <ref type="table" target="#tab_0">Table 10 that</ref> show that the average percentage of inconsistencies actually increased from Period 1 to 2 (from 9.1% to 10.0%). These differences in results arise because these analyses reflect different ways to estimate the prevalence of inconsistencies, each with its own advantages and disadvantages (see the section General Descriptives in Study 1 for details). However, despite these seemingly discrepant results for both methods, the effect of open data policy was invariably very small at best.</p><p>When we looked at gross inconsistencies, we found no evidence for an effect of Period on the probability that a result is grossly inconsistent (b1 = -0.186, 95% CI = [-1.140; 0.768], z = -0.382, p = .351, one-tailed).</p><p>The full details on the analyses of hypotheses 1 through 6 and the ad-hoc solutions to the convergence problems can be found in the Supplemental Information at https://osf.io/4gx53/ and in the R code at https://osf.io/8e3gr/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In Study 3, we documented the prevalence of reporting inconsistencies in the journal Psychological Science. We hypothesized that articles with any of the Open Practice Badges had a lower prevalence of inconsistencies and gross inconsistencies than articles without any badges, but we found no evidence to support this. Furthermore, we hypothesized that articles with an Open Data Badge in particular had a lower prevalence of inconsistencies and gross inconsistencies than articles without an Open Data Badge. We found that for articles published in 2014 there was a lower probability that a result was inconsistent if an article had an open data badge, but this pattern did not hold for other years or for gross inconsistencies. Finally, we hypothesized that the prevalence of inconsistencies and gross inconsistencies was lower from 2014 onwards, when PS installed the badge policy. We found evidence that the prevalence of inconsistencies was indeed lower from 2014 onwards than before 2014, but this only held when we looked at the multilevel logistic models and were not in line with the descriptives in <ref type="table" target="#tab_0">Table 10</ref>. Furthermore, we did not find a similar pattern for gross inconsistencies. Our results indicate that if there is any effect of the introduction of the policy on reporting inconsistencies, it is very small at best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory Findings across Studies 1, 2, and 3</head><p>We distinguish between confirmatory and exploratory analyses. Confirmatory analyses are intended to test a priori formulated hypotheses, as opposed to exploratory analyses, which are more data-driven. Although confirmatory findings are more reliable than exploratory findings, exploratory findings can be important in formulating new hypotheses. As long as the distinction is made clear, both confirmatory and exploratory findings have their own merits (see also <ref type="bibr" target="#b49">Wagenmakers, Wetzels, Borsboom, Maas, &amp; Kievit, 2012)</ref>.</p><p>The results of Studies 2 and 3 in the sections above can be considered purely confirmatory, since we preregistered the hypotheses, procedure, and analysis plans. This also means that the results of Study 1 cannot be considered purely confirmatory, because this study was not preregistered. Beside confirmatory analyses, we also performed several additional, more explorative analyses. We looked at cases in which data were promised but not delivered, the effectiveness of journal policies on data sharing, and whether articles with different types of gross inconsistencies also differ in how often they have open data. Finally, we also looked at the prevalence of inconsistencies over time, but since we did not find clear trends (similar to the findings of Nuijten et al., 2016), we only included these results in the Supplemental Information at https://osf.io/5j6tc/. We did not test any of the exploratory findings for statistical significance, because p-values are only interpretable in confirmatory tests (see <ref type="bibr" target="#b49">Wagenmakers et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data missing when promised</head><p>A large part of this study focuses on the availability of research data. Ideally, open data should follow the FAIR Guiding Principles <ref type="bibr">(Wilkinson et al., 2016)</ref>, which state that data should be Findable, Accessible, Interoperable, and Reusable. Here, we only focused on the first and least stringent of these principles: findability. However, in Study 2 (PLOS vs. FP) we noticed that in many cases articles stated that all data were available, whereas in fact this was not the case. We analyzed these cases in detail below.</p><p>We recorded 134 cases in articles from PLOS journals where data were promised but not available. This is as much as 29.0% of all PLOS articles that promised data. This is in line with the findings of <ref type="bibr">Chambers (2017, p. 86)</ref>, who found that 25% of a random sample of 50 PLOS papers employing brain-imaging methods stated their data was available, whereas in fact it was not. In FP,</p><p>we found a similar percentage: of the twelve articles that promised data, three articles (25.0%) did not have available data. In <ref type="table" target="#tab_0">Table 11</ref> we categorized all articles from Study 2 on whether data were promised and whether data were actually available, split up by journal. We examined papers that promised but did not deliver data according to the type of "missing" data. In a minority of the cases (N = 11), the data were hard or impossible to find due to broken URLs, links to Chinese websites, or directions to general data websites (e.g., http://osf.io).</p><p>The large majority of cases (N = 126, all in PLOS) were articles that only reported summary data, such as tables with means and standard deviations or bar plots, instead of actual raw data files. All but two of these cases were published after PLOS started requiring open data and every published article contained an explicit data availability statement. These data availability statements roughly fell in two categories: "Data Availability: The authors confirm that all data underlying the findings are fully available without restriction. All data are included within the manuscript" (N = 9) and "Data Availability: The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper" (italics added; N = 115).</p><p>Based on our findings, we speculate that there are two likely causes for the high rate of "missing" promised data in PLOS. Firstly, it is possible that the definition of "data" is unclear to the authors, PLOS editorial staff, or both. Perhaps summary data are considered enough information to comply with PLOS' open data regulations. Secondly, a lot of flexibility is introduced by allowing the data statement to promise all "relevant" data to be available. The word "relevant" is open to interpretation and might lead to underreporting of actual raw data files. We note that this high rate of missing promised open data is by no means unique for PLOS. A recent study found that as much as 40.5% of articles published in the journals Clinical Psychological <ref type="bibr">Science, Developmental Psychology, Journal of Experimental Psychology: Learning, Memory, and Cognition, and Journal of Personality and Social</ref> Psychology that promised open data did not deliver <ref type="bibr" target="#b27">(Kidwell et al., 2016)</ref>.</p><p>Whatever the cause may be, we are concerned about the high percentage of papers with missing open data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness open data policy</head><p>We noted that journal policy on sharing data seems highly effective.  <ref type="table" target="#tab_0">Table 12</ref>). Moreover, in 2013 and 2014, 100% of the articles in JDM contained open data (see <ref type="figure" target="#fig_7">Figure 4</ref>). In the similar journal JBDM that did not introduce a data policy, none of the articles had open data in period 1, and only 1.7% of the articles had open data in period 2 (see <ref type="table" target="#tab_0">Table 12</ref>). We found a similar pattern in Study 2. There, the articles in PLOS that were accompanied by open data increased from 4.5% to 55.9% after PLOS introduced a data sharing policy.</p><p>In the comparable open access journal FP without such a stringent policy, we see no such increase (1.4% to 1.3%; see <ref type="table" target="#tab_0">Table 12</ref>). Note that these percentages reflect whether data are actually available or not, so despite the worrying finding that roughly a third of the articles in Study 2 that promised data did not deliver (see the previous section), we still see a steep increase in the prevalence of open data in PLOS. In Study 3, we found that after the introduction of Open Practice Badges in PS, 16.9% of the articles earned an Open Data Badge. Previous research investigating the effectiveness of the badges in more detail found that after the introduction of the badges, data was more often available, correct, usable, and complete <ref type="bibr" target="#b27">(Kidwell et al., 2016)</ref>. These results are in line with the finding that journal submission guidelines in general can inspire desirable change in authors' behavior <ref type="bibr" target="#b22">(Giofrè, Cumming, Fresc, Boedker, &amp; Tressoldi, 2017</ref>; but see also <ref type="bibr" target="#b31">Morris &amp; Fritz, 2017</ref>).</p><p>Note, however, that our design is observational, which does not allow us to draw a causal conclusion. It is imaginable that there is an alternative explanation for the increase in data availability after data policies were introduced. For instance, it is possible that the introduction of data policies changed the image of these journals, which inspired "open-science-minded" researchers who always share their data to submit to these journals instead of elsewhere. In that case, it would not be the policy per se that increased data availability, but the way these journals present themselves. We would need an experimental design to be able to investigate whether data policies actually lead to higher data availability. For instance, one way to investigate this would be to have one or multiple journals randomly assign submissions to a "required data sharing condition" and a control condition in which no explicit requests concerning data sharing are made. This way, any systematic difference in the prevalence of statistical reporting inconsistencies between conditions is likely to be due to the presence or absence of a data sharing request.  Open data and inconsistencies in significant vs. non-significant findings</p><p>In previous research we found that gross inconsistencies were more common in results reported as significant (1.56%) than as non-significant (0.97%), suggesting evidence for a systematic bias towards finding significance <ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>. This finding can have several causes, ranging from deliberately rounding down non-significant p-values (see also <ref type="bibr" target="#b26">John, Loewenstein, &amp; Prelec, 2012)</ref> to publication bias, which would primarily cause the p-values that are wrongly rounded down to be published. Because of this apparent emphasis on finding significant results, we looked in more detail at the difference between gross inconsistencies in results reported as significant and reported as non-significant.</p><p>We first tried to replicate our previous finding that there seems to be a systematic bias towards significant findings, using the aggregated data of Studies 1, 2, and 3. Interestingly, we found no clear evidence for such a bias in the current data. Of all 56,716 results reported as significant,</p><p>1.26% was flagged as a gross inconsistency, as opposed to 1.23% of the 22,344 results reported as non-significant. <ref type="bibr">21</ref> Furthermore, we looked at whether the probability of data sharing was related to the type of gross inconsistencies in a paper. Specifically, we looked at the proportion of articles sharing data that 1) did not contain a gross inconsistency, 2) contained at least one gross inconsistency in general, 3) contained a gross inconsistency in a result reported as non-significant, and 4) contained a gross inconsistency in a result reported as significant. We speculated that if gross inconsistencies in favor of finding significant results as opposed to non-significant results would be intentional, authors would be reluctant to share data. We therefore expected that articles with gross inconsistencies, especially those in the direction of statistical significance, would be accompanied by open data less often than articles without any gross inconsistencies.</p><p>Interestingly, in the aggregated data of Studies 1, 2, and 3 we found no such pattern (see <ref type="table" target="#tab_0">Table 13</ref>). Articles without gross inconsistencies shared data in 8.6% of the cases, whereas articles with gross inconsistencies shared data slightly more often: in 10.3% of the cases. We also found that articles with gross inconsistencies in the direction of finding a significant result shared data more often (9.9%) than articles with gross inconsistencies in the direction of non-significance (8.3%). This finding is not in line with the notion that authors are more reluctant to share data when their articles contain gross inconsistencies in favor of finding significant results.</p><p>We also looked at a special case of gross inconsistencies in favor of significance: p-values that were reported as significant, but upon recalculation turned out to be p = .06. This case most closely resembles the questionable research practice (QRP) of wrongly rounding down p-values as defined in <ref type="bibr" target="#b0">(Agnoli, Wicherts, Veldkamp, Albiero, &amp; Cubelli, 2017;</ref><ref type="bibr" target="#b26">John et al., 2012)</ref>. If such cases in our data were indeed the result of intentional QRPs, we would expect articles with such gross inconsistencies to be less likely to share data than articles without gross inconsistencies. Our findings seem to be in line with this notion (see <ref type="table" target="#tab_0">Table 13</ref>). We found that articles that contained a pvalue wrongly rounded down from p = .06 to p &lt; .05 shared data in only 6.6% of the cases, as compared to articles without gross inconsistencies that shared data in 8.6% of the cases. Note that the sample sizes of these subgroup analyses are small, and these results should be interpreted with caution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We conducted three retrospective observational studies to test the hypotheses that data sharing and data sharing policy are negatively related to statistical reporting inconsistencies. Overall, we found that on average the prevalence of statistical inconsistencies was in line with the estimates of previous research (see <ref type="bibr" target="#b35">Nuijten et al., 2016</ref> for an overview). In Study 1, on average 9.3% of the pvalues in an article were inconsistent and 1.1% grossly inconsistent, in Study 2 these numbers were 13.0% and 1.6%, respectively, and in Study 3, 9.3% and 1.1%, respectively. Contrary to what we hypothesized, we did not find consistent evidence that these inconsistencies were related to data sharing or data sharing policies. In Study 2, we did find that the probability of an inconsistency increased less steeply over time in PLOS after they installed a data policy, as compared to FP, that did not install such a policy. However, we did not find a similar pattern for gross inconsistencies, or for the other journals in Studies 1 and 3. Although we considered meta-analyzing the findings of our three studies, we decided not to, for two reasons. First, the results of three studies do not consistently point to a positive or negative effect. Second and most importantly, the three contexts are very different, which questions the use of combining them in one meta-analysis. Note that a random-effects meta-analysis with just three studies is generally also considered not to be very useful.</p><p>We ran several exploratory analyses and found some interesting results. First and foremost, line with previous research that shows evidence that journal policy can encourage desirable change in research practices <ref type="bibr" target="#b22">(Giofrè et al., 2017;</ref><ref type="bibr" target="#b27">Kidwell et al., 2016)</ref>. Even though these results seem promising, they should be interpreted with care. These findings are not based on experimental data but on observational data, which only allow for correlational conclusions.</p><p>Even though data availability increased after open data policies were introduced, we did find that a surprisingly high number of cases in which an article stated the data were available, whereas in fact they were not. We found that roughly one third of the articles in PLOS and FP that promised open data did not deliver. This is comparable to the findings of <ref type="bibr">Chambers (2017, p. 86), and</ref><ref type="bibr" target="#b27">Kidwell et al. (2016)</ref>. <ref type="bibr" target="#b27">Kidwell et al. (2016)</ref>  Finally, contrary to previous findings <ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>, we found that gross inconsistencies in this sample do not seem to be biased towards finding significant results.</p><p>Furthermore, we found no evidence that articles with gross inconsistencies were less likely to have open data than articles without gross inconsistencies. Interestingly, we did find that articles were less likely to share data, when it contained a gross inconsistency in which a recalculated p-value of .06 was reported as &lt; .05. This finding could indicate that some of the gross inconsistencies are intentionally wrongly rounded down p-values, which would lead to reluctance in sharing data.</p><p>However, these findings are exploratory and based on a relatively small sample, so they should be interpreted with caution.</p><p>We recognize three main limitations in our studies. The first limitation is that our choice of retrospective observational designs limits the internal validity of the three studies, and prevents us from drawing causal conclusions. Because we did not randomly assign manuscripts to an "open data condition" and a control condition while keeping everything else constant, we were by definition not able to rule out alternative explanations for any relation between open data and reporting inconsistencies.</p><p>A second limitation is the lack of statistical power. Even though we downloaded a considerable number of articles for each study, the relatively low prevalence of inconsistencies dramatically decreases power to detect small effects. That said, we ran several power analyses that showed that if data sharing had a reasonable effect on the prevalence of inconsistencies, we should have had enough power to detect that. This means that even if data sharing or data sharing policy decreases inconsistencies, the effect is probably not strong enough to be of much practical value.</p><p>However, the situation was more problematic for detecting any effects on the prevalence of gross inconsistencies. Our power analyses in Study 3 revealed serious shortcomings of multilevel analysis to analyze low incidence rates (as with gross inconsistencies) when based on a small number of observations per level-2 unit (article, in our case). More specifically, in our power analysis we used the baseline probability for gross inconsistencies as found in previous research (1.2% in <ref type="bibr">PS;</ref><ref type="bibr" target="#b35">Nuijten et al., 2016)</ref>, and found that in this case the type I error does not equal .05 but approaches zero instead, and the power to detect extremely large effects may not even exceed .05. This problem holds for Studies 1, 2, and 3, and consequently we do not put too much trust in the results of the multilevel logistic analyses concerning gross inconsistencies. We decided to still include them in the paper for the sake of completeness and because we preregistered these analyses. More generally, we recommend against using multilevel logistic regression analyses as a statistical method to analyze nested data characterized by a low incidence rate (e.g., less than 5%) in combination with level-2 units having few observations (e.g., eight observations per level-2 unit).</p><p>The third main limitation is that we used automated software to detect reporting inconsistencies. Even though statcheck was extensively validated <ref type="bibr" target="#b35">(Nuijten et al., 2016</ref>; and see https://osf.io/36ybv/ for additional validity checks), it will never be as accurate as a manual search.</p><p>The main problem is that statcheck does not find all statistical results in a paper, due to variations in reporting style or problems in recognizing characters because of a journal's copy-editing process. It is possible that there is a systematic difference in the inconsistency rate between results that were or were not recognized by statcheck. For instance, maybe if researchers make an effort to report their results in APA style (which statcheck can detect), there is a lower probability of making a typo as compared to researchers who do not attempt to adhere to a strict reporting style. However, in statcheck's validity study there was no evidence for a systematic difference in reporting inconsistencies between results that were and were not picked up by statcheck, so we have no reason to assume that statcheck's estimates of the prevalence of inconsistencies is biased.</p><p>Taking the limitations into account, the results from these three studies are evidence against our hypotheses that data sharing and data sharing policies lead to fewer statistical reporting inconsistencies. We theorized that the precision needed to archive data in such a way that it is accessible and usable to others would also make typos and other errors in statistical reporting less likely. Additionally, we theorized that authors who are unsure about the quality of their analysis or know that there are errors in their work would be more reluctant to submit their work to a journal that requires data sharing. However, our data suggest that this is not the case; requiring data sharing in itself might not be enough to decrease the prevalence of statistical reporting inconsistencies in psychology.</p><p>Our findings are not directly in line with <ref type="bibr" target="#b52">Wicherts et al. (2011)</ref>, who found that reluctance to share data was related to, among other things, an increased rate of reporting inconsistencies. A meaningful difference between our studies is that we looked at whether data sets were published alongside the articles, whereas Wicherts et al. looked at (reluctance in) data sharing when explicitly requested. However, our findings are in line with those of <ref type="bibr" target="#b47">Veldkamp et al. (2014)</ref> and <ref type="bibr" target="#b46">Veldkamp, Hartgerink, Van Assen, and Wicherts (2017)</ref>, who did not find support for their suggested "co-pilot' model in which they theorize that if multiple authors work on the analyses, the probability for reporting inconsistencies should decrease. Their rationale was that shared responsibility for the analysis and results section should (partly) eliminate human error and therefore increase accuracy of the reported results. However, they did not find a relation between co-piloting and the prevalence of statistical reporting inconsistencies. The combined evidence of our three studies and previous literature seems to point to the conclusion that strategies to increase more rigorous data management such as sharing data and collaborating on analyses is not enough to prevent statistical reporting inconsistencies. Even though this collection of findings is based on a limited set of journals, we see no immediate reason to expect differences in other journals. To find out which strategies could be effective in preventing statistical reporting inconsistencies, we need more research to investigate what causes them.</p><p>One way to help decreasing reporting inconsistencies is to use programs and apps such as statcheck <ref type="bibr" target="#b17">(Epskamp &amp; Nuijten, 2014</ref>; http://statcheck.io), or p-checker <ref type="bibr" target="#b39">(Schönbrodt, 2015;</ref><ref type="bibr"></ref> http://shinyapps.org/apps/p-checker/) to quickly and easily check results for internal consistency.</p><p>These programs can be used by authors themselves before submitting a paper in order to avoid mistakes in the published paper and having to file a correction. Similarly, journals themselves can also include these extra checks during peer review. The journal Psychological Science started using statcheck in their peer review process last year to prevent inconsistencies from ending up in the literature (http://www.psychologicalscience.org/publications/psychological_science/ps-submissions;</p><p>retrieved on June 1, 2017), and the use of statcheck is recommended by the journals Stress &amp; Health <ref type="bibr" target="#b6">(Barber, 2017)</ref> and the new journal Advances in Methods and Practices in Psychological Science (http://www.psychologicalscience.org/publications/ampps/ampps-submission-guidelines;</p><p>retrieved on June 1, 2017). Another solution to decrease the prevalence of reporting errors is to make use of Analytic Review (AR; <ref type="bibr" target="#b38">Sakaluk, Williams, &amp; Biernat, 2014)</ref>, in which reviewers also check the analysis scripts and accompanying data files. The advantage of AR over automated programs is that a (human) reviewer can also check if the reported statistical analyses were the appropriate ones.</p><p>Even though we found no evidence that (recommended) data sharing is related to a decreased prevalence of statistical reporting inconsistencies, we still want to emphasize the importance of open data. Some of the greatest advantages of sharing data include, but are not limited to, the possibility to run secondary analyses to answer new questions, verify analyses of published work or examine the robustness of the original analyses, and compute specific effect sizes for meta-analyses (see <ref type="bibr" target="#b51">Wicherts, 2013)</ref>. Stating that "data are available upon request", as is APA policy, is often not enough to ensure availability <ref type="bibr" target="#b45">(Vanpaemel et al., 2015;</ref><ref type="bibr" target="#b53">Wicherts et al., 2006)</ref>. On top of that, sharing data upon request is not robust to time: how likely is it that the data are actually still available after ten years? Or fifty? Or even longer? <ref type="bibr" target="#b48">Vines et al. (2014)</ref> found that the odds of data actually being available upon request dropped by 17% per year. To ensure availability over time it is necessary to publish data in online repositories. An example of a platform for doing so is the Open Science Framework (http://osf.io). Availability of raw data does not guarantee usability or completeness, so it is desirable to build in checks or review of data sets. For instance, it is possible to publish your data in the Journal of Open Psychology Data, in which your data is reviewed to see if it is archived well. There have been concerns about data sharing pointing at issues such as privacy <ref type="bibr" target="#b20">(Finkel, Eastwick, &amp; Reis, 2015)</ref>, or the risk that "freeriders" will take advantage of your painstakingly collected data (but see <ref type="bibr" target="#b30">Longo &amp; Drazen, 2016)</ref>. These are valid concerns, but in most cases, it is easy to come up with solutions tailored to the situation. For instance, the majority of experiments in psychology do not concern sensitive data and can easily be anonymized, and there are options to publish data online privately, and only make it public after a pre-specified period of time in order to first publish findings from these data yourself. Moreover, there is evidence that data sharing is associated with an increased citation rate <ref type="bibr" target="#b37">(Piwowar, Day, &amp; Fridsma, 2007)</ref>.</p><p>In this paper, we used empirical methods to investigate one possible solution to the high prevalence of inconsistently reported statistical results. Reporting inconsistencies are only a small part of the problems related to the current "replication crisis" that psychology is facing (for an overview of these problems, see e.g., <ref type="bibr" target="#b40">Shrout &amp; Rodgers, 2017)</ref>. Even so, we think that it is useful to treat problems in our scientific system (no matter how small) as empirical questions that we can solve by applying the scientific method. Research that aims to do, such as this paper, adds to a growing body of literature on "meta-science", <ref type="bibr" target="#b25">(Ioannidis, Fanelli, Dunne, &amp; Goodman, 2015;</ref><ref type="bibr" target="#b32">Munafò et al., 2017)</ref>. Improving the quality of our research is a complex endeavor and we will need much more research to understand where the biggest problems lie, what caused them, and how we can solve them. Even though we still have a long way to go, it is encouraging to see that journal policies and research practices are changing to accommodate open science.</p><p>All the materials, participant data, and analysis scripts can be found on this paper's project page on the Open Science Framework: https://osf.io/538bc/. <ref type="figure" target="#fig_1">Figure 1</ref>. Per publication year and journal the average percentage of results within an article that was inconsistent or grossly inconsistent (the "inconsistency rate"). <ref type="figure" target="#fig_3">Figure 2</ref>. Per publication year and place published the average percentage of results within an article that was inconsistent or grossly inconsistent (the "inconsistency rate").  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure Titles and Legends</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>prevalence of inconsistencies. To test relations between inconsistencies and open data or open datapolicies, we used multilevel models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Per publication year and journal the average percentage of results within an article that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>psychological articles the open access journal Frontiers in Psychology (FP) and in journals from the major open access publisher PLOS. From March 1 st 2014 onwards PLOS required submissions to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Per publication year and place published the average percentage of results within an</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>0</head><label></label><figDesc>.169, 95% CI =[-0.266; 0.605], Z = 0.762, p = .446) or a main effect of Journal (b2 = -0.012, 95% CI =[-0.413; 0.438], Z = -0.063, p = .950). Note that our power analysis was based on the prevalence of inconsistencies, and not gross inconsistencies. The power of our analysis to find an effect of data sharing on the prevalence of gross inconsistencies is much lower since gross inconsistencies are much less prevalent.For our second set of hypotheses we tested whether results in articles that are accompanied by open data have a lower probability of being inconsistent and grossly inconsistent than results in articles that are not accompanied by open data, regardless of the journal in which they were published (open data effect). We found that the average percentage of inconsistencies in an article was 13.7% when an article had open data, and 12.9% when an article did not have open data. The average percentage of gross inconsistencies in an article was 2.1% and 1.5% for articles with and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>The average percentage of results within an article that was inconsistent or grossly inconsistent (the "inconsistency rate") per publication year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 1shows that the percentage of articles with open data increased dramatically right after JDM, PLOS, and PS introduced a data sharing policy (in 2011, 2014, and 2014, respectively), whereas JBDM and FP without a data policy did not show such an increase. Specifically, in Study 1 we saw that the percentage of articles in JDM with open data increased dramatically from 8.6% to 87.4% after the introduction of their data policy (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>The percentage of articles per journal and year that had open data. A solid circle indicates that there was no (stipulated) open data policy at this point, and an open circle indicates that there was. The different line colors indicate the different journals. The journal abbreviations indicate the following: JBDM = Journal of Behavioral Decision Making, JDM = Judgment and Decision Making, FP = Frontiers in Psychology, PLOS = Public Library of Science, and PS = Psychological Science.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>we found that installing an open data policy seems to be highly effective: the proportion of articles with open data increased rapidly after the journals started requiring or recommending open data, as compared to the prevalence of open data in journals without an open data policy over time. This is in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>The average percentage of results within an article that was inconsistent or grossly inconsistent (the "inconsistency rate") per publication year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>The percentage of articles per journal and year that had open data. A solid circle indicates that there was no (stipulated) open data policy at this point, and an open circle indicates that there was. The different line colors indicate the different journals. The journal abbreviations indicate the following: JBDM = Journal of Behavioral Decision Making, JDM = Judgment and Decision Making, FP = Frontiers in Psychology, PLOS = Public Library of Science, and PS = Psychological Science.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">Number of articles (N) downloaded per journal and time period: 2006 to February 2011 (T1; published</cell></row><row><cell cols="4">before open data policy of JDM), and from April 2011 to 2014 (T2; published after open data policy of</cell></row><row><cell>JDM).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>N in T1</cell><cell>N in T2</cell><cell>Total N</cell></row><row><cell>JBDM</cell><cell>157</cell><cell>149</cell><cell>306</cell></row><row><cell>JDM</cell><cell>236</cell><cell>222</cell><cell>458</cell></row><row><cell>Total</cell><cell>393</cell><cell>371</cell><cell>764</cell></row></table><note>changed their data policy and if it had stayed the same from 2006 to 2014, but unfortunately they did not reply. Based on information from web archives, we can see that in July 2017 this data policy was not yet part of the author guidelines and therefore does not affect our conclusions (information retrieved from https://web.archive.org/web/20170713015402/http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1099- 0771/homepage/ForAuthors.html, October 2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="8">Number of (gross) inconsistencies per journal (JDM = Judgment and Decision Making and JBDM =</cell></row><row><cell cols="8">Journal of Behavioral Decision Making) and time period (T1 = published in 2006-Feb 2011 and T2 =</cell></row><row><cell cols="7">published in April 2011-2014). In April 2011 JDM started encouraging open data.</cell></row><row><cell></cell><cell>#</cell><cell># articles</cell><cell># articles</cell><cell># APA</cell><cell>average #</cell><cell>average %</cell><cell>average %</cell></row><row><cell></cell><cell>articles</cell><cell>with APA</cell><cell>with APA</cell><cell>reported</cell><cell>APA reported</cell><cell>inconsistencies</cell><cell>gross</cell></row><row><cell></cell><cell></cell><cell>reported</cell><cell>reported</cell><cell>NHST</cell><cell>NHST results</cell><cell>per article</cell><cell>inconsistencies</cell></row><row><cell></cell><cell></cell><cell>NHST</cell><cell>NHST</cell><cell>results</cell><cell>per article</cell><cell></cell><cell>per article</cell></row><row><cell></cell><cell></cell><cell>results</cell><cell>results and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>open data</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">JBDM T1 157</cell><cell cols="2">117 (74.5%) 0</cell><cell>1,543</cell><cell>13.2</cell><cell>9.1%</cell><cell>1.4%</cell></row><row><cell></cell><cell>T2 149</cell><cell cols="2">118 (79.2%) 2</cell><cell>2,074</cell><cell>17.6</cell><cell>7.0%</cell><cell>0.6%</cell></row><row><cell>JDM</cell><cell>T1 236</cell><cell cols="2">128 (54.2%) 11</cell><cell>1,313</cell><cell>10.3</cell><cell>9.7%</cell><cell>1.1%</cell></row><row><cell></cell><cell>T2 222</cell><cell cols="2">135 (60.8%) 118</cell><cell>1,552</cell><cell>11.5</cell><cell>11.0%</cell><cell>1.1%</cell></row><row><cell>Total</cell><cell>764</cell><cell cols="2">498 (65.2%) 131</cell><cell>6,482</cell><cell>13.0</cell><cell>9.3%</cell><cell>1.1%</cell></row><row><cell></cell><cell cols="7">Our second hypothesis was that articles that are accompanied with open data contain fewer</cell></row><row><cell cols="8">(gross) inconsistencies than articles that are not accompanied with open data. Again, we observed</cell></row><row><cell cols="8">the opposite pattern in the prevalence of inconsistencies: on average, in articles without open data</cell></row><row><cell cols="8">8.8% of the results was inconsistent as opposed to 10.7% in articles with open data (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>To test this relation, we fitted a multilevel logistic regression model to see if Open Data predicts the probability that a p-value is a gross inconsistency, including a random effect for article.</figDesc><table><row><cell>respectively). Again, Open Data was not a significant predictor (b = 0.001, 95% CI = [-1.150; 1.153], Z = 0.002, p =</cell></row><row><cell>.998). A problem with this analysis is that the large majority of papers with open data were published</cell></row><row><cell>in JDM, which makes this analysis a comparison of the inconsistency rates in both journals. Since</cell></row><row><cell>we only have HTML files from JDM and only PDF files from JBDM, this comparison could therefore</cell></row><row><cell>reflect differences in the performance of statcheck instead of an actual difference in inconsistency</cell></row><row><cell>prevalence.</cell></row><row><cell>0 = the p-value is from an article</cell></row><row><cell>without open data, 1 = the p-value is from an article with open data), and a random effect for article.</cell></row><row><cell>Open Data did not significantly predict whether a p-value was inconsistent (b = 0.30, 95% CI = [-</cell></row><row><cell>0.069; 0.672], Z = 1.59, p = .111). Next, we looked at the relation between gross inconsistencies and</cell></row><row><cell>open data. We found a pattern in the predicted direction: articles with open data had on average a</cell></row><row><cell>lower rate of gross inconsistencies than articles without open data (1.0% of the results versus 1.1%,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Number of (gross) inconsistencies in articles with and without open data.</cell></row><row><cell>Open data?</cell><cell># articles with APA</cell><cell>average %</cell><cell>average % gross</cell></row><row><cell></cell><cell>reported NHST</cell><cell>inconsistencies per</cell><cell>inconsistencies per</cell></row><row><cell></cell><cell>results</cell><cell>article</cell><cell>article</cell></row><row><cell>No</cell><cell>367</cell><cell>8.8%</cell><cell>1.1%</cell></row><row><cell>Yes</cell><cell>131</cell><cell>10.7%</cell><cell>1.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>hypothesized that JDM would show a stronger decrease in (gross) inconsistencies than JBDM (open policy effect), and that p-values from articles accompanied by open data were less likely to be</figDesc><table><row><cell>inconsistent (open data effect). We found no evidence of an open policy effect or an open data</cell></row><row><cell>effect.</cell></row><row><cell>It is worth noting that even though we found no relation between data sharing policy and</cell></row><row><cell>reporting inconsistencies, the data sharing policy of JDM did result in the retraction of an article after</cell></row><row><cell>anomalies in the (open) data were discovered. 4 This emphasizes the potential importance of open</cell></row><row><cell>data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Number of research articles downloaded from PLOS and FP before and after PLOS introduced obligatory data sharing. All articles were published between March 8th 2010 and April 30th 2015.</figDesc><table><row><cell></cell><cell>Before PLOS' data sharing policy:</cell><cell>After PLOS' data sharing policy:</cell><cell></cell></row><row><cell></cell><cell>Received before March 1st 2014</cell><cell>Received after March 1st 2014</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Total</cell></row><row><cell>FP</cell><cell>1,819 articles</cell><cell>873 articles</cell><cell>2,692 articles</cell></row><row><cell>PLOS</cell><cell>7,700 articles</cell><cell>1,515 articles</cell><cell>9,215 articles</cell></row><row><cell>Total</cell><cell>9,519 articles</cell><cell>2,388 articles</cell><cell>11,907 articles</cell></row></table><note>Power analysis. Based on the number of downloaded articles and the previous results of Nuijten et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Number of (gross) inconsistencies per journal(FP and PLOS)  and time period (T1 = received before March 1st 2014 and T2 = received after March 1st 2014). PLOS required articles submitted after March 1st 2014 PLOS to be accompanied by open data.</figDesc><table><row><cell></cell><cell>#</cell><cell># articles</cell><cell># articles</cell><cell># APA</cell><cell>average #</cell><cell>average %</cell><cell>average %</cell></row><row><cell></cell><cell>articles</cell><cell>with APA</cell><cell>with APA</cell><cell>reported</cell><cell>APA</cell><cell>inconsistencies</cell><cell>gross</cell></row><row><cell></cell><cell></cell><cell>reported</cell><cell>reported</cell><cell>NHST</cell><cell>reported</cell><cell>per article</cell><cell>inconsistencies</cell></row><row><cell></cell><cell></cell><cell>NHST</cell><cell>NHST</cell><cell>results</cell><cell>NHST results</cell><cell></cell><cell>per article</cell></row><row><cell></cell><cell></cell><cell>results</cell><cell>results and</cell><cell></cell><cell>per article</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>open data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FP</cell><cell>T1 1,819</cell><cell>804 (44.2%)</cell><cell>11</cell><cell>11,079</cell><cell>13.8</cell><cell>13.1%</cell><cell>1.7%</cell></row><row><cell></cell><cell>T2 873</cell><cell>304 (34.8%)</cell><cell>4</cell><cell>2,432</cell><cell>8.0</cell><cell>16.2%</cell><cell>2.0%</cell></row><row><cell cols="2">PLOS T1 7,700</cell><cell>2,462</cell><cell>110</cell><cell>33,064</cell><cell>13.4</cell><cell>12.5%</cell><cell>1.4%</cell></row><row><cell></cell><cell></cell><cell>(32.0%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T2 1,515</cell><cell>447 (29.5%)</cell><cell>247</cell><cell>5,801</cell><cell>13.0</cell><cell>13.5%</cell><cell>1.7%</cell></row><row><cell>Total</cell><cell>11,907</cell><cell>4,017</cell><cell>372</cell><cell>52,376</cell><cell>13.0</cell><cell>13.0%</cell><cell>1.6%</cell></row><row><cell></cell><cell></cell><cell>(33.7%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>).We again theorized that open practices in general and data sharing in particular would decrease inconsistencies and gross inconsistencies. To test this, we focused on articles published in PS from 2014 onwards, because in this time frame the Open Practice Badges enable a Study 2 in this paper, is that authors have to meet certain criteria before they are awarded any of the Open Practice Badges. For instance, for an Open Data Badge authors need to publish their data in an open-access repository that is time-stamped, immutable, and permanent 14 . Therefore, in this study we are better able to assess whether an article actually has (high quality) open data than in Study 1 or Study 2.15  It is possible that articles published before 2014 also engaged in data sharing and other open practices, but due to feasibility constraints we did not attempt to code this.Furthermore, from July 2016 onwards, PS started using statcheck to screen articles for inconsistencies.16  In our study, we only included PS articles published up until May 2016, because any drop in the prevalence of statistical reporting inconsistencies after May 2016 could have been caused by the use of statcheck in the review process instead of the introduction of the Open Practice Badges.</figDesc><table /><note>straightforward check for the availability of data and/or engagement in other open practices (sharing materials and preregistration). One of the main advantages of this study as compared to Study 1 andTo investigate the relation between open practices in general and reporting inconsistencies, we tested the following two hypotheses (open practice effects), as stated in the preregistration at https://osf.io/6nujg/: "Statistical results in articles published in PS from 2014 onwards with one or more Open Practice</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Total number of downloaded research articles published before and after PS introduced the OpenPractice Badges, and how many of these articles were accompanied by the different badges.</figDesc><table><row><cell cols="2">Year published Total # articles</cell><cell>Open Data</cell><cell>Open Material</cell><cell>Preregistration</cell></row><row><cell></cell><cell>downloaded</cell><cell>Badge</cell><cell>Badge</cell><cell>Badge</cell></row><row><cell>2003-2013</cell><cell>2,305 18</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>2014-2016</cell><cell>574 19</cell><cell>97</cell><cell>69</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>Number of (gross) inconsistencies for articles published in PS after 2014 with at least one Open Practice Badge and without any badges.</figDesc><table><row><cell></cell><cell># articles</cell><cell># articles with</cell><cell># APA</cell><cell>average #</cell><cell>average %</cell><cell>average %</cell></row><row><cell></cell><cell>downloaded</cell><cell>APA reported</cell><cell>reported</cell><cell>APA reported</cell><cell>inconsistencies</cell><cell>gross</cell></row><row><cell></cell><cell></cell><cell>NHST results</cell><cell>NHST results</cell><cell>NHST results</cell><cell>per article</cell><cell>inconsistencies</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell>per article</cell><cell></cell><cell>per article</cell></row><row><cell>No Badges</cell><cell>469</cell><cell>351 (74.8%)</cell><cell>4240</cell><cell>9.7</cell><cell>9.7%</cell><cell>1.5%</cell></row><row><cell>Open Practice</cell><cell>105</cell><cell>75 (71.4%)</cell><cell>1039</cell><cell>10.3</cell><cell>11.8%</cell><cell>1.5%</cell></row><row><cell>Badge(s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>574</cell><cell>426 (74.2%)</cell><cell>5279</cell><cell>9.8</cell><cell>10.0%</cell><cell>1.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>Number of (gross) inconsistencies for articles published in PS after 2014 with and without an Open DataBadge.</figDesc><table><row><cell></cell><cell># articles</cell><cell># articles with</cell><cell># APA</cell><cell>average #</cell><cell>average %</cell><cell>average %</cell></row><row><cell></cell><cell>downloaded</cell><cell>APA reported</cell><cell>reported</cell><cell>APA reported</cell><cell>inconsistencies</cell><cell>gross</cell></row><row><cell></cell><cell></cell><cell>NHST results</cell><cell>NHST results</cell><cell>NHST results</cell><cell>per article</cell><cell>inconsistencies</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell>per article</cell><cell></cell><cell>per article</cell></row><row><cell>No Open Data</cell><cell>477</cell><cell>354 (74.2%)</cell><cell>4,259</cell><cell>9.8</cell><cell>9.6%</cell><cell>1.5%</cell></row><row><cell>Badges</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Open Data</cell><cell>97</cell><cell>72 (74.2%)</cell><cell>1,020</cell><cell>9.8</cell><cell>12.0%</cell><cell>1.6%</cell></row><row><cell>Badge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>574</cell><cell>426 (74.2%)</cell><cell>5,279</cell><cell>9.8</cell><cell>10.0%</cell><cell>1.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc>Results of the simple effects analysis to predict the probability that a result is inconsistent when Year is centered on 2014, 2015, and 2016. TheTable showsthe regression coefficients and their standard errors. The main predictor of interest, Open Data Badge, is printed in bold.</figDesc><table><row><cell>b (SE)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="7">Number of (gross) inconsistencies for articles published in PS before 2014 (Period 1) and from 2014</cell></row><row><cell cols="6">onwards (Period 2). From 2014 onwards PS started to award Open Practice Badges.</cell><cell></cell></row><row><cell></cell><cell># articles</cell><cell># articles with</cell><cell># APA</cell><cell>average #</cell><cell>average %</cell><cell>average %</cell></row><row><cell></cell><cell>downloaded</cell><cell>APA reported</cell><cell>reported</cell><cell>APA reported</cell><cell>inconsistencies</cell><cell>gross</cell></row><row><cell></cell><cell></cell><cell>NHST results</cell><cell>NHST results</cell><cell>NHST results</cell><cell>per article</cell><cell>inconsistencies</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell>per article</cell><cell></cell><cell>per article</cell></row><row><cell>Period 1</cell><cell>2,305</cell><cell>1,680 (72.9%)</cell><cell>15,647</cell><cell>10.0</cell><cell>9.1%</cell><cell>1.1%</cell></row><row><cell>Period 2</cell><cell>574</cell><cell>426 (74.2%)</cell><cell>5,279</cell><cell>9.8</cell><cell>10.0%</cell><cell>1.5%</cell></row><row><cell>Total</cell><cell>2,879</cell><cell>2,106 (73.2%)</cell><cell>20,926</cell><cell>9.9</cell><cell>9.3%</cell><cell>1.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11</head><label>11</label><figDesc>Number of articles in which data were promised or not and data were actually available or not, split up per journal. The cases in which data were promised but not available are printed in bold.</figDesc><table><row><cell>Journal</cell><cell>Data</cell><cell></cell><cell>Data Promised</cell></row><row><cell cols="2">Available</cell><cell></cell><cell></cell></row><row><cell>PLOS</cell><cell></cell><cell>Yes</cell><cell>No</cell><cell>Total</cell></row><row><cell>Yes</cell><cell></cell><cell>328</cell><cell>32</cell><cell>360</cell></row><row><cell>No</cell><cell></cell><cell>134</cell><cell>2,415</cell><cell>2,549</cell></row><row><cell cols="2">Total</cell><cell>462</cell><cell>2,447</cell><cell>2,909</cell></row><row><cell>FP</cell><cell></cell><cell>Yes</cell><cell>No</cell><cell>Total</cell></row><row><cell>Yes</cell><cell></cell><cell>9</cell><cell>6</cell><cell>15</cell></row><row><cell>No</cell><cell></cell><cell>3</cell><cell>1,090</cell><cell>1,093</cell></row><row><cell cols="2">Total</cell><cell>12</cell><cell>1,096</cell><cell>1,108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12</head><label>12</label><figDesc>Percentage of articles that was accompanied by open data, split up per journal and period. The periods were decided per study based on the dates that one of the journals implemented their open data policy.</figDesc><table><row><cell cols="2">% Articles with open data</cell></row><row><cell>Before implementation</cell><cell>After implementation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc>Categorization of all papers from Study 1, 2, and 3 with or without at least one (type of) gross inconsistency and whether they were accompanied by open data.</figDesc><table><row><cell>Articles that contain…</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>showed that of the articles from journals without badges that promised open data, only 40.5% actually had data available. Kidwell et al. also found that articles in Psychological Science with an Open Data Badge had a much higher probability of the data being available, usable, and complete. These data suggest that even though installing an open data policy increases the availability of open data, there needs to be an extra check at the journal to verify if open data statements are justified.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In the conversion from PDF to plain text, "=" signs were often translated to "¼". We adapted statcheck such that it would also recognize these cases. Furthermore, the downloaded articles contained a nonstandardly reported test results that statcheck wrongly recognized as chi-square tests. This we also fixed in this adapted version of statcheck.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See Uri Simonsohn's post on Data Colada: http://datacolada.org/2013/09/17/just_posting_it_works/ and the analysis of the case by Retraction Watch: http://retractionwatch.com/2013/09/10/real-problems-withretracted-shame-and-money-paper-revealed/#more-15597.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">FP's data policy: "To comply with best practice in their field of research, authors must also make certain types of data available to readers at time of publication in stable, community-supported repositories such as those listed below, unless in case of serious confidentiality concerns (for example, research involving human subjects). Although not mandatory, authors may also consider the deposition of additional datatypes (see below)." FP's editorial office let us know via email that they supported the TOP guidelines since 2015: "Frontiers supports the Transparency and Openness Promotion (TOP) guidelines, which state that materials, data, and code described in published works should be made available, without undue reservation, to any qualified researcher, to expedite work that builds on previous findings and enhance the reproducibility of the scientific record." Both quotes retrieved from http://home.frontiersin.org/about/author-guidelines, Materials and Data Policies, May 17, 2017.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Similar to articles in FP, PLOS articles also have a standard header indicating the type of article. Again, we used systematic text searches in R to identify the research articles, but for this it was not enough to only search for "Research Article", since this phrase could also just occur in the full text of the manuscript. We therefore also specified the context in which the phrase "Research Article" should occur. We included either the phrase "Open Access Peer-Reviewed Research Article" or "Browse Topics Research Article", rendering 7,700 articles from before the data sharing policy, and 1,515 articles from after the policy. 9 Note that this probability is smaller than one would expect based on the general inconsistency prevalence in<ref type="bibr" target="#b35">Nuijten et al. (2016)</ref>. This is due to the estimation method in the power analysis, which takes into account the random intercept, resulting in a lower probability of an inconsistency than observed directly in the data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Technically, we should call this variable "Journal/Publisher", since the results from PLOS did not all come from a single article. However, for the sake of readability and consistency with the preprint, we will call this variable "Journal".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">Note that this probability is lower than one would expect based on the general inconsistency prevalence of roughly .10 in PS<ref type="bibr" target="#b35">(Nuijten et al., 2016)</ref>. This is due to the estimation of the regression coefficients, which takes into account the random intercept, resulting in a lower probability of an inconsistency than observed directly in the data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">In the preregistration we stated that we had 2,307 articles in total, but this seems to have been a mistake.19  In the preregistration we stated that we had 576 articles in total, but this seems to have been a mistake.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">We thank Julia Rohrer for pointing this out to us in her review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">Note that this does not add up to the total sample size of 79,784 extracted APA reported NHST results (Study 1: N = 6,482; Study 2: N = 52,376; Study 3: N = 20,926). This is because results reported as p &lt; .07 could not be classified as significant or not significant, and these results were not included in this analysis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Sofie Swaans and Elise Crompvoets for their assistance in coding the articles for open data. We would also like to thank the reviewers Charlotte Hartwright, Julia Rohrer, and an anonymous reviewer for their comments. Their input has considerably improved this manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding Information</head><p>The preparation of this article was supported by The Innovational Research Incentives Scheme Vidi (no. 452-11-004) from the Netherlands Organization for Scientific Research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head><p>None declared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>"Supplemental Information 1: Detailed Analyses Study 3" and "Supplemental Information 2: Prevalence Reporting Inconsistencies over Time" can be found at https://osf.io/es5bv/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability Statement</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Questionable research practices among Italian research psychologists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agnoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L S</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cubelli</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0172792</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Public availability of published research data in high-impact journals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alsheikh-Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Al-Mallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0024357</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Publication Manual of the American Psychological Association</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>American Psychological Association</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
	<note>Sixth Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When Data Sharing Gets Close to 100%: What Human Paleogenetics Can Teach the Open Science Movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anagnostou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capocasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Milia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Battaggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Bisol</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0121409</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The (mis)reporting of statistical results in psychology journals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-011-0089-5</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="666" to="678" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Outlier removal and the relation with reporting errors and quality of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0103360</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">103360</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meticulous manuscripts, messy results: Working together for robust science reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Barber</surname></persName>
		</author>
		<idno type="DOI">10.1002/smi.2756</idno>
	</analytic>
	<monogr>
		<title level="j">Stress and Health</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acknowledgements and report for the year 2010</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inconsistencies between reported test statistics and p-values in two psychiatry journals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Starcevic</surname></persName>
		</author>
		<idno type="DOI">10.1002/mpr.225</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Methods in Psychiatric Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="202" to="207" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data access for the open access literature: PLOS&apos;s data policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ganley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Winker</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1001797</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1001797</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Consistency errors in p-values reported in Spanish psychology journals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Caperos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pardo</surname></persName>
		</author>
		<idno type="DOI">10.7334/psicothema2012.207</idno>
	</analytic>
	<monogr>
		<title level="j">Psicothema</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scientists Attitudes toward Data Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ceci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Technology &amp; Human Values</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">rplos: Interface to PLoS Journals search API</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boettiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=rplos" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>R package version 0.4.0</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cumming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kalinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Statistical reform in psychology: Is anything changing? Psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2007.01881.x</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="230" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Business not as usual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eich</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797613512465</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="6" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">statcheck: Extract statistics from articles and recompute p values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=statcheck" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>R package version 1.0.0.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=statcheck" />
		<title level="m">statcheck: Extract statistics from articles and recompute p values</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>R package version 1.0.1.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">statcheck: Extract statistics from articles and recompute p values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=statcheck" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>R package version 1.2.2.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Best research practices in psychology: Illustrating epistemological and pragmatic considerations with the case of relationship science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Eastwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Reis</surname></persName>
		</author>
		<idno type="DOI">10.1037/pspi0000007</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incongruence between test statistics and P values in medical papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garcia-Berthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alcaraz</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2288-4-13</idno>
	</analytic>
	<monogr>
		<title level="j">Bmc Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The influence of journal submission guidelines on authors&apos; reporting of statistics and use of open research practices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giofrè</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cumming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fresc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Boedker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tressoldi</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0175583</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Justifications for and obstacles to data sharing. Sharing research data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Hedrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="123" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The historical growth of statistical significance testing in psychologyand its future prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1177/0013164400605001</idno>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="661" to="681" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-research: Evaluation and Improvement of Research Methods and Practices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1002264</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measuring the prevalence of questionable research practices with incentives for truth-telling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prelec</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611430953</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="524" to="532" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kidwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baranski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Hardwicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Falkenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1002456</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Available upon Request: Field Experiment on Researchers&apos; Willingness to Share Supplementary Materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reuben</surname></persName>
		</author>
		<idno type="DOI">10.1080/08989621.2012.678688</idno>
	</analytic>
	<monogr>
		<title level="j">Accountability in Research: Policies and Quality Assurance</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="175" to="186" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sharing Data and Materials in Psychological Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Lindsay</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797617704015</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="699" to="702" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Drazen</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMe1516564</idno>
	</analytic>
	<monogr>
		<title level="j">The New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="276" to="277" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meeting the challenge of the Psychonomic Society&apos;s 2012 Guidelines on Statistical Issues: Some success and some room for improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A manifesto for reproducible science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Breckler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Promoting an open research culture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab2374</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">6242</biblScope>
			<biblScope unit="page" from="1422" to="1425" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The prevalence of statistical reporting errors in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-015-0664-2</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1205" to="1226" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<ptr target="https://osf.io/8tygq/" />
		<title level="m">Validity Study ofthe Tool&quot;statcheck&quot;to Discover Statistical Reporting Inconsistencies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sharing detailed research data is associated with increased citation rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Piwowar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Fridsma</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0000308</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">308</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analytic Review as a Solution to the Misreporting of Statistical Results in Psychological Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sakaluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biernat</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691614549257</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="660" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">p-checker: One-for-all p-value analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Schönbrodt</surname></persName>
		</author>
		<ptr target="http://shinyapps.org/apps/p-checker/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Psychology, Science, and Knowledge Construction: Broadening Perspectives from the Replication Crisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Rodgers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Just post it: The lesson from two cases of fabricated data detected by statistics alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797613480366</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1875" to="1888" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Publication decisions and their possible effects on inferences drawn from tests of significance -Or vice versa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<idno type="DOI">10.2307/2282137</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Publication decisions revisited -The effect of the outcome of statistical tests on the decision to publish and vice-versa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinkam</surname></persName>
		</author>
		<idno type="DOI">10.2307/2684823</idno>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="112" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sharing Scientific-Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinkam</surname></persName>
		</author>
		<idno type="DOI">10.1145/79173.79182</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the Acm</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="112" to="119" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are we wasting a good crisis? The availability of psychological research data after the storm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vanpaemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vermorgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deriemaecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Storms</surname></persName>
		</author>
		<idno type="DOI">10.1525/collabra.13</idno>
	</analytic>
	<monogr>
		<title level="j">Collabra</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shared responsibility for statistical analyses and statistical Reporting errors in psychology articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L S</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wicherts</surname></persName>
		</author>
		<ptr target="https://psyarxiv.com/g8cjq" />
	</analytic>
	<monogr>
		<title level="m">PLOS ONE</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Statistical reporting errors and collaboration on statistical analyses in psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L S</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dominguez-Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0114876</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The availability of research data declines rapidly with article age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Débarre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Rennison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="97" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An agenda for purely confirmatory research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L J</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612463078</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="632" to="638" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Psychology must learn a lesson from fraud case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.1038/480007a</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">480</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Science revolves around the data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.5334/jopd.e1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Psychology Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0026828</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The poor availability of psychological research data for reanalysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.61.7.726</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="726" to="728" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Aalbersberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Appleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Axton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The FAIR Guiding Principles for scientific data management and stewardship. Scientific data, 3, 160018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.18</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
